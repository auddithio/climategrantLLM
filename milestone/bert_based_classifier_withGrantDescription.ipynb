{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmCGk2w3meX0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "from transformers import BertForSequenceClassification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSVegkuAmeX1"
      },
      "source": [
        "# Helper Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyfhJnLJmeX2"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def remove_numbers(text):\n",
        "    if isinstance(text, str):\n",
        "        return re.sub(r'\\d+', '', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_year(string):\n",
        "    return re.sub(r'\\s20\\d{2}(\\s|\\-\\d{2})?', ' ', string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhAFuHCFmeX2"
      },
      "source": [
        "# Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUSAKlClmeX2",
        "outputId": "47597bb9-e502-48a6-f2f0-aa3ebe060eb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "79\n",
            "55\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-7d99546f415f>:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  clean_dataset['description'] = clean_dataset['description'].apply(remove_numbers)\n",
            "<ipython-input-3-7d99546f415f>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  clean_dataset['opportunitytitle'] = clean_dataset['opportunitytitle'].apply(remove_numbers)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "dataset = pd.read_csv('BIL Launchpad Case studies - Sheet1.csv')\n",
        "\n",
        "\"\"\"\n",
        "Cleaning the data:\n",
        "- only include columns we will use: 'Project Name', 'Project Description', 'Applicants', 'opportunitytitle', 'description'\n",
        "- drop column with missing information\n",
        "- currently, the grant opportunities include years, i will omit this from their title to group recurring grants together\n",
        "\"\"\"\n",
        "dataset = dataset[['Project Name', 'Project Description', 'Applicants', 'opportunitytitle', 'description']]\n",
        "clean_dataset = dataset.dropna()\n",
        "print(len(clean_dataset['opportunitytitle'].unique()))\n",
        "# print(dataset['opportunitytitle'].unique())\n",
        "clean_dataset['description'] = clean_dataset['description'].apply(remove_numbers)\n",
        "clean_dataset['opportunitytitle'] = clean_dataset['opportunitytitle'].apply(remove_numbers)\n",
        "\n",
        "print(len(clean_dataset['opportunitytitle'].unique()))\n",
        "# print(dataset['opportunitytitle'].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XaqDuhUmeX3"
      },
      "source": [
        "# Experiment 1: Classification of project <> grant name (multi-class text classification) (ignore description of program)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "5D2An8epmeX3",
        "outputId": "a2a6a5bb-e2a3-4d07-eb55-2c3145bc1481"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"simple_df\",\n  \"rows\": 4484,\n  \"fields\": [\n    {\n      \"column\": \"project_profile\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4401,\n        \"samples\": [\n          \"Erie Municipal Airport: Install Weather Reporting Equipment Install Weather Reporting Equipment\",\n          \"Chennault International Airport: This award funds the replacement of the exterior metal panel rain screen on the ATCT and will create resiliency as well as mitigate hazardous conditions resulting from the damaged screen, including electrical risks and environmental hazards. This upgrade will shield the base building from heavy rains, prevent water intrusions in storm conditions, and protect the tower's electrical, mechanical, elevator, and communication equipment. Chennault International Airport_LA_Airport Terminal Program\",\n          \"City of Novi: This award will be used by the City of Novi to develop a comprehensive safety action plan. Novi Safety Action Plan\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"opportunitytitle\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 55,\n        \"samples\": [\n          \"FY  Natural Gas Distribution Infrastructure Safety and Modernization Grant\",\n          \"Pilot Program for Transit-Oriented Development (TOD) Planning\",\n          \"The Infrastructure Investment and Jobs Act (IIJA) Notice of Funding Opportunity for America&#;s Marine Highway Program\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 18,\n        \"min\": 0,\n        \"max\": 54,\n        \"num_unique_values\": 55,\n        \"samples\": [\n          31,\n          5,\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "simple_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-79794c2d-eddf-4bbe-82fa-a651086b87e6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>project_profile</th>\n",
              "      <th>opportunitytitle</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Fairbanks International Airport: This award fu...</td>\n",
              "      <td>FY  Notice of Funding Opportunity: Bipartisan ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ted Stevens Anchorage International Airport: T...</td>\n",
              "      <td>FY  Notice of Funding Opportunity: Bipartisan ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Ted Stevens Anchorage International Airport: T...</td>\n",
              "      <td>FY  Notice of Funding Opportunity: Bipartisan ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Phoenix Sky Harbor International Airport: This...</td>\n",
              "      <td>FY  Notice of Funding Opportunity: Bipartisan ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Yuma International Airport: This award funds u...</td>\n",
              "      <td>FY  Notice of Funding Opportunity: Bipartisan ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5664</th>\n",
              "      <td>Maryland Department of Transportation (Marylan...</td>\n",
              "      <td>FY  Competitive Funding Opportunity: Rail Vehi...</td>\n",
              "      <td>53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5665</th>\n",
              "      <td>Southeastern Pennsylvania Transportation Autho...</td>\n",
              "      <td>FY  Competitive Funding Opportunity: Rail Vehi...</td>\n",
              "      <td>53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5666</th>\n",
              "      <td>Southeastern Pennsylvania Transportation Autho...</td>\n",
              "      <td>FY  Competitive Funding Opportunity: Rail Vehi...</td>\n",
              "      <td>53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5681</th>\n",
              "      <td>Colorado Department of Transportation: The Col...</td>\n",
              "      <td>*Grants for Buses and Bus Facilities Program</td>\n",
              "      <td>54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5682</th>\n",
              "      <td>Bi-State Development Agency: The Bi-State Deve...</td>\n",
              "      <td>*Grants for Buses and Bus Facilities Program</td>\n",
              "      <td>54</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4484 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-79794c2d-eddf-4bbe-82fa-a651086b87e6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-79794c2d-eddf-4bbe-82fa-a651086b87e6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-79794c2d-eddf-4bbe-82fa-a651086b87e6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-57b48771-9c90-4b0d-9ec3-1513e5cdd398\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-57b48771-9c90-4b0d-9ec3-1513e5cdd398')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-57b48771-9c90-4b0d-9ec3-1513e5cdd398 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_2c05697a-6de2-4ef9-98d3-5843efe6d400\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('simple_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_2c05697a-6de2-4ef9-98d3-5843efe6d400 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('simple_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                        project_profile  \\\n",
              "0     Fairbanks International Airport: This award fu...   \n",
              "1     Ted Stevens Anchorage International Airport: T...   \n",
              "2     Ted Stevens Anchorage International Airport: T...   \n",
              "3     Phoenix Sky Harbor International Airport: This...   \n",
              "4     Yuma International Airport: This award funds u...   \n",
              "...                                                 ...   \n",
              "5664  Maryland Department of Transportation (Marylan...   \n",
              "5665  Southeastern Pennsylvania Transportation Autho...   \n",
              "5666  Southeastern Pennsylvania Transportation Autho...   \n",
              "5681  Colorado Department of Transportation: The Col...   \n",
              "5682  Bi-State Development Agency: The Bi-State Deve...   \n",
              "\n",
              "                                       opportunitytitle  label  \n",
              "0     FY  Notice of Funding Opportunity: Bipartisan ...      0  \n",
              "1     FY  Notice of Funding Opportunity: Bipartisan ...      0  \n",
              "2     FY  Notice of Funding Opportunity: Bipartisan ...      0  \n",
              "3     FY  Notice of Funding Opportunity: Bipartisan ...      0  \n",
              "4     FY  Notice of Funding Opportunity: Bipartisan ...      0  \n",
              "...                                                 ...    ...  \n",
              "5664  FY  Competitive Funding Opportunity: Rail Vehi...     53  \n",
              "5665  FY  Competitive Funding Opportunity: Rail Vehi...     53  \n",
              "5666  FY  Competitive Funding Opportunity: Rail Vehi...     53  \n",
              "5681       *Grants for Buses and Bus Facilities Program     54  \n",
              "5682       *Grants for Buses and Bus Facilities Program     54  \n",
              "\n",
              "[4484 rows x 3 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create new dataset of project + project name and grant title\n",
        "simple_df = clean_dataset[['Project Name', 'Project Description', 'Applicants', 'opportunitytitle']]\n",
        "simple_df['project_profile'] =  simple_df['Applicants'] + ': ' + simple_df['Project Description'] + ' ' + simple_df['Project Name']\n",
        "simple_df = simple_df[['project_profile','opportunitytitle'] ]\n",
        "possible_grants = simple_df['opportunitytitle'].unique()\n",
        "\n",
        "#substitute label with number instead\n",
        "label_dict = {}\n",
        "for index, possible_label in enumerate(possible_grants):\n",
        "    label_dict[possible_label] = index\n",
        "\n",
        "simple_df['label'] = simple_df['opportunitytitle'].replace(label_dict)\n",
        "simple_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LMOnzlAomeX3",
        "outputId": "51f04f9f-29ae-469d-dd23-0b92ba417064"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Fairbanks International Airport: This award funds the replacement of the passenger boarding bridge at Gate 3. Fairbanks International Airport'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "simple_df['project_profile'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmidLEizmeX3"
      },
      "source": [
        "# Experiment 2: Classification of project <> grant name (multi-class text classification) (with description of program)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GswARBGumeX3"
      },
      "outputs": [],
      "source": [
        "def remove_year(string):\n",
        "    return re.sub(r'\\s20\\d{2}(\\s|\\-\\d{2})?', ' ', string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "id": "sJBC1_XAmeX3",
        "outputId": "7c68e6df-b0a5-4f4e-ebf4-fad5c2259176"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "79\n",
            "55\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-8-1698cac7b4a5>:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  clean_dataset['description'] = clean_dataset['description'].apply(remove_numbers)\n",
            "<ipython-input-8-1698cac7b4a5>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  clean_dataset['opportunitytitle'] = clean_dataset['opportunitytitle'].apply(remove_numbers)\n",
            "<ipython-input-8-1698cac7b4a5>:20: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  simple_df['project_profile'] =  simple_df['Applicants'] + ': ' + simple_df['Project Description'] + ' ' + simple_df['Project Name']\n",
            "<ipython-input-8-1698cac7b4a5>:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  simple_df['grant_profile'] =  simple_df['opportunitytitle'] + ': ' + simple_df['description']\n",
            "<ipython-input-8-1698cac7b4a5>:30: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  simple_df['label'] = simple_df['grant_profile'].replace(label_dict)\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'FY  Notice of Funding Opportunity: Bipartisan Infrastructure Law (BIL) Airport Terminal Program (ATP) Grants: The Department of Transportation (DOT), Federal Aviation Administration (FAA) announces the opportunity to apply for approximately $ billion in FY  discretionary funds for the Airport Terminal Program (ATP), made available under the Infrastructure Investment and Jobs Act of  (IIJA), Pub. L. -, herein referred to as the Bipartisan Infrastructure Law (BIL). The purpose of the ATP is to make annual grants available to eligible airports for airport terminal and airport-owned Airport Traffic Control Towers development projects that address the aging infrastructure of our nation&#;s airports. In addition, ATP grants will align with DOT&#;s Strategic Framework FY- at https://www.transportation.gov/administrations/office-policy/fy--strategic-framework. The FY  ATP will be implemented consistent with law and in alignment with the priorities in Executive Order , Implementation of the Infrastructure Investments and Jobs Act ( FR ), which are to invest efficiently and equitably; promote the competitiveness of the U.S. economy; improve job opportunities by focusing on high labor standards; strengthen infrastructure resilience to all hazards including climate change; and to effectively coordinate with State, local, Tribal, and territorial government partners. '"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = pd.read_csv('BIL Launchpad Case studies - Sheet1.csv')\n",
        "\n",
        "\"\"\"\n",
        "Cleaning the data:\n",
        "- only include columns we will use: 'Project Name', 'Project Description', 'Applicants', 'opportunitytitle', 'description'\n",
        "- drop column with missing information\n",
        "- currently, the grant opportunities include years, i will omit this from their title to group recurring grants together\n",
        "\"\"\"\n",
        "dataset = dataset[['Project Name', 'Project Description', 'Applicants', 'opportunitytitle', 'description']]\n",
        "clean_dataset = dataset.dropna()\n",
        "print(len(clean_dataset['opportunitytitle'].unique()))\n",
        "# print(dataset['opportunitytitle'].unique())\n",
        "clean_dataset['description'] = clean_dataset['description'].apply(remove_numbers)\n",
        "clean_dataset['opportunitytitle'] = clean_dataset['opportunitytitle'].apply(remove_numbers)\n",
        "\n",
        "print(len(clean_dataset['opportunitytitle'].unique()))\n",
        "# print(dataset['opportunitytitle'].unique())\n",
        "\n",
        "simple_df = clean_dataset\n",
        "simple_df['project_profile'] =  simple_df['Applicants'] + ': ' + simple_df['Project Description'] + ' ' + simple_df['Project Name']\n",
        "simple_df['grant_profile'] =  simple_df['opportunitytitle'] + ': ' + simple_df['description']\n",
        "simple_df = simple_df[['project_profile','grant_profile'] ]\n",
        "possible_grants = simple_df['grant_profile'].unique()\n",
        "\n",
        "#substitute label with number instead\n",
        "label_dict = {}\n",
        "for index, possible_label in enumerate(possible_grants):\n",
        "    label_dict[possible_label] = index\n",
        "\n",
        "simple_df['label'] = simple_df['grant_profile'].replace(label_dict)\n",
        "simple_df['grant_profile'].unique()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2E168PnmeX4"
      },
      "source": [
        "# Train validation split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TlLvvVEsmeX4",
        "outputId": "b26702b6-8fef-472b-95e5-9ef5cc9d1110"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-d306b4faeb30>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  simple_df['data_type'] = ['not_set']*simple_df.shape[0]\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"simple_df\",\n  \"rows\": 131,\n  \"fields\": [\n    {\n      \"column\": \"project_profile\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 89,\n        \"min\": 1,\n        \"max\": 923,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          42,\n          79,\n          38\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-251f6ad7-53e1-4d2a-90d0-cf575f8b4519\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>project_profile</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>grant_profile</th>\n",
              "      <th>label</th>\n",
              "      <th>data_type</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">Competitive Funding Opportunity: Pilot Program for Transit-Oriented Development (TOD) Planning: The Pilot Program for TOD Planning helps support FTA&amp;#;s mission of improving public transportation for America&amp;#;s communities by providing funding to local communities to integrate land use and transportation planning around a new fixed guideway or core capacity improvement project. Per statute, any comprehensive planning funded through the program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations. Additional information is available at: https://www.transit.dot.gov/TODPilot.Notice of Funding Opportunity (NOFO): Notice of Funding Opportunity for PILOT PROGRAM FOR TRANSIT-ORIENTED DEVELOPMENT (TOD) PLANNING. The Federal Transit Administration (FTA) announces the opportunity to apply for $,, of funding under the Pilot Program for TOD Planning (Catalog of Federal Domestic Assistance #.). As required by federal transit law and subject to funding availability, funds will be awarded competitively to support comprehensive planning associated with new fixed guideway and core capacity improvement projects. FTA anticipates minimum grant awards of $, and maximum grant awards of $,,. Synopses and full announcement are posted on Grants.gov as opportunity FTA---TPE-TODP. The Pilot Program for TOD Planning is intended to fund comprehensive planning that supports economic development, ridership, multimodal connectivity and accessibility, increased transit access for pedestrian and bicycle traffic, and mixed-use development near transit stations. The program also encourages identification of infrastructure needs and engagement with the private sector.Consistent with statutory direction, FTA is seeking comprehensive planning projects covering an entire transit capital project corridor, rather than proposals that involve planning for individual station areas or only a small section of the corridor. To ensure any proposed planning work reflects the needs and aspirations of the local community and results in concrete, specific deliverables and outcomes, transit project sponsors must partner with entities with land use planning authority in the transit project corridor to conduct the planning work.Link and Instructions for attaching the supplemental form to the SF-: All applicants must complete a supplemental form (PDF) specific to the Pilot Program for TOD Planning and attach it to their submission in Grants.gov. See section D of the NOFO for information about additional required application contents.Webinar: FTA will conduct a webinar for prospective applicants after the NOFO is published in the Federal Register. Further information will be posted on FTA&amp;#;s website (https://www.transit.dot.gov/TODPilot) when available. Dates: An applicant must submit a proposal electronically by : p.m. Eastern Daylight Time on June , . Any agency intending to apply should initiate the process of registering on the Grants.gov site immediately to ensure completion of registration before the submission deadline.</th>\n",
              "      <th rowspan=\"2\" valign=\"top\">17</th>\n",
              "      <th>train</th>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>val</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">Tribal Transportation Program Safety Fund: A Notice of Funding Availability (NOFA) for Tribal Transportation Program Safety Funds (TTPSF) has been published. This notice announces the availability of $. million for projects in three categories: safety plans; data assessment, improvement, and analysis activities; and infrastructure improvement and other eligible activities as listed in  U.S.C.(a)() The deadline for submissions is //. For complete instructions on preparing and submitting an application for TTPSF, please download the application information and NOFO at https://flh.fhwa.dot.gov/programs/ttp/safety/ttpsf.htm Please submit your application directly to FHWA through the &amp;quot;apply now&amp;quot; button at: https://flh.fhwa.dot.gov/programs/ttp/safety/ttpsf.htm</th>\n",
              "      <th rowspan=\"2\" valign=\"top\">8</th>\n",
              "      <th>train</th>\n",
              "      <td>76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>val</th>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Tribal Transportation Program Safety Fund: Eligible projects described in section (a)() are strategies, activities, and projects on a public road that are consistent with a transportation safety plan and that (i) correct or improve a hazardous road location or feature, or (ii) address a highway safety problem. TTPSF emphasizes the development of strategic transportation safety plans using a data-driven process as a means for Tribes to identify transportation safety needs and determine how those needs will be addressed in Tribal communities. FHWA has identified four eligibility categories: transportation safety plans; data assessment, improvement, and analysis activities; systemic roadway departure countermeasures; and infrastructure improvements and other eligible activities as listed in  U.S.C. &amp;#; (a)().</th>\n",
              "      <th>59</th>\n",
              "      <th>train</th>\n",
              "      <td>72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Strengthening Mobility and Revolutionizing Transportation (SMART) Grants Program: The purpose of this notice is to solicit applications for Strengthening Mobility and Revolutionizing Transportation (SMART) grants. Funds for the fiscal year (FY)  SMART Grants Program are to be awarded on a competitive basis to conduct demonstration projects focused on advanced smart city or community technologies and systems to improve transportation efficiency and safety.</th>\n",
              "      <th>46</th>\n",
              "      <th>val</th>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">The Infrastructure Investment and Jobs Act (IIJA) Notice of Funding Opportunity for America&amp;#;s Marine Highway Program: This notice announces the availability of funding for grants and establishes selection criteria and application requirements for the America&amp;#;s Marine Highway Program (&amp;#;AMHP&amp;#;). The purpose of this program is to make grants available to previously designated Marine Highway Projects that support the development and expansion of documented vessels or port and landside infrastructure. The Department also seeks eligible grant projects that will strengthen American supply chains. The U.S. Department of Transportation (&amp;#;DOT&amp;#; or &amp;#;Department&amp;#;) will award Marine Highway Grants to implement projects or components of projects previously designated by the Secretary of Transportation (&amp;#;Secretary&amp;#;) under the AMHP. Only Marine Highway Projects the Secretary designates before the Notice of Funding Opportunity (&amp;#;NOFO&amp;#;) closing date are eligible for funding as described in this notice. TIMING OF GRANT APPLICATIONS: Applications must be received by the Maritime Administration (&amp;#;MARAD&amp;#;) by : p.m. E.D.T. on April , . ADDRESSES: Grant applications must be submitted electronically using Grants.gov (https://www.grants.gov). Please be aware that you must complete the Grants.gov registration process before submitting your application and that the registration process usually takes  to  weeks to complete. Applicants are strongly encouraged to make submissions in advance of the deadline. FOR FURTHER INFORMATION CONTACT: Fred Jones, Office of Ports &amp;amp; Waterways Planning, Room W&amp;#;, Maritime Administration, U.S. Department of Transportation,  New Jersey Avenue S.E., Washington, D.C. , phone --, or email Fred.Jones@dot.gov. Persons who use a telecommunications device for the deaf (TDD) may call the Federal Information Relay Service (FIRS) at --- to contact the above individual during business hours. The FIRS is available twenty-four hours a day, seven days a week, to leave a message or question with the above individual. You will receive a reply during regular business hours.</th>\n",
              "      <th rowspan=\"2\" valign=\"top\">37</th>\n",
              "      <th>train</th>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>val</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">United States Marine Highway Grants: This funding opportunity solicits applications for fiscal year (FY)  United States Marine Highway Program (USMHP) grants. Funds for FY  USMHP grants will be awarded on a competitive basis to assist in funding eligible projects for the purpose of developing, expanding, or promoting marine highway transportation. This opportunity announces the availability of up to $,, million in funding for grants under this program and establishes selection criteria and application requirements. All USMHP grant recipients must meet all applicable Federal requirements, including domestic content (&amp;#;Buy America&amp;#;) requirements. This program was formerly known as the America&amp;#;s Marine Highway Program</th>\n",
              "      <th rowspan=\"2\" valign=\"top\">60</th>\n",
              "      <th>train</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>val</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>131 rows × 1 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-251f6ad7-53e1-4d2a-90d0-cf575f8b4519')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-251f6ad7-53e1-4d2a-90d0-cf575f8b4519 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-251f6ad7-53e1-4d2a-90d0-cf575f8b4519');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-520fc6d9-cf51-4686-b0c9-885bde2eafab\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-520fc6d9-cf51-4686-b0c9-885bde2eafab')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-520fc6d9-cf51-4686-b0c9-885bde2eafab button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                                    project_profile\n",
              "grant_profile                                      label data_type                 \n",
              " Competitive Funding Opportunity: Pilot Program... 17    train                   17\n",
              "                                                         val                      3\n",
              " Tribal Transportation Program Safety Fund: A N... 8     train                   76\n",
              "                                                         val                     14\n",
              " Tribal Transportation Program Safety Fund: Eli... 59    train                   72\n",
              "...                                                                             ...\n",
              "Strengthening Mobility and Revolutionizing Tran... 46    val                      9\n",
              "The Infrastructure Investment and Jobs Act (IIJ... 37    train                   10\n",
              "                                                         val                      2\n",
              "United States Marine Highway Grants: This fundi... 60    train                    7\n",
              "                                                         val                      1\n",
              "\n",
              "[131 rows x 1 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(simple_df.index.values,\n",
        "                                                  simple_df.label.values,\n",
        "                                                  test_size=0.15,\n",
        "                                                  random_state=42,\n",
        "                                                  stratify=simple_df.label.values)\n",
        "\n",
        "simple_df['data_type'] = ['not_set']*simple_df.shape[0]\n",
        "\n",
        "simple_df.loc[X_train, 'data_type'] = 'train'\n",
        "simple_df.loc[X_val, 'data_type'] = 'val'\n",
        "\n",
        "simple_df.groupby(['grant_profile', 'label', 'data_type']).count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqaJFkA3meX4"
      },
      "source": [
        "# Tokenisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344,
          "referenced_widgets": [
            "61e82fa50b6c4b98b166f2a73c288f14",
            "3b903a5d88bf4737beb7299d165bab7c",
            "9ddbe5383fdb48ef9d3a63e6e6106d44",
            "e2962b7bfbe14310932793ea5e012e98",
            "d2f1a606386243a981af6b56a1fe19ae",
            "3b46d51217c44e4abb30b9fa0c6596cf",
            "5961f6c0192c452e844096f092580918",
            "36a7d955174445ecac05c9d8fa3fd9ec",
            "6266808d204d482494a2b0a9a6ae334e",
            "6305174988e94dbca05f031f77e46a1e",
            "acaa8d00d4a64d4c98fafefd39ba8227",
            "af2e59b19d5b4ec0baaaba2ee08d1705",
            "8f19a8ebe77840c69de1be7c03e1ac28",
            "1537b7fdf8b24e77b61cd59c604ab9e0",
            "08904902faf647b8badf670c2a896ccd",
            "7d3d456323ac47d8b0bfbd7c7c790b10",
            "7515b04cf8f74d1db3ba611c0c22f6ba",
            "4d7b3eb5467442bd87b7572a544bbf90",
            "db7d101fceb34439a923c05ab5884473",
            "35a7f22923664a0ebfe2560c75304408",
            "df4586f25a6c4747853b0eeaf92cb8be",
            "6d720d71541c4273bad8ca1a193e3442",
            "c71aa7dd97314bec83f2b3c68d0756fb",
            "40d7cf3f22024298b7d1c84186a89d0f",
            "20f24451cfd54b8fb25ad406d83ed2cb",
            "9f8dd81cdc844e189773595dbae46f9f",
            "2199b42250d64509a805a35ac0bbc2c5",
            "3c7679d3f7d948d59ad5b14b3a7d47df",
            "9b4baa66e7b04af9b40ad8ca40f78288",
            "249752ef2ffd43879dee3ffa79a8085a",
            "6842cb3de0de4dd0a3e332475b201187",
            "09d677f517f44884b9c9ce51c74205b2",
            "702ab38848094b389eef42c893fd7acb",
            "d1d9bdd9d14a4787bfc7d0b211922a54",
            "3f7b7f0609c64acda60aca0f905b5544",
            "7f042b55edf747bc95724cee68f82678",
            "ff903b7f4a00497da6f71e919e9abbc2",
            "c84130085b164e83b5ea7856c89702e4",
            "dfeb6567dbd34663b653a3be11703f75",
            "38ea332441be4e72916786db98a20f4d",
            "85c8eb45f06a4ae7b6156cdd7154ac59",
            "928c9bdd69c741c8b1b0cb9b235ab3cf",
            "b198510cfef749b9be826a7d72eb55e3",
            "9463c6aeaba5401fa6a7e180620fef2c"
          ]
        },
        "id": "8tbURBTBmeX4",
        "outputId": "d53dd231-cca0-4cb7-d131-aa5e392b2fdf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "61e82fa50b6c4b98b166f2a73c288f14",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af2e59b19d5b4ec0baaaba2ee08d1705",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c71aa7dd97314bec83f2b3c68d0756fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d1d9bdd9d14a4787bfc7d0b211922a54",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Constructs a BERT tokenizer. Based on WordPiece.\n",
        "Instantiate a pre-trained BERT model configuration to encode our data.\n",
        "To convert all the titles from text into encoded form, we use a function called batch_encode_plus , and we will proceed train and validation data separately.\n",
        "The 1st parameter inside the above function is the title text.\n",
        "add_special_tokens=True means the sequences will be encoded with the special tokens relative to their model.\n",
        "When batching sequences together, we set return_attention_mask=True, so it will return the attention mask according to the specific tokenizer defined by the max_length attribute.\n",
        "We also want to pad all the titles to certain maximum length.\n",
        "We actually do not need to set max_length=256, but just to play it safe.\n",
        "return_tensors='pt' to return PyTorch.\n",
        "And then we need to split the data into input_ids, attention_masks and labels.\n",
        "Finally, after we get encoded data set, we can create training data and validation data.\n",
        "\"\"\"\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',\n",
        "                                          do_lower_case=True)\n",
        "\n",
        "encoded_data_train = tokenizer.batch_encode_plus(\n",
        "    simple_df[simple_df.data_type=='train'].project_profile.values,\n",
        "    add_special_tokens=True,\n",
        "    return_attention_mask=True,\n",
        "    pad_to_max_length=True,\n",
        "    # max_length=256,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "encoded_data_val = tokenizer.batch_encode_plus(\n",
        "    simple_df[simple_df.data_type=='val'].project_profile.values,\n",
        "    add_special_tokens=True,\n",
        "    return_attention_mask=True,\n",
        "    pad_to_max_length=True,\n",
        "    # max_length=256,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "input_ids_train = encoded_data_train['input_ids']\n",
        "attention_masks_train = encoded_data_train['attention_mask']\n",
        "labels_train = torch.tensor(simple_df[simple_df.data_type=='train'].label.values)\n",
        "\n",
        "input_ids_val = encoded_data_val['input_ids']\n",
        "attention_masks_val = encoded_data_val['attention_mask']\n",
        "labels_val = torch.tensor(simple_df[simple_df.data_type=='val'].label.values)\n",
        "\n",
        "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
        "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ88Y1QimeX4"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123,
          "referenced_widgets": [
            "54e0a3f25f2e4cc5944cdaebfc6d53d0",
            "058de3ed86494076a7f46bb14f73dd0f",
            "1dcabdf7b755433397f62e622fcac84f",
            "f1b7e30c7f974117a2e5d86273a1ec37",
            "3778647f0eee400ebc6f17f5dc5fa713",
            "e29f8df22452467594d82ba6a80428ef",
            "74222ae9422a4159aba66d3b8d9f5919",
            "a899e69a08114a3399c819396da3ba7d",
            "9ee448de73a442419c04e66f8c88c208",
            "8077cfaafe5d48dcaa6b06c67af23bac",
            "d111f49a84d541838dd2f88913335065"
          ]
        },
        "id": "DU99Bev2meX4",
        "outputId": "8482ed8e-c451-43d4-d264-ad07212a3b7f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "54e0a3f25f2e4cc5944cdaebfc6d53d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
        "                                                      num_labels=len(label_dict),\n",
        "                                                      output_attentions=False,\n",
        "                                                      output_hidden_states=False)\n",
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.cuda.current_device()\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ajsA0UZmeX4"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 3\n",
        "\n",
        "dataloader_train = DataLoader(dataset_train,\n",
        "                              sampler=RandomSampler(dataset_train),\n",
        "                              batch_size=batch_size)\n",
        "\n",
        "dataloader_validation = DataLoader(dataset_val,\n",
        "                                   sampler=SequentialSampler(dataset_val),\n",
        "                                   batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Qtq_GaEmeX4",
        "outputId": "35886ab2-76a3-4917-f28f-6247ca83b044"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr=1e-5,\n",
        "                  eps=1e-8)\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=0,\n",
        "                                            num_training_steps=len(dataloader_train)*epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SlrjgXfmeX4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def f1_score_func(preds, labels):\n",
        "    preds_flat = torch.argmax(preds, dim=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return f1_score(labels_flat.cpu().numpy(), preds_flat.cpu().numpy(), average='weighted')\n",
        "\n",
        "def accuracy_per_class(preds, labels):\n",
        "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
        "\n",
        "    preds_flat = torch.argmax(preds, dim=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "\n",
        "    class_accuracies = {}\n",
        "    for label in torch.unique(labels_flat):\n",
        "        y_preds = preds_flat[labels_flat == label]\n",
        "        y_true = labels_flat[labels_flat == label]\n",
        "        accuracy = (y_preds == label).sum().item() / len(y_true)\n",
        "        class_accuracies[label_dict_inverse[label.item()]] = accuracy\n",
        "        print(f'Class: {label_dict_inverse[label.item()]}')\n",
        "        print(f'Accuracy: {(y_preds == label).sum().item()}/{len(y_true)} ({accuracy:.4f})\\n')\n",
        "\n",
        "    return class_accuracies\n",
        "\n",
        "def recall_at_k(y_true, y_pred, k=5):\n",
        "    num_samples = y_true.size(0)\n",
        "    recall_count = 0\n",
        "\n",
        "    for true_label, predictions in zip(y_true, y_pred):\n",
        "        top_k_predictions = torch.topk(predictions, k).indices  # Get indices of the top k predictions\n",
        "        if true_label in top_k_predictions:\n",
        "            recall_count += 1\n",
        "\n",
        "    recall_at_k_score = recall_count / num_samples\n",
        "    return recall_at_k_score\n",
        "\n",
        "def evaluate(dataloader_val):\n",
        "    model.eval()\n",
        "\n",
        "    loss_val_total = 0\n",
        "    predictions, true_vals = [], []\n",
        "\n",
        "    for batch in dataloader_val:\n",
        "\n",
        "        batch = tuple(b.to(device) for b in batch)\n",
        "\n",
        "        inputs = {'input_ids':      batch[0],\n",
        "                  'attention_mask': batch[1],\n",
        "                  'labels':         batch[2],\n",
        "                 }\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "        loss_val_total += loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu()\n",
        "        label_ids = inputs['labels'].cpu()\n",
        "        predictions.append(logits)\n",
        "        true_vals.append(label_ids)\n",
        "\n",
        "    loss_val_avg = loss_val_total / len(dataloader_val)\n",
        "\n",
        "    predictions = torch.cat(predictions, dim=0)\n",
        "    true_vals = torch.cat(true_vals, dim=0)\n",
        "\n",
        "    recall_at_5 = recall_at_k(true_vals, predictions, k=5)\n",
        "\n",
        "    return loss_val_avg, predictions, true_vals, recall_at_5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkK_h5iRmeX5",
        "outputId": "533e2823-a908-4ad6-97f3-a0b224d163f2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/5 [00:00<?, ?it/s]\n",
            "Epoch 1:   0%|          | 0/1271 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:   0%|          | 0/1271 [00:02<?, ?it/s, training_loss=1.373]\u001b[A\n",
            "Epoch 1:   0%|          | 1/1271 [00:02<42:32,  2.01s/it, training_loss=1.373]\u001b[A\n",
            "Epoch 1:   0%|          | 1/1271 [00:02<42:32,  2.01s/it, training_loss=1.362]\u001b[A\n",
            "Epoch 1:   0%|          | 2/1271 [00:02<21:03,  1.00it/s, training_loss=1.362]\u001b[A\n",
            "Epoch 1:   0%|          | 2/1271 [00:02<21:03,  1.00it/s, training_loss=1.432]\u001b[A\n",
            "Epoch 1:   0%|          | 3/1271 [00:02<14:16,  1.48it/s, training_loss=1.432]\u001b[A\n",
            "Epoch 1:   0%|          | 3/1271 [00:02<14:16,  1.48it/s, training_loss=1.431]\u001b[A\n",
            "Epoch 1:   0%|          | 4/1271 [00:02<11:03,  1.91it/s, training_loss=1.431]\u001b[A\n",
            "Epoch 1:   0%|          | 4/1271 [00:03<11:03,  1.91it/s, training_loss=1.440]\u001b[A\n",
            "Epoch 1:   0%|          | 5/1271 [00:03<09:16,  2.28it/s, training_loss=1.440]\u001b[A\n",
            "Epoch 1:   0%|          | 5/1271 [00:03<09:16,  2.28it/s, training_loss=1.468]\u001b[A\n",
            "Epoch 1:   0%|          | 6/1271 [00:03<08:12,  2.57it/s, training_loss=1.468]\u001b[A\n",
            "Epoch 1:   0%|          | 6/1271 [00:03<08:12,  2.57it/s, training_loss=1.482]\u001b[A\n",
            "Epoch 1:   1%|          | 7/1271 [00:03<07:28,  2.82it/s, training_loss=1.482]\u001b[A\n",
            "Epoch 1:   1%|          | 7/1271 [00:04<07:28,  2.82it/s, training_loss=1.351]\u001b[A\n",
            "Epoch 1:   1%|          | 8/1271 [00:04<07:01,  3.00it/s, training_loss=1.351]\u001b[A\n",
            "Epoch 1:   1%|          | 8/1271 [00:04<07:01,  3.00it/s, training_loss=1.286]\u001b[A\n",
            "Epoch 1:   1%|          | 9/1271 [00:04<06:42,  3.14it/s, training_loss=1.286]\u001b[A\n",
            "Epoch 1:   1%|          | 9/1271 [00:04<06:42,  3.14it/s, training_loss=1.356]\u001b[A\n",
            "Epoch 1:   1%|          | 10/1271 [00:04<06:29,  3.23it/s, training_loss=1.356]\u001b[A\n",
            "Epoch 1:   1%|          | 10/1271 [00:04<06:29,  3.23it/s, training_loss=1.418]\u001b[A\n",
            "Epoch 1:   1%|          | 11/1271 [00:04<06:22,  3.30it/s, training_loss=1.418]\u001b[A\n",
            "Epoch 1:   1%|          | 11/1271 [00:05<06:22,  3.30it/s, training_loss=1.447]\u001b[A\n",
            "Epoch 1:   1%|          | 12/1271 [00:05<06:17,  3.34it/s, training_loss=1.447]\u001b[A\n",
            "Epoch 1:   1%|          | 12/1271 [00:05<06:17,  3.34it/s, training_loss=1.335]\u001b[A\n",
            "Epoch 1:   1%|          | 13/1271 [00:05<06:12,  3.38it/s, training_loss=1.335]\u001b[A\n",
            "Epoch 1:   1%|          | 13/1271 [00:05<06:12,  3.38it/s, training_loss=1.436]\u001b[A\n",
            "Epoch 1:   1%|          | 14/1271 [00:05<06:09,  3.40it/s, training_loss=1.436]\u001b[A\n",
            "Epoch 1:   1%|          | 14/1271 [00:06<06:09,  3.40it/s, training_loss=1.391]\u001b[A\n",
            "Epoch 1:   1%|          | 15/1271 [00:06<06:07,  3.42it/s, training_loss=1.391]\u001b[A\n",
            "Epoch 1:   1%|          | 15/1271 [00:06<06:07,  3.42it/s, training_loss=1.320]\u001b[A\n",
            "Epoch 1:   1%|▏         | 16/1271 [00:06<06:06,  3.42it/s, training_loss=1.320]\u001b[A\n",
            "Epoch 1:   1%|▏         | 16/1271 [00:06<06:06,  3.42it/s, training_loss=1.413]\u001b[A\n",
            "Epoch 1:   1%|▏         | 17/1271 [00:06<06:05,  3.44it/s, training_loss=1.413]\u001b[A\n",
            "Epoch 1:   1%|▏         | 17/1271 [00:06<06:05,  3.44it/s, training_loss=1.387]\u001b[A\n",
            "Epoch 1:   1%|▏         | 18/1271 [00:06<06:03,  3.44it/s, training_loss=1.387]\u001b[A\n",
            "Epoch 1:   1%|▏         | 18/1271 [00:07<06:03,  3.44it/s, training_loss=1.347]\u001b[A\n",
            "Epoch 1:   1%|▏         | 19/1271 [00:07<06:03,  3.44it/s, training_loss=1.347]\u001b[A\n",
            "Epoch 1:   1%|▏         | 19/1271 [00:07<06:03,  3.44it/s, training_loss=1.347]\u001b[A\n",
            "Epoch 1:   2%|▏         | 20/1271 [00:07<06:04,  3.43it/s, training_loss=1.347]\u001b[A\n",
            "Epoch 1:   2%|▏         | 20/1271 [00:07<06:04,  3.43it/s, training_loss=1.319]\u001b[A\n",
            "Epoch 1:   2%|▏         | 21/1271 [00:07<06:02,  3.44it/s, training_loss=1.319]\u001b[A\n",
            "Epoch 1:   2%|▏         | 21/1271 [00:08<06:02,  3.44it/s, training_loss=1.343]\u001b[A\n",
            "Epoch 1:   2%|▏         | 22/1271 [00:08<06:01,  3.46it/s, training_loss=1.343]\u001b[A\n",
            "Epoch 1:   2%|▏         | 22/1271 [00:08<06:01,  3.46it/s, training_loss=1.423]\u001b[A\n",
            "Epoch 1:   2%|▏         | 23/1271 [00:08<06:01,  3.45it/s, training_loss=1.423]\u001b[A\n",
            "Epoch 1:   2%|▏         | 23/1271 [00:08<06:01,  3.45it/s, training_loss=1.378]\u001b[A\n",
            "Epoch 1:   2%|▏         | 24/1271 [00:08<06:01,  3.45it/s, training_loss=1.378]\u001b[A\n",
            "Epoch 1:   2%|▏         | 24/1271 [00:08<06:01,  3.45it/s, training_loss=1.346]\u001b[A\n",
            "Epoch 1:   2%|▏         | 25/1271 [00:08<06:01,  3.45it/s, training_loss=1.346]\u001b[A\n",
            "Epoch 1:   2%|▏         | 25/1271 [00:09<06:01,  3.45it/s, training_loss=1.374]\u001b[A\n",
            "Epoch 1:   2%|▏         | 26/1271 [00:09<05:51,  3.54it/s, training_loss=1.374]\u001b[A\n",
            "Epoch 1:   2%|▏         | 26/1271 [00:09<05:51,  3.54it/s, training_loss=1.423]\u001b[A\n",
            "Epoch 1:   2%|▏         | 27/1271 [00:09<05:56,  3.49it/s, training_loss=1.423]\u001b[A\n",
            "Epoch 1:   2%|▏         | 27/1271 [00:09<05:56,  3.49it/s, training_loss=1.313]\u001b[A\n",
            "Epoch 1:   2%|▏         | 28/1271 [00:09<05:58,  3.47it/s, training_loss=1.313]\u001b[A\n",
            "Epoch 1:   2%|▏         | 28/1271 [00:10<05:58,  3.47it/s, training_loss=1.287]\u001b[A\n",
            "Epoch 1:   2%|▏         | 29/1271 [00:10<06:01,  3.44it/s, training_loss=1.287]\u001b[A\n",
            "Epoch 1:   2%|▏         | 29/1271 [00:10<06:01,  3.44it/s, training_loss=1.174]\u001b[A\n",
            "Epoch 1:   2%|▏         | 30/1271 [00:10<06:01,  3.43it/s, training_loss=1.174]\u001b[A\n",
            "Epoch 1:   2%|▏         | 30/1271 [00:10<06:01,  3.43it/s, training_loss=1.288]\u001b[A\n",
            "Epoch 1:   2%|▏         | 31/1271 [00:10<06:01,  3.43it/s, training_loss=1.288]\u001b[A\n",
            "Epoch 1:   2%|▏         | 31/1271 [00:10<06:01,  3.43it/s, training_loss=1.428]\u001b[A\n",
            "Epoch 1:   3%|▎         | 32/1271 [00:10<06:01,  3.43it/s, training_loss=1.428]\u001b[A\n",
            "Epoch 1:   3%|▎         | 32/1271 [00:11<06:01,  3.43it/s, training_loss=1.210]\u001b[A\n",
            "Epoch 1:   3%|▎         | 33/1271 [00:11<05:55,  3.48it/s, training_loss=1.210]\u001b[A\n",
            "Epoch 1:   3%|▎         | 33/1271 [00:11<05:55,  3.48it/s, training_loss=1.215]\u001b[A\n",
            "Epoch 1:   3%|▎         | 34/1271 [00:11<05:56,  3.47it/s, training_loss=1.215]\u001b[A\n",
            "Epoch 1:   3%|▎         | 34/1271 [00:11<05:56,  3.47it/s, training_loss=1.279]\u001b[A\n",
            "Epoch 1:   3%|▎         | 35/1271 [00:11<05:56,  3.47it/s, training_loss=1.279]\u001b[A\n",
            "Epoch 1:   3%|▎         | 35/1271 [00:12<05:56,  3.47it/s, training_loss=1.323]\u001b[A\n",
            "Epoch 1:   3%|▎         | 36/1271 [00:12<05:59,  3.44it/s, training_loss=1.323]\u001b[A\n",
            "Epoch 1:   3%|▎         | 36/1271 [00:12<05:59,  3.44it/s, training_loss=1.210]\u001b[A\n",
            "Epoch 1:   3%|▎         | 37/1271 [00:12<06:02,  3.40it/s, training_loss=1.210]\u001b[A\n",
            "Epoch 1:   3%|▎         | 37/1271 [00:12<06:02,  3.40it/s, training_loss=1.355]\u001b[A\n",
            "Epoch 1:   3%|▎         | 38/1271 [00:12<06:03,  3.39it/s, training_loss=1.355]\u001b[A\n",
            "Epoch 1:   3%|▎         | 38/1271 [00:13<06:03,  3.39it/s, training_loss=1.260]\u001b[A\n",
            "Epoch 1:   3%|▎         | 39/1271 [00:13<06:00,  3.41it/s, training_loss=1.260]\u001b[A\n",
            "Epoch 1:   3%|▎         | 39/1271 [00:13<06:00,  3.41it/s, training_loss=1.155]\u001b[A\n",
            "Epoch 1:   3%|▎         | 40/1271 [00:13<05:59,  3.42it/s, training_loss=1.155]\u001b[A\n",
            "Epoch 1:   3%|▎         | 40/1271 [00:13<05:59,  3.42it/s, training_loss=1.432]\u001b[A\n",
            "Epoch 1:   3%|▎         | 41/1271 [00:13<05:58,  3.43it/s, training_loss=1.432]\u001b[A\n",
            "Epoch 1:   3%|▎         | 41/1271 [00:13<05:58,  3.43it/s, training_loss=1.446]\u001b[A\n",
            "Epoch 1:   3%|▎         | 42/1271 [00:13<06:00,  3.41it/s, training_loss=1.446]\u001b[A\n",
            "Epoch 1:   3%|▎         | 42/1271 [00:14<06:00,  3.41it/s, training_loss=1.063]\u001b[A\n",
            "Epoch 1:   3%|▎         | 43/1271 [00:14<06:01,  3.40it/s, training_loss=1.063]\u001b[A\n",
            "Epoch 1:   3%|▎         | 43/1271 [00:14<06:01,  3.40it/s, training_loss=1.344]\u001b[A\n",
            "Epoch 1:   3%|▎         | 44/1271 [00:14<06:00,  3.41it/s, training_loss=1.344]\u001b[A\n",
            "Epoch 1:   3%|▎         | 44/1271 [00:14<06:00,  3.41it/s, training_loss=1.427]\u001b[A\n",
            "Epoch 1:   4%|▎         | 45/1271 [00:14<06:00,  3.40it/s, training_loss=1.427]\u001b[A\n",
            "Epoch 1:   4%|▎         | 45/1271 [00:15<06:00,  3.40it/s, training_loss=1.319]\u001b[A\n",
            "Epoch 1:   4%|▎         | 46/1271 [00:15<05:58,  3.41it/s, training_loss=1.319]\u001b[A\n",
            "Epoch 1:   4%|▎         | 46/1271 [00:15<05:58,  3.41it/s, training_loss=1.492]\u001b[A\n",
            "Epoch 1:   4%|▎         | 47/1271 [00:15<05:56,  3.43it/s, training_loss=1.492]\u001b[A\n",
            "Epoch 1:   4%|▎         | 47/1271 [00:15<05:56,  3.43it/s, training_loss=1.227]\u001b[A\n",
            "Epoch 1:   4%|▍         | 48/1271 [00:15<05:54,  3.45it/s, training_loss=1.227]\u001b[A\n",
            "Epoch 1:   4%|▍         | 48/1271 [00:15<05:54,  3.45it/s, training_loss=1.493]\u001b[A\n",
            "Epoch 1:   4%|▍         | 49/1271 [00:15<05:55,  3.44it/s, training_loss=1.493]\u001b[A\n",
            "Epoch 1:   4%|▍         | 49/1271 [00:16<05:55,  3.44it/s, training_loss=1.380]\u001b[A\n",
            "Epoch 1:   4%|▍         | 50/1271 [00:16<05:55,  3.44it/s, training_loss=1.380]\u001b[A\n",
            "Epoch 1:   4%|▍         | 50/1271 [00:16<05:55,  3.44it/s, training_loss=1.313]\u001b[A\n",
            "Epoch 1:   4%|▍         | 51/1271 [00:16<05:54,  3.44it/s, training_loss=1.313]\u001b[A\n",
            "Epoch 1:   4%|▍         | 51/1271 [00:16<05:54,  3.44it/s, training_loss=1.334]\u001b[A\n",
            "Epoch 1:   4%|▍         | 52/1271 [00:16<05:53,  3.45it/s, training_loss=1.334]\u001b[A\n",
            "Epoch 1:   4%|▍         | 52/1271 [00:17<05:53,  3.45it/s, training_loss=1.317]\u001b[A\n",
            "Epoch 1:   4%|▍         | 53/1271 [00:17<05:53,  3.45it/s, training_loss=1.317]\u001b[A\n",
            "Epoch 1:   4%|▍         | 53/1271 [00:17<05:53,  3.45it/s, training_loss=1.439]\u001b[A\n",
            "Epoch 1:   4%|▍         | 54/1271 [00:17<05:53,  3.44it/s, training_loss=1.439]\u001b[A\n",
            "Epoch 1:   4%|▍         | 54/1271 [00:17<05:53,  3.44it/s, training_loss=1.143]\u001b[A\n",
            "Epoch 1:   4%|▍         | 55/1271 [00:17<05:53,  3.44it/s, training_loss=1.143]\u001b[A\n",
            "Epoch 1:   4%|▍         | 55/1271 [00:17<05:53,  3.44it/s, training_loss=1.228]\u001b[A\n",
            "Epoch 1:   4%|▍         | 56/1271 [00:17<05:53,  3.44it/s, training_loss=1.228]\u001b[A\n",
            "Epoch 1:   4%|▍         | 56/1271 [00:18<05:53,  3.44it/s, training_loss=1.353]\u001b[A\n",
            "Epoch 1:   4%|▍         | 57/1271 [00:18<05:54,  3.42it/s, training_loss=1.353]\u001b[A\n",
            "Epoch 1:   4%|▍         | 57/1271 [00:18<05:54,  3.42it/s, training_loss=1.180]\u001b[A\n",
            "Epoch 1:   5%|▍         | 58/1271 [00:18<05:53,  3.43it/s, training_loss=1.180]\u001b[A\n",
            "Epoch 1:   5%|▍         | 58/1271 [00:18<05:53,  3.43it/s, training_loss=1.044]\u001b[A\n",
            "Epoch 1:   5%|▍         | 59/1271 [00:18<05:52,  3.44it/s, training_loss=1.044]\u001b[A\n",
            "Epoch 1:   5%|▍         | 59/1271 [00:19<05:52,  3.44it/s, training_loss=1.170]\u001b[A\n",
            "Epoch 1:   5%|▍         | 60/1271 [00:19<05:52,  3.43it/s, training_loss=1.170]\u001b[A\n",
            "Epoch 1:   5%|▍         | 60/1271 [00:19<05:52,  3.43it/s, training_loss=1.029]\u001b[A\n",
            "Epoch 1:   5%|▍         | 61/1271 [00:19<05:52,  3.44it/s, training_loss=1.029]\u001b[A\n",
            "Epoch 1:   5%|▍         | 61/1271 [00:19<05:52,  3.44it/s, training_loss=1.036]\u001b[A\n",
            "Epoch 1:   5%|▍         | 62/1271 [00:19<05:52,  3.43it/s, training_loss=1.036]\u001b[A\n",
            "Epoch 1:   5%|▍         | 62/1271 [00:20<05:52,  3.43it/s, training_loss=1.445]\u001b[A\n",
            "Epoch 1:   5%|▍         | 63/1271 [00:20<05:51,  3.44it/s, training_loss=1.445]\u001b[A\n",
            "Epoch 1:   5%|▍         | 63/1271 [00:20<05:51,  3.44it/s, training_loss=1.390]\u001b[A\n",
            "Epoch 1:   5%|▌         | 64/1271 [00:20<05:51,  3.43it/s, training_loss=1.390]\u001b[A\n",
            "Epoch 1:   5%|▌         | 64/1271 [00:20<05:51,  3.43it/s, training_loss=1.199]\u001b[A\n",
            "Epoch 1:   5%|▌         | 65/1271 [00:20<05:46,  3.48it/s, training_loss=1.199]\u001b[A\n",
            "Epoch 1:   5%|▌         | 65/1271 [00:20<05:46,  3.48it/s, training_loss=1.333]\u001b[A\n",
            "Epoch 1:   5%|▌         | 66/1271 [00:20<05:50,  3.44it/s, training_loss=1.333]\u001b[A\n",
            "Epoch 1:   5%|▌         | 66/1271 [00:21<05:50,  3.44it/s, training_loss=1.411]\u001b[A\n",
            "Epoch 1:   5%|▌         | 67/1271 [00:21<05:51,  3.43it/s, training_loss=1.411]\u001b[A\n",
            "Epoch 1:   5%|▌         | 67/1271 [00:21<05:51,  3.43it/s, training_loss=1.225]\u001b[A\n",
            "Epoch 1:   5%|▌         | 68/1271 [00:21<05:49,  3.44it/s, training_loss=1.225]\u001b[A\n",
            "Epoch 1:   5%|▌         | 68/1271 [00:21<05:49,  3.44it/s, training_loss=1.401]\u001b[A\n",
            "Epoch 1:   5%|▌         | 69/1271 [00:21<05:50,  3.43it/s, training_loss=1.401]\u001b[A\n",
            "Epoch 1:   5%|▌         | 69/1271 [00:22<05:50,  3.43it/s, training_loss=1.398]\u001b[A\n",
            "Epoch 1:   6%|▌         | 70/1271 [00:22<05:49,  3.43it/s, training_loss=1.398]\u001b[A\n",
            "Epoch 1:   6%|▌         | 70/1271 [00:22<05:49,  3.43it/s, training_loss=1.479]\u001b[A\n",
            "Epoch 1:   6%|▌         | 71/1271 [00:22<05:50,  3.42it/s, training_loss=1.479]\u001b[A\n",
            "Epoch 1:   6%|▌         | 71/1271 [00:22<05:50,  3.42it/s, training_loss=1.476]\u001b[A\n",
            "Epoch 1:   6%|▌         | 72/1271 [00:22<05:45,  3.47it/s, training_loss=1.476]\u001b[A\n",
            "Epoch 1:   6%|▌         | 72/1271 [00:22<05:45,  3.47it/s, training_loss=1.346]\u001b[A\n",
            "Epoch 1:   6%|▌         | 73/1271 [00:22<05:48,  3.44it/s, training_loss=1.346]\u001b[A\n",
            "Epoch 1:   6%|▌         | 73/1271 [00:23<05:48,  3.44it/s, training_loss=0.946]\u001b[A\n",
            "Epoch 1:   6%|▌         | 74/1271 [00:23<05:49,  3.42it/s, training_loss=0.946]\u001b[A\n",
            "Epoch 1:   6%|▌         | 74/1271 [00:23<05:49,  3.42it/s, training_loss=1.506]\u001b[A\n",
            "Epoch 1:   6%|▌         | 75/1271 [00:23<05:50,  3.41it/s, training_loss=1.506]\u001b[A\n",
            "Epoch 1:   6%|▌         | 75/1271 [00:23<05:50,  3.41it/s, training_loss=1.228]\u001b[A\n",
            "Epoch 1:   6%|▌         | 76/1271 [00:23<05:49,  3.42it/s, training_loss=1.228]\u001b[A\n",
            "Epoch 1:   6%|▌         | 76/1271 [00:24<05:49,  3.42it/s, training_loss=1.345]\u001b[A\n",
            "Epoch 1:   6%|▌         | 77/1271 [00:24<05:49,  3.42it/s, training_loss=1.345]\u001b[A\n",
            "Epoch 1:   6%|▌         | 77/1271 [00:24<05:49,  3.42it/s, training_loss=1.043]\u001b[A\n",
            "Epoch 1:   6%|▌         | 78/1271 [00:24<05:48,  3.43it/s, training_loss=1.043]\u001b[A\n",
            "Epoch 1:   6%|▌         | 78/1271 [00:24<05:48,  3.43it/s, training_loss=0.955]\u001b[A\n",
            "Epoch 1:   6%|▌         | 79/1271 [00:24<05:47,  3.43it/s, training_loss=0.955]\u001b[A\n",
            "Epoch 1:   6%|▌         | 79/1271 [00:24<05:47,  3.43it/s, training_loss=1.376]\u001b[A\n",
            "Epoch 1:   6%|▋         | 80/1271 [00:24<05:48,  3.42it/s, training_loss=1.376]\u001b[A\n",
            "Epoch 1:   6%|▋         | 80/1271 [00:25<05:48,  3.42it/s, training_loss=1.038]\u001b[A\n",
            "Epoch 1:   6%|▋         | 81/1271 [00:25<05:48,  3.41it/s, training_loss=1.038]\u001b[A\n",
            "Epoch 1:   6%|▋         | 81/1271 [00:25<05:48,  3.41it/s, training_loss=1.301]\u001b[A\n",
            "Epoch 1:   6%|▋         | 82/1271 [00:25<05:49,  3.40it/s, training_loss=1.301]\u001b[A\n",
            "Epoch 1:   6%|▋         | 82/1271 [00:25<05:49,  3.40it/s, training_loss=1.395]\u001b[A\n",
            "Epoch 1:   7%|▋         | 83/1271 [00:25<05:48,  3.41it/s, training_loss=1.395]\u001b[A\n",
            "Epoch 1:   7%|▋         | 83/1271 [00:26<05:48,  3.41it/s, training_loss=0.883]\u001b[A\n",
            "Epoch 1:   7%|▋         | 84/1271 [00:26<05:47,  3.42it/s, training_loss=0.883]\u001b[A\n",
            "Epoch 1:   7%|▋         | 84/1271 [00:26<05:47,  3.42it/s, training_loss=1.401]\u001b[A\n",
            "Epoch 1:   7%|▋         | 85/1271 [00:26<05:46,  3.42it/s, training_loss=1.401]\u001b[A\n",
            "Epoch 1:   7%|▋         | 85/1271 [00:26<05:46,  3.42it/s, training_loss=1.285]\u001b[A\n",
            "Epoch 1:   7%|▋         | 86/1271 [00:26<05:46,  3.42it/s, training_loss=1.285]\u001b[A\n",
            "Epoch 1:   7%|▋         | 86/1271 [00:27<05:46,  3.42it/s, training_loss=1.081]\u001b[A\n",
            "Epoch 1:   7%|▋         | 87/1271 [00:27<05:45,  3.42it/s, training_loss=1.081]\u001b[A\n",
            "Epoch 1:   7%|▋         | 87/1271 [00:27<05:45,  3.42it/s, training_loss=0.833]\u001b[A\n",
            "Epoch 1:   7%|▋         | 88/1271 [00:27<05:45,  3.42it/s, training_loss=0.833]\u001b[A\n",
            "Epoch 1:   7%|▋         | 88/1271 [00:27<05:45,  3.42it/s, training_loss=1.164]\u001b[A\n",
            "Epoch 1:   7%|▋         | 89/1271 [00:27<05:45,  3.42it/s, training_loss=1.164]\u001b[A\n",
            "Epoch 1:   7%|▋         | 89/1271 [00:27<05:45,  3.42it/s, training_loss=1.170]\u001b[A\n",
            "Epoch 1:   7%|▋         | 90/1271 [00:27<05:44,  3.42it/s, training_loss=1.170]\u001b[A\n",
            "Epoch 1:   7%|▋         | 90/1271 [00:28<05:44,  3.42it/s, training_loss=1.263]\u001b[A\n",
            "Epoch 1:   7%|▋         | 91/1271 [00:28<05:44,  3.42it/s, training_loss=1.263]\u001b[A\n",
            "Epoch 1:   7%|▋         | 91/1271 [00:28<05:44,  3.42it/s, training_loss=1.320]\u001b[A\n",
            "Epoch 1:   7%|▋         | 92/1271 [00:28<05:43,  3.43it/s, training_loss=1.320]\u001b[A\n",
            "Epoch 1:   7%|▋         | 92/1271 [00:28<05:43,  3.43it/s, training_loss=1.105]\u001b[A\n",
            "Epoch 1:   7%|▋         | 93/1271 [00:28<05:44,  3.42it/s, training_loss=1.105]\u001b[A\n",
            "Epoch 1:   7%|▋         | 93/1271 [00:29<05:44,  3.42it/s, training_loss=1.332]\u001b[A\n",
            "Epoch 1:   7%|▋         | 94/1271 [00:29<05:44,  3.42it/s, training_loss=1.332]\u001b[A\n",
            "Epoch 1:   7%|▋         | 94/1271 [00:29<05:44,  3.42it/s, training_loss=1.141]\u001b[A\n",
            "Epoch 1:   7%|▋         | 95/1271 [00:29<05:44,  3.42it/s, training_loss=1.141]\u001b[A\n",
            "Epoch 1:   7%|▋         | 95/1271 [00:29<05:44,  3.42it/s, training_loss=1.279]\u001b[A\n",
            "Epoch 1:   8%|▊         | 96/1271 [00:29<05:44,  3.41it/s, training_loss=1.279]\u001b[A\n",
            "Epoch 1:   8%|▊         | 96/1271 [00:29<05:44,  3.41it/s, training_loss=1.224]\u001b[A\n",
            "Epoch 1:   8%|▊         | 97/1271 [00:29<05:45,  3.40it/s, training_loss=1.224]\u001b[A\n",
            "Epoch 1:   8%|▊         | 97/1271 [00:30<05:45,  3.40it/s, training_loss=1.336]\u001b[A\n",
            "Epoch 1:   8%|▊         | 98/1271 [00:30<05:42,  3.42it/s, training_loss=1.336]\u001b[A\n",
            "Epoch 1:   8%|▊         | 98/1271 [00:30<05:42,  3.42it/s, training_loss=1.397]\u001b[A\n",
            "Epoch 1:   8%|▊         | 99/1271 [00:30<05:41,  3.43it/s, training_loss=1.397]\u001b[A\n",
            "Epoch 1:   8%|▊         | 99/1271 [00:30<05:41,  3.43it/s, training_loss=1.456]\u001b[A\n",
            "Epoch 1:   8%|▊         | 100/1271 [00:30<05:41,  3.43it/s, training_loss=1.456]\u001b[A\n",
            "Epoch 1:   8%|▊         | 100/1271 [00:31<05:41,  3.43it/s, training_loss=1.251]\u001b[A\n",
            "Epoch 1:   8%|▊         | 101/1271 [00:31<05:40,  3.43it/s, training_loss=1.251]\u001b[A\n",
            "Epoch 1:   8%|▊         | 101/1271 [00:31<05:40,  3.43it/s, training_loss=1.137]\u001b[A\n",
            "Epoch 1:   8%|▊         | 102/1271 [00:31<05:40,  3.43it/s, training_loss=1.137]\u001b[A\n",
            "Epoch 1:   8%|▊         | 102/1271 [00:31<05:40,  3.43it/s, training_loss=1.314]\u001b[A\n",
            "Epoch 1:   8%|▊         | 103/1271 [00:31<05:40,  3.43it/s, training_loss=1.314]\u001b[A\n",
            "Epoch 1:   8%|▊         | 103/1271 [00:31<05:40,  3.43it/s, training_loss=1.389]\u001b[A\n",
            "Epoch 1:   8%|▊         | 104/1271 [00:31<05:40,  3.43it/s, training_loss=1.389]\u001b[A\n",
            "Epoch 1:   8%|▊         | 104/1271 [00:32<05:40,  3.43it/s, training_loss=1.226]\u001b[A\n",
            "Epoch 1:   8%|▊         | 105/1271 [00:32<05:40,  3.42it/s, training_loss=1.226]\u001b[A\n",
            "Epoch 1:   8%|▊         | 105/1271 [00:32<05:40,  3.42it/s, training_loss=1.206]\u001b[A\n",
            "Epoch 1:   8%|▊         | 106/1271 [00:32<05:40,  3.42it/s, training_loss=1.206]\u001b[A\n",
            "Epoch 1:   8%|▊         | 106/1271 [00:32<05:40,  3.42it/s, training_loss=0.708]\u001b[A\n",
            "Epoch 1:   8%|▊         | 107/1271 [00:32<05:39,  3.43it/s, training_loss=0.708]\u001b[A\n",
            "Epoch 1:   8%|▊         | 107/1271 [00:33<05:39,  3.43it/s, training_loss=1.062]\u001b[A\n",
            "Epoch 1:   8%|▊         | 108/1271 [00:33<05:37,  3.45it/s, training_loss=1.062]\u001b[A\n",
            "Epoch 1:   8%|▊         | 108/1271 [00:33<05:37,  3.45it/s, training_loss=1.447]\u001b[A\n",
            "Epoch 1:   9%|▊         | 109/1271 [00:33<05:40,  3.42it/s, training_loss=1.447]\u001b[A\n",
            "Epoch 1:   9%|▊         | 109/1271 [00:33<05:40,  3.42it/s, training_loss=1.165]\u001b[A\n",
            "Epoch 1:   9%|▊         | 110/1271 [00:33<05:41,  3.40it/s, training_loss=1.165]\u001b[A\n",
            "Epoch 1:   9%|▊         | 110/1271 [00:34<05:41,  3.40it/s, training_loss=1.308]\u001b[A\n",
            "Epoch 1:   9%|▊         | 111/1271 [00:34<05:40,  3.41it/s, training_loss=1.308]\u001b[A\n",
            "Epoch 1:   9%|▊         | 111/1271 [00:34<05:40,  3.41it/s, training_loss=1.273]\u001b[A\n",
            "Epoch 1:   9%|▉         | 112/1271 [00:34<05:40,  3.40it/s, training_loss=1.273]\u001b[A\n",
            "Epoch 1:   9%|▉         | 112/1271 [00:34<05:40,  3.40it/s, training_loss=1.382]\u001b[A\n",
            "Epoch 1:   9%|▉         | 113/1271 [00:34<05:39,  3.41it/s, training_loss=1.382]\u001b[A\n",
            "Epoch 1:   9%|▉         | 113/1271 [00:34<05:39,  3.41it/s, training_loss=0.844]\u001b[A\n",
            "Epoch 1:   9%|▉         | 114/1271 [00:34<05:40,  3.40it/s, training_loss=0.844]\u001b[A\n",
            "Epoch 1:   9%|▉         | 114/1271 [00:35<05:40,  3.40it/s, training_loss=1.394]\u001b[A\n",
            "Epoch 1:   9%|▉         | 115/1271 [00:35<05:41,  3.39it/s, training_loss=1.394]\u001b[A\n",
            "Epoch 1:   9%|▉         | 115/1271 [00:35<05:41,  3.39it/s, training_loss=1.080]\u001b[A\n",
            "Epoch 1:   9%|▉         | 116/1271 [00:35<05:40,  3.39it/s, training_loss=1.080]\u001b[A\n",
            "Epoch 1:   9%|▉         | 116/1271 [00:35<05:40,  3.39it/s, training_loss=1.088]\u001b[A\n",
            "Epoch 1:   9%|▉         | 117/1271 [00:35<05:36,  3.43it/s, training_loss=1.088]\u001b[A\n",
            "Epoch 1:   9%|▉         | 117/1271 [00:36<05:36,  3.43it/s, training_loss=1.403]\u001b[A\n",
            "Epoch 1:   9%|▉         | 118/1271 [00:36<05:39,  3.39it/s, training_loss=1.403]\u001b[A\n",
            "Epoch 1:   9%|▉         | 118/1271 [00:36<05:39,  3.39it/s, training_loss=0.743]\u001b[A\n",
            "Epoch 1:   9%|▉         | 119/1271 [00:36<05:40,  3.38it/s, training_loss=0.743]\u001b[A\n",
            "Epoch 1:   9%|▉         | 119/1271 [00:36<05:40,  3.38it/s, training_loss=0.833]\u001b[A\n",
            "Epoch 1:   9%|▉         | 120/1271 [00:36<05:40,  3.38it/s, training_loss=0.833]\u001b[A\n",
            "Epoch 1:   9%|▉         | 120/1271 [00:36<05:40,  3.38it/s, training_loss=1.126]\u001b[A\n",
            "Epoch 1:  10%|▉         | 121/1271 [00:36<05:39,  3.39it/s, training_loss=1.126]\u001b[A\n",
            "Epoch 1:  10%|▉         | 121/1271 [00:37<05:39,  3.39it/s, training_loss=1.124]\u001b[A\n",
            "Epoch 1:  10%|▉         | 122/1271 [00:37<05:38,  3.40it/s, training_loss=1.124]\u001b[A\n",
            "Epoch 1:  10%|▉         | 122/1271 [00:37<05:38,  3.40it/s, training_loss=0.786]\u001b[A\n",
            "Epoch 1:  10%|▉         | 123/1271 [00:37<05:37,  3.40it/s, training_loss=0.786]\u001b[A\n",
            "Epoch 1:  10%|▉         | 123/1271 [00:37<05:37,  3.40it/s, training_loss=1.327]\u001b[A\n",
            "Epoch 1:  10%|▉         | 124/1271 [00:37<05:36,  3.41it/s, training_loss=1.327]\u001b[A\n",
            "Epoch 1:  10%|▉         | 124/1271 [00:38<05:36,  3.41it/s, training_loss=1.365]\u001b[A\n",
            "Epoch 1:  10%|▉         | 125/1271 [00:38<05:37,  3.39it/s, training_loss=1.365]\u001b[A\n",
            "Epoch 1:  10%|▉         | 125/1271 [00:38<05:37,  3.39it/s, training_loss=1.391]\u001b[A\n",
            "Epoch 1:  10%|▉         | 126/1271 [00:38<05:37,  3.39it/s, training_loss=1.391]\u001b[A\n",
            "Epoch 1:  10%|▉         | 126/1271 [00:38<05:37,  3.39it/s, training_loss=1.346]\u001b[A\n",
            "Epoch 1:  10%|▉         | 127/1271 [00:38<05:37,  3.39it/s, training_loss=1.346]\u001b[A\n",
            "Epoch 1:  10%|▉         | 127/1271 [00:39<05:37,  3.39it/s, training_loss=1.398]\u001b[A\n",
            "Epoch 1:  10%|█         | 128/1271 [00:39<05:38,  3.38it/s, training_loss=1.398]\u001b[A\n",
            "Epoch 1:  10%|█         | 128/1271 [00:39<05:38,  3.38it/s, training_loss=1.191]\u001b[A\n",
            "Epoch 1:  10%|█         | 129/1271 [00:39<05:37,  3.39it/s, training_loss=1.191]\u001b[A\n",
            "Epoch 1:  10%|█         | 129/1271 [00:39<05:37,  3.39it/s, training_loss=1.255]\u001b[A\n",
            "Epoch 1:  10%|█         | 130/1271 [00:39<05:36,  3.39it/s, training_loss=1.255]\u001b[A\n",
            "Epoch 1:  10%|█         | 130/1271 [00:39<05:36,  3.39it/s, training_loss=0.810]\u001b[A\n",
            "Epoch 1:  10%|█         | 131/1271 [00:39<05:36,  3.39it/s, training_loss=0.810]\u001b[A\n",
            "Epoch 1:  10%|█         | 131/1271 [00:40<05:36,  3.39it/s, training_loss=1.399]\u001b[A\n",
            "Epoch 1:  10%|█         | 132/1271 [00:40<05:36,  3.39it/s, training_loss=1.399]\u001b[A\n",
            "Epoch 1:  10%|█         | 132/1271 [00:40<05:36,  3.39it/s, training_loss=0.950]\u001b[A\n",
            "Epoch 1:  10%|█         | 133/1271 [00:40<05:36,  3.38it/s, training_loss=0.950]\u001b[A\n",
            "Epoch 1:  10%|█         | 133/1271 [00:40<05:36,  3.38it/s, training_loss=1.093]\u001b[A\n",
            "Epoch 1:  11%|█         | 134/1271 [00:40<05:35,  3.39it/s, training_loss=1.093]\u001b[A\n",
            "Epoch 1:  11%|█         | 134/1271 [00:41<05:35,  3.39it/s, training_loss=1.019]\u001b[A\n",
            "Epoch 1:  11%|█         | 135/1271 [00:41<05:35,  3.39it/s, training_loss=1.019]\u001b[A\n",
            "Epoch 1:  11%|█         | 135/1271 [00:41<05:35,  3.39it/s, training_loss=1.315]\u001b[A\n",
            "Epoch 1:  11%|█         | 136/1271 [00:41<05:34,  3.39it/s, training_loss=1.315]\u001b[A\n",
            "Epoch 1:  11%|█         | 136/1271 [00:41<05:34,  3.39it/s, training_loss=1.122]\u001b[A\n",
            "Epoch 1:  11%|█         | 137/1271 [00:41<05:34,  3.39it/s, training_loss=1.122]\u001b[A\n",
            "Epoch 1:  11%|█         | 137/1271 [00:41<05:34,  3.39it/s, training_loss=0.742]\u001b[A\n",
            "Epoch 1:  11%|█         | 138/1271 [00:41<05:33,  3.40it/s, training_loss=0.742]\u001b[A\n",
            "Epoch 1:  11%|█         | 138/1271 [00:42<05:33,  3.40it/s, training_loss=0.933]\u001b[A\n",
            "Epoch 1:  11%|█         | 139/1271 [00:42<05:35,  3.37it/s, training_loss=0.933]\u001b[A\n",
            "Epoch 1:  11%|█         | 139/1271 [00:42<05:35,  3.37it/s, training_loss=1.231]\u001b[A\n",
            "Epoch 1:  11%|█         | 140/1271 [00:42<05:35,  3.37it/s, training_loss=1.231]\u001b[A\n",
            "Epoch 1:  11%|█         | 140/1271 [00:42<05:35,  3.37it/s, training_loss=1.017]\u001b[A\n",
            "Epoch 1:  11%|█         | 141/1271 [00:42<05:34,  3.38it/s, training_loss=1.017]\u001b[A\n",
            "Epoch 1:  11%|█         | 141/1271 [00:43<05:34,  3.38it/s, training_loss=1.062]\u001b[A\n",
            "Epoch 1:  11%|█         | 142/1271 [00:43<05:34,  3.37it/s, training_loss=1.062]\u001b[A\n",
            "Epoch 1:  11%|█         | 142/1271 [00:43<05:34,  3.37it/s, training_loss=1.200]\u001b[A\n",
            "Epoch 1:  11%|█▏        | 143/1271 [00:43<05:35,  3.36it/s, training_loss=1.200]\u001b[A\n",
            "Epoch 1:  11%|█▏        | 143/1271 [00:43<05:35,  3.36it/s, training_loss=1.378]\u001b[A\n",
            "Epoch 1:  11%|█▏        | 144/1271 [00:43<05:36,  3.35it/s, training_loss=1.378]\u001b[A\n",
            "Epoch 1:  11%|█▏        | 144/1271 [00:44<05:36,  3.35it/s, training_loss=1.017]\u001b[A\n",
            "Epoch 1:  11%|█▏        | 145/1271 [00:44<05:35,  3.35it/s, training_loss=1.017]\u001b[A\n",
            "Epoch 1:  11%|█▏        | 145/1271 [00:44<05:35,  3.35it/s, training_loss=1.266]\u001b[A\n",
            "Epoch 1:  11%|█▏        | 146/1271 [00:44<05:37,  3.33it/s, training_loss=1.266]\u001b[A\n",
            "Epoch 1:  11%|█▏        | 146/1271 [00:44<05:37,  3.33it/s, training_loss=0.376]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 147/1271 [00:44<05:35,  3.35it/s, training_loss=0.376]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 147/1271 [00:44<05:35,  3.35it/s, training_loss=1.297]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 148/1271 [00:44<05:34,  3.36it/s, training_loss=1.297]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 148/1271 [00:45<05:34,  3.36it/s, training_loss=1.156]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 149/1271 [00:45<05:33,  3.36it/s, training_loss=1.156]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 149/1271 [00:45<05:33,  3.36it/s, training_loss=1.044]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 150/1271 [00:45<05:32,  3.37it/s, training_loss=1.044]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 150/1271 [00:45<05:32,  3.37it/s, training_loss=1.581]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 151/1271 [00:45<05:31,  3.38it/s, training_loss=1.581]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 151/1271 [00:46<05:31,  3.38it/s, training_loss=1.222]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 152/1271 [00:46<05:30,  3.38it/s, training_loss=1.222]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 152/1271 [00:46<05:30,  3.38it/s, training_loss=0.964]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 153/1271 [00:46<05:29,  3.39it/s, training_loss=0.964]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 153/1271 [00:46<05:29,  3.39it/s, training_loss=1.244]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 154/1271 [00:46<05:30,  3.38it/s, training_loss=1.244]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 154/1271 [00:47<05:30,  3.38it/s, training_loss=0.972]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 155/1271 [00:47<05:29,  3.39it/s, training_loss=0.972]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 155/1271 [00:47<05:29,  3.39it/s, training_loss=1.203]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 156/1271 [00:47<05:28,  3.39it/s, training_loss=1.203]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 156/1271 [00:47<05:28,  3.39it/s, training_loss=1.463]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 157/1271 [00:47<05:29,  3.38it/s, training_loss=1.463]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 157/1271 [00:47<05:29,  3.38it/s, training_loss=1.167]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 158/1271 [00:47<05:28,  3.39it/s, training_loss=1.167]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 158/1271 [00:48<05:28,  3.39it/s, training_loss=1.099]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 159/1271 [00:48<05:29,  3.38it/s, training_loss=1.099]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 159/1271 [00:48<05:29,  3.38it/s, training_loss=0.805]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 160/1271 [00:48<05:29,  3.38it/s, training_loss=0.805]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 160/1271 [00:48<05:29,  3.38it/s, training_loss=1.381]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 161/1271 [00:48<05:29,  3.37it/s, training_loss=1.381]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 161/1271 [00:49<05:29,  3.37it/s, training_loss=0.893]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 162/1271 [00:49<05:30,  3.36it/s, training_loss=0.893]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 162/1271 [00:49<05:30,  3.36it/s, training_loss=1.264]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 163/1271 [00:49<05:30,  3.35it/s, training_loss=1.264]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 163/1271 [00:49<05:30,  3.35it/s, training_loss=1.317]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 164/1271 [00:49<05:30,  3.35it/s, training_loss=1.317]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 164/1271 [00:50<05:30,  3.35it/s, training_loss=1.385]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 165/1271 [00:50<05:31,  3.33it/s, training_loss=1.385]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 165/1271 [00:50<05:31,  3.33it/s, training_loss=0.729]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 166/1271 [00:50<05:29,  3.36it/s, training_loss=0.729]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 166/1271 [00:50<05:29,  3.36it/s, training_loss=1.206]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 167/1271 [00:50<05:28,  3.36it/s, training_loss=1.206]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 167/1271 [00:50<05:28,  3.36it/s, training_loss=1.006]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 168/1271 [00:50<05:25,  3.38it/s, training_loss=1.006]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 168/1271 [00:51<05:25,  3.38it/s, training_loss=0.942]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 169/1271 [00:51<05:25,  3.38it/s, training_loss=0.942]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 169/1271 [00:51<05:25,  3.38it/s, training_loss=1.017]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 170/1271 [00:51<05:25,  3.38it/s, training_loss=1.017]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 170/1271 [00:51<05:25,  3.38it/s, training_loss=1.237]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 171/1271 [00:51<05:27,  3.36it/s, training_loss=1.237]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 171/1271 [00:52<05:27,  3.36it/s, training_loss=1.006]\u001b[A\n",
            "Epoch 1:  14%|█▎        | 172/1271 [00:52<05:26,  3.36it/s, training_loss=1.006]\u001b[A\n",
            "Epoch 1:  14%|█▎        | 172/1271 [00:52<05:26,  3.36it/s, training_loss=0.338]\u001b[A\n",
            "Epoch 1:  14%|█▎        | 173/1271 [00:52<05:25,  3.37it/s, training_loss=0.338]\u001b[A\n",
            "Epoch 1:  14%|█▎        | 173/1271 [00:52<05:25,  3.37it/s, training_loss=0.525]\u001b[A\n",
            "Epoch 1:  14%|█▎        | 174/1271 [00:52<05:24,  3.38it/s, training_loss=0.525]\u001b[A\n",
            "Epoch 1:  14%|█▎        | 174/1271 [00:52<05:24,  3.38it/s, training_loss=0.406]\u001b[A\n",
            "Epoch 1:  14%|█▍        | 175/1271 [00:52<05:23,  3.39it/s, training_loss=0.406]\u001b[A\n",
            "Epoch 1:  14%|█▍        | 175/1271 [00:53<05:23,  3.39it/s, training_loss=1.242]\u001b[A\n",
            "Epoch 1:  14%|█▍        | 176/1271 [00:53<05:23,  3.38it/s, training_loss=1.242]\u001b[A\n",
            "Epoch 1:  14%|█▍        | 176/1271 [00:53<05:23,  3.38it/s, training_loss=1.392]\u001b[A\n",
            "Epoch 1:  14%|█▍        | 177/1271 [00:53<05:24,  3.37it/s, training_loss=1.392]\u001b[A\n",
            "Epoch 1:  14%|█▍        | 177/1271 [00:53<05:24,  3.37it/s, training_loss=1.392]\u001b[A\n",
            "Epoch 1:  14%|█▍        | 178/1271 [00:53<05:26,  3.35it/s, training_loss=1.392]\u001b[A\n",
            "Epoch 1:  14%|█▍        | 178/1271 [00:54<05:26,  3.35it/s, training_loss=0.867]\u001b[A\n",
            "Epoch 1:  14%|█▍        | 179/1271 [00:54<05:25,  3.36it/s, training_loss=0.867]\u001b[A\n",
            "Epoch 1:  14%|█▍        | 179/1271 [00:54<05:25,  3.36it/s, training_loss=0.947]\u001b[A\n",
            "Epoch 1:  14%|█▍        | 180/1271 [00:54<05:24,  3.36it/s, training_loss=0.947]\u001b[A\n",
            "Epoch 1:  14%|█▍        | 180/1271 [00:54<05:24,  3.36it/s, training_loss=0.977]\u001b[A\n",
            "Epoch 1:  14%|█▍        | 181/1271 [00:54<05:24,  3.36it/s, training_loss=0.977]\u001b[A\n",
            "Epoch 1:  14%|█▍        | 181/1271 [00:55<05:24,  3.36it/s, training_loss=0.855]\u001b[A\n",
            "Epoch 1:  14%|█▍        | 182/1271 [00:55<05:23,  3.36it/s, training_loss=0.855]\u001b[A\n",
            "Epoch 1:  14%|█▍        | 182/1271 [00:55<05:23,  3.36it/s, training_loss=1.186]\u001b[A\n",
            "Epoch 1:  14%|█▍        | 183/1271 [00:55<05:24,  3.35it/s, training_loss=1.186]\u001b[A\n",
            "Epoch 1:  14%|█▍        | 183/1271 [00:55<05:24,  3.35it/s, training_loss=1.274]\u001b[A\n",
            "Epoch 1:  14%|█▍        | 184/1271 [00:55<05:23,  3.36it/s, training_loss=1.274]\u001b[A\n",
            "Epoch 1:  14%|█▍        | 184/1271 [00:55<05:23,  3.36it/s, training_loss=0.890]\u001b[A\n",
            "Epoch 1:  15%|█▍        | 185/1271 [00:55<05:23,  3.35it/s, training_loss=0.890]\u001b[A\n",
            "Epoch 1:  15%|█▍        | 185/1271 [00:56<05:23,  3.35it/s, training_loss=0.920]\u001b[A\n",
            "Epoch 1:  15%|█▍        | 186/1271 [00:56<05:22,  3.36it/s, training_loss=0.920]\u001b[A\n",
            "Epoch 1:  15%|█▍        | 186/1271 [00:56<05:22,  3.36it/s, training_loss=1.173]\u001b[A\n",
            "Epoch 1:  15%|█▍        | 187/1271 [00:56<05:23,  3.35it/s, training_loss=1.173]\u001b[A\n",
            "Epoch 1:  15%|█▍        | 187/1271 [00:56<05:23,  3.35it/s, training_loss=0.702]\u001b[A\n",
            "Epoch 1:  15%|█▍        | 188/1271 [00:56<05:23,  3.35it/s, training_loss=0.702]\u001b[A\n",
            "Epoch 1:  15%|█▍        | 188/1271 [00:57<05:23,  3.35it/s, training_loss=0.644]\u001b[A\n",
            "Epoch 1:  15%|█▍        | 189/1271 [00:57<05:23,  3.35it/s, training_loss=0.644]\u001b[A\n",
            "Epoch 1:  15%|█▍        | 189/1271 [00:57<05:23,  3.35it/s, training_loss=1.315]\u001b[A\n",
            "Epoch 1:  15%|█▍        | 190/1271 [00:57<05:19,  3.38it/s, training_loss=1.315]\u001b[A\n",
            "Epoch 1:  15%|█▍        | 190/1271 [00:57<05:19,  3.38it/s, training_loss=0.240]\u001b[A\n",
            "Epoch 1:  15%|█▌        | 191/1271 [00:57<05:20,  3.37it/s, training_loss=0.240]\u001b[A\n",
            "Epoch 1:  15%|█▌        | 191/1271 [00:58<05:20,  3.37it/s, training_loss=0.490]\u001b[A\n",
            "Epoch 1:  15%|█▌        | 192/1271 [00:58<05:20,  3.37it/s, training_loss=0.490]\u001b[A\n",
            "Epoch 1:  15%|█▌        | 192/1271 [00:58<05:20,  3.37it/s, training_loss=1.237]\u001b[A\n",
            "Epoch 1:  15%|█▌        | 193/1271 [00:58<05:20,  3.37it/s, training_loss=1.237]\u001b[A\n",
            "Epoch 1:  15%|█▌        | 193/1271 [00:58<05:20,  3.37it/s, training_loss=1.391]\u001b[A\n",
            "Epoch 1:  15%|█▌        | 194/1271 [00:58<05:20,  3.36it/s, training_loss=1.391]\u001b[A\n",
            "Epoch 1:  15%|█▌        | 194/1271 [00:58<05:20,  3.36it/s, training_loss=1.334]\u001b[A\n",
            "Epoch 1:  15%|█▌        | 195/1271 [00:58<05:20,  3.36it/s, training_loss=1.334]\u001b[A\n",
            "Epoch 1:  15%|█▌        | 195/1271 [00:59<05:20,  3.36it/s, training_loss=0.895]\u001b[A\n",
            "Epoch 1:  15%|█▌        | 196/1271 [00:59<05:19,  3.36it/s, training_loss=0.895]\u001b[A\n",
            "Epoch 1:  15%|█▌        | 196/1271 [00:59<05:19,  3.36it/s, training_loss=0.696]\u001b[A\n",
            "Epoch 1:  15%|█▌        | 197/1271 [00:59<05:20,  3.36it/s, training_loss=0.696]\u001b[A\n",
            "Epoch 1:  15%|█▌        | 197/1271 [00:59<05:20,  3.36it/s, training_loss=1.112]\u001b[A\n",
            "Epoch 1:  16%|█▌        | 198/1271 [00:59<05:20,  3.35it/s, training_loss=1.112]\u001b[A\n",
            "Epoch 1:  16%|█▌        | 198/1271 [01:00<05:20,  3.35it/s, training_loss=1.128]\u001b[A\n",
            "Epoch 1:  16%|█▌        | 199/1271 [01:00<05:19,  3.36it/s, training_loss=1.128]\u001b[A\n",
            "Epoch 1:  16%|█▌        | 199/1271 [01:00<05:19,  3.36it/s, training_loss=1.247]\u001b[A\n",
            "Epoch 1:  16%|█▌        | 200/1271 [01:00<05:19,  3.35it/s, training_loss=1.247]\u001b[A\n",
            "Epoch 1:  16%|█▌        | 200/1271 [01:00<05:19,  3.35it/s, training_loss=0.945]\u001b[A\n",
            "Epoch 1:  16%|█▌        | 201/1271 [01:00<05:19,  3.35it/s, training_loss=0.945]\u001b[A\n",
            "Epoch 1:  16%|█▌        | 201/1271 [01:01<05:19,  3.35it/s, training_loss=0.963]\u001b[A\n",
            "Epoch 1:  16%|█▌        | 202/1271 [01:01<05:20,  3.34it/s, training_loss=0.963]\u001b[A\n",
            "Epoch 1:  16%|█▌        | 202/1271 [01:01<05:20,  3.34it/s, training_loss=0.894]\u001b[A\n",
            "Epoch 1:  16%|█▌        | 203/1271 [01:01<05:19,  3.34it/s, training_loss=0.894]\u001b[A\n",
            "Epoch 1:  16%|█▌        | 203/1271 [01:01<05:19,  3.34it/s, training_loss=0.639]\u001b[A\n",
            "Epoch 1:  16%|█▌        | 204/1271 [01:01<05:19,  3.34it/s, training_loss=0.639]\u001b[A\n",
            "Epoch 1:  16%|█▌        | 204/1271 [01:01<05:19,  3.34it/s, training_loss=0.644]\u001b[A\n",
            "Epoch 1:  16%|█▌        | 205/1271 [01:01<05:17,  3.35it/s, training_loss=0.644]\u001b[A\n",
            "Epoch 1:  16%|█▌        | 205/1271 [01:02<05:17,  3.35it/s, training_loss=1.375]\u001b[A\n",
            "Epoch 1:  16%|█▌        | 206/1271 [01:02<05:16,  3.36it/s, training_loss=1.375]\u001b[A\n",
            "Epoch 1:  16%|█▌        | 206/1271 [01:02<05:16,  3.36it/s, training_loss=0.746]\u001b[A\n",
            "Epoch 1:  16%|█▋        | 207/1271 [01:02<05:14,  3.38it/s, training_loss=0.746]\u001b[A\n",
            "Epoch 1:  16%|█▋        | 207/1271 [01:02<05:14,  3.38it/s, training_loss=0.819]\u001b[A\n",
            "Epoch 1:  16%|█▋        | 208/1271 [01:02<05:15,  3.37it/s, training_loss=0.819]\u001b[A\n",
            "Epoch 1:  16%|█▋        | 208/1271 [01:03<05:15,  3.37it/s, training_loss=1.285]\u001b[A\n",
            "Epoch 1:  16%|█▋        | 209/1271 [01:03<05:15,  3.37it/s, training_loss=1.285]\u001b[A\n",
            "Epoch 1:  16%|█▋        | 209/1271 [01:03<05:15,  3.37it/s, training_loss=1.267]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 210/1271 [01:03<05:15,  3.36it/s, training_loss=1.267]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 210/1271 [01:03<05:15,  3.36it/s, training_loss=1.295]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 211/1271 [01:03<05:16,  3.35it/s, training_loss=1.295]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 211/1271 [01:03<05:16,  3.35it/s, training_loss=0.439]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 212/1271 [01:04<05:16,  3.35it/s, training_loss=0.439]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 212/1271 [01:04<05:16,  3.35it/s, training_loss=1.285]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 213/1271 [01:04<05:18,  3.33it/s, training_loss=1.285]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 213/1271 [01:04<05:18,  3.33it/s, training_loss=1.246]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 214/1271 [01:04<05:17,  3.33it/s, training_loss=1.246]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 214/1271 [01:04<05:17,  3.33it/s, training_loss=0.837]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 215/1271 [01:04<05:17,  3.33it/s, training_loss=0.837]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 215/1271 [01:05<05:17,  3.33it/s, training_loss=0.882]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 216/1271 [01:05<05:17,  3.33it/s, training_loss=0.882]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 216/1271 [01:05<05:17,  3.33it/s, training_loss=1.318]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 217/1271 [01:05<05:18,  3.31it/s, training_loss=1.318]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 217/1271 [01:05<05:18,  3.31it/s, training_loss=0.379]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 218/1271 [01:05<05:17,  3.32it/s, training_loss=0.379]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 218/1271 [01:06<05:17,  3.32it/s, training_loss=0.917]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 219/1271 [01:06<05:15,  3.33it/s, training_loss=0.917]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 219/1271 [01:06<05:15,  3.33it/s, training_loss=1.030]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 220/1271 [01:06<05:18,  3.30it/s, training_loss=1.030]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 220/1271 [01:06<05:18,  3.30it/s, training_loss=1.188]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 221/1271 [01:06<05:17,  3.31it/s, training_loss=1.188]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 221/1271 [01:07<05:17,  3.31it/s, training_loss=1.041]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 222/1271 [01:07<05:16,  3.32it/s, training_loss=1.041]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 222/1271 [01:07<05:16,  3.32it/s, training_loss=0.360]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 223/1271 [01:07<05:13,  3.34it/s, training_loss=0.360]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 223/1271 [01:07<05:13,  3.34it/s, training_loss=0.969]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 224/1271 [01:07<05:12,  3.35it/s, training_loss=0.969]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 224/1271 [01:07<05:12,  3.35it/s, training_loss=1.199]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 225/1271 [01:07<05:12,  3.35it/s, training_loss=1.199]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 225/1271 [01:08<05:12,  3.35it/s, training_loss=0.706]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 226/1271 [01:08<05:12,  3.35it/s, training_loss=0.706]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 226/1271 [01:08<05:12,  3.35it/s, training_loss=0.368]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 227/1271 [01:08<05:11,  3.36it/s, training_loss=0.368]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 227/1271 [01:08<05:11,  3.36it/s, training_loss=0.780]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 228/1271 [01:08<05:10,  3.35it/s, training_loss=0.780]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 228/1271 [01:09<05:10,  3.35it/s, training_loss=0.544]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 229/1271 [01:09<05:11,  3.35it/s, training_loss=0.544]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 229/1271 [01:09<05:11,  3.35it/s, training_loss=0.989]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 230/1271 [01:09<05:13,  3.32it/s, training_loss=0.989]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 230/1271 [01:09<05:13,  3.32it/s, training_loss=0.461]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 231/1271 [01:09<05:12,  3.33it/s, training_loss=0.461]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 231/1271 [01:10<05:12,  3.33it/s, training_loss=1.389]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 232/1271 [01:10<05:11,  3.33it/s, training_loss=1.389]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 232/1271 [01:10<05:11,  3.33it/s, training_loss=0.705]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 233/1271 [01:10<05:12,  3.32it/s, training_loss=0.705]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 233/1271 [01:10<05:12,  3.32it/s, training_loss=0.944]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 234/1271 [01:10<05:11,  3.33it/s, training_loss=0.944]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 234/1271 [01:10<05:11,  3.33it/s, training_loss=0.896]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 235/1271 [01:10<05:10,  3.34it/s, training_loss=0.896]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 235/1271 [01:11<05:10,  3.34it/s, training_loss=0.158]\u001b[A\n",
            "Epoch 1:  19%|█▊        | 236/1271 [01:11<05:08,  3.35it/s, training_loss=0.158]\u001b[A\n",
            "Epoch 1:  19%|█▊        | 236/1271 [01:11<05:08,  3.35it/s, training_loss=0.651]\u001b[A\n",
            "Epoch 1:  19%|█▊        | 237/1271 [01:11<05:08,  3.35it/s, training_loss=0.651]\u001b[A\n",
            "Epoch 1:  19%|█▊        | 237/1271 [01:11<05:08,  3.35it/s, training_loss=0.664]\u001b[A\n",
            "Epoch 1:  19%|█▊        | 238/1271 [01:11<05:08,  3.35it/s, training_loss=0.664]\u001b[A\n",
            "Epoch 1:  19%|█▊        | 238/1271 [01:12<05:08,  3.35it/s, training_loss=1.105]\u001b[A\n",
            "Epoch 1:  19%|█▉        | 239/1271 [01:12<05:09,  3.34it/s, training_loss=1.105]\u001b[A\n",
            "Epoch 1:  19%|█▉        | 239/1271 [01:12<05:09,  3.34it/s, training_loss=1.297]\u001b[A\n",
            "Epoch 1:  19%|█▉        | 240/1271 [01:12<05:08,  3.34it/s, training_loss=1.297]\u001b[A\n",
            "Epoch 1:  19%|█▉        | 240/1271 [01:12<05:08,  3.34it/s, training_loss=1.347]\u001b[A\n",
            "Epoch 1:  19%|█▉        | 241/1271 [01:12<05:09,  3.33it/s, training_loss=1.347]\u001b[A\n",
            "Epoch 1:  19%|█▉        | 241/1271 [01:12<05:09,  3.33it/s, training_loss=1.412]\u001b[A\n",
            "Epoch 1:  19%|█▉        | 242/1271 [01:13<05:08,  3.33it/s, training_loss=1.412]\u001b[A\n",
            "Epoch 1:  19%|█▉        | 242/1271 [01:13<05:08,  3.33it/s, training_loss=1.225]\u001b[A\n",
            "Epoch 1:  19%|█▉        | 243/1271 [01:13<05:08,  3.33it/s, training_loss=1.225]\u001b[A\n",
            "Epoch 1:  19%|█▉        | 243/1271 [01:13<05:08,  3.33it/s, training_loss=1.197]\u001b[A\n",
            "Epoch 1:  19%|█▉        | 244/1271 [01:13<05:07,  3.34it/s, training_loss=1.197]\u001b[A\n",
            "Epoch 1:  19%|█▉        | 244/1271 [01:13<05:07,  3.34it/s, training_loss=0.136]\u001b[A\n",
            "Epoch 1:  19%|█▉        | 245/1271 [01:13<05:06,  3.35it/s, training_loss=0.136]\u001b[A\n",
            "Epoch 1:  19%|█▉        | 245/1271 [01:14<05:06,  3.35it/s, training_loss=0.478]\u001b[A\n",
            "Epoch 1:  19%|█▉        | 246/1271 [01:14<05:05,  3.36it/s, training_loss=0.478]\u001b[A\n",
            "Epoch 1:  19%|█▉        | 246/1271 [01:14<05:05,  3.36it/s, training_loss=0.760]\u001b[A\n",
            "Epoch 1:  19%|█▉        | 247/1271 [01:14<05:05,  3.35it/s, training_loss=0.760]\u001b[A\n",
            "Epoch 1:  19%|█▉        | 247/1271 [01:14<05:05,  3.35it/s, training_loss=0.525]\u001b[A\n",
            "Epoch 1:  20%|█▉        | 248/1271 [01:14<05:05,  3.35it/s, training_loss=0.525]\u001b[A\n",
            "Epoch 1:  20%|█▉        | 248/1271 [01:15<05:05,  3.35it/s, training_loss=0.951]\u001b[A\n",
            "Epoch 1:  20%|█▉        | 249/1271 [01:15<05:04,  3.35it/s, training_loss=0.951]\u001b[A\n",
            "Epoch 1:  20%|█▉        | 249/1271 [01:15<05:04,  3.35it/s, training_loss=1.416]\u001b[A\n",
            "Epoch 1:  20%|█▉        | 250/1271 [01:15<05:04,  3.35it/s, training_loss=1.416]\u001b[A\n",
            "Epoch 1:  20%|█▉        | 250/1271 [01:15<05:04,  3.35it/s, training_loss=0.702]\u001b[A\n",
            "Epoch 1:  20%|█▉        | 251/1271 [01:15<05:04,  3.35it/s, training_loss=0.702]\u001b[A\n",
            "Epoch 1:  20%|█▉        | 251/1271 [01:15<05:04,  3.35it/s, training_loss=1.336]\u001b[A\n",
            "Epoch 1:  20%|█▉        | 252/1271 [01:15<05:05,  3.33it/s, training_loss=1.336]\u001b[A\n",
            "Epoch 1:  20%|█▉        | 252/1271 [01:16<05:05,  3.33it/s, training_loss=0.444]\u001b[A\n",
            "Epoch 1:  20%|█▉        | 253/1271 [01:16<05:06,  3.33it/s, training_loss=0.444]\u001b[A\n",
            "Epoch 1:  20%|█▉        | 253/1271 [01:16<05:06,  3.33it/s, training_loss=1.501]\u001b[A\n",
            "Epoch 1:  20%|█▉        | 254/1271 [01:16<05:05,  3.33it/s, training_loss=1.501]\u001b[A\n",
            "Epoch 1:  20%|█▉        | 254/1271 [01:16<05:05,  3.33it/s, training_loss=1.253]\u001b[A\n",
            "Epoch 1:  20%|██        | 255/1271 [01:16<05:03,  3.34it/s, training_loss=1.253]\u001b[A\n",
            "Epoch 1:  20%|██        | 255/1271 [01:17<05:03,  3.34it/s, training_loss=1.267]\u001b[A\n",
            "Epoch 1:  20%|██        | 256/1271 [01:17<05:05,  3.32it/s, training_loss=1.267]\u001b[A\n",
            "Epoch 1:  20%|██        | 256/1271 [01:17<05:05,  3.32it/s, training_loss=0.501]\u001b[A\n",
            "Epoch 1:  20%|██        | 257/1271 [01:17<05:04,  3.33it/s, training_loss=0.501]\u001b[A\n",
            "Epoch 1:  20%|██        | 257/1271 [01:17<05:04,  3.33it/s, training_loss=0.876]\u001b[A\n",
            "Epoch 1:  20%|██        | 258/1271 [01:17<05:05,  3.31it/s, training_loss=0.876]\u001b[A\n",
            "Epoch 1:  20%|██        | 258/1271 [01:18<05:05,  3.31it/s, training_loss=1.017]\u001b[A\n",
            "Epoch 1:  20%|██        | 259/1271 [01:18<05:04,  3.32it/s, training_loss=1.017]\u001b[A\n",
            "Epoch 1:  20%|██        | 259/1271 [01:18<05:04,  3.32it/s, training_loss=0.442]\u001b[A\n",
            "Epoch 1:  20%|██        | 260/1271 [01:18<05:06,  3.30it/s, training_loss=0.442]\u001b[A\n",
            "Epoch 1:  20%|██        | 260/1271 [01:18<05:06,  3.30it/s, training_loss=0.898]\u001b[A\n",
            "Epoch 1:  21%|██        | 261/1271 [01:18<05:06,  3.30it/s, training_loss=0.898]\u001b[A\n",
            "Epoch 1:  21%|██        | 261/1271 [01:19<05:06,  3.30it/s, training_loss=0.952]\u001b[A\n",
            "Epoch 1:  21%|██        | 262/1271 [01:19<05:05,  3.31it/s, training_loss=0.952]\u001b[A\n",
            "Epoch 1:  21%|██        | 262/1271 [01:19<05:05,  3.31it/s, training_loss=0.638]\u001b[A\n",
            "Epoch 1:  21%|██        | 263/1271 [01:19<05:03,  3.32it/s, training_loss=0.638]\u001b[A\n",
            "Epoch 1:  21%|██        | 263/1271 [01:19<05:03,  3.32it/s, training_loss=0.478]\u001b[A\n",
            "Epoch 1:  21%|██        | 264/1271 [01:19<05:04,  3.31it/s, training_loss=0.478]\u001b[A\n",
            "Epoch 1:  21%|██        | 264/1271 [01:19<05:04,  3.31it/s, training_loss=1.254]\u001b[A\n",
            "Epoch 1:  21%|██        | 265/1271 [01:19<05:03,  3.31it/s, training_loss=1.254]\u001b[A\n",
            "Epoch 1:  21%|██        | 265/1271 [01:20<05:03,  3.31it/s, training_loss=1.498]\u001b[A\n",
            "Epoch 1:  21%|██        | 266/1271 [01:20<05:03,  3.31it/s, training_loss=1.498]\u001b[A\n",
            "Epoch 1:  21%|██        | 266/1271 [01:20<05:03,  3.31it/s, training_loss=0.888]\u001b[A\n",
            "Epoch 1:  21%|██        | 267/1271 [01:20<05:02,  3.31it/s, training_loss=0.888]\u001b[A\n",
            "Epoch 1:  21%|██        | 267/1271 [01:20<05:02,  3.31it/s, training_loss=0.685]\u001b[A\n",
            "Epoch 1:  21%|██        | 268/1271 [01:20<05:02,  3.32it/s, training_loss=0.685]\u001b[A\n",
            "Epoch 1:  21%|██        | 268/1271 [01:21<05:02,  3.32it/s, training_loss=1.090]\u001b[A\n",
            "Epoch 1:  21%|██        | 269/1271 [01:21<05:01,  3.32it/s, training_loss=1.090]\u001b[A\n",
            "Epoch 1:  21%|██        | 269/1271 [01:21<05:01,  3.32it/s, training_loss=1.251]\u001b[A\n",
            "Epoch 1:  21%|██        | 270/1271 [01:21<05:02,  3.30it/s, training_loss=1.251]\u001b[A\n",
            "Epoch 1:  21%|██        | 270/1271 [01:21<05:02,  3.30it/s, training_loss=0.540]\u001b[A\n",
            "Epoch 1:  21%|██▏       | 271/1271 [01:21<05:02,  3.30it/s, training_loss=0.540]\u001b[A\n",
            "Epoch 1:  21%|██▏       | 271/1271 [01:22<05:02,  3.30it/s, training_loss=0.997]\u001b[A\n",
            "Epoch 1:  21%|██▏       | 272/1271 [01:22<05:02,  3.30it/s, training_loss=0.997]\u001b[A\n",
            "Epoch 1:  21%|██▏       | 272/1271 [01:22<05:02,  3.30it/s, training_loss=0.898]\u001b[A\n",
            "Epoch 1:  21%|██▏       | 273/1271 [01:22<05:04,  3.28it/s, training_loss=0.898]\u001b[A\n",
            "Epoch 1:  21%|██▏       | 273/1271 [01:22<05:04,  3.28it/s, training_loss=0.984]\u001b[A\n",
            "Epoch 1:  22%|██▏       | 274/1271 [01:22<05:03,  3.28it/s, training_loss=0.984]\u001b[A\n",
            "Epoch 1:  22%|██▏       | 274/1271 [01:22<05:03,  3.28it/s, training_loss=1.252]\u001b[A\n",
            "Epoch 1:  22%|██▏       | 275/1271 [01:22<05:03,  3.28it/s, training_loss=1.252]\u001b[A\n",
            "Epoch 1:  22%|██▏       | 275/1271 [01:23<05:03,  3.28it/s, training_loss=0.088]\u001b[A\n",
            "Epoch 1:  22%|██▏       | 276/1271 [01:23<05:02,  3.29it/s, training_loss=0.088]\u001b[A\n",
            "Epoch 1:  22%|██▏       | 276/1271 [01:23<05:02,  3.29it/s, training_loss=0.782]\u001b[A\n",
            "Epoch 1:  22%|██▏       | 277/1271 [01:23<05:02,  3.29it/s, training_loss=0.782]\u001b[A\n",
            "Epoch 1:  22%|██▏       | 277/1271 [01:23<05:02,  3.29it/s, training_loss=0.683]\u001b[A\n",
            "Epoch 1:  22%|██▏       | 278/1271 [01:23<05:00,  3.31it/s, training_loss=0.683]\u001b[A\n",
            "Epoch 1:  22%|██▏       | 278/1271 [01:24<05:00,  3.31it/s, training_loss=1.310]\u001b[A\n",
            "Epoch 1:  22%|██▏       | 279/1271 [01:24<04:59,  3.31it/s, training_loss=1.310]\u001b[A\n",
            "Epoch 1:  22%|██▏       | 279/1271 [01:24<04:59,  3.31it/s, training_loss=0.833]\u001b[A\n",
            "Epoch 1:  22%|██▏       | 280/1271 [01:24<04:58,  3.32it/s, training_loss=0.833]\u001b[A\n",
            "Epoch 1:  22%|██▏       | 280/1271 [01:24<04:58,  3.32it/s, training_loss=1.424]\u001b[A\n",
            "Epoch 1:  22%|██▏       | 281/1271 [01:24<04:58,  3.32it/s, training_loss=1.424]\u001b[A\n",
            "Epoch 1:  22%|██▏       | 281/1271 [01:25<04:58,  3.32it/s, training_loss=0.958]\u001b[A\n",
            "Epoch 1:  22%|██▏       | 282/1271 [01:25<04:59,  3.30it/s, training_loss=0.958]\u001b[A\n",
            "Epoch 1:  22%|██▏       | 282/1271 [01:25<04:59,  3.30it/s, training_loss=0.996]\u001b[A\n",
            "Epoch 1:  22%|██▏       | 283/1271 [01:25<05:03,  3.26it/s, training_loss=0.996]\u001b[A\n",
            "Epoch 1:  22%|██▏       | 283/1271 [01:25<05:03,  3.26it/s, training_loss=1.015]\u001b[A\n",
            "Epoch 1:  22%|██▏       | 284/1271 [01:25<05:02,  3.27it/s, training_loss=1.015]\u001b[A\n",
            "Epoch 1:  22%|██▏       | 284/1271 [01:25<05:02,  3.27it/s, training_loss=0.519]\u001b[A\n",
            "Epoch 1:  22%|██▏       | 285/1271 [01:25<04:59,  3.29it/s, training_loss=0.519]\u001b[A\n",
            "Epoch 1:  22%|██▏       | 285/1271 [01:26<04:59,  3.29it/s, training_loss=0.670]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 286/1271 [01:26<04:57,  3.31it/s, training_loss=0.670]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 286/1271 [01:26<04:57,  3.31it/s, training_loss=0.749]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 287/1271 [01:26<04:57,  3.31it/s, training_loss=0.749]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 287/1271 [01:26<04:57,  3.31it/s, training_loss=1.211]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 288/1271 [01:26<04:57,  3.30it/s, training_loss=1.211]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 288/1271 [01:27<04:57,  3.30it/s, training_loss=0.805]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 289/1271 [01:27<04:58,  3.29it/s, training_loss=0.805]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 289/1271 [01:27<04:58,  3.29it/s, training_loss=0.316]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 290/1271 [01:27<04:56,  3.30it/s, training_loss=0.316]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 290/1271 [01:27<04:56,  3.30it/s, training_loss=1.235]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 291/1271 [01:27<04:56,  3.31it/s, training_loss=1.235]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 291/1271 [01:28<04:56,  3.31it/s, training_loss=1.202]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 292/1271 [01:28<04:57,  3.29it/s, training_loss=1.202]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 292/1271 [01:28<04:57,  3.29it/s, training_loss=0.777]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 293/1271 [01:28<04:58,  3.28it/s, training_loss=0.777]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 293/1271 [01:28<04:58,  3.28it/s, training_loss=1.034]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 294/1271 [01:28<04:58,  3.28it/s, training_loss=1.034]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 294/1271 [01:29<04:58,  3.28it/s, training_loss=1.084]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 295/1271 [01:29<04:58,  3.27it/s, training_loss=1.084]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 295/1271 [01:29<04:58,  3.27it/s, training_loss=0.256]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 296/1271 [01:29<04:58,  3.27it/s, training_loss=0.256]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 296/1271 [01:29<04:58,  3.27it/s, training_loss=0.881]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 297/1271 [01:29<04:59,  3.25it/s, training_loss=0.881]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 297/1271 [01:29<04:59,  3.25it/s, training_loss=0.676]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 298/1271 [01:29<04:58,  3.26it/s, training_loss=0.676]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 298/1271 [01:30<04:58,  3.26it/s, training_loss=0.889]\u001b[A\n",
            "Epoch 1:  24%|██▎       | 299/1271 [01:30<04:57,  3.27it/s, training_loss=0.889]\u001b[A\n",
            "Epoch 1:  24%|██▎       | 299/1271 [01:30<04:57,  3.27it/s, training_loss=1.333]\u001b[A\n",
            "Epoch 1:  24%|██▎       | 300/1271 [01:30<04:55,  3.29it/s, training_loss=1.333]\u001b[A\n",
            "Epoch 1:  24%|██▎       | 300/1271 [01:30<04:55,  3.29it/s, training_loss=1.203]\u001b[A\n",
            "Epoch 1:  24%|██▎       | 301/1271 [01:30<04:54,  3.30it/s, training_loss=1.203]\u001b[A\n",
            "Epoch 1:  24%|██▎       | 301/1271 [01:31<04:54,  3.30it/s, training_loss=0.505]\u001b[A\n",
            "Epoch 1:  24%|██▍       | 302/1271 [01:31<04:53,  3.30it/s, training_loss=0.505]\u001b[A\n",
            "Epoch 1:  24%|██▍       | 302/1271 [01:31<04:53,  3.30it/s, training_loss=0.965]\u001b[A\n",
            "Epoch 1:  24%|██▍       | 303/1271 [01:31<04:54,  3.28it/s, training_loss=0.965]\u001b[A\n",
            "Epoch 1:  24%|██▍       | 303/1271 [01:31<04:54,  3.28it/s, training_loss=0.822]\u001b[A\n",
            "Epoch 1:  24%|██▍       | 304/1271 [01:31<04:55,  3.27it/s, training_loss=0.822]\u001b[A\n",
            "Epoch 1:  24%|██▍       | 304/1271 [01:32<04:55,  3.27it/s, training_loss=0.782]\u001b[A\n",
            "Epoch 1:  24%|██▍       | 305/1271 [01:32<04:56,  3.26it/s, training_loss=0.782]\u001b[A\n",
            "Epoch 1:  24%|██▍       | 305/1271 [01:32<04:56,  3.26it/s, training_loss=0.827]\u001b[A\n",
            "Epoch 1:  24%|██▍       | 306/1271 [01:32<04:55,  3.27it/s, training_loss=0.827]\u001b[A\n",
            "Epoch 1:  24%|██▍       | 306/1271 [01:32<04:55,  3.27it/s, training_loss=1.001]\u001b[A\n",
            "Epoch 1:  24%|██▍       | 307/1271 [01:32<04:54,  3.28it/s, training_loss=1.001]\u001b[A\n",
            "Epoch 1:  24%|██▍       | 307/1271 [01:32<04:54,  3.28it/s, training_loss=1.280]\u001b[A\n",
            "Epoch 1:  24%|██▍       | 308/1271 [01:32<04:54,  3.27it/s, training_loss=1.280]\u001b[A\n",
            "Epoch 1:  24%|██▍       | 308/1271 [01:33<04:54,  3.27it/s, training_loss=1.318]\u001b[A\n",
            "Epoch 1:  24%|██▍       | 309/1271 [01:33<04:55,  3.25it/s, training_loss=1.318]\u001b[A\n",
            "Epoch 1:  24%|██▍       | 309/1271 [01:33<04:55,  3.25it/s, training_loss=0.524]\u001b[A\n",
            "Epoch 1:  24%|██▍       | 310/1271 [01:33<04:54,  3.27it/s, training_loss=0.524]\u001b[A\n",
            "Epoch 1:  24%|██▍       | 310/1271 [01:33<04:54,  3.27it/s, training_loss=0.867]\u001b[A\n",
            "Epoch 1:  24%|██▍       | 311/1271 [01:33<04:52,  3.28it/s, training_loss=0.867]\u001b[A\n",
            "Epoch 1:  24%|██▍       | 311/1271 [01:34<04:52,  3.28it/s, training_loss=0.851]\u001b[A\n",
            "Epoch 1:  25%|██▍       | 312/1271 [01:34<04:50,  3.30it/s, training_loss=0.851]\u001b[A\n",
            "Epoch 1:  25%|██▍       | 312/1271 [01:34<04:50,  3.30it/s, training_loss=1.301]\u001b[A\n",
            "Epoch 1:  25%|██▍       | 313/1271 [01:34<04:52,  3.27it/s, training_loss=1.301]\u001b[A\n",
            "Epoch 1:  25%|██▍       | 313/1271 [01:34<04:52,  3.27it/s, training_loss=0.605]\u001b[A\n",
            "Epoch 1:  25%|██▍       | 314/1271 [01:34<04:51,  3.28it/s, training_loss=0.605]\u001b[A\n",
            "Epoch 1:  25%|██▍       | 314/1271 [01:35<04:51,  3.28it/s, training_loss=0.387]\u001b[A\n",
            "Epoch 1:  25%|██▍       | 315/1271 [01:35<04:51,  3.28it/s, training_loss=0.387]\u001b[A\n",
            "Epoch 1:  25%|██▍       | 315/1271 [01:35<04:51,  3.28it/s, training_loss=1.251]\u001b[A\n",
            "Epoch 1:  25%|██▍       | 316/1271 [01:35<04:52,  3.26it/s, training_loss=1.251]\u001b[A\n",
            "Epoch 1:  25%|██▍       | 316/1271 [01:35<04:52,  3.26it/s, training_loss=0.447]\u001b[A\n",
            "Epoch 1:  25%|██▍       | 317/1271 [01:35<04:52,  3.27it/s, training_loss=0.447]\u001b[A\n",
            "Epoch 1:  25%|██▍       | 317/1271 [01:36<04:52,  3.27it/s, training_loss=0.952]\u001b[A\n",
            "Epoch 1:  25%|██▌       | 318/1271 [01:36<04:51,  3.27it/s, training_loss=0.952]\u001b[A\n",
            "Epoch 1:  25%|██▌       | 318/1271 [01:36<04:51,  3.27it/s, training_loss=0.787]\u001b[A\n",
            "Epoch 1:  25%|██▌       | 319/1271 [01:36<04:50,  3.27it/s, training_loss=0.787]\u001b[A\n",
            "Epoch 1:  25%|██▌       | 319/1271 [01:36<04:50,  3.27it/s, training_loss=1.016]\u001b[A\n",
            "Epoch 1:  25%|██▌       | 320/1271 [01:36<04:52,  3.26it/s, training_loss=1.016]\u001b[A\n",
            "Epoch 1:  25%|██▌       | 320/1271 [01:36<04:52,  3.26it/s, training_loss=1.290]\u001b[A\n",
            "Epoch 1:  25%|██▌       | 321/1271 [01:36<04:51,  3.26it/s, training_loss=1.290]\u001b[A\n",
            "Epoch 1:  25%|██▌       | 321/1271 [01:37<04:51,  3.26it/s, training_loss=0.911]\u001b[A\n",
            "Epoch 1:  25%|██▌       | 322/1271 [01:37<04:50,  3.26it/s, training_loss=0.911]\u001b[A\n",
            "Epoch 1:  25%|██▌       | 322/1271 [01:37<04:50,  3.26it/s, training_loss=1.456]\u001b[A\n",
            "Epoch 1:  25%|██▌       | 323/1271 [01:37<04:51,  3.25it/s, training_loss=1.456]\u001b[A\n",
            "Epoch 1:  25%|██▌       | 323/1271 [01:37<04:51,  3.25it/s, training_loss=0.972]\u001b[A\n",
            "Epoch 1:  25%|██▌       | 324/1271 [01:37<04:50,  3.27it/s, training_loss=0.972]\u001b[A\n",
            "Epoch 1:  25%|██▌       | 324/1271 [01:38<04:50,  3.27it/s, training_loss=1.209]\u001b[A\n",
            "Epoch 1:  26%|██▌       | 325/1271 [01:38<04:48,  3.27it/s, training_loss=1.209]\u001b[A\n",
            "Epoch 1:  26%|██▌       | 325/1271 [01:38<04:48,  3.27it/s, training_loss=0.781]\u001b[A\n",
            "Epoch 1:  26%|██▌       | 326/1271 [01:38<04:49,  3.26it/s, training_loss=0.781]\u001b[A\n",
            "Epoch 1:  26%|██▌       | 326/1271 [01:38<04:49,  3.26it/s, training_loss=0.917]\u001b[A\n",
            "Epoch 1:  26%|██▌       | 327/1271 [01:38<04:51,  3.24it/s, training_loss=0.917]\u001b[A\n",
            "Epoch 1:  26%|██▌       | 327/1271 [01:39<04:51,  3.24it/s, training_loss=1.063]\u001b[A\n",
            "Epoch 1:  26%|██▌       | 328/1271 [01:39<04:52,  3.22it/s, training_loss=1.063]\u001b[A\n",
            "Epoch 1:  26%|██▌       | 328/1271 [01:39<04:52,  3.22it/s, training_loss=1.302]\u001b[A\n",
            "Epoch 1:  26%|██▌       | 329/1271 [01:39<04:51,  3.24it/s, training_loss=1.302]\u001b[A\n",
            "Epoch 1:  26%|██▌       | 329/1271 [01:39<04:51,  3.24it/s, training_loss=0.868]\u001b[A\n",
            "Epoch 1:  26%|██▌       | 330/1271 [01:39<04:51,  3.23it/s, training_loss=0.868]\u001b[A\n",
            "Epoch 1:  26%|██▌       | 330/1271 [01:40<04:51,  3.23it/s, training_loss=0.548]\u001b[A\n",
            "Epoch 1:  26%|██▌       | 331/1271 [01:40<04:50,  3.24it/s, training_loss=0.548]\u001b[A\n",
            "Epoch 1:  26%|██▌       | 331/1271 [01:40<04:50,  3.24it/s, training_loss=0.929]\u001b[A\n",
            "Epoch 1:  26%|██▌       | 332/1271 [01:40<04:48,  3.25it/s, training_loss=0.929]\u001b[A\n",
            "Epoch 1:  26%|██▌       | 332/1271 [01:40<04:48,  3.25it/s, training_loss=0.861]\u001b[A\n",
            "Epoch 1:  26%|██▌       | 333/1271 [01:40<04:47,  3.26it/s, training_loss=0.861]\u001b[A\n",
            "Epoch 1:  26%|██▌       | 333/1271 [01:40<04:47,  3.26it/s, training_loss=1.101]\u001b[A\n",
            "Epoch 1:  26%|██▋       | 334/1271 [01:40<04:47,  3.26it/s, training_loss=1.101]\u001b[A\n",
            "Epoch 1:  26%|██▋       | 334/1271 [01:41<04:47,  3.26it/s, training_loss=0.806]\u001b[A\n",
            "Epoch 1:  26%|██▋       | 335/1271 [01:41<04:48,  3.25it/s, training_loss=0.806]\u001b[A\n",
            "Epoch 1:  26%|██▋       | 335/1271 [01:41<04:48,  3.25it/s, training_loss=0.808]\u001b[A\n",
            "Epoch 1:  26%|██▋       | 336/1271 [01:41<04:53,  3.18it/s, training_loss=0.808]\u001b[A\n",
            "Epoch 1:  26%|██▋       | 336/1271 [01:41<04:53,  3.18it/s, training_loss=0.818]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 337/1271 [01:41<04:49,  3.22it/s, training_loss=0.818]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 337/1271 [01:42<04:49,  3.22it/s, training_loss=1.239]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 338/1271 [01:42<04:48,  3.23it/s, training_loss=1.239]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 338/1271 [01:42<04:48,  3.23it/s, training_loss=0.852]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 339/1271 [01:42<04:49,  3.22it/s, training_loss=0.852]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 339/1271 [01:42<04:49,  3.22it/s, training_loss=1.234]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 340/1271 [01:42<04:50,  3.20it/s, training_loss=1.234]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 340/1271 [01:43<04:50,  3.20it/s, training_loss=0.983]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 341/1271 [01:43<04:58,  3.12it/s, training_loss=0.983]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 341/1271 [01:43<04:58,  3.12it/s, training_loss=1.387]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 342/1271 [01:43<04:53,  3.17it/s, training_loss=1.387]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 342/1271 [01:43<04:53,  3.17it/s, training_loss=1.096]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 343/1271 [01:43<04:50,  3.19it/s, training_loss=1.096]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 343/1271 [01:44<04:50,  3.19it/s, training_loss=0.433]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 344/1271 [01:44<04:53,  3.16it/s, training_loss=0.433]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 344/1271 [01:44<04:53,  3.16it/s, training_loss=0.517]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 345/1271 [01:44<05:03,  3.05it/s, training_loss=0.517]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 345/1271 [01:44<05:03,  3.05it/s, training_loss=1.237]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 346/1271 [01:44<05:11,  2.97it/s, training_loss=1.237]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 346/1271 [01:45<05:11,  2.97it/s, training_loss=1.103]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 347/1271 [01:45<05:09,  2.99it/s, training_loss=1.103]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 347/1271 [01:45<05:09,  2.99it/s, training_loss=0.828]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 348/1271 [01:45<05:01,  3.07it/s, training_loss=0.828]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 348/1271 [01:45<05:01,  3.07it/s, training_loss=0.419]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 349/1271 [01:45<04:55,  3.12it/s, training_loss=0.419]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 349/1271 [01:46<04:55,  3.12it/s, training_loss=1.184]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 350/1271 [01:46<04:47,  3.21it/s, training_loss=1.184]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 350/1271 [01:46<04:47,  3.21it/s, training_loss=0.770]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 351/1271 [01:46<04:46,  3.21it/s, training_loss=0.770]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 351/1271 [01:46<04:46,  3.21it/s, training_loss=1.081]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 352/1271 [01:46<04:47,  3.19it/s, training_loss=1.081]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 352/1271 [01:47<04:47,  3.19it/s, training_loss=1.000]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 353/1271 [01:47<04:45,  3.21it/s, training_loss=1.000]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 353/1271 [01:47<04:45,  3.21it/s, training_loss=0.910]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 354/1271 [01:47<04:45,  3.22it/s, training_loss=0.910]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 354/1271 [01:47<04:45,  3.22it/s, training_loss=1.266]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 355/1271 [01:47<04:43,  3.23it/s, training_loss=1.266]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 355/1271 [01:47<04:43,  3.23it/s, training_loss=0.939]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 356/1271 [01:47<04:41,  3.25it/s, training_loss=0.939]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 356/1271 [01:48<04:41,  3.25it/s, training_loss=0.078]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 357/1271 [01:48<04:39,  3.26it/s, training_loss=0.078]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 357/1271 [01:48<04:39,  3.26it/s, training_loss=0.634]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 358/1271 [01:48<04:39,  3.26it/s, training_loss=0.634]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 358/1271 [01:48<04:39,  3.26it/s, training_loss=0.786]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 359/1271 [01:48<04:41,  3.24it/s, training_loss=0.786]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 359/1271 [01:49<04:41,  3.24it/s, training_loss=1.444]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 360/1271 [01:49<04:40,  3.24it/s, training_loss=1.444]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 360/1271 [01:49<04:40,  3.24it/s, training_loss=0.393]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 361/1271 [01:49<04:39,  3.26it/s, training_loss=0.393]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 361/1271 [01:49<04:39,  3.26it/s, training_loss=0.789]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 362/1271 [01:49<04:38,  3.26it/s, training_loss=0.789]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 362/1271 [01:50<04:38,  3.26it/s, training_loss=1.322]\u001b[A\n",
            "Epoch 1:  29%|██▊       | 363/1271 [01:50<04:38,  3.26it/s, training_loss=1.322]\u001b[A\n",
            "Epoch 1:  29%|██▊       | 363/1271 [01:50<04:38,  3.26it/s, training_loss=1.289]\u001b[A\n",
            "Epoch 1:  29%|██▊       | 364/1271 [01:50<04:39,  3.25it/s, training_loss=1.289]\u001b[A\n",
            "Epoch 1:  29%|██▊       | 364/1271 [01:50<04:39,  3.25it/s, training_loss=1.342]\u001b[A\n",
            "Epoch 1:  29%|██▊       | 365/1271 [01:50<04:38,  3.25it/s, training_loss=1.342]\u001b[A\n",
            "Epoch 1:  29%|██▊       | 365/1271 [01:50<04:38,  3.25it/s, training_loss=0.834]\u001b[A\n",
            "Epoch 1:  29%|██▉       | 366/1271 [01:51<04:37,  3.26it/s, training_loss=0.834]\u001b[A\n",
            "Epoch 1:  29%|██▉       | 366/1271 [01:51<04:37,  3.26it/s, training_loss=0.765]\u001b[A\n",
            "Epoch 1:  29%|██▉       | 367/1271 [01:51<04:37,  3.26it/s, training_loss=0.765]\u001b[A\n",
            "Epoch 1:  29%|██▉       | 367/1271 [01:51<04:37,  3.26it/s, training_loss=1.217]\u001b[A\n",
            "Epoch 1:  29%|██▉       | 368/1271 [01:51<04:37,  3.26it/s, training_loss=1.217]\u001b[A\n",
            "Epoch 1:  29%|██▉       | 368/1271 [01:51<04:37,  3.26it/s, training_loss=0.819]\u001b[A\n",
            "Epoch 1:  29%|██▉       | 369/1271 [01:51<04:36,  3.26it/s, training_loss=0.819]\u001b[A\n",
            "Epoch 1:  29%|██▉       | 369/1271 [01:52<04:36,  3.26it/s, training_loss=0.718]\u001b[A\n",
            "Epoch 1:  29%|██▉       | 370/1271 [01:52<04:35,  3.27it/s, training_loss=0.718]\u001b[A\n",
            "Epoch 1:  29%|██▉       | 370/1271 [01:52<04:35,  3.27it/s, training_loss=1.265]\u001b[A\n",
            "Epoch 1:  29%|██▉       | 371/1271 [01:52<04:36,  3.25it/s, training_loss=1.265]\u001b[A\n",
            "Epoch 1:  29%|██▉       | 371/1271 [01:52<04:36,  3.25it/s, training_loss=1.241]\u001b[A\n",
            "Epoch 1:  29%|██▉       | 372/1271 [01:52<04:38,  3.23it/s, training_loss=1.241]\u001b[A\n",
            "Epoch 1:  29%|██▉       | 372/1271 [01:53<04:38,  3.23it/s, training_loss=0.861]\u001b[A\n",
            "Epoch 1:  29%|██▉       | 373/1271 [01:53<04:39,  3.22it/s, training_loss=0.861]\u001b[A\n",
            "Epoch 1:  29%|██▉       | 373/1271 [01:53<04:39,  3.22it/s, training_loss=0.679]\u001b[A\n",
            "Epoch 1:  29%|██▉       | 374/1271 [01:53<04:38,  3.22it/s, training_loss=0.679]\u001b[A\n",
            "Epoch 1:  29%|██▉       | 374/1271 [01:53<04:38,  3.22it/s, training_loss=0.517]\u001b[A\n",
            "Epoch 1:  30%|██▉       | 375/1271 [01:53<04:37,  3.22it/s, training_loss=0.517]\u001b[A\n",
            "Epoch 1:  30%|██▉       | 375/1271 [01:54<04:37,  3.22it/s, training_loss=0.779]\u001b[A\n",
            "Epoch 1:  30%|██▉       | 376/1271 [01:54<04:36,  3.23it/s, training_loss=0.779]\u001b[A\n",
            "Epoch 1:  30%|██▉       | 376/1271 [01:54<04:36,  3.23it/s, training_loss=0.974]\u001b[A\n",
            "Epoch 1:  30%|██▉       | 377/1271 [01:54<04:35,  3.24it/s, training_loss=0.974]\u001b[A\n",
            "Epoch 1:  30%|██▉       | 377/1271 [01:54<04:35,  3.24it/s, training_loss=1.097]\u001b[A\n",
            "Epoch 1:  30%|██▉       | 378/1271 [01:54<04:36,  3.23it/s, training_loss=1.097]\u001b[A\n",
            "Epoch 1:  30%|██▉       | 378/1271 [01:55<04:36,  3.23it/s, training_loss=1.234]\u001b[A\n",
            "Epoch 1:  30%|██▉       | 379/1271 [01:55<04:35,  3.24it/s, training_loss=1.234]\u001b[A\n",
            "Epoch 1:  30%|██▉       | 379/1271 [01:55<04:35,  3.24it/s, training_loss=0.745]\u001b[A\n",
            "Epoch 1:  30%|██▉       | 380/1271 [01:55<04:36,  3.22it/s, training_loss=0.745]\u001b[A\n",
            "Epoch 1:  30%|██▉       | 380/1271 [01:55<04:36,  3.22it/s, training_loss=1.162]\u001b[A\n",
            "Epoch 1:  30%|██▉       | 381/1271 [01:55<04:35,  3.23it/s, training_loss=1.162]\u001b[A\n",
            "Epoch 1:  30%|██▉       | 381/1271 [01:55<04:35,  3.23it/s, training_loss=0.780]\u001b[A\n",
            "Epoch 1:  30%|███       | 382/1271 [01:55<04:37,  3.20it/s, training_loss=0.780]\u001b[A\n",
            "Epoch 1:  30%|███       | 382/1271 [01:56<04:37,  3.20it/s, training_loss=1.231]\u001b[A\n",
            "Epoch 1:  30%|███       | 383/1271 [01:56<04:36,  3.22it/s, training_loss=1.231]\u001b[A\n",
            "Epoch 1:  30%|███       | 383/1271 [01:56<04:36,  3.22it/s, training_loss=0.786]\u001b[A\n",
            "Epoch 1:  30%|███       | 384/1271 [01:56<04:34,  3.23it/s, training_loss=0.786]\u001b[A\n",
            "Epoch 1:  30%|███       | 384/1271 [01:56<04:34,  3.23it/s, training_loss=0.448]\u001b[A\n",
            "Epoch 1:  30%|███       | 385/1271 [01:56<04:33,  3.23it/s, training_loss=0.448]\u001b[A\n",
            "Epoch 1:  30%|███       | 385/1271 [01:57<04:33,  3.23it/s, training_loss=0.700]\u001b[A\n",
            "Epoch 1:  30%|███       | 386/1271 [01:57<04:33,  3.24it/s, training_loss=0.700]\u001b[A\n",
            "Epoch 1:  30%|███       | 386/1271 [01:57<04:33,  3.24it/s, training_loss=0.767]\u001b[A\n",
            "Epoch 1:  30%|███       | 387/1271 [01:57<04:33,  3.23it/s, training_loss=0.767]\u001b[A\n",
            "Epoch 1:  30%|███       | 387/1271 [01:57<04:33,  3.23it/s, training_loss=0.591]\u001b[A\n",
            "Epoch 1:  31%|███       | 388/1271 [01:57<04:34,  3.22it/s, training_loss=0.591]\u001b[A\n",
            "Epoch 1:  31%|███       | 388/1271 [01:58<04:34,  3.22it/s, training_loss=1.298]\u001b[A\n",
            "Epoch 1:  31%|███       | 389/1271 [01:58<04:34,  3.21it/s, training_loss=1.298]\u001b[A\n",
            "Epoch 1:  31%|███       | 389/1271 [01:58<04:34,  3.21it/s, training_loss=1.393]\u001b[A\n",
            "Epoch 1:  31%|███       | 390/1271 [01:58<04:33,  3.22it/s, training_loss=1.393]\u001b[A\n",
            "Epoch 1:  31%|███       | 390/1271 [01:58<04:33,  3.22it/s, training_loss=0.581]\u001b[A\n",
            "Epoch 1:  31%|███       | 391/1271 [01:58<04:32,  3.23it/s, training_loss=0.581]\u001b[A\n",
            "Epoch 1:  31%|███       | 391/1271 [01:59<04:32,  3.23it/s, training_loss=0.330]\u001b[A\n",
            "Epoch 1:  31%|███       | 392/1271 [01:59<04:33,  3.21it/s, training_loss=0.330]\u001b[A\n",
            "Epoch 1:  31%|███       | 392/1271 [01:59<04:33,  3.21it/s, training_loss=0.773]\u001b[A\n",
            "Epoch 1:  31%|███       | 393/1271 [01:59<04:33,  3.21it/s, training_loss=0.773]\u001b[A\n",
            "Epoch 1:  31%|███       | 393/1271 [01:59<04:33,  3.21it/s, training_loss=0.917]\u001b[A\n",
            "Epoch 1:  31%|███       | 394/1271 [01:59<04:32,  3.22it/s, training_loss=0.917]\u001b[A\n",
            "Epoch 1:  31%|███       | 394/1271 [01:59<04:32,  3.22it/s, training_loss=0.820]\u001b[A\n",
            "Epoch 1:  31%|███       | 395/1271 [01:59<04:31,  3.23it/s, training_loss=0.820]\u001b[A\n",
            "Epoch 1:  31%|███       | 395/1271 [02:00<04:31,  3.23it/s, training_loss=0.807]\u001b[A\n",
            "Epoch 1:  31%|███       | 396/1271 [02:00<04:31,  3.22it/s, training_loss=0.807]\u001b[A\n",
            "Epoch 1:  31%|███       | 396/1271 [02:00<04:31,  3.22it/s, training_loss=0.954]\u001b[A\n",
            "Epoch 1:  31%|███       | 397/1271 [02:00<04:30,  3.24it/s, training_loss=0.954]\u001b[A\n",
            "Epoch 1:  31%|███       | 397/1271 [02:00<04:30,  3.24it/s, training_loss=1.017]\u001b[A\n",
            "Epoch 1:  31%|███▏      | 398/1271 [02:00<04:29,  3.24it/s, training_loss=1.017]\u001b[A\n",
            "Epoch 1:  31%|███▏      | 398/1271 [02:01<04:29,  3.24it/s, training_loss=0.709]\u001b[A\n",
            "Epoch 1:  31%|███▏      | 399/1271 [02:01<04:29,  3.23it/s, training_loss=0.709]\u001b[A\n",
            "Epoch 1:  31%|███▏      | 399/1271 [02:01<04:29,  3.23it/s, training_loss=0.594]\u001b[A\n",
            "Epoch 1:  31%|███▏      | 400/1271 [02:01<04:29,  3.24it/s, training_loss=0.594]\u001b[A\n",
            "Epoch 1:  31%|███▏      | 400/1271 [02:01<04:29,  3.24it/s, training_loss=0.054]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 401/1271 [02:01<04:27,  3.26it/s, training_loss=0.054]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 401/1271 [02:02<04:27,  3.26it/s, training_loss=1.013]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 402/1271 [02:02<04:27,  3.25it/s, training_loss=1.013]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 402/1271 [02:02<04:27,  3.25it/s, training_loss=0.988]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 403/1271 [02:02<04:27,  3.24it/s, training_loss=0.988]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 403/1271 [02:02<04:27,  3.24it/s, training_loss=0.654]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 404/1271 [02:02<04:26,  3.25it/s, training_loss=0.654]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 404/1271 [02:03<04:26,  3.25it/s, training_loss=0.871]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 405/1271 [02:03<04:27,  3.24it/s, training_loss=0.871]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 405/1271 [02:03<04:27,  3.24it/s, training_loss=0.829]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 406/1271 [02:03<04:26,  3.25it/s, training_loss=0.829]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 406/1271 [02:03<04:26,  3.25it/s, training_loss=1.033]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 407/1271 [02:03<04:25,  3.25it/s, training_loss=1.033]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 407/1271 [02:03<04:25,  3.25it/s, training_loss=0.872]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 408/1271 [02:03<04:24,  3.26it/s, training_loss=0.872]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 408/1271 [02:04<04:24,  3.26it/s, training_loss=0.327]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 409/1271 [02:04<04:25,  3.24it/s, training_loss=0.327]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 409/1271 [02:04<04:25,  3.24it/s, training_loss=0.811]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 410/1271 [02:04<04:27,  3.22it/s, training_loss=0.811]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 410/1271 [02:04<04:27,  3.22it/s, training_loss=0.973]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 411/1271 [02:04<04:26,  3.23it/s, training_loss=0.973]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 411/1271 [02:05<04:26,  3.23it/s, training_loss=0.694]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 412/1271 [02:05<04:26,  3.22it/s, training_loss=0.694]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 412/1271 [02:05<04:26,  3.22it/s, training_loss=1.081]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 413/1271 [02:05<04:24,  3.24it/s, training_loss=1.081]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 413/1271 [02:05<04:24,  3.24it/s, training_loss=0.741]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 414/1271 [02:05<04:24,  3.24it/s, training_loss=0.741]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 414/1271 [02:06<04:24,  3.24it/s, training_loss=1.097]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 415/1271 [02:06<04:25,  3.22it/s, training_loss=1.097]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 415/1271 [02:06<04:25,  3.22it/s, training_loss=1.146]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 416/1271 [02:06<04:25,  3.22it/s, training_loss=1.146]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 416/1271 [02:06<04:25,  3.22it/s, training_loss=0.906]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 417/1271 [02:06<04:23,  3.24it/s, training_loss=0.906]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 417/1271 [02:07<04:23,  3.24it/s, training_loss=0.511]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 418/1271 [02:07<04:24,  3.22it/s, training_loss=0.511]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 418/1271 [02:07<04:24,  3.22it/s, training_loss=0.856]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 419/1271 [02:07<04:23,  3.23it/s, training_loss=0.856]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 419/1271 [02:07<04:23,  3.23it/s, training_loss=0.922]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 420/1271 [02:07<04:22,  3.24it/s, training_loss=0.922]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 420/1271 [02:08<04:22,  3.24it/s, training_loss=1.057]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 421/1271 [02:08<04:21,  3.24it/s, training_loss=1.057]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 421/1271 [02:08<04:21,  3.24it/s, training_loss=0.647]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 422/1271 [02:08<04:23,  3.22it/s, training_loss=0.647]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 422/1271 [02:08<04:23,  3.22it/s, training_loss=0.482]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 423/1271 [02:08<04:24,  3.21it/s, training_loss=0.482]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 423/1271 [02:08<04:24,  3.21it/s, training_loss=0.580]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 424/1271 [02:08<04:23,  3.22it/s, training_loss=0.580]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 424/1271 [02:09<04:23,  3.22it/s, training_loss=0.694]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 425/1271 [02:09<04:21,  3.23it/s, training_loss=0.694]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 425/1271 [02:09<04:21,  3.23it/s, training_loss=0.590]\u001b[A\n",
            "Epoch 1:  34%|███▎      | 426/1271 [02:09<04:20,  3.24it/s, training_loss=0.590]\u001b[A\n",
            "Epoch 1:  34%|███▎      | 426/1271 [02:09<04:20,  3.24it/s, training_loss=0.969]\u001b[A\n",
            "Epoch 1:  34%|███▎      | 427/1271 [02:09<04:22,  3.22it/s, training_loss=0.969]\u001b[A\n",
            "Epoch 1:  34%|███▎      | 427/1271 [02:10<04:22,  3.22it/s, training_loss=0.476]\u001b[A\n",
            "Epoch 1:  34%|███▎      | 428/1271 [02:10<04:21,  3.23it/s, training_loss=0.476]\u001b[A\n",
            "Epoch 1:  34%|███▎      | 428/1271 [02:10<04:21,  3.23it/s, training_loss=0.805]\u001b[A\n",
            "Epoch 1:  34%|███▍      | 429/1271 [02:10<04:20,  3.23it/s, training_loss=0.805]\u001b[A\n",
            "Epoch 1:  34%|███▍      | 429/1271 [02:10<04:20,  3.23it/s, training_loss=1.079]\u001b[A\n",
            "Epoch 1:  34%|███▍      | 430/1271 [02:10<04:20,  3.23it/s, training_loss=1.079]\u001b[A\n",
            "Epoch 1:  34%|███▍      | 430/1271 [02:11<04:20,  3.23it/s, training_loss=0.720]\u001b[A\n",
            "Epoch 1:  34%|███▍      | 431/1271 [02:11<04:19,  3.23it/s, training_loss=0.720]\u001b[A\n",
            "Epoch 1:  34%|███▍      | 431/1271 [02:11<04:19,  3.23it/s, training_loss=1.157]\u001b[A\n",
            "Epoch 1:  34%|███▍      | 432/1271 [02:11<04:20,  3.22it/s, training_loss=1.157]\u001b[A\n",
            "Epoch 1:  34%|███▍      | 432/1271 [02:11<04:20,  3.22it/s, training_loss=0.473]\u001b[A\n",
            "Epoch 1:  34%|███▍      | 433/1271 [02:11<04:20,  3.21it/s, training_loss=0.473]\u001b[A\n",
            "Epoch 1:  34%|███▍      | 433/1271 [02:12<04:20,  3.21it/s, training_loss=1.202]\u001b[A\n",
            "Epoch 1:  34%|███▍      | 434/1271 [02:12<04:21,  3.20it/s, training_loss=1.202]\u001b[A\n",
            "Epoch 1:  34%|███▍      | 434/1271 [02:12<04:21,  3.20it/s, training_loss=0.352]\u001b[A\n",
            "Epoch 1:  34%|███▍      | 435/1271 [02:12<04:19,  3.22it/s, training_loss=0.352]\u001b[A\n",
            "Epoch 1:  34%|███▍      | 435/1271 [02:12<04:19,  3.22it/s, training_loss=0.408]\u001b[A\n",
            "Epoch 1:  34%|███▍      | 436/1271 [02:12<04:19,  3.22it/s, training_loss=0.408]\u001b[A\n",
            "Epoch 1:  34%|███▍      | 436/1271 [02:12<04:19,  3.22it/s, training_loss=0.455]\u001b[A\n",
            "Epoch 1:  34%|███▍      | 437/1271 [02:12<04:18,  3.23it/s, training_loss=0.455]\u001b[A\n",
            "Epoch 1:  34%|███▍      | 437/1271 [02:13<04:18,  3.23it/s, training_loss=0.689]\u001b[A\n",
            "Epoch 1:  34%|███▍      | 438/1271 [02:13<04:16,  3.24it/s, training_loss=0.689]\u001b[A\n",
            "Epoch 1:  34%|███▍      | 438/1271 [02:13<04:16,  3.24it/s, training_loss=0.805]\u001b[A\n",
            "Epoch 1:  35%|███▍      | 439/1271 [02:13<04:18,  3.22it/s, training_loss=0.805]\u001b[A\n",
            "Epoch 1:  35%|███▍      | 439/1271 [02:13<04:18,  3.22it/s, training_loss=1.040]\u001b[A\n",
            "Epoch 1:  35%|███▍      | 440/1271 [02:13<04:17,  3.23it/s, training_loss=1.040]\u001b[A\n",
            "Epoch 1:  35%|███▍      | 440/1271 [02:14<04:17,  3.23it/s, training_loss=1.316]\u001b[A\n",
            "Epoch 1:  35%|███▍      | 441/1271 [02:14<04:17,  3.22it/s, training_loss=1.316]\u001b[A\n",
            "Epoch 1:  35%|███▍      | 441/1271 [02:14<04:17,  3.22it/s, training_loss=0.680]\u001b[A\n",
            "Epoch 1:  35%|███▍      | 442/1271 [02:14<04:16,  3.23it/s, training_loss=0.680]\u001b[A\n",
            "Epoch 1:  35%|███▍      | 442/1271 [02:14<04:16,  3.23it/s, training_loss=0.907]\u001b[A\n",
            "Epoch 1:  35%|███▍      | 443/1271 [02:14<04:15,  3.24it/s, training_loss=0.907]\u001b[A\n",
            "Epoch 1:  35%|███▍      | 443/1271 [02:15<04:15,  3.24it/s, training_loss=0.703]\u001b[A\n",
            "Epoch 1:  35%|███▍      | 444/1271 [02:15<04:14,  3.25it/s, training_loss=0.703]\u001b[A\n",
            "Epoch 1:  35%|███▍      | 444/1271 [02:15<04:14,  3.25it/s, training_loss=0.285]\u001b[A\n",
            "Epoch 1:  35%|███▌      | 445/1271 [02:15<04:15,  3.23it/s, training_loss=0.285]\u001b[A\n",
            "Epoch 1:  35%|███▌      | 445/1271 [02:15<04:15,  3.23it/s, training_loss=0.811]\u001b[A\n",
            "Epoch 1:  35%|███▌      | 446/1271 [02:15<04:14,  3.24it/s, training_loss=0.811]\u001b[A\n",
            "Epoch 1:  35%|███▌      | 446/1271 [02:16<04:14,  3.24it/s, training_loss=0.510]\u001b[A\n",
            "Epoch 1:  35%|███▌      | 447/1271 [02:16<04:13,  3.25it/s, training_loss=0.510]\u001b[A\n",
            "Epoch 1:  35%|███▌      | 447/1271 [02:16<04:13,  3.25it/s, training_loss=0.940]\u001b[A\n",
            "Epoch 1:  35%|███▌      | 448/1271 [02:16<04:12,  3.25it/s, training_loss=0.940]\u001b[A\n",
            "Epoch 1:  35%|███▌      | 448/1271 [02:16<04:12,  3.25it/s, training_loss=1.044]\u001b[A\n",
            "Epoch 1:  35%|███▌      | 449/1271 [02:16<04:11,  3.27it/s, training_loss=1.044]\u001b[A\n",
            "Epoch 1:  35%|███▌      | 449/1271 [02:16<04:11,  3.27it/s, training_loss=1.143]\u001b[A\n",
            "Epoch 1:  35%|███▌      | 450/1271 [02:16<04:13,  3.24it/s, training_loss=1.143]\u001b[A\n",
            "Epoch 1:  35%|███▌      | 450/1271 [02:17<04:13,  3.24it/s, training_loss=0.991]\u001b[A\n",
            "Epoch 1:  35%|███▌      | 451/1271 [02:17<04:12,  3.24it/s, training_loss=0.991]\u001b[A\n",
            "Epoch 1:  35%|███▌      | 451/1271 [02:17<04:12,  3.24it/s, training_loss=0.552]\u001b[A\n",
            "Epoch 1:  36%|███▌      | 452/1271 [02:17<04:11,  3.26it/s, training_loss=0.552]\u001b[A\n",
            "Epoch 1:  36%|███▌      | 452/1271 [02:17<04:11,  3.26it/s, training_loss=1.246]\u001b[A\n",
            "Epoch 1:  36%|███▌      | 453/1271 [02:17<04:12,  3.24it/s, training_loss=1.246]\u001b[A\n",
            "Epoch 1:  36%|███▌      | 453/1271 [02:18<04:12,  3.24it/s, training_loss=0.908]\u001b[A\n",
            "Epoch 1:  36%|███▌      | 454/1271 [02:18<04:13,  3.23it/s, training_loss=0.908]\u001b[A\n",
            "Epoch 1:  36%|███▌      | 454/1271 [02:18<04:13,  3.23it/s, training_loss=1.196]\u001b[A\n",
            "Epoch 1:  36%|███▌      | 455/1271 [02:18<04:12,  3.23it/s, training_loss=1.196]\u001b[A\n",
            "Epoch 1:  36%|███▌      | 455/1271 [02:18<04:12,  3.23it/s, training_loss=0.665]\u001b[A\n",
            "Epoch 1:  36%|███▌      | 456/1271 [02:18<04:11,  3.24it/s, training_loss=0.665]\u001b[A\n",
            "Epoch 1:  36%|███▌      | 456/1271 [02:19<04:11,  3.24it/s, training_loss=1.138]\u001b[A\n",
            "Epoch 1:  36%|███▌      | 457/1271 [02:19<04:10,  3.25it/s, training_loss=1.138]\u001b[A\n",
            "Epoch 1:  36%|███▌      | 457/1271 [02:19<04:10,  3.25it/s, training_loss=0.937]\u001b[A\n",
            "Epoch 1:  36%|███▌      | 458/1271 [02:19<04:09,  3.25it/s, training_loss=0.937]\u001b[A\n",
            "Epoch 1:  36%|███▌      | 458/1271 [02:19<04:09,  3.25it/s, training_loss=0.948]\u001b[A\n",
            "Epoch 1:  36%|███▌      | 459/1271 [02:19<04:10,  3.24it/s, training_loss=0.948]\u001b[A\n",
            "Epoch 1:  36%|███▌      | 459/1271 [02:20<04:10,  3.24it/s, training_loss=0.285]\u001b[A\n",
            "Epoch 1:  36%|███▌      | 460/1271 [02:20<04:10,  3.24it/s, training_loss=0.285]\u001b[A\n",
            "Epoch 1:  36%|███▌      | 460/1271 [02:20<04:10,  3.24it/s, training_loss=0.532]\u001b[A\n",
            "Epoch 1:  36%|███▋      | 461/1271 [02:20<04:09,  3.25it/s, training_loss=0.532]\u001b[A\n",
            "Epoch 1:  36%|███▋      | 461/1271 [02:20<04:09,  3.25it/s, training_loss=0.714]\u001b[A\n",
            "Epoch 1:  36%|███▋      | 462/1271 [02:20<04:08,  3.26it/s, training_loss=0.714]\u001b[A\n",
            "Epoch 1:  36%|███▋      | 462/1271 [02:20<04:08,  3.26it/s, training_loss=0.527]\u001b[A\n",
            "Epoch 1:  36%|███▋      | 463/1271 [02:20<04:08,  3.25it/s, training_loss=0.527]\u001b[A\n",
            "Epoch 1:  36%|███▋      | 463/1271 [02:21<04:08,  3.25it/s, training_loss=0.784]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 464/1271 [02:21<04:10,  3.22it/s, training_loss=0.784]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 464/1271 [02:21<04:10,  3.22it/s, training_loss=0.615]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 465/1271 [02:21<04:11,  3.21it/s, training_loss=0.615]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 465/1271 [02:21<04:11,  3.21it/s, training_loss=0.932]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 466/1271 [02:21<04:11,  3.20it/s, training_loss=0.932]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 466/1271 [02:22<04:11,  3.20it/s, training_loss=0.650]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 467/1271 [02:22<04:10,  3.21it/s, training_loss=0.650]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 467/1271 [02:22<04:10,  3.21it/s, training_loss=1.151]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 468/1271 [02:22<04:09,  3.22it/s, training_loss=1.151]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 468/1271 [02:22<04:09,  3.22it/s, training_loss=0.854]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 469/1271 [02:22<04:08,  3.23it/s, training_loss=0.854]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 469/1271 [02:23<04:08,  3.23it/s, training_loss=1.294]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 470/1271 [02:23<04:07,  3.23it/s, training_loss=1.294]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 470/1271 [02:23<04:07,  3.23it/s, training_loss=0.255]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 471/1271 [02:23<04:06,  3.25it/s, training_loss=0.255]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 471/1271 [02:23<04:06,  3.25it/s, training_loss=0.987]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 472/1271 [02:23<04:07,  3.23it/s, training_loss=0.987]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 472/1271 [02:24<04:07,  3.23it/s, training_loss=1.203]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 473/1271 [02:24<04:06,  3.23it/s, training_loss=1.203]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 473/1271 [02:24<04:06,  3.23it/s, training_loss=1.220]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 474/1271 [02:24<04:06,  3.23it/s, training_loss=1.220]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 474/1271 [02:24<04:06,  3.23it/s, training_loss=0.728]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 475/1271 [02:24<04:06,  3.23it/s, training_loss=0.728]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 475/1271 [02:25<04:06,  3.23it/s, training_loss=0.993]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 476/1271 [02:25<04:08,  3.20it/s, training_loss=0.993]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 476/1271 [02:25<04:08,  3.20it/s, training_loss=0.470]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 477/1271 [02:25<04:08,  3.20it/s, training_loss=0.470]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 477/1271 [02:25<04:08,  3.20it/s, training_loss=1.056]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 478/1271 [02:25<04:08,  3.19it/s, training_loss=1.056]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 478/1271 [02:25<04:08,  3.19it/s, training_loss=0.598]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 479/1271 [02:25<04:09,  3.18it/s, training_loss=0.598]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 479/1271 [02:26<04:09,  3.18it/s, training_loss=0.466]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 480/1271 [02:26<04:07,  3.20it/s, training_loss=0.466]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 480/1271 [02:26<04:07,  3.20it/s, training_loss=0.667]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 481/1271 [02:26<04:06,  3.20it/s, training_loss=0.667]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 481/1271 [02:26<04:06,  3.20it/s, training_loss=1.058]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 482/1271 [02:26<04:05,  3.21it/s, training_loss=1.058]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 482/1271 [02:27<04:05,  3.21it/s, training_loss=0.751]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 483/1271 [02:27<04:04,  3.22it/s, training_loss=0.751]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 483/1271 [02:27<04:04,  3.22it/s, training_loss=0.878]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 484/1271 [02:27<04:05,  3.21it/s, training_loss=0.878]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 484/1271 [02:27<04:05,  3.21it/s, training_loss=0.914]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 485/1271 [02:27<04:05,  3.20it/s, training_loss=0.914]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 485/1271 [02:28<04:05,  3.20it/s, training_loss=1.025]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 486/1271 [02:28<04:06,  3.19it/s, training_loss=1.025]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 486/1271 [02:28<04:06,  3.19it/s, training_loss=0.833]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 487/1271 [02:28<04:05,  3.19it/s, training_loss=0.833]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 487/1271 [02:28<04:05,  3.19it/s, training_loss=0.902]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 488/1271 [02:28<04:03,  3.22it/s, training_loss=0.902]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 488/1271 [02:29<04:03,  3.22it/s, training_loss=0.848]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 489/1271 [02:29<04:01,  3.24it/s, training_loss=0.848]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 489/1271 [02:29<04:01,  3.24it/s, training_loss=0.572]\u001b[A\n",
            "Epoch 1:  39%|███▊      | 490/1271 [02:29<04:00,  3.25it/s, training_loss=0.572]\u001b[A\n",
            "Epoch 1:  39%|███▊      | 490/1271 [02:29<04:00,  3.25it/s, training_loss=0.657]\u001b[A\n",
            "Epoch 1:  39%|███▊      | 491/1271 [02:29<03:59,  3.26it/s, training_loss=0.657]\u001b[A\n",
            "Epoch 1:  39%|███▊      | 491/1271 [02:30<03:59,  3.26it/s, training_loss=1.057]\u001b[A\n",
            "Epoch 1:  39%|███▊      | 492/1271 [02:30<03:59,  3.25it/s, training_loss=1.057]\u001b[A\n",
            "Epoch 1:  39%|███▊      | 492/1271 [02:30<03:59,  3.25it/s, training_loss=0.150]\u001b[A\n",
            "Epoch 1:  39%|███▉      | 493/1271 [02:30<03:59,  3.25it/s, training_loss=0.150]\u001b[A\n",
            "Epoch 1:  39%|███▉      | 493/1271 [02:30<03:59,  3.25it/s, training_loss=0.744]\u001b[A\n",
            "Epoch 1:  39%|███▉      | 494/1271 [02:30<03:58,  3.26it/s, training_loss=0.744]\u001b[A\n",
            "Epoch 1:  39%|███▉      | 494/1271 [02:30<03:58,  3.26it/s, training_loss=0.864]\u001b[A\n",
            "Epoch 1:  39%|███▉      | 495/1271 [02:30<03:58,  3.25it/s, training_loss=0.864]\u001b[A\n",
            "Epoch 1:  39%|███▉      | 495/1271 [02:31<03:58,  3.25it/s, training_loss=0.777]\u001b[A\n",
            "Epoch 1:  39%|███▉      | 496/1271 [02:31<03:59,  3.23it/s, training_loss=0.777]\u001b[A\n",
            "Epoch 1:  39%|███▉      | 496/1271 [02:31<03:59,  3.23it/s, training_loss=1.173]\u001b[A\n",
            "Epoch 1:  39%|███▉      | 497/1271 [02:31<04:01,  3.20it/s, training_loss=1.173]\u001b[A\n",
            "Epoch 1:  39%|███▉      | 497/1271 [02:31<04:01,  3.20it/s, training_loss=0.633]\u001b[A\n",
            "Epoch 1:  39%|███▉      | 498/1271 [02:31<04:02,  3.19it/s, training_loss=0.633]\u001b[A\n",
            "Epoch 1:  39%|███▉      | 498/1271 [02:32<04:02,  3.19it/s, training_loss=0.637]\u001b[A\n",
            "Epoch 1:  39%|███▉      | 499/1271 [02:32<04:00,  3.20it/s, training_loss=0.637]\u001b[A\n",
            "Epoch 1:  39%|███▉      | 499/1271 [02:32<04:00,  3.20it/s, training_loss=0.738]\u001b[A\n",
            "Epoch 1:  39%|███▉      | 500/1271 [02:32<03:59,  3.21it/s, training_loss=0.738]\u001b[A\n",
            "Epoch 1:  39%|███▉      | 500/1271 [02:32<03:59,  3.21it/s, training_loss=0.951]\u001b[A\n",
            "Epoch 1:  39%|███▉      | 501/1271 [02:32<03:58,  3.23it/s, training_loss=0.951]\u001b[A\n",
            "Epoch 1:  39%|███▉      | 501/1271 [02:33<03:58,  3.23it/s, training_loss=0.850]\u001b[A\n",
            "Epoch 1:  39%|███▉      | 502/1271 [02:33<03:57,  3.23it/s, training_loss=0.850]\u001b[A\n",
            "Epoch 1:  39%|███▉      | 502/1271 [02:33<03:57,  3.23it/s, training_loss=0.832]\u001b[A\n",
            "Epoch 1:  40%|███▉      | 503/1271 [02:33<03:57,  3.23it/s, training_loss=0.832]\u001b[A\n",
            "Epoch 1:  40%|███▉      | 503/1271 [02:33<03:57,  3.23it/s, training_loss=0.159]\u001b[A\n",
            "Epoch 1:  40%|███▉      | 504/1271 [02:33<03:57,  3.23it/s, training_loss=0.159]\u001b[A\n",
            "Epoch 1:  40%|███▉      | 504/1271 [02:34<03:57,  3.23it/s, training_loss=1.082]\u001b[A\n",
            "Epoch 1:  40%|███▉      | 505/1271 [02:34<03:56,  3.23it/s, training_loss=1.082]\u001b[A\n",
            "Epoch 1:  40%|███▉      | 505/1271 [02:34<03:56,  3.23it/s, training_loss=1.068]\u001b[A\n",
            "Epoch 1:  40%|███▉      | 506/1271 [02:34<03:57,  3.22it/s, training_loss=1.068]\u001b[A\n",
            "Epoch 1:  40%|███▉      | 506/1271 [02:34<03:57,  3.22it/s, training_loss=0.638]\u001b[A\n",
            "Epoch 1:  40%|███▉      | 507/1271 [02:34<03:56,  3.23it/s, training_loss=0.638]\u001b[A\n",
            "Epoch 1:  40%|███▉      | 507/1271 [02:34<03:56,  3.23it/s, training_loss=1.218]\u001b[A\n",
            "Epoch 1:  40%|███▉      | 508/1271 [02:34<03:56,  3.22it/s, training_loss=1.218]\u001b[A\n",
            "Epoch 1:  40%|███▉      | 508/1271 [02:35<03:56,  3.22it/s, training_loss=0.488]\u001b[A\n",
            "Epoch 1:  40%|████      | 509/1271 [02:35<03:56,  3.23it/s, training_loss=0.488]\u001b[A\n",
            "Epoch 1:  40%|████      | 509/1271 [02:35<03:56,  3.23it/s, training_loss=0.955]\u001b[A\n",
            "Epoch 1:  40%|████      | 510/1271 [02:35<03:56,  3.22it/s, training_loss=0.955]\u001b[A\n",
            "Epoch 1:  40%|████      | 510/1271 [02:35<03:56,  3.22it/s, training_loss=0.352]\u001b[A\n",
            "Epoch 1:  40%|████      | 511/1271 [02:35<03:56,  3.21it/s, training_loss=0.352]\u001b[A\n",
            "Epoch 1:  40%|████      | 511/1271 [02:36<03:56,  3.21it/s, training_loss=0.974]\u001b[A\n",
            "Epoch 1:  40%|████      | 512/1271 [02:36<03:55,  3.22it/s, training_loss=0.974]\u001b[A\n",
            "Epoch 1:  40%|████      | 512/1271 [02:36<03:55,  3.22it/s, training_loss=1.023]\u001b[A\n",
            "Epoch 1:  40%|████      | 513/1271 [02:36<03:55,  3.22it/s, training_loss=1.023]\u001b[A\n",
            "Epoch 1:  40%|████      | 513/1271 [02:36<03:55,  3.22it/s, training_loss=0.930]\u001b[A\n",
            "Epoch 1:  40%|████      | 514/1271 [02:36<03:55,  3.21it/s, training_loss=0.930]\u001b[A\n",
            "Epoch 1:  40%|████      | 514/1271 [02:37<03:55,  3.21it/s, training_loss=0.518]\u001b[A\n",
            "Epoch 1:  41%|████      | 515/1271 [02:37<03:56,  3.20it/s, training_loss=0.518]\u001b[A\n",
            "Epoch 1:  41%|████      | 515/1271 [02:37<03:56,  3.20it/s, training_loss=0.466]\u001b[A\n",
            "Epoch 1:  41%|████      | 516/1271 [02:37<03:54,  3.21it/s, training_loss=0.466]\u001b[A\n",
            "Epoch 1:  41%|████      | 516/1271 [02:37<03:54,  3.21it/s, training_loss=0.572]\u001b[A\n",
            "Epoch 1:  41%|████      | 517/1271 [02:37<03:55,  3.21it/s, training_loss=0.572]\u001b[A\n",
            "Epoch 1:  41%|████      | 517/1271 [02:38<03:55,  3.21it/s, training_loss=1.044]\u001b[A\n",
            "Epoch 1:  41%|████      | 518/1271 [02:38<03:55,  3.19it/s, training_loss=1.044]\u001b[A\n",
            "Epoch 1:  41%|████      | 518/1271 [02:38<03:55,  3.19it/s, training_loss=0.126]\u001b[A\n",
            "Epoch 1:  41%|████      | 519/1271 [02:38<03:56,  3.18it/s, training_loss=0.126]\u001b[A\n",
            "Epoch 1:  41%|████      | 519/1271 [02:38<03:56,  3.18it/s, training_loss=0.837]\u001b[A\n",
            "Epoch 1:  41%|████      | 520/1271 [02:38<03:55,  3.19it/s, training_loss=0.837]\u001b[A\n",
            "Epoch 1:  41%|████      | 520/1271 [02:39<03:55,  3.19it/s, training_loss=0.503]\u001b[A\n",
            "Epoch 1:  41%|████      | 521/1271 [02:39<03:54,  3.19it/s, training_loss=0.503]\u001b[A\n",
            "Epoch 1:  41%|████      | 521/1271 [02:39<03:54,  3.19it/s, training_loss=0.771]\u001b[A\n",
            "Epoch 1:  41%|████      | 522/1271 [02:39<03:53,  3.20it/s, training_loss=0.771]\u001b[A\n",
            "Epoch 1:  41%|████      | 522/1271 [02:39<03:53,  3.20it/s, training_loss=0.869]\u001b[A\n",
            "Epoch 1:  41%|████      | 523/1271 [02:39<03:52,  3.21it/s, training_loss=0.869]\u001b[A\n",
            "Epoch 1:  41%|████      | 523/1271 [02:39<03:52,  3.21it/s, training_loss=1.105]\u001b[A\n",
            "Epoch 1:  41%|████      | 524/1271 [02:39<03:52,  3.22it/s, training_loss=1.105]\u001b[A\n",
            "Epoch 1:  41%|████      | 524/1271 [02:40<03:52,  3.22it/s, training_loss=0.869]\u001b[A\n",
            "Epoch 1:  41%|████▏     | 525/1271 [02:40<03:53,  3.20it/s, training_loss=0.869]\u001b[A\n",
            "Epoch 1:  41%|████▏     | 525/1271 [02:40<03:53,  3.20it/s, training_loss=1.136]\u001b[A\n",
            "Epoch 1:  41%|████▏     | 526/1271 [02:40<03:52,  3.20it/s, training_loss=1.136]\u001b[A\n",
            "Epoch 1:  41%|████▏     | 526/1271 [02:40<03:52,  3.20it/s, training_loss=0.929]\u001b[A\n",
            "Epoch 1:  41%|████▏     | 527/1271 [02:40<03:51,  3.21it/s, training_loss=0.929]\u001b[A\n",
            "Epoch 1:  41%|████▏     | 527/1271 [02:41<03:51,  3.21it/s, training_loss=0.981]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 528/1271 [02:41<03:51,  3.21it/s, training_loss=0.981]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 528/1271 [02:41<03:51,  3.21it/s, training_loss=0.383]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 529/1271 [02:41<03:51,  3.20it/s, training_loss=0.383]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 529/1271 [02:41<03:51,  3.20it/s, training_loss=0.497]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 530/1271 [02:41<03:51,  3.21it/s, training_loss=0.497]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 530/1271 [02:42<03:51,  3.21it/s, training_loss=1.192]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 531/1271 [02:42<03:50,  3.21it/s, training_loss=1.192]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 531/1271 [02:42<03:50,  3.21it/s, training_loss=0.459]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 532/1271 [02:42<03:50,  3.21it/s, training_loss=0.459]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 532/1271 [02:42<03:50,  3.21it/s, training_loss=0.499]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 533/1271 [02:42<03:51,  3.18it/s, training_loss=0.499]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 533/1271 [02:43<03:51,  3.18it/s, training_loss=0.969]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 534/1271 [02:43<03:52,  3.18it/s, training_loss=0.969]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 534/1271 [02:43<03:52,  3.18it/s, training_loss=0.767]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 535/1271 [02:43<03:51,  3.18it/s, training_loss=0.767]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 535/1271 [02:43<03:51,  3.18it/s, training_loss=1.289]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 536/1271 [02:43<03:52,  3.16it/s, training_loss=1.289]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 536/1271 [02:44<03:52,  3.16it/s, training_loss=1.232]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 537/1271 [02:44<03:52,  3.16it/s, training_loss=1.232]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 537/1271 [02:44<03:52,  3.16it/s, training_loss=1.105]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 538/1271 [02:44<03:51,  3.17it/s, training_loss=1.105]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 538/1271 [02:44<03:51,  3.17it/s, training_loss=0.885]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 539/1271 [02:44<03:50,  3.18it/s, training_loss=0.885]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 539/1271 [02:44<03:50,  3.18it/s, training_loss=1.145]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 540/1271 [02:44<03:50,  3.18it/s, training_loss=1.145]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 540/1271 [02:45<03:50,  3.18it/s, training_loss=1.061]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 541/1271 [02:45<03:51,  3.16it/s, training_loss=1.061]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 541/1271 [02:45<03:51,  3.16it/s, training_loss=0.455]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 542/1271 [02:45<03:50,  3.17it/s, training_loss=0.455]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 542/1271 [02:45<03:50,  3.17it/s, training_loss=0.728]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 543/1271 [02:45<03:50,  3.16it/s, training_loss=0.728]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 543/1271 [02:46<03:50,  3.16it/s, training_loss=0.972]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 544/1271 [02:46<03:50,  3.16it/s, training_loss=0.972]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 544/1271 [02:46<03:50,  3.16it/s, training_loss=0.927]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 545/1271 [02:46<03:50,  3.14it/s, training_loss=0.927]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 545/1271 [02:46<03:50,  3.14it/s, training_loss=0.833]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 546/1271 [02:46<03:49,  3.16it/s, training_loss=0.833]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 546/1271 [02:47<03:49,  3.16it/s, training_loss=0.453]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 547/1271 [02:47<03:47,  3.18it/s, training_loss=0.453]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 547/1271 [02:47<03:47,  3.18it/s, training_loss=0.800]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 548/1271 [02:47<03:47,  3.18it/s, training_loss=0.800]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 548/1271 [02:47<03:47,  3.18it/s, training_loss=0.473]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 549/1271 [02:47<03:44,  3.21it/s, training_loss=0.473]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 549/1271 [02:48<03:44,  3.21it/s, training_loss=0.231]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 550/1271 [02:48<03:44,  3.21it/s, training_loss=0.231]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 550/1271 [02:48<03:44,  3.21it/s, training_loss=0.904]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 551/1271 [02:48<03:45,  3.20it/s, training_loss=0.904]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 551/1271 [02:48<03:45,  3.20it/s, training_loss=1.093]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 552/1271 [02:48<03:47,  3.16it/s, training_loss=1.093]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 552/1271 [02:49<03:47,  3.16it/s, training_loss=1.243]\u001b[A\n",
            "Epoch 1:  44%|████▎     | 553/1271 [02:49<03:47,  3.16it/s, training_loss=1.243]\u001b[A\n",
            "Epoch 1:  44%|████▎     | 553/1271 [02:49<03:47,  3.16it/s, training_loss=1.097]\u001b[A\n",
            "Epoch 1:  44%|████▎     | 554/1271 [02:49<03:46,  3.16it/s, training_loss=1.097]\u001b[A\n",
            "Epoch 1:  44%|████▎     | 554/1271 [02:49<03:46,  3.16it/s, training_loss=1.263]\u001b[A\n",
            "Epoch 1:  44%|████▎     | 555/1271 [02:49<03:46,  3.16it/s, training_loss=1.263]\u001b[A\n",
            "Epoch 1:  44%|████▎     | 555/1271 [02:50<03:46,  3.16it/s, training_loss=0.435]\u001b[A\n",
            "Epoch 1:  44%|████▎     | 556/1271 [02:50<03:45,  3.17it/s, training_loss=0.435]\u001b[A\n",
            "Epoch 1:  44%|████▎     | 556/1271 [02:50<03:45,  3.17it/s, training_loss=0.606]\u001b[A\n",
            "Epoch 1:  44%|████▍     | 557/1271 [02:50<03:45,  3.16it/s, training_loss=0.606]\u001b[A\n",
            "Epoch 1:  44%|████▍     | 557/1271 [02:50<03:45,  3.16it/s, training_loss=0.573]\u001b[A\n",
            "Epoch 1:  44%|████▍     | 558/1271 [02:50<03:47,  3.14it/s, training_loss=0.573]\u001b[A\n",
            "Epoch 1:  44%|████▍     | 558/1271 [02:50<03:47,  3.14it/s, training_loss=0.926]\u001b[A\n",
            "Epoch 1:  44%|████▍     | 559/1271 [02:50<03:46,  3.14it/s, training_loss=0.926]\u001b[A\n",
            "Epoch 1:  44%|████▍     | 559/1271 [02:51<03:46,  3.14it/s, training_loss=0.398]\u001b[A\n",
            "Epoch 1:  44%|████▍     | 560/1271 [02:51<03:45,  3.15it/s, training_loss=0.398]\u001b[A\n",
            "Epoch 1:  44%|████▍     | 560/1271 [02:51<03:45,  3.15it/s, training_loss=1.035]\u001b[A\n",
            "Epoch 1:  44%|████▍     | 561/1271 [02:51<03:46,  3.14it/s, training_loss=1.035]\u001b[A\n",
            "Epoch 1:  44%|████▍     | 561/1271 [02:51<03:46,  3.14it/s, training_loss=1.050]\u001b[A\n",
            "Epoch 1:  44%|████▍     | 562/1271 [02:51<03:45,  3.14it/s, training_loss=1.050]\u001b[A\n",
            "Epoch 1:  44%|████▍     | 562/1271 [02:52<03:45,  3.14it/s, training_loss=0.722]\u001b[A\n",
            "Epoch 1:  44%|████▍     | 563/1271 [02:52<03:44,  3.15it/s, training_loss=0.722]\u001b[A\n",
            "Epoch 1:  44%|████▍     | 563/1271 [02:52<03:44,  3.15it/s, training_loss=0.850]\u001b[A\n",
            "Epoch 1:  44%|████▍     | 564/1271 [02:52<03:44,  3.15it/s, training_loss=0.850]\u001b[A\n",
            "Epoch 1:  44%|████▍     | 564/1271 [02:52<03:44,  3.15it/s, training_loss=0.386]\u001b[A\n",
            "Epoch 1:  44%|████▍     | 565/1271 [02:52<03:45,  3.12it/s, training_loss=0.386]\u001b[A\n",
            "Epoch 1:  44%|████▍     | 565/1271 [02:53<03:45,  3.12it/s, training_loss=0.966]\u001b[A\n",
            "Epoch 1:  45%|████▍     | 566/1271 [02:53<03:46,  3.12it/s, training_loss=0.966]\u001b[A\n",
            "Epoch 1:  45%|████▍     | 566/1271 [02:53<03:46,  3.12it/s, training_loss=1.105]\u001b[A\n",
            "Epoch 1:  45%|████▍     | 567/1271 [02:53<03:45,  3.13it/s, training_loss=1.105]\u001b[A\n",
            "Epoch 1:  45%|████▍     | 567/1271 [02:53<03:45,  3.13it/s, training_loss=1.070]\u001b[A\n",
            "Epoch 1:  45%|████▍     | 568/1271 [02:53<03:44,  3.14it/s, training_loss=1.070]\u001b[A\n",
            "Epoch 1:  45%|████▍     | 568/1271 [02:54<03:44,  3.14it/s, training_loss=0.350]\u001b[A\n",
            "Epoch 1:  45%|████▍     | 569/1271 [02:54<03:43,  3.14it/s, training_loss=0.350]\u001b[A\n",
            "Epoch 1:  45%|████▍     | 569/1271 [02:54<03:43,  3.14it/s, training_loss=0.827]\u001b[A\n",
            "Epoch 1:  45%|████▍     | 570/1271 [02:54<03:44,  3.13it/s, training_loss=0.827]\u001b[A\n",
            "Epoch 1:  45%|████▍     | 570/1271 [02:54<03:44,  3.13it/s, training_loss=1.374]\u001b[A\n",
            "Epoch 1:  45%|████▍     | 571/1271 [02:54<03:44,  3.11it/s, training_loss=1.374]\u001b[A\n",
            "Epoch 1:  45%|████▍     | 571/1271 [02:55<03:44,  3.11it/s, training_loss=0.567]\u001b[A\n",
            "Epoch 1:  45%|████▌     | 572/1271 [02:55<03:43,  3.12it/s, training_loss=0.567]\u001b[A\n",
            "Epoch 1:  45%|████▌     | 572/1271 [02:55<03:43,  3.12it/s, training_loss=0.719]\u001b[A\n",
            "Epoch 1:  45%|████▌     | 573/1271 [02:55<03:43,  3.12it/s, training_loss=0.719]\u001b[A\n",
            "Epoch 1:  45%|████▌     | 573/1271 [02:55<03:43,  3.12it/s, training_loss=0.735]\u001b[A\n",
            "Epoch 1:  45%|████▌     | 574/1271 [02:55<03:43,  3.12it/s, training_loss=0.735]\u001b[A\n",
            "Epoch 1:  45%|████▌     | 574/1271 [02:56<03:43,  3.12it/s, training_loss=0.425]\u001b[A\n",
            "Epoch 1:  45%|████▌     | 575/1271 [02:56<03:42,  3.13it/s, training_loss=0.425]\u001b[A\n",
            "Epoch 1:  45%|████▌     | 575/1271 [02:56<03:42,  3.13it/s, training_loss=0.562]\u001b[A\n",
            "Epoch 1:  45%|████▌     | 576/1271 [02:56<03:44,  3.10it/s, training_loss=0.562]\u001b[A\n",
            "Epoch 1:  45%|████▌     | 576/1271 [02:56<03:44,  3.10it/s, training_loss=0.445]\u001b[A\n",
            "Epoch 1:  45%|████▌     | 577/1271 [02:56<03:43,  3.10it/s, training_loss=0.445]\u001b[A\n",
            "Epoch 1:  45%|████▌     | 577/1271 [02:57<03:43,  3.10it/s, training_loss=1.323]\u001b[A\n",
            "Epoch 1:  45%|████▌     | 578/1271 [02:57<03:44,  3.09it/s, training_loss=1.323]\u001b[A\n",
            "Epoch 1:  45%|████▌     | 578/1271 [02:57<03:44,  3.09it/s, training_loss=0.854]\u001b[A\n",
            "Epoch 1:  46%|████▌     | 579/1271 [02:57<03:42,  3.10it/s, training_loss=0.854]\u001b[A\n",
            "Epoch 1:  46%|████▌     | 579/1271 [02:57<03:42,  3.10it/s, training_loss=0.899]\u001b[A\n",
            "Epoch 1:  46%|████▌     | 580/1271 [02:57<03:42,  3.10it/s, training_loss=0.899]\u001b[A\n",
            "Epoch 1:  46%|████▌     | 580/1271 [02:58<03:42,  3.10it/s, training_loss=1.120]\u001b[A\n",
            "Epoch 1:  46%|████▌     | 581/1271 [02:58<03:43,  3.08it/s, training_loss=1.120]\u001b[A\n",
            "Epoch 1:  46%|████▌     | 581/1271 [02:58<03:43,  3.08it/s, training_loss=1.043]\u001b[A\n",
            "Epoch 1:  46%|████▌     | 582/1271 [02:58<03:43,  3.09it/s, training_loss=1.043]\u001b[A\n",
            "Epoch 1:  46%|████▌     | 582/1271 [02:58<03:43,  3.09it/s, training_loss=0.577]\u001b[A\n",
            "Epoch 1:  46%|████▌     | 583/1271 [02:58<03:41,  3.10it/s, training_loss=0.577]\u001b[A\n",
            "Epoch 1:  46%|████▌     | 583/1271 [02:59<03:41,  3.10it/s, training_loss=0.496]\u001b[A\n",
            "Epoch 1:  46%|████▌     | 584/1271 [02:59<03:40,  3.11it/s, training_loss=0.496]\u001b[A\n",
            "Epoch 1:  46%|████▌     | 584/1271 [02:59<03:40,  3.11it/s, training_loss=0.171]\u001b[A\n",
            "Epoch 1:  46%|████▌     | 585/1271 [02:59<03:40,  3.11it/s, training_loss=0.171]\u001b[A\n",
            "Epoch 1:  46%|████▌     | 585/1271 [02:59<03:40,  3.11it/s, training_loss=0.642]\u001b[A\n",
            "Epoch 1:  46%|████▌     | 586/1271 [02:59<03:41,  3.10it/s, training_loss=0.642]\u001b[A\n",
            "Epoch 1:  46%|████▌     | 586/1271 [02:59<03:41,  3.10it/s, training_loss=0.424]\u001b[A\n",
            "Epoch 1:  46%|████▌     | 587/1271 [02:59<03:39,  3.11it/s, training_loss=0.424]\u001b[A\n",
            "Epoch 1:  46%|████▌     | 587/1271 [03:00<03:39,  3.11it/s, training_loss=1.194]\u001b[A\n",
            "Epoch 1:  46%|████▋     | 588/1271 [03:00<03:39,  3.11it/s, training_loss=1.194]\u001b[A\n",
            "Epoch 1:  46%|████▋     | 588/1271 [03:00<03:39,  3.11it/s, training_loss=0.086]\u001b[A\n",
            "Epoch 1:  46%|████▋     | 589/1271 [03:00<03:38,  3.12it/s, training_loss=0.086]\u001b[A\n",
            "Epoch 1:  46%|████▋     | 589/1271 [03:00<03:38,  3.12it/s, training_loss=0.036]\u001b[A\n",
            "Epoch 1:  46%|████▋     | 590/1271 [03:00<03:38,  3.11it/s, training_loss=0.036]\u001b[A\n",
            "Epoch 1:  46%|████▋     | 590/1271 [03:01<03:38,  3.11it/s, training_loss=0.567]\u001b[A\n",
            "Epoch 1:  46%|████▋     | 591/1271 [03:01<03:38,  3.11it/s, training_loss=0.567]\u001b[A\n",
            "Epoch 1:  46%|████▋     | 591/1271 [03:01<03:38,  3.11it/s, training_loss=0.916]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 592/1271 [03:01<03:37,  3.13it/s, training_loss=0.916]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 592/1271 [03:01<03:37,  3.13it/s, training_loss=0.423]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 593/1271 [03:01<03:37,  3.12it/s, training_loss=0.423]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 593/1271 [03:02<03:37,  3.12it/s, training_loss=0.740]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 594/1271 [03:02<03:36,  3.12it/s, training_loss=0.740]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 594/1271 [03:02<03:36,  3.12it/s, training_loss=0.454]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 595/1271 [03:02<03:37,  3.11it/s, training_loss=0.454]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 595/1271 [03:02<03:37,  3.11it/s, training_loss=0.294]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 596/1271 [03:02<03:37,  3.11it/s, training_loss=0.294]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 596/1271 [03:03<03:37,  3.11it/s, training_loss=0.627]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 597/1271 [03:03<03:38,  3.09it/s, training_loss=0.627]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 597/1271 [03:03<03:38,  3.09it/s, training_loss=0.397]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 598/1271 [03:03<03:37,  3.09it/s, training_loss=0.397]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 598/1271 [03:03<03:37,  3.09it/s, training_loss=0.687]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 599/1271 [03:03<03:37,  3.09it/s, training_loss=0.687]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 599/1271 [03:04<03:37,  3.09it/s, training_loss=0.636]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 600/1271 [03:04<03:37,  3.09it/s, training_loss=0.636]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 600/1271 [03:04<03:37,  3.09it/s, training_loss=0.908]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 601/1271 [03:04<03:36,  3.10it/s, training_loss=0.908]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 601/1271 [03:04<03:36,  3.10it/s, training_loss=0.962]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 602/1271 [03:04<03:36,  3.08it/s, training_loss=0.962]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 602/1271 [03:05<03:36,  3.08it/s, training_loss=0.620]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 603/1271 [03:05<03:36,  3.09it/s, training_loss=0.620]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 603/1271 [03:05<03:36,  3.09it/s, training_loss=0.904]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 604/1271 [03:05<03:35,  3.09it/s, training_loss=0.904]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 604/1271 [03:05<03:35,  3.09it/s, training_loss=0.840]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 605/1271 [03:05<03:34,  3.10it/s, training_loss=0.840]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 605/1271 [03:06<03:34,  3.10it/s, training_loss=0.759]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 606/1271 [03:06<03:35,  3.08it/s, training_loss=0.759]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 606/1271 [03:06<03:35,  3.08it/s, training_loss=0.580]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 607/1271 [03:06<03:36,  3.07it/s, training_loss=0.580]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 607/1271 [03:06<03:36,  3.07it/s, training_loss=0.677]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 608/1271 [03:06<03:35,  3.08it/s, training_loss=0.677]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 608/1271 [03:07<03:35,  3.08it/s, training_loss=0.443]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 609/1271 [03:07<03:35,  3.08it/s, training_loss=0.443]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 609/1271 [03:07<03:35,  3.08it/s, training_loss=0.797]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 610/1271 [03:07<03:34,  3.08it/s, training_loss=0.797]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 610/1271 [03:07<03:34,  3.08it/s, training_loss=0.590]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 611/1271 [03:07<03:33,  3.09it/s, training_loss=0.590]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 611/1271 [03:08<03:33,  3.09it/s, training_loss=0.613]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 612/1271 [03:08<03:34,  3.08it/s, training_loss=0.613]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 612/1271 [03:08<03:34,  3.08it/s, training_loss=0.574]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 613/1271 [03:08<03:33,  3.08it/s, training_loss=0.574]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 613/1271 [03:08<03:33,  3.08it/s, training_loss=1.066]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 614/1271 [03:08<03:33,  3.08it/s, training_loss=1.066]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 614/1271 [03:09<03:33,  3.08it/s, training_loss=0.461]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 615/1271 [03:09<03:32,  3.09it/s, training_loss=0.461]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 615/1271 [03:09<03:32,  3.09it/s, training_loss=0.529]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 616/1271 [03:09<03:31,  3.09it/s, training_loss=0.529]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 616/1271 [03:09<03:31,  3.09it/s, training_loss=0.354]\u001b[A\n",
            "Epoch 1:  49%|████▊     | 617/1271 [03:09<03:31,  3.09it/s, training_loss=0.354]\u001b[A\n",
            "Epoch 1:  49%|████▊     | 617/1271 [03:09<03:31,  3.09it/s, training_loss=0.581]\u001b[A\n",
            "Epoch 1:  49%|████▊     | 618/1271 [03:09<03:31,  3.09it/s, training_loss=0.581]\u001b[A\n",
            "Epoch 1:  49%|████▊     | 618/1271 [03:10<03:31,  3.09it/s, training_loss=0.854]\u001b[A\n",
            "Epoch 1:  49%|████▊     | 619/1271 [03:10<03:31,  3.09it/s, training_loss=0.854]\u001b[A\n",
            "Epoch 1:  49%|████▊     | 619/1271 [03:10<03:31,  3.09it/s, training_loss=1.131]\u001b[A\n",
            "Epoch 1:  49%|████▉     | 620/1271 [03:10<03:31,  3.07it/s, training_loss=1.131]\u001b[A\n",
            "Epoch 1:  49%|████▉     | 620/1271 [03:10<03:31,  3.07it/s, training_loss=0.594]\u001b[A\n",
            "Epoch 1:  49%|████▉     | 621/1271 [03:10<03:30,  3.08it/s, training_loss=0.594]\u001b[A\n",
            "Epoch 1:  49%|████▉     | 621/1271 [03:11<03:30,  3.08it/s, training_loss=0.714]\u001b[A\n",
            "Epoch 1:  49%|████▉     | 622/1271 [03:11<03:31,  3.07it/s, training_loss=0.714]\u001b[A\n",
            "Epoch 1:  49%|████▉     | 622/1271 [03:11<03:31,  3.07it/s, training_loss=0.393]\u001b[A\n",
            "Epoch 1:  49%|████▉     | 623/1271 [03:11<03:30,  3.08it/s, training_loss=0.393]\u001b[A\n",
            "Epoch 1:  49%|████▉     | 623/1271 [03:11<03:30,  3.08it/s, training_loss=0.085]\u001b[A\n",
            "Epoch 1:  49%|████▉     | 624/1271 [03:11<03:29,  3.09it/s, training_loss=0.085]\u001b[A\n",
            "Epoch 1:  49%|████▉     | 624/1271 [03:12<03:29,  3.09it/s, training_loss=0.684]\u001b[A\n",
            "Epoch 1:  49%|████▉     | 625/1271 [03:12<03:28,  3.09it/s, training_loss=0.684]\u001b[A\n",
            "Epoch 1:  49%|████▉     | 625/1271 [03:12<03:28,  3.09it/s, training_loss=0.408]\u001b[A\n",
            "Epoch 1:  49%|████▉     | 626/1271 [03:12<03:30,  3.07it/s, training_loss=0.408]\u001b[A\n",
            "Epoch 1:  49%|████▉     | 626/1271 [03:12<03:30,  3.07it/s, training_loss=0.592]\u001b[A\n",
            "Epoch 1:  49%|████▉     | 627/1271 [03:12<03:28,  3.09it/s, training_loss=0.592]\u001b[A\n",
            "Epoch 1:  49%|████▉     | 627/1271 [03:13<03:28,  3.09it/s, training_loss=0.027]\u001b[A\n",
            "Epoch 1:  49%|████▉     | 628/1271 [03:13<03:27,  3.10it/s, training_loss=0.027]\u001b[A\n",
            "Epoch 1:  49%|████▉     | 628/1271 [03:13<03:27,  3.10it/s, training_loss=0.808]\u001b[A\n",
            "Epoch 1:  49%|████▉     | 629/1271 [03:13<03:28,  3.08it/s, training_loss=0.808]\u001b[A\n",
            "Epoch 1:  49%|████▉     | 629/1271 [03:13<03:28,  3.08it/s, training_loss=0.705]\u001b[A\n",
            "Epoch 1:  50%|████▉     | 630/1271 [03:13<03:27,  3.09it/s, training_loss=0.705]\u001b[A\n",
            "Epoch 1:  50%|████▉     | 630/1271 [03:14<03:27,  3.09it/s, training_loss=0.937]\u001b[A\n",
            "Epoch 1:  50%|████▉     | 631/1271 [03:14<03:26,  3.09it/s, training_loss=0.937]\u001b[A\n",
            "Epoch 1:  50%|████▉     | 631/1271 [03:14<03:26,  3.09it/s, training_loss=0.649]\u001b[A\n",
            "Epoch 1:  50%|████▉     | 632/1271 [03:14<03:25,  3.10it/s, training_loss=0.649]\u001b[A\n",
            "Epoch 1:  50%|████▉     | 632/1271 [03:14<03:25,  3.10it/s, training_loss=0.867]\u001b[A\n",
            "Epoch 1:  50%|████▉     | 633/1271 [03:14<03:26,  3.09it/s, training_loss=0.867]\u001b[A\n",
            "Epoch 1:  50%|████▉     | 633/1271 [03:15<03:26,  3.09it/s, training_loss=0.816]\u001b[A\n",
            "Epoch 1:  50%|████▉     | 634/1271 [03:15<03:25,  3.10it/s, training_loss=0.816]\u001b[A\n",
            "Epoch 1:  50%|████▉     | 634/1271 [03:15<03:25,  3.10it/s, training_loss=0.757]\u001b[A\n",
            "Epoch 1:  50%|████▉     | 635/1271 [03:15<03:24,  3.11it/s, training_loss=0.757]\u001b[A\n",
            "Epoch 1:  50%|████▉     | 635/1271 [03:15<03:24,  3.11it/s, training_loss=1.080]\u001b[A\n",
            "Epoch 1:  50%|█████     | 636/1271 [03:15<03:24,  3.11it/s, training_loss=1.080]\u001b[A\n",
            "Epoch 1:  50%|█████     | 636/1271 [03:16<03:24,  3.11it/s, training_loss=0.761]\u001b[A\n",
            "Epoch 1:  50%|█████     | 637/1271 [03:16<03:23,  3.12it/s, training_loss=0.761]\u001b[A\n",
            "Epoch 1:  50%|█████     | 637/1271 [03:16<03:23,  3.12it/s, training_loss=0.578]\u001b[A\n",
            "Epoch 1:  50%|█████     | 638/1271 [03:16<03:23,  3.12it/s, training_loss=0.578]\u001b[A\n",
            "Epoch 1:  50%|█████     | 638/1271 [03:16<03:23,  3.12it/s, training_loss=0.690]\u001b[A\n",
            "Epoch 1:  50%|█████     | 639/1271 [03:16<03:22,  3.12it/s, training_loss=0.690]\u001b[A\n",
            "Epoch 1:  50%|█████     | 639/1271 [03:17<03:22,  3.12it/s, training_loss=0.623]\u001b[A\n",
            "Epoch 1:  50%|█████     | 640/1271 [03:17<03:22,  3.12it/s, training_loss=0.623]\u001b[A\n",
            "Epoch 1:  50%|█████     | 640/1271 [03:17<03:22,  3.12it/s, training_loss=0.399]\u001b[A\n",
            "Epoch 1:  50%|█████     | 641/1271 [03:17<03:21,  3.12it/s, training_loss=0.399]\u001b[A\n",
            "Epoch 1:  50%|█████     | 641/1271 [03:17<03:21,  3.12it/s, training_loss=0.465]\u001b[A\n",
            "Epoch 1:  51%|█████     | 642/1271 [03:17<03:21,  3.13it/s, training_loss=0.465]\u001b[A\n",
            "Epoch 1:  51%|█████     | 642/1271 [03:18<03:21,  3.13it/s, training_loss=0.879]\u001b[A\n",
            "Epoch 1:  51%|█████     | 643/1271 [03:18<03:21,  3.12it/s, training_loss=0.879]\u001b[A\n",
            "Epoch 1:  51%|█████     | 643/1271 [03:18<03:21,  3.12it/s, training_loss=0.981]\u001b[A\n",
            "Epoch 1:  51%|█████     | 644/1271 [03:18<03:21,  3.11it/s, training_loss=0.981]\u001b[A\n",
            "Epoch 1:  51%|█████     | 644/1271 [03:18<03:21,  3.11it/s, training_loss=0.642]\u001b[A\n",
            "Epoch 1:  51%|█████     | 645/1271 [03:18<03:22,  3.10it/s, training_loss=0.642]\u001b[A\n",
            "Epoch 1:  51%|█████     | 645/1271 [03:19<03:22,  3.10it/s, training_loss=0.453]\u001b[A\n",
            "Epoch 1:  51%|█████     | 646/1271 [03:19<03:22,  3.09it/s, training_loss=0.453]\u001b[A\n",
            "Epoch 1:  51%|█████     | 646/1271 [03:19<03:22,  3.09it/s, training_loss=0.305]\u001b[A\n",
            "Epoch 1:  51%|█████     | 647/1271 [03:19<03:21,  3.10it/s, training_loss=0.305]\u001b[A\n",
            "Epoch 1:  51%|█████     | 647/1271 [03:19<03:21,  3.10it/s, training_loss=0.964]\u001b[A\n",
            "Epoch 1:  51%|█████     | 648/1271 [03:19<03:22,  3.08it/s, training_loss=0.964]\u001b[A\n",
            "Epoch 1:  51%|█████     | 648/1271 [03:20<03:22,  3.08it/s, training_loss=0.380]\u001b[A\n",
            "Epoch 1:  51%|█████     | 649/1271 [03:20<03:22,  3.07it/s, training_loss=0.380]\u001b[A\n",
            "Epoch 1:  51%|█████     | 649/1271 [03:20<03:22,  3.07it/s, training_loss=0.579]\u001b[A\n",
            "Epoch 1:  51%|█████     | 650/1271 [03:20<03:22,  3.07it/s, training_loss=0.579]\u001b[A\n",
            "Epoch 1:  51%|█████     | 650/1271 [03:20<03:22,  3.07it/s, training_loss=0.498]\u001b[A\n",
            "Epoch 1:  51%|█████     | 651/1271 [03:20<03:20,  3.09it/s, training_loss=0.498]\u001b[A\n",
            "Epoch 1:  51%|█████     | 651/1271 [03:20<03:20,  3.09it/s, training_loss=0.314]\u001b[A\n",
            "Epoch 1:  51%|█████▏    | 652/1271 [03:20<03:20,  3.08it/s, training_loss=0.314]\u001b[A\n",
            "Epoch 1:  51%|█████▏    | 652/1271 [03:21<03:20,  3.08it/s, training_loss=0.036]\u001b[A\n",
            "Epoch 1:  51%|█████▏    | 653/1271 [03:21<03:19,  3.10it/s, training_loss=0.036]\u001b[A\n",
            "Epoch 1:  51%|█████▏    | 653/1271 [03:21<03:19,  3.10it/s, training_loss=0.840]\u001b[A\n",
            "Epoch 1:  51%|█████▏    | 654/1271 [03:21<03:19,  3.09it/s, training_loss=0.840]\u001b[A\n",
            "Epoch 1:  51%|█████▏    | 654/1271 [03:21<03:19,  3.09it/s, training_loss=0.776]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 655/1271 [03:21<03:19,  3.09it/s, training_loss=0.776]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 655/1271 [03:22<03:19,  3.09it/s, training_loss=0.650]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 656/1271 [03:22<03:18,  3.11it/s, training_loss=0.650]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 656/1271 [03:22<03:18,  3.11it/s, training_loss=0.279]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 657/1271 [03:22<03:16,  3.12it/s, training_loss=0.279]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 657/1271 [03:22<03:16,  3.12it/s, training_loss=0.856]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 658/1271 [03:22<03:16,  3.12it/s, training_loss=0.856]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 658/1271 [03:23<03:16,  3.12it/s, training_loss=0.377]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 659/1271 [03:23<03:16,  3.11it/s, training_loss=0.377]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 659/1271 [03:23<03:16,  3.11it/s, training_loss=1.050]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 660/1271 [03:23<03:16,  3.11it/s, training_loss=1.050]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 660/1271 [03:23<03:16,  3.11it/s, training_loss=0.824]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 661/1271 [03:23<03:15,  3.11it/s, training_loss=0.824]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 661/1271 [03:24<03:15,  3.11it/s, training_loss=0.307]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 662/1271 [03:24<03:15,  3.12it/s, training_loss=0.307]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 662/1271 [03:24<03:15,  3.12it/s, training_loss=1.015]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 663/1271 [03:24<03:15,  3.11it/s, training_loss=1.015]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 663/1271 [03:24<03:15,  3.11it/s, training_loss=0.758]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 664/1271 [03:24<03:15,  3.11it/s, training_loss=0.758]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 664/1271 [03:25<03:15,  3.11it/s, training_loss=0.956]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 665/1271 [03:25<03:14,  3.11it/s, training_loss=0.956]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 665/1271 [03:25<03:14,  3.11it/s, training_loss=0.974]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 666/1271 [03:25<03:15,  3.10it/s, training_loss=0.974]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 666/1271 [03:25<03:15,  3.10it/s, training_loss=0.530]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 667/1271 [03:25<03:15,  3.10it/s, training_loss=0.530]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 667/1271 [03:26<03:15,  3.10it/s, training_loss=1.093]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 668/1271 [03:26<03:14,  3.11it/s, training_loss=1.093]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 668/1271 [03:26<03:14,  3.11it/s, training_loss=0.391]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 669/1271 [03:26<03:13,  3.11it/s, training_loss=0.391]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 669/1271 [03:26<03:13,  3.11it/s, training_loss=0.727]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 670/1271 [03:26<03:13,  3.11it/s, training_loss=0.727]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 670/1271 [03:27<03:13,  3.11it/s, training_loss=0.516]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 671/1271 [03:27<03:12,  3.11it/s, training_loss=0.516]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 671/1271 [03:27<03:12,  3.11it/s, training_loss=1.143]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 672/1271 [03:27<03:11,  3.13it/s, training_loss=1.143]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 672/1271 [03:27<03:11,  3.13it/s, training_loss=0.327]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 673/1271 [03:27<03:11,  3.13it/s, training_loss=0.327]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 673/1271 [03:28<03:11,  3.13it/s, training_loss=0.920]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 674/1271 [03:28<03:11,  3.12it/s, training_loss=0.920]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 674/1271 [03:28<03:11,  3.12it/s, training_loss=0.675]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 675/1271 [03:28<03:10,  3.12it/s, training_loss=0.675]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 675/1271 [03:28<03:10,  3.12it/s, training_loss=0.775]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 676/1271 [03:28<03:10,  3.13it/s, training_loss=0.775]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 676/1271 [03:29<03:10,  3.13it/s, training_loss=0.130]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 677/1271 [03:29<03:10,  3.12it/s, training_loss=0.130]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 677/1271 [03:29<03:10,  3.12it/s, training_loss=0.632]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 678/1271 [03:29<03:10,  3.12it/s, training_loss=0.632]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 678/1271 [03:29<03:10,  3.12it/s, training_loss=0.690]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 679/1271 [03:29<03:09,  3.12it/s, training_loss=0.690]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 679/1271 [03:29<03:09,  3.12it/s, training_loss=1.135]\u001b[A\n",
            "Epoch 1:  54%|█████▎    | 680/1271 [03:29<03:09,  3.12it/s, training_loss=1.135]\u001b[A\n",
            "Epoch 1:  54%|█████▎    | 680/1271 [03:30<03:09,  3.12it/s, training_loss=0.905]\u001b[A\n",
            "Epoch 1:  54%|█████▎    | 681/1271 [03:30<03:09,  3.11it/s, training_loss=0.905]\u001b[A\n",
            "Epoch 1:  54%|█████▎    | 681/1271 [03:30<03:09,  3.11it/s, training_loss=1.157]\u001b[A\n",
            "Epoch 1:  54%|█████▎    | 682/1271 [03:30<03:10,  3.09it/s, training_loss=1.157]\u001b[A\n",
            "Epoch 1:  54%|█████▎    | 682/1271 [03:30<03:10,  3.09it/s, training_loss=0.347]\u001b[A\n",
            "Epoch 1:  54%|█████▎    | 683/1271 [03:30<03:10,  3.09it/s, training_loss=0.347]\u001b[A\n",
            "Epoch 1:  54%|█████▎    | 683/1271 [03:31<03:10,  3.09it/s, training_loss=0.466]\u001b[A\n",
            "Epoch 1:  54%|█████▍    | 684/1271 [03:31<03:09,  3.10it/s, training_loss=0.466]\u001b[A\n",
            "Epoch 1:  54%|█████▍    | 684/1271 [03:31<03:09,  3.10it/s, training_loss=0.869]\u001b[A\n",
            "Epoch 1:  54%|█████▍    | 685/1271 [03:31<03:08,  3.11it/s, training_loss=0.869]\u001b[A\n",
            "Epoch 1:  54%|█████▍    | 685/1271 [03:31<03:08,  3.11it/s, training_loss=1.059]\u001b[A\n",
            "Epoch 1:  54%|█████▍    | 686/1271 [03:31<03:07,  3.11it/s, training_loss=1.059]\u001b[A\n",
            "Epoch 1:  54%|█████▍    | 686/1271 [03:32<03:07,  3.11it/s, training_loss=1.050]\u001b[A\n",
            "Epoch 1:  54%|█████▍    | 687/1271 [03:32<03:08,  3.09it/s, training_loss=1.050]\u001b[A\n",
            "Epoch 1:  54%|█████▍    | 687/1271 [03:32<03:08,  3.09it/s, training_loss=0.281]\u001b[A\n",
            "Epoch 1:  54%|█████▍    | 688/1271 [03:32<03:08,  3.09it/s, training_loss=0.281]\u001b[A\n",
            "Epoch 1:  54%|█████▍    | 688/1271 [03:32<03:08,  3.09it/s, training_loss=0.619]\u001b[A\n",
            "Epoch 1:  54%|█████▍    | 689/1271 [03:32<03:09,  3.07it/s, training_loss=0.619]\u001b[A\n",
            "Epoch 1:  54%|█████▍    | 689/1271 [03:33<03:09,  3.07it/s, training_loss=0.551]\u001b[A\n",
            "Epoch 1:  54%|█████▍    | 690/1271 [03:33<03:08,  3.08it/s, training_loss=0.551]\u001b[A\n",
            "Epoch 1:  54%|█████▍    | 690/1271 [03:33<03:08,  3.08it/s, training_loss=0.406]\u001b[A\n",
            "Epoch 1:  54%|█████▍    | 691/1271 [03:33<03:07,  3.09it/s, training_loss=0.406]\u001b[A\n",
            "Epoch 1:  54%|█████▍    | 691/1271 [03:33<03:07,  3.09it/s, training_loss=0.577]\u001b[A\n",
            "Epoch 1:  54%|█████▍    | 692/1271 [03:33<03:07,  3.09it/s, training_loss=0.577]\u001b[A\n",
            "Epoch 1:  54%|█████▍    | 692/1271 [03:34<03:07,  3.09it/s, training_loss=0.766]\u001b[A\n",
            "Epoch 1:  55%|█████▍    | 693/1271 [03:34<03:06,  3.10it/s, training_loss=0.766]\u001b[A\n",
            "Epoch 1:  55%|█████▍    | 693/1271 [03:34<03:06,  3.10it/s, training_loss=1.122]\u001b[A\n",
            "Epoch 1:  55%|█████▍    | 694/1271 [03:34<03:06,  3.10it/s, training_loss=1.122]\u001b[A\n",
            "Epoch 1:  55%|█████▍    | 694/1271 [03:34<03:06,  3.10it/s, training_loss=0.321]\u001b[A\n",
            "Epoch 1:  55%|█████▍    | 695/1271 [03:34<03:04,  3.11it/s, training_loss=0.321]\u001b[A\n",
            "Epoch 1:  55%|█████▍    | 695/1271 [03:35<03:04,  3.11it/s, training_loss=1.108]\u001b[A\n",
            "Epoch 1:  55%|█████▍    | 696/1271 [03:35<03:04,  3.11it/s, training_loss=1.108]\u001b[A\n",
            "Epoch 1:  55%|█████▍    | 696/1271 [03:35<03:04,  3.11it/s, training_loss=0.865]\u001b[A\n",
            "Epoch 1:  55%|█████▍    | 697/1271 [03:35<03:05,  3.10it/s, training_loss=0.865]\u001b[A\n",
            "Epoch 1:  55%|█████▍    | 697/1271 [03:35<03:05,  3.10it/s, training_loss=0.941]\u001b[A\n",
            "Epoch 1:  55%|█████▍    | 698/1271 [03:35<03:04,  3.11it/s, training_loss=0.941]\u001b[A\n",
            "Epoch 1:  55%|█████▍    | 698/1271 [03:36<03:04,  3.11it/s, training_loss=1.027]\u001b[A\n",
            "Epoch 1:  55%|█████▍    | 699/1271 [03:36<03:04,  3.10it/s, training_loss=1.027]\u001b[A\n",
            "Epoch 1:  55%|█████▍    | 699/1271 [03:36<03:04,  3.10it/s, training_loss=0.367]\u001b[A\n",
            "Epoch 1:  55%|█████▌    | 700/1271 [03:36<03:03,  3.12it/s, training_loss=0.367]\u001b[A\n",
            "Epoch 1:  55%|█████▌    | 700/1271 [03:36<03:03,  3.12it/s, training_loss=0.645]\u001b[A\n",
            "Epoch 1:  55%|█████▌    | 701/1271 [03:36<03:02,  3.13it/s, training_loss=0.645]\u001b[A\n",
            "Epoch 1:  55%|█████▌    | 701/1271 [03:37<03:02,  3.13it/s, training_loss=0.800]\u001b[A\n",
            "Epoch 1:  55%|█████▌    | 702/1271 [03:37<03:02,  3.12it/s, training_loss=0.800]\u001b[A\n",
            "Epoch 1:  55%|█████▌    | 702/1271 [03:37<03:02,  3.12it/s, training_loss=0.809]\u001b[A\n",
            "Epoch 1:  55%|█████▌    | 703/1271 [03:37<03:02,  3.11it/s, training_loss=0.809]\u001b[A\n",
            "Epoch 1:  55%|█████▌    | 703/1271 [03:37<03:02,  3.11it/s, training_loss=0.717]\u001b[A\n",
            "Epoch 1:  55%|█████▌    | 704/1271 [03:37<03:02,  3.11it/s, training_loss=0.717]\u001b[A\n",
            "Epoch 1:  55%|█████▌    | 704/1271 [03:38<03:02,  3.11it/s, training_loss=0.390]\u001b[A\n",
            "Epoch 1:  55%|█████▌    | 705/1271 [03:38<03:01,  3.12it/s, training_loss=0.390]\u001b[A\n",
            "Epoch 1:  55%|█████▌    | 705/1271 [03:38<03:01,  3.12it/s, training_loss=1.107]\u001b[A\n",
            "Epoch 1:  56%|█████▌    | 706/1271 [03:38<03:00,  3.13it/s, training_loss=1.107]\u001b[A\n",
            "Epoch 1:  56%|█████▌    | 706/1271 [03:38<03:00,  3.13it/s, training_loss=0.665]\u001b[A\n",
            "Epoch 1:  56%|█████▌    | 707/1271 [03:38<03:00,  3.13it/s, training_loss=0.665]\u001b[A\n",
            "Epoch 1:  56%|█████▌    | 707/1271 [03:38<03:00,  3.13it/s, training_loss=0.849]\u001b[A\n",
            "Epoch 1:  56%|█████▌    | 708/1271 [03:38<03:00,  3.13it/s, training_loss=0.849]\u001b[A\n",
            "Epoch 1:  56%|█████▌    | 708/1271 [03:39<03:00,  3.13it/s, training_loss=0.605]\u001b[A\n",
            "Epoch 1:  56%|█████▌    | 709/1271 [03:39<03:00,  3.12it/s, training_loss=0.605]\u001b[A\n",
            "Epoch 1:  56%|█████▌    | 709/1271 [03:39<03:00,  3.12it/s, training_loss=0.986]\u001b[A\n",
            "Epoch 1:  56%|█████▌    | 710/1271 [03:39<02:59,  3.13it/s, training_loss=0.986]\u001b[A\n",
            "Epoch 1:  56%|█████▌    | 710/1271 [03:39<02:59,  3.13it/s, training_loss=0.409]\u001b[A\n",
            "Epoch 1:  56%|█████▌    | 711/1271 [03:39<02:58,  3.14it/s, training_loss=0.409]\u001b[A\n",
            "Epoch 1:  56%|█████▌    | 711/1271 [03:40<02:58,  3.14it/s, training_loss=0.658]\u001b[A\n",
            "Epoch 1:  56%|█████▌    | 712/1271 [03:40<02:58,  3.13it/s, training_loss=0.658]\u001b[A\n",
            "Epoch 1:  56%|█████▌    | 712/1271 [03:40<02:58,  3.13it/s, training_loss=0.978]\u001b[A\n",
            "Epoch 1:  56%|█████▌    | 713/1271 [03:40<03:00,  3.10it/s, training_loss=0.978]\u001b[A\n",
            "Epoch 1:  56%|█████▌    | 713/1271 [03:40<03:00,  3.10it/s, training_loss=0.747]\u001b[A\n",
            "Epoch 1:  56%|█████▌    | 714/1271 [03:40<02:59,  3.10it/s, training_loss=0.747]\u001b[A\n",
            "Epoch 1:  56%|█████▌    | 714/1271 [03:41<02:59,  3.10it/s, training_loss=0.686]\u001b[A\n",
            "Epoch 1:  56%|█████▋    | 715/1271 [03:41<02:58,  3.11it/s, training_loss=0.686]\u001b[A\n",
            "Epoch 1:  56%|█████▋    | 715/1271 [03:41<02:58,  3.11it/s, training_loss=0.815]\u001b[A\n",
            "Epoch 1:  56%|█████▋    | 716/1271 [03:41<02:58,  3.11it/s, training_loss=0.815]\u001b[A\n",
            "Epoch 1:  56%|█████▋    | 716/1271 [03:41<02:58,  3.11it/s, training_loss=0.531]\u001b[A\n",
            "Epoch 1:  56%|█████▋    | 717/1271 [03:41<02:58,  3.10it/s, training_loss=0.531]\u001b[A\n",
            "Epoch 1:  56%|█████▋    | 717/1271 [03:42<02:58,  3.10it/s, training_loss=0.827]\u001b[A\n",
            "Epoch 1:  56%|█████▋    | 718/1271 [03:42<02:57,  3.11it/s, training_loss=0.827]\u001b[A\n",
            "Epoch 1:  56%|█████▋    | 718/1271 [03:42<02:57,  3.11it/s, training_loss=0.335]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 719/1271 [03:42<02:57,  3.12it/s, training_loss=0.335]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 719/1271 [03:42<02:57,  3.12it/s, training_loss=0.815]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 720/1271 [03:42<02:56,  3.11it/s, training_loss=0.815]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 720/1271 [03:43<02:56,  3.11it/s, training_loss=0.504]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 721/1271 [03:43<02:57,  3.11it/s, training_loss=0.504]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 721/1271 [03:43<02:57,  3.11it/s, training_loss=0.822]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 722/1271 [03:43<02:57,  3.10it/s, training_loss=0.822]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 722/1271 [03:43<02:57,  3.10it/s, training_loss=1.143]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 723/1271 [03:43<02:56,  3.10it/s, training_loss=1.143]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 723/1271 [03:44<02:56,  3.10it/s, training_loss=0.996]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 724/1271 [03:44<02:56,  3.10it/s, training_loss=0.996]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 724/1271 [03:44<02:56,  3.10it/s, training_loss=0.167]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 725/1271 [03:44<02:56,  3.09it/s, training_loss=0.167]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 725/1271 [03:44<02:56,  3.09it/s, training_loss=0.537]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 726/1271 [03:44<02:57,  3.08it/s, training_loss=0.537]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 726/1271 [03:45<02:57,  3.08it/s, training_loss=0.281]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 727/1271 [03:45<02:56,  3.09it/s, training_loss=0.281]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 727/1271 [03:45<02:56,  3.09it/s, training_loss=0.863]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 728/1271 [03:45<02:55,  3.10it/s, training_loss=0.863]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 728/1271 [03:45<02:55,  3.10it/s, training_loss=0.335]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 729/1271 [03:45<02:54,  3.10it/s, training_loss=0.335]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 729/1271 [03:46<02:54,  3.10it/s, training_loss=0.776]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 730/1271 [03:46<02:54,  3.10it/s, training_loss=0.776]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 730/1271 [03:46<02:54,  3.10it/s, training_loss=0.581]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 731/1271 [03:46<02:53,  3.11it/s, training_loss=0.581]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 731/1271 [03:46<02:53,  3.11it/s, training_loss=0.450]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 732/1271 [03:46<02:52,  3.12it/s, training_loss=0.450]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 732/1271 [03:47<02:52,  3.12it/s, training_loss=0.985]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 733/1271 [03:47<02:52,  3.12it/s, training_loss=0.985]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 733/1271 [03:47<02:52,  3.12it/s, training_loss=0.812]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 734/1271 [03:47<02:51,  3.13it/s, training_loss=0.812]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 734/1271 [03:47<02:51,  3.13it/s, training_loss=0.063]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 735/1271 [03:47<02:50,  3.14it/s, training_loss=0.063]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 735/1271 [03:47<02:50,  3.14it/s, training_loss=0.972]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 736/1271 [03:47<02:51,  3.12it/s, training_loss=0.972]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 736/1271 [03:48<02:51,  3.12it/s, training_loss=0.762]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 737/1271 [03:48<02:52,  3.09it/s, training_loss=0.762]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 737/1271 [03:48<02:52,  3.09it/s, training_loss=0.533]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 738/1271 [03:48<02:51,  3.11it/s, training_loss=0.533]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 738/1271 [03:48<02:51,  3.11it/s, training_loss=0.567]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 739/1271 [03:48<02:51,  3.10it/s, training_loss=0.567]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 739/1271 [03:49<02:51,  3.10it/s, training_loss=0.746]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 740/1271 [03:49<02:50,  3.11it/s, training_loss=0.746]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 740/1271 [03:49<02:50,  3.11it/s, training_loss=0.737]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 741/1271 [03:49<02:49,  3.13it/s, training_loss=0.737]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 741/1271 [03:49<02:49,  3.13it/s, training_loss=0.681]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 742/1271 [03:49<02:49,  3.12it/s, training_loss=0.681]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 742/1271 [03:50<02:49,  3.12it/s, training_loss=0.318]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 743/1271 [03:50<02:48,  3.13it/s, training_loss=0.318]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 743/1271 [03:50<02:48,  3.13it/s, training_loss=0.484]\u001b[A\n",
            "Epoch 1:  59%|█████▊    | 744/1271 [03:50<02:48,  3.13it/s, training_loss=0.484]\u001b[A\n",
            "Epoch 1:  59%|█████▊    | 744/1271 [03:50<02:48,  3.13it/s, training_loss=0.327]\u001b[A\n",
            "Epoch 1:  59%|█████▊    | 745/1271 [03:50<02:46,  3.15it/s, training_loss=0.327]\u001b[A\n",
            "Epoch 1:  59%|█████▊    | 745/1271 [03:51<02:46,  3.15it/s, training_loss=0.466]\u001b[A\n",
            "Epoch 1:  59%|█████▊    | 746/1271 [03:51<02:47,  3.13it/s, training_loss=0.466]\u001b[A\n",
            "Epoch 1:  59%|█████▊    | 746/1271 [03:51<02:47,  3.13it/s, training_loss=0.514]\u001b[A\n",
            "Epoch 1:  59%|█████▉    | 747/1271 [03:51<02:47,  3.13it/s, training_loss=0.514]\u001b[A\n",
            "Epoch 1:  59%|█████▉    | 747/1271 [03:51<02:47,  3.13it/s, training_loss=0.676]\u001b[A\n",
            "Epoch 1:  59%|█████▉    | 748/1271 [03:51<02:48,  3.11it/s, training_loss=0.676]\u001b[A\n",
            "Epoch 1:  59%|█████▉    | 748/1271 [03:52<02:48,  3.11it/s, training_loss=0.767]\u001b[A\n",
            "Epoch 1:  59%|█████▉    | 749/1271 [03:52<02:49,  3.08it/s, training_loss=0.767]\u001b[A\n",
            "Epoch 1:  59%|█████▉    | 749/1271 [03:52<02:49,  3.08it/s, training_loss=0.948]\u001b[A\n",
            "Epoch 1:  59%|█████▉    | 750/1271 [03:52<02:48,  3.10it/s, training_loss=0.948]\u001b[A\n",
            "Epoch 1:  59%|█████▉    | 750/1271 [03:52<02:48,  3.10it/s, training_loss=0.311]\u001b[A\n",
            "Epoch 1:  59%|█████▉    | 751/1271 [03:52<02:48,  3.09it/s, training_loss=0.311]\u001b[A\n",
            "Epoch 1:  59%|█████▉    | 751/1271 [03:53<02:48,  3.09it/s, training_loss=0.265]\u001b[A\n",
            "Epoch 1:  59%|█████▉    | 752/1271 [03:53<02:47,  3.10it/s, training_loss=0.265]\u001b[A\n",
            "Epoch 1:  59%|█████▉    | 752/1271 [03:53<02:47,  3.10it/s, training_loss=0.020]\u001b[A\n",
            "Epoch 1:  59%|█████▉    | 753/1271 [03:53<02:46,  3.11it/s, training_loss=0.020]\u001b[A\n",
            "Epoch 1:  59%|█████▉    | 753/1271 [03:53<02:46,  3.11it/s, training_loss=0.559]\u001b[A\n",
            "Epoch 1:  59%|█████▉    | 754/1271 [03:53<02:45,  3.13it/s, training_loss=0.559]\u001b[A\n",
            "Epoch 1:  59%|█████▉    | 754/1271 [03:54<02:45,  3.13it/s, training_loss=0.643]\u001b[A\n",
            "Epoch 1:  59%|█████▉    | 755/1271 [03:54<02:46,  3.11it/s, training_loss=0.643]\u001b[A\n",
            "Epoch 1:  59%|█████▉    | 755/1271 [03:54<02:46,  3.11it/s, training_loss=0.736]\u001b[A\n",
            "Epoch 1:  59%|█████▉    | 756/1271 [03:54<02:44,  3.12it/s, training_loss=0.736]\u001b[A\n",
            "Epoch 1:  59%|█████▉    | 756/1271 [03:54<02:44,  3.12it/s, training_loss=0.558]\u001b[A\n",
            "Epoch 1:  60%|█████▉    | 757/1271 [03:54<02:44,  3.13it/s, training_loss=0.558]\u001b[A\n",
            "Epoch 1:  60%|█████▉    | 757/1271 [03:55<02:44,  3.13it/s, training_loss=0.914]\u001b[A\n",
            "Epoch 1:  60%|█████▉    | 758/1271 [03:55<02:43,  3.13it/s, training_loss=0.914]\u001b[A\n",
            "Epoch 1:  60%|█████▉    | 758/1271 [03:55<02:43,  3.13it/s, training_loss=0.560]\u001b[A\n",
            "Epoch 1:  60%|█████▉    | 759/1271 [03:55<02:44,  3.11it/s, training_loss=0.560]\u001b[A\n",
            "Epoch 1:  60%|█████▉    | 759/1271 [03:55<02:44,  3.11it/s, training_loss=0.718]\u001b[A\n",
            "Epoch 1:  60%|█████▉    | 760/1271 [03:55<02:43,  3.13it/s, training_loss=0.718]\u001b[A\n",
            "Epoch 1:  60%|█████▉    | 760/1271 [03:56<02:43,  3.13it/s, training_loss=0.753]\u001b[A\n",
            "Epoch 1:  60%|█████▉    | 761/1271 [03:56<02:43,  3.13it/s, training_loss=0.753]\u001b[A\n",
            "Epoch 1:  60%|█████▉    | 761/1271 [03:56<02:43,  3.13it/s, training_loss=0.665]\u001b[A\n",
            "Epoch 1:  60%|█████▉    | 762/1271 [03:56<02:42,  3.13it/s, training_loss=0.665]\u001b[A\n",
            "Epoch 1:  60%|█████▉    | 762/1271 [03:56<02:42,  3.13it/s, training_loss=0.746]\u001b[A\n",
            "Epoch 1:  60%|██████    | 763/1271 [03:56<02:42,  3.13it/s, training_loss=0.746]\u001b[A\n",
            "Epoch 1:  60%|██████    | 763/1271 [03:56<02:42,  3.13it/s, training_loss=0.560]\u001b[A\n",
            "Epoch 1:  60%|██████    | 764/1271 [03:56<02:41,  3.13it/s, training_loss=0.560]\u001b[A\n",
            "Epoch 1:  60%|██████    | 764/1271 [03:57<02:41,  3.13it/s, training_loss=1.059]\u001b[A\n",
            "Epoch 1:  60%|██████    | 765/1271 [03:57<02:43,  3.10it/s, training_loss=1.059]\u001b[A\n",
            "Epoch 1:  60%|██████    | 765/1271 [03:57<02:43,  3.10it/s, training_loss=0.052]\u001b[A\n",
            "Epoch 1:  60%|██████    | 766/1271 [03:57<02:42,  3.11it/s, training_loss=0.052]\u001b[A\n",
            "Epoch 1:  60%|██████    | 766/1271 [03:57<02:42,  3.11it/s, training_loss=0.898]\u001b[A\n",
            "Epoch 1:  60%|██████    | 767/1271 [03:57<02:42,  3.10it/s, training_loss=0.898]\u001b[A\n",
            "Epoch 1:  60%|██████    | 767/1271 [03:58<02:42,  3.10it/s, training_loss=0.509]\u001b[A\n",
            "Epoch 1:  60%|██████    | 768/1271 [03:58<02:41,  3.11it/s, training_loss=0.509]\u001b[A\n",
            "Epoch 1:  60%|██████    | 768/1271 [03:58<02:41,  3.11it/s, training_loss=0.913]\u001b[A\n",
            "Epoch 1:  61%|██████    | 769/1271 [03:58<02:41,  3.11it/s, training_loss=0.913]\u001b[A\n",
            "Epoch 1:  61%|██████    | 769/1271 [03:58<02:41,  3.11it/s, training_loss=0.307]\u001b[A\n",
            "Epoch 1:  61%|██████    | 770/1271 [03:58<02:39,  3.13it/s, training_loss=0.307]\u001b[A\n",
            "Epoch 1:  61%|██████    | 770/1271 [03:59<02:39,  3.13it/s, training_loss=0.639]\u001b[A\n",
            "Epoch 1:  61%|██████    | 771/1271 [03:59<02:39,  3.14it/s, training_loss=0.639]\u001b[A\n",
            "Epoch 1:  61%|██████    | 771/1271 [03:59<02:39,  3.14it/s, training_loss=0.131]\u001b[A\n",
            "Epoch 1:  61%|██████    | 772/1271 [03:59<02:39,  3.13it/s, training_loss=0.131]\u001b[A\n",
            "Epoch 1:  61%|██████    | 772/1271 [03:59<02:39,  3.13it/s, training_loss=0.973]\u001b[A\n",
            "Epoch 1:  61%|██████    | 773/1271 [03:59<02:40,  3.10it/s, training_loss=0.973]\u001b[A\n",
            "Epoch 1:  61%|██████    | 773/1271 [04:00<02:40,  3.10it/s, training_loss=0.517]\u001b[A\n",
            "Epoch 1:  61%|██████    | 774/1271 [04:00<02:39,  3.11it/s, training_loss=0.517]\u001b[A\n",
            "Epoch 1:  61%|██████    | 774/1271 [04:00<02:39,  3.11it/s, training_loss=0.886]\u001b[A\n",
            "Epoch 1:  61%|██████    | 775/1271 [04:00<02:39,  3.11it/s, training_loss=0.886]\u001b[A\n",
            "Epoch 1:  61%|██████    | 775/1271 [04:00<02:39,  3.11it/s, training_loss=0.280]\u001b[A\n",
            "Epoch 1:  61%|██████    | 776/1271 [04:00<02:38,  3.12it/s, training_loss=0.280]\u001b[A\n",
            "Epoch 1:  61%|██████    | 776/1271 [04:01<02:38,  3.12it/s, training_loss=0.786]\u001b[A\n",
            "Epoch 1:  61%|██████    | 777/1271 [04:01<02:37,  3.13it/s, training_loss=0.786]\u001b[A\n",
            "Epoch 1:  61%|██████    | 777/1271 [04:01<02:37,  3.13it/s, training_loss=0.392]\u001b[A\n",
            "Epoch 1:  61%|██████    | 778/1271 [04:01<02:37,  3.12it/s, training_loss=0.392]\u001b[A\n",
            "Epoch 1:  61%|██████    | 778/1271 [04:01<02:37,  3.12it/s, training_loss=0.454]\u001b[A\n",
            "Epoch 1:  61%|██████▏   | 779/1271 [04:01<02:38,  3.11it/s, training_loss=0.454]\u001b[A\n",
            "Epoch 1:  61%|██████▏   | 779/1271 [04:02<02:38,  3.11it/s, training_loss=0.675]\u001b[A\n",
            "Epoch 1:  61%|██████▏   | 780/1271 [04:02<02:37,  3.11it/s, training_loss=0.675]\u001b[A\n",
            "Epoch 1:  61%|██████▏   | 780/1271 [04:02<02:37,  3.11it/s, training_loss=0.959]\u001b[A\n",
            "Epoch 1:  61%|██████▏   | 781/1271 [04:02<02:37,  3.11it/s, training_loss=0.959]\u001b[A\n",
            "Epoch 1:  61%|██████▏   | 781/1271 [04:02<02:37,  3.11it/s, training_loss=0.244]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 782/1271 [04:02<02:37,  3.11it/s, training_loss=0.244]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 782/1271 [04:03<02:37,  3.11it/s, training_loss=0.746]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 783/1271 [04:03<02:36,  3.11it/s, training_loss=0.746]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 783/1271 [04:03<02:36,  3.11it/s, training_loss=0.196]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 784/1271 [04:03<02:35,  3.13it/s, training_loss=0.196]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 784/1271 [04:03<02:35,  3.13it/s, training_loss=0.369]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 785/1271 [04:03<02:35,  3.12it/s, training_loss=0.369]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 785/1271 [04:04<02:35,  3.12it/s, training_loss=0.742]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 786/1271 [04:04<02:35,  3.12it/s, training_loss=0.742]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 786/1271 [04:04<02:35,  3.12it/s, training_loss=0.686]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 787/1271 [04:04<02:34,  3.12it/s, training_loss=0.686]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 787/1271 [04:04<02:34,  3.12it/s, training_loss=0.427]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 788/1271 [04:04<02:34,  3.13it/s, training_loss=0.427]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 788/1271 [04:04<02:34,  3.13it/s, training_loss=0.444]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 789/1271 [04:04<02:33,  3.14it/s, training_loss=0.444]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 789/1271 [04:05<02:33,  3.14it/s, training_loss=0.424]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 790/1271 [04:05<02:33,  3.14it/s, training_loss=0.424]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 790/1271 [04:05<02:33,  3.14it/s, training_loss=0.623]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 791/1271 [04:05<02:33,  3.13it/s, training_loss=0.623]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 791/1271 [04:05<02:33,  3.13it/s, training_loss=1.080]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 792/1271 [04:05<02:33,  3.13it/s, training_loss=1.080]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 792/1271 [04:06<02:33,  3.13it/s, training_loss=0.099]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 793/1271 [04:06<02:32,  3.13it/s, training_loss=0.099]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 793/1271 [04:06<02:32,  3.13it/s, training_loss=0.809]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 794/1271 [04:06<02:33,  3.12it/s, training_loss=0.809]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 794/1271 [04:06<02:33,  3.12it/s, training_loss=0.684]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 795/1271 [04:06<02:32,  3.12it/s, training_loss=0.684]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 795/1271 [04:07<02:32,  3.12it/s, training_loss=0.628]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 796/1271 [04:07<02:32,  3.11it/s, training_loss=0.628]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 796/1271 [04:07<02:32,  3.11it/s, training_loss=0.592]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 797/1271 [04:07<02:32,  3.11it/s, training_loss=0.592]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 797/1271 [04:07<02:32,  3.11it/s, training_loss=0.891]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 798/1271 [04:07<02:32,  3.10it/s, training_loss=0.891]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 798/1271 [04:08<02:32,  3.10it/s, training_loss=0.782]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 799/1271 [04:08<02:31,  3.11it/s, training_loss=0.782]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 799/1271 [04:08<02:31,  3.11it/s, training_loss=0.710]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 800/1271 [04:08<02:32,  3.09it/s, training_loss=0.710]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 800/1271 [04:08<02:32,  3.09it/s, training_loss=0.570]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 801/1271 [04:08<02:31,  3.09it/s, training_loss=0.570]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 801/1271 [04:09<02:31,  3.09it/s, training_loss=0.487]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 802/1271 [04:09<02:31,  3.09it/s, training_loss=0.487]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 802/1271 [04:09<02:31,  3.09it/s, training_loss=0.712]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 803/1271 [04:09<02:29,  3.12it/s, training_loss=0.712]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 803/1271 [04:09<02:29,  3.12it/s, training_loss=0.798]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 804/1271 [04:09<02:29,  3.13it/s, training_loss=0.798]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 804/1271 [04:10<02:29,  3.13it/s, training_loss=0.688]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 805/1271 [04:10<02:29,  3.12it/s, training_loss=0.688]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 805/1271 [04:10<02:29,  3.12it/s, training_loss=0.437]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 806/1271 [04:10<02:28,  3.12it/s, training_loss=0.437]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 806/1271 [04:10<02:28,  3.12it/s, training_loss=0.702]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 807/1271 [04:10<02:29,  3.10it/s, training_loss=0.702]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 807/1271 [04:11<02:29,  3.10it/s, training_loss=0.904]\u001b[A\n",
            "Epoch 1:  64%|██████▎   | 808/1271 [04:11<02:29,  3.09it/s, training_loss=0.904]\u001b[A\n",
            "Epoch 1:  64%|██████▎   | 808/1271 [04:11<02:29,  3.09it/s, training_loss=0.539]\u001b[A\n",
            "Epoch 1:  64%|██████▎   | 809/1271 [04:11<02:30,  3.08it/s, training_loss=0.539]\u001b[A\n",
            "Epoch 1:  64%|██████▎   | 809/1271 [04:11<02:30,  3.08it/s, training_loss=0.989]\u001b[A\n",
            "Epoch 1:  64%|██████▎   | 810/1271 [04:11<02:29,  3.09it/s, training_loss=0.989]\u001b[A\n",
            "Epoch 1:  64%|██████▎   | 810/1271 [04:12<02:29,  3.09it/s, training_loss=0.445]\u001b[A\n",
            "Epoch 1:  64%|██████▍   | 811/1271 [04:12<02:28,  3.10it/s, training_loss=0.445]\u001b[A\n",
            "Epoch 1:  64%|██████▍   | 811/1271 [04:12<02:28,  3.10it/s, training_loss=0.899]\u001b[A\n",
            "Epoch 1:  64%|██████▍   | 812/1271 [04:12<02:28,  3.10it/s, training_loss=0.899]\u001b[A\n",
            "Epoch 1:  64%|██████▍   | 812/1271 [04:12<02:28,  3.10it/s, training_loss=0.748]\u001b[A\n",
            "Epoch 1:  64%|██████▍   | 813/1271 [04:12<02:28,  3.09it/s, training_loss=0.748]\u001b[A\n",
            "Epoch 1:  64%|██████▍   | 813/1271 [04:13<02:28,  3.09it/s, training_loss=1.162]\u001b[A\n",
            "Epoch 1:  64%|██████▍   | 814/1271 [04:13<02:28,  3.09it/s, training_loss=1.162]\u001b[A\n",
            "Epoch 1:  64%|██████▍   | 814/1271 [04:13<02:28,  3.09it/s, training_loss=0.691]\u001b[A\n",
            "Epoch 1:  64%|██████▍   | 815/1271 [04:13<02:26,  3.10it/s, training_loss=0.691]\u001b[A\n",
            "Epoch 1:  64%|██████▍   | 815/1271 [04:13<02:26,  3.10it/s, training_loss=0.862]\u001b[A\n",
            "Epoch 1:  64%|██████▍   | 816/1271 [04:13<02:26,  3.11it/s, training_loss=0.862]\u001b[A\n",
            "Epoch 1:  64%|██████▍   | 816/1271 [04:13<02:26,  3.11it/s, training_loss=0.379]\u001b[A\n",
            "Epoch 1:  64%|██████▍   | 817/1271 [04:14<02:25,  3.11it/s, training_loss=0.379]\u001b[A\n",
            "Epoch 1:  64%|██████▍   | 817/1271 [04:14<02:25,  3.11it/s, training_loss=0.352]\u001b[A\n",
            "Epoch 1:  64%|██████▍   | 818/1271 [04:14<02:24,  3.14it/s, training_loss=0.352]\u001b[A\n",
            "Epoch 1:  64%|██████▍   | 818/1271 [04:14<02:24,  3.14it/s, training_loss=1.034]\u001b[A\n",
            "Epoch 1:  64%|██████▍   | 819/1271 [04:14<02:25,  3.11it/s, training_loss=1.034]\u001b[A\n",
            "Epoch 1:  64%|██████▍   | 819/1271 [04:14<02:25,  3.11it/s, training_loss=0.674]\u001b[A\n",
            "Epoch 1:  65%|██████▍   | 820/1271 [04:14<02:24,  3.12it/s, training_loss=0.674]\u001b[A\n",
            "Epoch 1:  65%|██████▍   | 820/1271 [04:15<02:24,  3.12it/s, training_loss=0.852]\u001b[A\n",
            "Epoch 1:  65%|██████▍   | 821/1271 [04:15<02:24,  3.12it/s, training_loss=0.852]\u001b[A\n",
            "Epoch 1:  65%|██████▍   | 821/1271 [04:15<02:24,  3.12it/s, training_loss=0.836]\u001b[A\n",
            "Epoch 1:  65%|██████▍   | 822/1271 [04:15<02:23,  3.13it/s, training_loss=0.836]\u001b[A\n",
            "Epoch 1:  65%|██████▍   | 822/1271 [04:15<02:23,  3.13it/s, training_loss=0.701]\u001b[A\n",
            "Epoch 1:  65%|██████▍   | 823/1271 [04:15<02:23,  3.13it/s, training_loss=0.701]\u001b[A\n",
            "Epoch 1:  65%|██████▍   | 823/1271 [04:16<02:23,  3.13it/s, training_loss=0.425]\u001b[A\n",
            "Epoch 1:  65%|██████▍   | 824/1271 [04:16<02:21,  3.15it/s, training_loss=0.425]\u001b[A\n",
            "Epoch 1:  65%|██████▍   | 824/1271 [04:16<02:21,  3.15it/s, training_loss=0.588]\u001b[A\n",
            "Epoch 1:  65%|██████▍   | 825/1271 [04:16<02:21,  3.14it/s, training_loss=0.588]\u001b[A\n",
            "Epoch 1:  65%|██████▍   | 825/1271 [04:16<02:21,  3.14it/s, training_loss=0.939]\u001b[A\n",
            "Epoch 1:  65%|██████▍   | 826/1271 [04:16<02:21,  3.15it/s, training_loss=0.939]\u001b[A\n",
            "Epoch 1:  65%|██████▍   | 826/1271 [04:17<02:21,  3.15it/s, training_loss=0.308]\u001b[A\n",
            "Epoch 1:  65%|██████▌   | 827/1271 [04:17<02:20,  3.16it/s, training_loss=0.308]\u001b[A\n",
            "Epoch 1:  65%|██████▌   | 827/1271 [04:17<02:20,  3.16it/s, training_loss=0.245]\u001b[A\n",
            "Epoch 1:  65%|██████▌   | 828/1271 [04:17<02:20,  3.16it/s, training_loss=0.245]\u001b[A\n",
            "Epoch 1:  65%|██████▌   | 828/1271 [04:17<02:20,  3.16it/s, training_loss=0.355]\u001b[A\n",
            "Epoch 1:  65%|██████▌   | 829/1271 [04:17<02:20,  3.14it/s, training_loss=0.355]\u001b[A\n",
            "Epoch 1:  65%|██████▌   | 829/1271 [04:18<02:20,  3.14it/s, training_loss=0.688]\u001b[A\n",
            "Epoch 1:  65%|██████▌   | 830/1271 [04:18<02:19,  3.15it/s, training_loss=0.688]\u001b[A\n",
            "Epoch 1:  65%|██████▌   | 830/1271 [04:18<02:19,  3.15it/s, training_loss=0.090]\u001b[A\n",
            "Epoch 1:  65%|██████▌   | 831/1271 [04:18<02:19,  3.15it/s, training_loss=0.090]\u001b[A\n",
            "Epoch 1:  65%|██████▌   | 831/1271 [04:18<02:19,  3.15it/s, training_loss=0.102]\u001b[A\n",
            "Epoch 1:  65%|██████▌   | 832/1271 [04:18<02:19,  3.15it/s, training_loss=0.102]\u001b[A\n",
            "Epoch 1:  65%|██████▌   | 832/1271 [04:19<02:19,  3.15it/s, training_loss=0.046]\u001b[A\n",
            "Epoch 1:  66%|██████▌   | 833/1271 [04:19<02:19,  3.14it/s, training_loss=0.046]\u001b[A\n",
            "Epoch 1:  66%|██████▌   | 833/1271 [04:19<02:19,  3.14it/s, training_loss=0.254]\u001b[A\n",
            "Epoch 1:  66%|██████▌   | 834/1271 [04:19<02:20,  3.12it/s, training_loss=0.254]\u001b[A\n",
            "Epoch 1:  66%|██████▌   | 834/1271 [04:19<02:20,  3.12it/s, training_loss=0.541]\u001b[A\n",
            "Epoch 1:  66%|██████▌   | 835/1271 [04:19<02:19,  3.12it/s, training_loss=0.541]\u001b[A\n",
            "Epoch 1:  66%|██████▌   | 835/1271 [04:20<02:19,  3.12it/s, training_loss=0.321]\u001b[A\n",
            "Epoch 1:  66%|██████▌   | 836/1271 [04:20<02:19,  3.12it/s, training_loss=0.321]\u001b[A\n",
            "Epoch 1:  66%|██████▌   | 836/1271 [04:20<02:19,  3.12it/s, training_loss=0.694]\u001b[A\n",
            "Epoch 1:  66%|██████▌   | 837/1271 [04:20<02:19,  3.11it/s, training_loss=0.694]\u001b[A\n",
            "Epoch 1:  66%|██████▌   | 837/1271 [04:20<02:19,  3.11it/s, training_loss=0.234]\u001b[A\n",
            "Epoch 1:  66%|██████▌   | 838/1271 [04:20<02:18,  3.13it/s, training_loss=0.234]\u001b[A\n",
            "Epoch 1:  66%|██████▌   | 838/1271 [04:21<02:18,  3.13it/s, training_loss=0.742]\u001b[A\n",
            "Epoch 1:  66%|██████▌   | 839/1271 [04:21<02:18,  3.13it/s, training_loss=0.742]\u001b[A\n",
            "Epoch 1:  66%|██████▌   | 839/1271 [04:21<02:18,  3.13it/s, training_loss=0.630]\u001b[A\n",
            "Epoch 1:  66%|██████▌   | 840/1271 [04:21<02:17,  3.13it/s, training_loss=0.630]\u001b[A\n",
            "Epoch 1:  66%|██████▌   | 840/1271 [04:21<02:17,  3.13it/s, training_loss=0.613]\u001b[A\n",
            "Epoch 1:  66%|██████▌   | 841/1271 [04:21<02:18,  3.11it/s, training_loss=0.613]\u001b[A\n",
            "Epoch 1:  66%|██████▌   | 841/1271 [04:21<02:18,  3.11it/s, training_loss=0.524]\u001b[A\n",
            "Epoch 1:  66%|██████▌   | 842/1271 [04:21<02:17,  3.12it/s, training_loss=0.524]\u001b[A\n",
            "Epoch 1:  66%|██████▌   | 842/1271 [04:22<02:17,  3.12it/s, training_loss=0.739]\u001b[A\n",
            "Epoch 1:  66%|██████▋   | 843/1271 [04:22<02:16,  3.13it/s, training_loss=0.739]\u001b[A\n",
            "Epoch 1:  66%|██████▋   | 843/1271 [04:22<02:16,  3.13it/s, training_loss=0.527]\u001b[A\n",
            "Epoch 1:  66%|██████▋   | 844/1271 [04:22<02:16,  3.13it/s, training_loss=0.527]\u001b[A\n",
            "Epoch 1:  66%|██████▋   | 844/1271 [04:22<02:16,  3.13it/s, training_loss=0.913]\u001b[A\n",
            "Epoch 1:  66%|██████▋   | 845/1271 [04:22<02:16,  3.12it/s, training_loss=0.913]\u001b[A\n",
            "Epoch 1:  66%|██████▋   | 845/1271 [04:23<02:16,  3.12it/s, training_loss=0.731]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 846/1271 [04:23<02:16,  3.11it/s, training_loss=0.731]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 846/1271 [04:23<02:16,  3.11it/s, training_loss=0.621]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 847/1271 [04:23<02:16,  3.11it/s, training_loss=0.621]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 847/1271 [04:23<02:16,  3.11it/s, training_loss=1.259]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 848/1271 [04:23<02:16,  3.11it/s, training_loss=1.259]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 848/1271 [04:24<02:16,  3.11it/s, training_loss=0.598]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 849/1271 [04:24<02:16,  3.10it/s, training_loss=0.598]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 849/1271 [04:24<02:16,  3.10it/s, training_loss=0.478]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 850/1271 [04:24<02:15,  3.10it/s, training_loss=0.478]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 850/1271 [04:24<02:15,  3.10it/s, training_loss=0.376]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 851/1271 [04:24<02:15,  3.11it/s, training_loss=0.376]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 851/1271 [04:25<02:15,  3.11it/s, training_loss=0.964]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 852/1271 [04:25<02:14,  3.10it/s, training_loss=0.964]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 852/1271 [04:25<02:14,  3.10it/s, training_loss=0.691]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 853/1271 [04:25<02:14,  3.10it/s, training_loss=0.691]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 853/1271 [04:25<02:14,  3.10it/s, training_loss=0.618]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 854/1271 [04:25<02:15,  3.08it/s, training_loss=0.618]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 854/1271 [04:26<02:15,  3.08it/s, training_loss=0.734]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 855/1271 [04:26<02:15,  3.08it/s, training_loss=0.734]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 855/1271 [04:26<02:15,  3.08it/s, training_loss=0.833]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 856/1271 [04:26<02:15,  3.07it/s, training_loss=0.833]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 856/1271 [04:26<02:15,  3.07it/s, training_loss=0.630]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 857/1271 [04:26<02:15,  3.07it/s, training_loss=0.630]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 857/1271 [04:27<02:15,  3.07it/s, training_loss=0.262]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 858/1271 [04:27<02:13,  3.08it/s, training_loss=0.262]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 858/1271 [04:27<02:13,  3.08it/s, training_loss=0.252]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 859/1271 [04:27<02:13,  3.09it/s, training_loss=0.252]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 859/1271 [04:27<02:13,  3.09it/s, training_loss=0.924]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 860/1271 [04:27<02:12,  3.10it/s, training_loss=0.924]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 860/1271 [04:28<02:12,  3.10it/s, training_loss=0.882]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 861/1271 [04:28<02:13,  3.08it/s, training_loss=0.882]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 861/1271 [04:28<02:13,  3.08it/s, training_loss=0.789]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 862/1271 [04:28<02:12,  3.09it/s, training_loss=0.789]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 862/1271 [04:28<02:12,  3.09it/s, training_loss=0.737]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 863/1271 [04:28<02:11,  3.09it/s, training_loss=0.737]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 863/1271 [04:29<02:11,  3.09it/s, training_loss=0.511]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 864/1271 [04:29<02:11,  3.10it/s, training_loss=0.511]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 864/1271 [04:29<02:11,  3.10it/s, training_loss=0.269]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 865/1271 [04:29<02:10,  3.12it/s, training_loss=0.269]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 865/1271 [04:29<02:10,  3.12it/s, training_loss=0.736]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 866/1271 [04:29<02:10,  3.11it/s, training_loss=0.736]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 866/1271 [04:30<02:10,  3.11it/s, training_loss=0.087]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 867/1271 [04:30<02:09,  3.13it/s, training_loss=0.087]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 867/1271 [04:30<02:09,  3.13it/s, training_loss=1.051]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 868/1271 [04:30<02:09,  3.10it/s, training_loss=1.051]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 868/1271 [04:30<02:09,  3.10it/s, training_loss=0.513]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 869/1271 [04:30<02:09,  3.11it/s, training_loss=0.513]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 869/1271 [04:31<02:09,  3.11it/s, training_loss=1.281]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 870/1271 [04:31<02:09,  3.11it/s, training_loss=1.281]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 870/1271 [04:31<02:09,  3.11it/s, training_loss=1.101]\u001b[A\n",
            "Epoch 1:  69%|██████▊   | 871/1271 [04:31<02:08,  3.11it/s, training_loss=1.101]\u001b[A\n",
            "Epoch 1:  69%|██████▊   | 871/1271 [04:31<02:08,  3.11it/s, training_loss=0.720]\u001b[A\n",
            "Epoch 1:  69%|██████▊   | 872/1271 [04:31<02:08,  3.09it/s, training_loss=0.720]\u001b[A\n",
            "Epoch 1:  69%|██████▊   | 872/1271 [04:31<02:08,  3.09it/s, training_loss=0.866]\u001b[A\n",
            "Epoch 1:  69%|██████▊   | 873/1271 [04:31<02:08,  3.11it/s, training_loss=0.866]\u001b[A\n",
            "Epoch 1:  69%|██████▊   | 873/1271 [04:32<02:08,  3.11it/s, training_loss=0.906]\u001b[A\n",
            "Epoch 1:  69%|██████▉   | 874/1271 [04:32<02:07,  3.12it/s, training_loss=0.906]\u001b[A\n",
            "Epoch 1:  69%|██████▉   | 874/1271 [04:32<02:07,  3.12it/s, training_loss=0.533]\u001b[A\n",
            "Epoch 1:  69%|██████▉   | 875/1271 [04:32<02:07,  3.11it/s, training_loss=0.533]\u001b[A\n",
            "Epoch 1:  69%|██████▉   | 875/1271 [04:32<02:07,  3.11it/s, training_loss=0.986]\u001b[A\n",
            "Epoch 1:  69%|██████▉   | 876/1271 [04:32<02:06,  3.12it/s, training_loss=0.986]\u001b[A\n",
            "Epoch 1:  69%|██████▉   | 876/1271 [04:33<02:06,  3.12it/s, training_loss=1.058]\u001b[A\n",
            "Epoch 1:  69%|██████▉   | 877/1271 [04:33<02:06,  3.11it/s, training_loss=1.058]\u001b[A\n",
            "Epoch 1:  69%|██████▉   | 877/1271 [04:33<02:06,  3.11it/s, training_loss=0.684]\u001b[A\n",
            "Epoch 1:  69%|██████▉   | 878/1271 [04:33<02:06,  3.11it/s, training_loss=0.684]\u001b[A\n",
            "Epoch 1:  69%|██████▉   | 878/1271 [04:33<02:06,  3.11it/s, training_loss=0.424]\u001b[A\n",
            "Epoch 1:  69%|██████▉   | 879/1271 [04:33<02:06,  3.11it/s, training_loss=0.424]\u001b[A\n",
            "Epoch 1:  69%|██████▉   | 879/1271 [04:34<02:06,  3.11it/s, training_loss=0.383]\u001b[A\n",
            "Epoch 1:  69%|██████▉   | 880/1271 [04:34<02:06,  3.10it/s, training_loss=0.383]\u001b[A\n",
            "Epoch 1:  69%|██████▉   | 880/1271 [04:34<02:06,  3.10it/s, training_loss=0.372]\u001b[A\n",
            "Epoch 1:  69%|██████▉   | 881/1271 [04:34<02:05,  3.10it/s, training_loss=0.372]\u001b[A\n",
            "Epoch 1:  69%|██████▉   | 881/1271 [04:34<02:05,  3.10it/s, training_loss=0.609]\u001b[A\n",
            "Epoch 1:  69%|██████▉   | 882/1271 [04:34<02:06,  3.08it/s, training_loss=0.609]\u001b[A\n",
            "Epoch 1:  69%|██████▉   | 882/1271 [04:35<02:06,  3.08it/s, training_loss=0.679]\u001b[A\n",
            "Epoch 1:  69%|██████▉   | 883/1271 [04:35<02:05,  3.08it/s, training_loss=0.679]\u001b[A\n",
            "Epoch 1:  69%|██████▉   | 883/1271 [04:35<02:05,  3.08it/s, training_loss=0.540]\u001b[A\n",
            "Epoch 1:  70%|██████▉   | 884/1271 [04:35<02:05,  3.09it/s, training_loss=0.540]\u001b[A\n",
            "Epoch 1:  70%|██████▉   | 884/1271 [04:35<02:05,  3.09it/s, training_loss=0.358]\u001b[A\n",
            "Epoch 1:  70%|██████▉   | 885/1271 [04:35<02:04,  3.11it/s, training_loss=0.358]\u001b[A\n",
            "Epoch 1:  70%|██████▉   | 885/1271 [04:36<02:04,  3.11it/s, training_loss=0.835]\u001b[A\n",
            "Epoch 1:  70%|██████▉   | 886/1271 [04:36<02:03,  3.12it/s, training_loss=0.835]\u001b[A\n",
            "Epoch 1:  70%|██████▉   | 886/1271 [04:36<02:03,  3.12it/s, training_loss=0.677]\u001b[A\n",
            "Epoch 1:  70%|██████▉   | 887/1271 [04:36<02:02,  3.12it/s, training_loss=0.677]\u001b[A\n",
            "Epoch 1:  70%|██████▉   | 887/1271 [04:36<02:02,  3.12it/s, training_loss=1.003]\u001b[A\n",
            "Epoch 1:  70%|██████▉   | 888/1271 [04:36<02:03,  3.10it/s, training_loss=1.003]\u001b[A\n",
            "Epoch 1:  70%|██████▉   | 888/1271 [04:37<02:03,  3.10it/s, training_loss=0.225]\u001b[A\n",
            "Epoch 1:  70%|██████▉   | 889/1271 [04:37<02:02,  3.12it/s, training_loss=0.225]\u001b[A\n",
            "Epoch 1:  70%|██████▉   | 889/1271 [04:37<02:02,  3.12it/s, training_loss=0.451]\u001b[A\n",
            "Epoch 1:  70%|███████   | 890/1271 [04:37<02:01,  3.13it/s, training_loss=0.451]\u001b[A\n",
            "Epoch 1:  70%|███████   | 890/1271 [04:37<02:01,  3.13it/s, training_loss=0.065]\u001b[A\n",
            "Epoch 1:  70%|███████   | 891/1271 [04:37<02:01,  3.13it/s, training_loss=0.065]\u001b[A\n",
            "Epoch 1:  70%|███████   | 891/1271 [04:38<02:01,  3.13it/s, training_loss=0.494]\u001b[A\n",
            "Epoch 1:  70%|███████   | 892/1271 [04:38<02:01,  3.13it/s, training_loss=0.494]\u001b[A\n",
            "Epoch 1:  70%|███████   | 892/1271 [04:38<02:01,  3.13it/s, training_loss=0.513]\u001b[A\n",
            "Epoch 1:  70%|███████   | 893/1271 [04:38<02:00,  3.13it/s, training_loss=0.513]\u001b[A\n",
            "Epoch 1:  70%|███████   | 893/1271 [04:38<02:00,  3.13it/s, training_loss=0.318]\u001b[A\n",
            "Epoch 1:  70%|███████   | 894/1271 [04:38<02:00,  3.12it/s, training_loss=0.318]\u001b[A\n",
            "Epoch 1:  70%|███████   | 894/1271 [04:39<02:00,  3.12it/s, training_loss=0.947]\u001b[A\n",
            "Epoch 1:  70%|███████   | 895/1271 [04:39<02:01,  3.10it/s, training_loss=0.947]\u001b[A\n",
            "Epoch 1:  70%|███████   | 895/1271 [04:39<02:01,  3.10it/s, training_loss=0.809]\u001b[A\n",
            "Epoch 1:  70%|███████   | 896/1271 [04:39<02:00,  3.10it/s, training_loss=0.809]\u001b[A\n",
            "Epoch 1:  70%|███████   | 896/1271 [04:39<02:00,  3.10it/s, training_loss=0.883]\u001b[A\n",
            "Epoch 1:  71%|███████   | 897/1271 [04:39<02:01,  3.08it/s, training_loss=0.883]\u001b[A\n",
            "Epoch 1:  71%|███████   | 897/1271 [04:40<02:01,  3.08it/s, training_loss=0.646]\u001b[A\n",
            "Epoch 1:  71%|███████   | 898/1271 [04:40<02:01,  3.08it/s, training_loss=0.646]\u001b[A\n",
            "Epoch 1:  71%|███████   | 898/1271 [04:40<02:01,  3.08it/s, training_loss=0.414]\u001b[A\n",
            "Epoch 1:  71%|███████   | 899/1271 [04:40<02:00,  3.08it/s, training_loss=0.414]\u001b[A\n",
            "Epoch 1:  71%|███████   | 899/1271 [04:40<02:00,  3.08it/s, training_loss=0.915]\u001b[A\n",
            "Epoch 1:  71%|███████   | 900/1271 [04:40<02:00,  3.09it/s, training_loss=0.915]\u001b[A\n",
            "Epoch 1:  71%|███████   | 900/1271 [04:40<02:00,  3.09it/s, training_loss=0.193]\u001b[A\n",
            "Epoch 1:  71%|███████   | 901/1271 [04:40<01:59,  3.09it/s, training_loss=0.193]\u001b[A\n",
            "Epoch 1:  71%|███████   | 901/1271 [04:41<01:59,  3.09it/s, training_loss=0.089]\u001b[A\n",
            "Epoch 1:  71%|███████   | 902/1271 [04:41<01:58,  3.10it/s, training_loss=0.089]\u001b[A\n",
            "Epoch 1:  71%|███████   | 902/1271 [04:41<01:58,  3.10it/s, training_loss=0.647]\u001b[A\n",
            "Epoch 1:  71%|███████   | 903/1271 [04:41<01:58,  3.12it/s, training_loss=0.647]\u001b[A\n",
            "Epoch 1:  71%|███████   | 903/1271 [04:41<01:58,  3.12it/s, training_loss=0.558]\u001b[A\n",
            "Epoch 1:  71%|███████   | 904/1271 [04:41<01:58,  3.10it/s, training_loss=0.558]\u001b[A\n",
            "Epoch 1:  71%|███████   | 904/1271 [04:42<01:58,  3.10it/s, training_loss=0.280]\u001b[A\n",
            "Epoch 1:  71%|███████   | 905/1271 [04:42<01:57,  3.12it/s, training_loss=0.280]\u001b[A\n",
            "Epoch 1:  71%|███████   | 905/1271 [04:42<01:57,  3.12it/s, training_loss=0.550]\u001b[A\n",
            "Epoch 1:  71%|███████▏  | 906/1271 [04:42<01:57,  3.11it/s, training_loss=0.550]\u001b[A\n",
            "Epoch 1:  71%|███████▏  | 906/1271 [04:42<01:57,  3.11it/s, training_loss=1.000]\u001b[A\n",
            "Epoch 1:  71%|███████▏  | 907/1271 [04:42<01:56,  3.12it/s, training_loss=1.000]\u001b[A\n",
            "Epoch 1:  71%|███████▏  | 907/1271 [04:43<01:56,  3.12it/s, training_loss=0.534]\u001b[A\n",
            "Epoch 1:  71%|███████▏  | 908/1271 [04:43<01:56,  3.13it/s, training_loss=0.534]\u001b[A\n",
            "Epoch 1:  71%|███████▏  | 908/1271 [04:43<01:56,  3.13it/s, training_loss=0.630]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 909/1271 [04:43<01:55,  3.12it/s, training_loss=0.630]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 909/1271 [04:43<01:55,  3.12it/s, training_loss=0.555]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 910/1271 [04:43<01:55,  3.12it/s, training_loss=0.555]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 910/1271 [04:44<01:55,  3.12it/s, training_loss=0.136]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 911/1271 [04:44<01:55,  3.11it/s, training_loss=0.136]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 911/1271 [04:44<01:55,  3.11it/s, training_loss=0.446]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 912/1271 [04:44<01:55,  3.12it/s, training_loss=0.446]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 912/1271 [04:44<01:55,  3.12it/s, training_loss=0.041]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 913/1271 [04:44<01:54,  3.12it/s, training_loss=0.041]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 913/1271 [04:45<01:54,  3.12it/s, training_loss=0.516]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 914/1271 [04:45<01:54,  3.12it/s, training_loss=0.516]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 914/1271 [04:45<01:54,  3.12it/s, training_loss=0.762]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 915/1271 [04:45<01:53,  3.13it/s, training_loss=0.762]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 915/1271 [04:45<01:53,  3.13it/s, training_loss=1.168]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 916/1271 [04:45<01:53,  3.12it/s, training_loss=1.168]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 916/1271 [04:46<01:53,  3.12it/s, training_loss=0.562]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 917/1271 [04:46<01:53,  3.12it/s, training_loss=0.562]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 917/1271 [04:46<01:53,  3.12it/s, training_loss=0.280]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 918/1271 [04:46<01:53,  3.12it/s, training_loss=0.280]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 918/1271 [04:46<01:53,  3.12it/s, training_loss=0.465]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 919/1271 [04:46<01:52,  3.12it/s, training_loss=0.465]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 919/1271 [04:47<01:52,  3.12it/s, training_loss=1.143]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 920/1271 [04:47<01:52,  3.13it/s, training_loss=1.143]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 920/1271 [04:47<01:52,  3.13it/s, training_loss=0.630]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 921/1271 [04:47<01:51,  3.14it/s, training_loss=0.630]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 921/1271 [04:47<01:51,  3.14it/s, training_loss=0.467]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 922/1271 [04:47<01:52,  3.11it/s, training_loss=0.467]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 922/1271 [04:48<01:52,  3.11it/s, training_loss=0.666]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 923/1271 [04:48<01:52,  3.11it/s, training_loss=0.666]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 923/1271 [04:48<01:52,  3.11it/s, training_loss=0.742]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 924/1271 [04:48<01:51,  3.12it/s, training_loss=0.742]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 924/1271 [04:48<01:51,  3.12it/s, training_loss=0.816]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 925/1271 [04:48<01:51,  3.10it/s, training_loss=0.816]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 925/1271 [04:49<01:51,  3.10it/s, training_loss=0.730]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 926/1271 [04:49<01:51,  3.08it/s, training_loss=0.730]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 926/1271 [04:49<01:51,  3.08it/s, training_loss=0.335]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 927/1271 [04:49<01:51,  3.09it/s, training_loss=0.335]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 927/1271 [04:49<01:51,  3.09it/s, training_loss=0.841]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 928/1271 [04:49<01:50,  3.10it/s, training_loss=0.841]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 928/1271 [04:49<01:50,  3.10it/s, training_loss=0.219]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 929/1271 [04:49<01:50,  3.10it/s, training_loss=0.219]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 929/1271 [04:50<01:50,  3.10it/s, training_loss=0.399]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 930/1271 [04:50<01:49,  3.11it/s, training_loss=0.399]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 930/1271 [04:50<01:49,  3.11it/s, training_loss=0.344]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 931/1271 [04:50<01:48,  3.12it/s, training_loss=0.344]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 931/1271 [04:50<01:48,  3.12it/s, training_loss=0.889]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 932/1271 [04:50<01:48,  3.12it/s, training_loss=0.889]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 932/1271 [04:51<01:48,  3.12it/s, training_loss=0.103]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 933/1271 [04:51<01:48,  3.12it/s, training_loss=0.103]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 933/1271 [04:51<01:48,  3.12it/s, training_loss=0.700]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 934/1271 [04:51<01:48,  3.12it/s, training_loss=0.700]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 934/1271 [04:51<01:48,  3.12it/s, training_loss=1.162]\u001b[A\n",
            "Epoch 1:  74%|███████▎  | 935/1271 [04:51<01:47,  3.12it/s, training_loss=1.162]\u001b[A\n",
            "Epoch 1:  74%|███████▎  | 935/1271 [04:52<01:47,  3.12it/s, training_loss=0.847]\u001b[A\n",
            "Epoch 1:  74%|███████▎  | 936/1271 [04:52<01:47,  3.13it/s, training_loss=0.847]\u001b[A\n",
            "Epoch 1:  74%|███████▎  | 936/1271 [04:52<01:47,  3.13it/s, training_loss=0.509]\u001b[A\n",
            "Epoch 1:  74%|███████▎  | 937/1271 [04:52<01:46,  3.12it/s, training_loss=0.509]\u001b[A\n",
            "Epoch 1:  74%|███████▎  | 937/1271 [04:52<01:46,  3.12it/s, training_loss=0.542]\u001b[A\n",
            "Epoch 1:  74%|███████▍  | 938/1271 [04:52<01:47,  3.10it/s, training_loss=0.542]\u001b[A\n",
            "Epoch 1:  74%|███████▍  | 938/1271 [04:53<01:47,  3.10it/s, training_loss=0.747]\u001b[A\n",
            "Epoch 1:  74%|███████▍  | 939/1271 [04:53<01:47,  3.08it/s, training_loss=0.747]\u001b[A\n",
            "Epoch 1:  74%|███████▍  | 939/1271 [04:53<01:47,  3.08it/s, training_loss=0.506]\u001b[A\n",
            "Epoch 1:  74%|███████▍  | 940/1271 [04:53<01:46,  3.10it/s, training_loss=0.506]\u001b[A\n",
            "Epoch 1:  74%|███████▍  | 940/1271 [04:53<01:46,  3.10it/s, training_loss=0.418]\u001b[A\n",
            "Epoch 1:  74%|███████▍  | 941/1271 [04:53<01:46,  3.11it/s, training_loss=0.418]\u001b[A\n",
            "Epoch 1:  74%|███████▍  | 941/1271 [04:54<01:46,  3.11it/s, training_loss=0.769]\u001b[A\n",
            "Epoch 1:  74%|███████▍  | 942/1271 [04:54<01:46,  3.08it/s, training_loss=0.769]\u001b[A\n",
            "Epoch 1:  74%|███████▍  | 942/1271 [04:54<01:46,  3.08it/s, training_loss=0.163]\u001b[A\n",
            "Epoch 1:  74%|███████▍  | 943/1271 [04:54<01:46,  3.07it/s, training_loss=0.163]\u001b[A\n",
            "Epoch 1:  74%|███████▍  | 943/1271 [04:54<01:46,  3.07it/s, training_loss=0.399]\u001b[A\n",
            "Epoch 1:  74%|███████▍  | 944/1271 [04:54<01:46,  3.06it/s, training_loss=0.399]\u001b[A\n",
            "Epoch 1:  74%|███████▍  | 944/1271 [04:55<01:46,  3.06it/s, training_loss=0.774]\u001b[A\n",
            "Epoch 1:  74%|███████▍  | 945/1271 [04:55<01:45,  3.09it/s, training_loss=0.774]\u001b[A\n",
            "Epoch 1:  74%|███████▍  | 945/1271 [04:55<01:45,  3.09it/s, training_loss=0.912]\u001b[A\n",
            "Epoch 1:  74%|███████▍  | 946/1271 [04:55<01:44,  3.11it/s, training_loss=0.912]\u001b[A\n",
            "Epoch 1:  74%|███████▍  | 946/1271 [04:55<01:44,  3.11it/s, training_loss=0.804]\u001b[A\n",
            "Epoch 1:  75%|███████▍  | 947/1271 [04:55<01:44,  3.10it/s, training_loss=0.804]\u001b[A\n",
            "Epoch 1:  75%|███████▍  | 947/1271 [04:56<01:44,  3.10it/s, training_loss=0.033]\u001b[A\n",
            "Epoch 1:  75%|███████▍  | 948/1271 [04:56<01:43,  3.11it/s, training_loss=0.033]\u001b[A\n",
            "Epoch 1:  75%|███████▍  | 948/1271 [04:56<01:43,  3.11it/s, training_loss=1.113]\u001b[A\n",
            "Epoch 1:  75%|███████▍  | 949/1271 [04:56<01:43,  3.13it/s, training_loss=1.113]\u001b[A\n",
            "Epoch 1:  75%|███████▍  | 949/1271 [04:56<01:43,  3.13it/s, training_loss=0.947]\u001b[A\n",
            "Epoch 1:  75%|███████▍  | 950/1271 [04:56<01:42,  3.13it/s, training_loss=0.947]\u001b[A\n",
            "Epoch 1:  75%|███████▍  | 950/1271 [04:57<01:42,  3.13it/s, training_loss=0.878]\u001b[A\n",
            "Epoch 1:  75%|███████▍  | 951/1271 [04:57<01:43,  3.10it/s, training_loss=0.878]\u001b[A\n",
            "Epoch 1:  75%|███████▍  | 951/1271 [04:57<01:43,  3.10it/s, training_loss=0.195]\u001b[A\n",
            "Epoch 1:  75%|███████▍  | 952/1271 [04:57<01:42,  3.10it/s, training_loss=0.195]\u001b[A\n",
            "Epoch 1:  75%|███████▍  | 952/1271 [04:57<01:42,  3.10it/s, training_loss=0.688]\u001b[A\n",
            "Epoch 1:  75%|███████▍  | 953/1271 [04:57<01:42,  3.11it/s, training_loss=0.688]\u001b[A\n",
            "Epoch 1:  75%|███████▍  | 953/1271 [04:58<01:42,  3.11it/s, training_loss=1.135]\u001b[A\n",
            "Epoch 1:  75%|███████▌  | 954/1271 [04:58<01:42,  3.10it/s, training_loss=1.135]\u001b[A\n",
            "Epoch 1:  75%|███████▌  | 954/1271 [04:58<01:42,  3.10it/s, training_loss=1.029]\u001b[A\n",
            "Epoch 1:  75%|███████▌  | 955/1271 [04:58<01:42,  3.09it/s, training_loss=1.029]\u001b[A\n",
            "Epoch 1:  75%|███████▌  | 955/1271 [04:58<01:42,  3.09it/s, training_loss=0.508]\u001b[A\n",
            "Epoch 1:  75%|███████▌  | 956/1271 [04:58<01:41,  3.09it/s, training_loss=0.508]\u001b[A\n",
            "Epoch 1:  75%|███████▌  | 956/1271 [04:59<01:41,  3.09it/s, training_loss=0.351]\u001b[A\n",
            "Epoch 1:  75%|███████▌  | 957/1271 [04:59<01:41,  3.10it/s, training_loss=0.351]\u001b[A\n",
            "Epoch 1:  75%|███████▌  | 957/1271 [04:59<01:41,  3.10it/s, training_loss=0.749]\u001b[A\n",
            "Epoch 1:  75%|███████▌  | 958/1271 [04:59<01:40,  3.11it/s, training_loss=0.749]\u001b[A\n",
            "Epoch 1:  75%|███████▌  | 958/1271 [04:59<01:40,  3.11it/s, training_loss=0.831]\u001b[A\n",
            "Epoch 1:  75%|███████▌  | 959/1271 [04:59<01:40,  3.11it/s, training_loss=0.831]\u001b[A\n",
            "Epoch 1:  75%|███████▌  | 959/1271 [04:59<01:40,  3.11it/s, training_loss=0.382]\u001b[A\n",
            "Epoch 1:  76%|███████▌  | 960/1271 [04:59<01:39,  3.13it/s, training_loss=0.382]\u001b[A\n",
            "Epoch 1:  76%|███████▌  | 960/1271 [05:00<01:39,  3.13it/s, training_loss=0.408]\u001b[A\n",
            "Epoch 1:  76%|███████▌  | 961/1271 [05:00<01:38,  3.13it/s, training_loss=0.408]\u001b[A\n",
            "Epoch 1:  76%|███████▌  | 961/1271 [05:00<01:38,  3.13it/s, training_loss=0.551]\u001b[A\n",
            "Epoch 1:  76%|███████▌  | 962/1271 [05:00<01:39,  3.12it/s, training_loss=0.551]\u001b[A\n",
            "Epoch 1:  76%|███████▌  | 962/1271 [05:00<01:39,  3.12it/s, training_loss=0.860]\u001b[A\n",
            "Epoch 1:  76%|███████▌  | 963/1271 [05:00<01:38,  3.12it/s, training_loss=0.860]\u001b[A\n",
            "Epoch 1:  76%|███████▌  | 963/1271 [05:01<01:38,  3.12it/s, training_loss=0.874]\u001b[A\n",
            "Epoch 1:  76%|███████▌  | 964/1271 [05:01<01:38,  3.12it/s, training_loss=0.874]\u001b[A\n",
            "Epoch 1:  76%|███████▌  | 964/1271 [05:01<01:38,  3.12it/s, training_loss=0.472]\u001b[A\n",
            "Epoch 1:  76%|███████▌  | 965/1271 [05:01<01:39,  3.09it/s, training_loss=0.472]\u001b[A\n",
            "Epoch 1:  76%|███████▌  | 965/1271 [05:01<01:39,  3.09it/s, training_loss=0.698]\u001b[A\n",
            "Epoch 1:  76%|███████▌  | 966/1271 [05:01<01:39,  3.08it/s, training_loss=0.698]\u001b[A\n",
            "Epoch 1:  76%|███████▌  | 966/1271 [05:02<01:39,  3.08it/s, training_loss=0.526]\u001b[A\n",
            "Epoch 1:  76%|███████▌  | 967/1271 [05:02<01:38,  3.08it/s, training_loss=0.526]\u001b[A\n",
            "Epoch 1:  76%|███████▌  | 967/1271 [05:02<01:38,  3.08it/s, training_loss=0.838]\u001b[A\n",
            "Epoch 1:  76%|███████▌  | 968/1271 [05:02<01:37,  3.10it/s, training_loss=0.838]\u001b[A\n",
            "Epoch 1:  76%|███████▌  | 968/1271 [05:02<01:37,  3.10it/s, training_loss=1.042]\u001b[A\n",
            "Epoch 1:  76%|███████▌  | 969/1271 [05:02<01:37,  3.09it/s, training_loss=1.042]\u001b[A\n",
            "Epoch 1:  76%|███████▌  | 969/1271 [05:03<01:37,  3.09it/s, training_loss=0.715]\u001b[A\n",
            "Epoch 1:  76%|███████▋  | 970/1271 [05:03<01:37,  3.09it/s, training_loss=0.715]\u001b[A\n",
            "Epoch 1:  76%|███████▋  | 970/1271 [05:03<01:37,  3.09it/s, training_loss=0.611]\u001b[A\n",
            "Epoch 1:  76%|███████▋  | 971/1271 [05:03<01:37,  3.09it/s, training_loss=0.611]\u001b[A\n",
            "Epoch 1:  76%|███████▋  | 971/1271 [05:03<01:37,  3.09it/s, training_loss=1.098]\u001b[A\n",
            "Epoch 1:  76%|███████▋  | 972/1271 [05:03<01:37,  3.07it/s, training_loss=1.098]\u001b[A\n",
            "Epoch 1:  76%|███████▋  | 972/1271 [05:04<01:37,  3.07it/s, training_loss=0.296]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 973/1271 [05:04<01:36,  3.08it/s, training_loss=0.296]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 973/1271 [05:04<01:36,  3.08it/s, training_loss=0.309]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 974/1271 [05:04<01:35,  3.10it/s, training_loss=0.309]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 974/1271 [05:04<01:35,  3.10it/s, training_loss=0.547]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 975/1271 [05:04<01:34,  3.12it/s, training_loss=0.547]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 975/1271 [05:05<01:34,  3.12it/s, training_loss=1.112]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 976/1271 [05:05<01:34,  3.11it/s, training_loss=1.112]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 976/1271 [05:05<01:34,  3.11it/s, training_loss=0.669]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 977/1271 [05:05<01:34,  3.11it/s, training_loss=0.669]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 977/1271 [05:05<01:34,  3.11it/s, training_loss=0.849]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 978/1271 [05:05<01:34,  3.11it/s, training_loss=0.849]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 978/1271 [05:06<01:34,  3.11it/s, training_loss=0.542]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 979/1271 [05:06<01:34,  3.10it/s, training_loss=0.542]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 979/1271 [05:06<01:34,  3.10it/s, training_loss=0.295]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 980/1271 [05:06<01:34,  3.09it/s, training_loss=0.295]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 980/1271 [05:06<01:34,  3.09it/s, training_loss=0.297]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 981/1271 [05:06<01:33,  3.10it/s, training_loss=0.297]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 981/1271 [05:07<01:33,  3.10it/s, training_loss=0.723]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 982/1271 [05:07<01:33,  3.10it/s, training_loss=0.723]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 982/1271 [05:07<01:33,  3.10it/s, training_loss=0.799]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 983/1271 [05:07<01:32,  3.10it/s, training_loss=0.799]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 983/1271 [05:07<01:32,  3.10it/s, training_loss=1.336]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 984/1271 [05:07<01:32,  3.11it/s, training_loss=1.336]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 984/1271 [05:08<01:32,  3.11it/s, training_loss=0.399]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 985/1271 [05:08<01:31,  3.11it/s, training_loss=0.399]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 985/1271 [05:08<01:31,  3.11it/s, training_loss=0.300]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 986/1271 [05:08<01:31,  3.13it/s, training_loss=0.300]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 986/1271 [05:08<01:31,  3.13it/s, training_loss=0.249]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 987/1271 [05:08<01:31,  3.11it/s, training_loss=0.249]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 987/1271 [05:08<01:31,  3.11it/s, training_loss=0.506]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 988/1271 [05:08<01:30,  3.11it/s, training_loss=0.506]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 988/1271 [05:09<01:30,  3.11it/s, training_loss=0.310]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 989/1271 [05:09<01:30,  3.12it/s, training_loss=0.310]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 989/1271 [05:09<01:30,  3.12it/s, training_loss=0.741]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 990/1271 [05:09<01:30,  3.10it/s, training_loss=0.741]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 990/1271 [05:09<01:30,  3.10it/s, training_loss=0.708]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 991/1271 [05:09<01:30,  3.11it/s, training_loss=0.708]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 991/1271 [05:10<01:30,  3.11it/s, training_loss=0.029]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 992/1271 [05:10<01:29,  3.12it/s, training_loss=0.029]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 992/1271 [05:10<01:29,  3.12it/s, training_loss=0.805]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 993/1271 [05:10<01:29,  3.12it/s, training_loss=0.805]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 993/1271 [05:10<01:29,  3.12it/s, training_loss=0.796]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 994/1271 [05:10<01:29,  3.11it/s, training_loss=0.796]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 994/1271 [05:11<01:29,  3.11it/s, training_loss=0.770]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 995/1271 [05:11<01:28,  3.11it/s, training_loss=0.770]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 995/1271 [05:11<01:28,  3.11it/s, training_loss=0.593]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 996/1271 [05:11<01:28,  3.10it/s, training_loss=0.593]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 996/1271 [05:11<01:28,  3.10it/s, training_loss=0.744]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 997/1271 [05:11<01:28,  3.10it/s, training_loss=0.744]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 997/1271 [05:12<01:28,  3.10it/s, training_loss=0.706]\u001b[A\n",
            "Epoch 1:  79%|███████▊  | 998/1271 [05:12<01:28,  3.09it/s, training_loss=0.706]\u001b[A\n",
            "Epoch 1:  79%|███████▊  | 998/1271 [05:12<01:28,  3.09it/s, training_loss=0.032]\u001b[A\n",
            "Epoch 1:  79%|███████▊  | 999/1271 [05:12<01:27,  3.10it/s, training_loss=0.032]\u001b[A\n",
            "Epoch 1:  79%|███████▊  | 999/1271 [05:12<01:27,  3.10it/s, training_loss=0.774]\u001b[A\n",
            "Epoch 1:  79%|███████▊  | 1000/1271 [05:12<01:27,  3.09it/s, training_loss=0.774]\u001b[A\n",
            "Epoch 1:  79%|███████▊  | 1000/1271 [05:13<01:27,  3.09it/s, training_loss=0.532]\u001b[A\n",
            "Epoch 1:  79%|███████▉  | 1001/1271 [05:13<01:27,  3.10it/s, training_loss=0.532]\u001b[A\n",
            "Epoch 1:  79%|███████▉  | 1001/1271 [05:13<01:27,  3.10it/s, training_loss=0.541]\u001b[A\n",
            "Epoch 1:  79%|███████▉  | 1002/1271 [05:13<01:26,  3.10it/s, training_loss=0.541]\u001b[A\n",
            "Epoch 1:  79%|███████▉  | 1002/1271 [05:13<01:26,  3.10it/s, training_loss=0.961]\u001b[A\n",
            "Epoch 1:  79%|███████▉  | 1003/1271 [05:13<01:26,  3.10it/s, training_loss=0.961]\u001b[A\n",
            "Epoch 1:  79%|███████▉  | 1003/1271 [05:14<01:26,  3.10it/s, training_loss=0.662]\u001b[A\n",
            "Epoch 1:  79%|███████▉  | 1004/1271 [05:14<01:26,  3.09it/s, training_loss=0.662]\u001b[A\n",
            "Epoch 1:  79%|███████▉  | 1004/1271 [05:14<01:26,  3.09it/s, training_loss=0.389]\u001b[A\n",
            "Epoch 1:  79%|███████▉  | 1005/1271 [05:14<01:25,  3.10it/s, training_loss=0.389]\u001b[A\n",
            "Epoch 1:  79%|███████▉  | 1005/1271 [05:14<01:25,  3.10it/s, training_loss=0.994]\u001b[A\n",
            "Epoch 1:  79%|███████▉  | 1006/1271 [05:14<01:25,  3.10it/s, training_loss=0.994]\u001b[A\n",
            "Epoch 1:  79%|███████▉  | 1006/1271 [05:15<01:25,  3.10it/s, training_loss=0.320]\u001b[A\n",
            "Epoch 1:  79%|███████▉  | 1007/1271 [05:15<01:24,  3.11it/s, training_loss=0.320]\u001b[A\n",
            "Epoch 1:  79%|███████▉  | 1007/1271 [05:15<01:24,  3.11it/s, training_loss=0.838]\u001b[A\n",
            "Epoch 1:  79%|███████▉  | 1008/1271 [05:15<01:25,  3.08it/s, training_loss=0.838]\u001b[A\n",
            "Epoch 1:  79%|███████▉  | 1008/1271 [05:15<01:25,  3.08it/s, training_loss=0.644]\u001b[A\n",
            "Epoch 1:  79%|███████▉  | 1009/1271 [05:15<01:25,  3.07it/s, training_loss=0.644]\u001b[A\n",
            "Epoch 1:  79%|███████▉  | 1009/1271 [05:16<01:25,  3.07it/s, training_loss=0.261]\u001b[A\n",
            "Epoch 1:  79%|███████▉  | 1010/1271 [05:16<01:25,  3.06it/s, training_loss=0.261]\u001b[A\n",
            "Epoch 1:  79%|███████▉  | 1010/1271 [05:16<01:25,  3.06it/s, training_loss=0.321]\u001b[A\n",
            "Epoch 1:  80%|███████▉  | 1011/1271 [05:16<01:24,  3.06it/s, training_loss=0.321]\u001b[A\n",
            "Epoch 1:  80%|███████▉  | 1011/1271 [05:16<01:24,  3.06it/s, training_loss=1.040]\u001b[A\n",
            "Epoch 1:  80%|███████▉  | 1012/1271 [05:16<01:23,  3.10it/s, training_loss=1.040]\u001b[A\n",
            "Epoch 1:  80%|███████▉  | 1012/1271 [05:17<01:23,  3.10it/s, training_loss=0.211]\u001b[A\n",
            "Epoch 1:  80%|███████▉  | 1013/1271 [05:17<01:23,  3.08it/s, training_loss=0.211]\u001b[A\n",
            "Epoch 1:  80%|███████▉  | 1013/1271 [05:17<01:23,  3.08it/s, training_loss=0.300]\u001b[A\n",
            "Epoch 1:  80%|███████▉  | 1014/1271 [05:17<01:23,  3.09it/s, training_loss=0.300]\u001b[A\n",
            "Epoch 1:  80%|███████▉  | 1014/1271 [05:17<01:23,  3.09it/s, training_loss=0.224]\u001b[A\n",
            "Epoch 1:  80%|███████▉  | 1015/1271 [05:17<01:22,  3.11it/s, training_loss=0.224]\u001b[A\n",
            "Epoch 1:  80%|███████▉  | 1015/1271 [05:18<01:22,  3.11it/s, training_loss=0.751]\u001b[A\n",
            "Epoch 1:  80%|███████▉  | 1016/1271 [05:18<01:21,  3.12it/s, training_loss=0.751]\u001b[A\n",
            "Epoch 1:  80%|███████▉  | 1016/1271 [05:18<01:21,  3.12it/s, training_loss=0.755]\u001b[A\n",
            "Epoch 1:  80%|████████  | 1017/1271 [05:18<01:21,  3.12it/s, training_loss=0.755]\u001b[A\n",
            "Epoch 1:  80%|████████  | 1017/1271 [05:18<01:21,  3.12it/s, training_loss=0.609]\u001b[A\n",
            "Epoch 1:  80%|████████  | 1018/1271 [05:18<01:21,  3.12it/s, training_loss=0.609]\u001b[A\n",
            "Epoch 1:  80%|████████  | 1018/1271 [05:18<01:21,  3.12it/s, training_loss=0.548]\u001b[A\n",
            "Epoch 1:  80%|████████  | 1019/1271 [05:18<01:20,  3.11it/s, training_loss=0.548]\u001b[A\n",
            "Epoch 1:  80%|████████  | 1019/1271 [05:19<01:20,  3.11it/s, training_loss=0.285]\u001b[A\n",
            "Epoch 1:  80%|████████  | 1020/1271 [05:19<01:21,  3.09it/s, training_loss=0.285]\u001b[A\n",
            "Epoch 1:  80%|████████  | 1020/1271 [05:19<01:21,  3.09it/s, training_loss=0.822]\u001b[A\n",
            "Epoch 1:  80%|████████  | 1021/1271 [05:19<01:21,  3.07it/s, training_loss=0.822]\u001b[A\n",
            "Epoch 1:  80%|████████  | 1021/1271 [05:19<01:21,  3.07it/s, training_loss=0.518]\u001b[A\n",
            "Epoch 1:  80%|████████  | 1022/1271 [05:19<01:20,  3.08it/s, training_loss=0.518]\u001b[A\n",
            "Epoch 1:  80%|████████  | 1022/1271 [05:20<01:20,  3.08it/s, training_loss=0.417]\u001b[A\n",
            "Epoch 1:  80%|████████  | 1023/1271 [05:20<01:20,  3.10it/s, training_loss=0.417]\u001b[A\n",
            "Epoch 1:  80%|████████  | 1023/1271 [05:20<01:20,  3.10it/s, training_loss=0.711]\u001b[A\n",
            "Epoch 1:  81%|████████  | 1024/1271 [05:20<01:20,  3.09it/s, training_loss=0.711]\u001b[A\n",
            "Epoch 1:  81%|████████  | 1024/1271 [05:20<01:20,  3.09it/s, training_loss=0.267]\u001b[A\n",
            "Epoch 1:  81%|████████  | 1025/1271 [05:20<01:19,  3.09it/s, training_loss=0.267]\u001b[A\n",
            "Epoch 1:  81%|████████  | 1025/1271 [05:21<01:19,  3.09it/s, training_loss=0.831]\u001b[A\n",
            "Epoch 1:  81%|████████  | 1026/1271 [05:21<01:18,  3.10it/s, training_loss=0.831]\u001b[A\n",
            "Epoch 1:  81%|████████  | 1026/1271 [05:21<01:18,  3.10it/s, training_loss=0.261]\u001b[A\n",
            "Epoch 1:  81%|████████  | 1027/1271 [05:21<01:18,  3.12it/s, training_loss=0.261]\u001b[A\n",
            "Epoch 1:  81%|████████  | 1027/1271 [05:21<01:18,  3.12it/s, training_loss=0.662]\u001b[A\n",
            "Epoch 1:  81%|████████  | 1028/1271 [05:21<01:18,  3.11it/s, training_loss=0.662]\u001b[A\n",
            "Epoch 1:  81%|████████  | 1028/1271 [05:22<01:18,  3.11it/s, training_loss=0.464]\u001b[A\n",
            "Epoch 1:  81%|████████  | 1029/1271 [05:22<01:17,  3.11it/s, training_loss=0.464]\u001b[A\n",
            "Epoch 1:  81%|████████  | 1029/1271 [05:22<01:17,  3.11it/s, training_loss=0.951]\u001b[A\n",
            "Epoch 1:  81%|████████  | 1030/1271 [05:22<01:17,  3.12it/s, training_loss=0.951]\u001b[A\n",
            "Epoch 1:  81%|████████  | 1030/1271 [05:22<01:17,  3.12it/s, training_loss=0.619]\u001b[A\n",
            "Epoch 1:  81%|████████  | 1031/1271 [05:22<01:17,  3.09it/s, training_loss=0.619]\u001b[A\n",
            "Epoch 1:  81%|████████  | 1031/1271 [05:23<01:17,  3.09it/s, training_loss=0.453]\u001b[A\n",
            "Epoch 1:  81%|████████  | 1032/1271 [05:23<01:17,  3.08it/s, training_loss=0.453]\u001b[A\n",
            "Epoch 1:  81%|████████  | 1032/1271 [05:23<01:17,  3.08it/s, training_loss=0.514]\u001b[A\n",
            "Epoch 1:  81%|████████▏ | 1033/1271 [05:23<01:17,  3.08it/s, training_loss=0.514]\u001b[A\n",
            "Epoch 1:  81%|████████▏ | 1033/1271 [05:23<01:17,  3.08it/s, training_loss=0.538]\u001b[A\n",
            "Epoch 1:  81%|████████▏ | 1034/1271 [05:23<01:16,  3.10it/s, training_loss=0.538]\u001b[A\n",
            "Epoch 1:  81%|████████▏ | 1034/1271 [05:24<01:16,  3.10it/s, training_loss=1.031]\u001b[A\n",
            "Epoch 1:  81%|████████▏ | 1035/1271 [05:24<01:16,  3.08it/s, training_loss=1.031]\u001b[A\n",
            "Epoch 1:  81%|████████▏ | 1035/1271 [05:24<01:16,  3.08it/s, training_loss=0.581]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 1036/1271 [05:24<01:15,  3.09it/s, training_loss=0.581]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 1036/1271 [05:24<01:15,  3.09it/s, training_loss=0.469]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 1037/1271 [05:24<01:15,  3.10it/s, training_loss=0.469]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 1037/1271 [05:25<01:15,  3.10it/s, training_loss=0.585]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 1038/1271 [05:25<01:15,  3.10it/s, training_loss=0.585]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 1038/1271 [05:25<01:15,  3.10it/s, training_loss=0.594]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 1039/1271 [05:25<01:15,  3.09it/s, training_loss=0.594]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 1039/1271 [05:25<01:15,  3.09it/s, training_loss=0.423]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 1040/1271 [05:25<01:14,  3.11it/s, training_loss=0.423]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 1040/1271 [05:26<01:14,  3.11it/s, training_loss=0.041]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 1041/1271 [05:26<01:13,  3.12it/s, training_loss=0.041]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 1041/1271 [05:26<01:13,  3.12it/s, training_loss=0.748]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 1042/1271 [05:26<01:13,  3.13it/s, training_loss=0.748]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 1042/1271 [05:26<01:13,  3.13it/s, training_loss=0.829]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 1043/1271 [05:26<01:13,  3.12it/s, training_loss=0.829]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 1043/1271 [05:27<01:13,  3.12it/s, training_loss=0.791]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 1044/1271 [05:27<01:12,  3.12it/s, training_loss=0.791]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 1044/1271 [05:27<01:12,  3.12it/s, training_loss=0.392]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 1045/1271 [05:27<01:12,  3.12it/s, training_loss=0.392]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 1045/1271 [05:27<01:12,  3.12it/s, training_loss=0.030]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 1046/1271 [05:27<01:12,  3.12it/s, training_loss=0.030]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 1046/1271 [05:28<01:12,  3.12it/s, training_loss=0.907]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 1047/1271 [05:28<01:11,  3.12it/s, training_loss=0.907]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 1047/1271 [05:28<01:11,  3.12it/s, training_loss=0.702]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 1048/1271 [05:28<01:11,  3.10it/s, training_loss=0.702]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 1048/1271 [05:28<01:11,  3.10it/s, training_loss=0.301]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 1049/1271 [05:28<01:11,  3.11it/s, training_loss=0.301]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 1049/1271 [05:28<01:11,  3.11it/s, training_loss=0.306]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 1050/1271 [05:28<01:11,  3.08it/s, training_loss=0.306]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 1050/1271 [05:29<01:11,  3.08it/s, training_loss=0.442]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 1051/1271 [05:29<01:11,  3.08it/s, training_loss=0.442]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 1051/1271 [05:29<01:11,  3.08it/s, training_loss=0.273]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 1052/1271 [05:29<01:10,  3.09it/s, training_loss=0.273]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 1052/1271 [05:29<01:10,  3.09it/s, training_loss=0.410]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 1053/1271 [05:29<01:10,  3.09it/s, training_loss=0.410]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 1053/1271 [05:30<01:10,  3.09it/s, training_loss=0.459]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 1054/1271 [05:30<01:10,  3.09it/s, training_loss=0.459]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 1054/1271 [05:30<01:10,  3.09it/s, training_loss=0.785]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 1055/1271 [05:30<01:09,  3.11it/s, training_loss=0.785]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 1055/1271 [05:30<01:09,  3.11it/s, training_loss=0.469]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 1056/1271 [05:30<01:08,  3.12it/s, training_loss=0.469]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 1056/1271 [05:31<01:08,  3.12it/s, training_loss=0.800]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 1057/1271 [05:31<01:08,  3.11it/s, training_loss=0.800]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 1057/1271 [05:31<01:08,  3.11it/s, training_loss=0.439]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 1058/1271 [05:31<01:08,  3.11it/s, training_loss=0.439]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 1058/1271 [05:31<01:08,  3.11it/s, training_loss=0.723]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 1059/1271 [05:31<01:07,  3.12it/s, training_loss=0.723]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 1059/1271 [05:32<01:07,  3.12it/s, training_loss=0.784]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 1060/1271 [05:32<01:08,  3.08it/s, training_loss=0.784]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 1060/1271 [05:32<01:08,  3.08it/s, training_loss=0.049]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 1061/1271 [05:32<01:08,  3.08it/s, training_loss=0.049]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 1061/1271 [05:32<01:08,  3.08it/s, training_loss=0.234]\u001b[A\n",
            "Epoch 1:  84%|████████▎ | 1062/1271 [05:32<01:07,  3.10it/s, training_loss=0.234]\u001b[A\n",
            "Epoch 1:  84%|████████▎ | 1062/1271 [05:33<01:07,  3.10it/s, training_loss=0.825]\u001b[A\n",
            "Epoch 1:  84%|████████▎ | 1063/1271 [05:33<01:07,  3.10it/s, training_loss=0.825]\u001b[A\n",
            "Epoch 1:  84%|████████▎ | 1063/1271 [05:33<01:07,  3.10it/s, training_loss=0.249]\u001b[A\n",
            "Epoch 1:  84%|████████▎ | 1064/1271 [05:33<01:06,  3.10it/s, training_loss=0.249]\u001b[A\n",
            "Epoch 1:  84%|████████▎ | 1064/1271 [05:33<01:06,  3.10it/s, training_loss=0.854]\u001b[A\n",
            "Epoch 1:  84%|████████▍ | 1065/1271 [05:33<01:06,  3.10it/s, training_loss=0.854]\u001b[A\n",
            "Epoch 1:  84%|████████▍ | 1065/1271 [05:34<01:06,  3.10it/s, training_loss=0.713]\u001b[A\n",
            "Epoch 1:  84%|████████▍ | 1066/1271 [05:34<01:05,  3.11it/s, training_loss=0.713]\u001b[A\n",
            "Epoch 1:  84%|████████▍ | 1066/1271 [05:34<01:05,  3.11it/s, training_loss=0.967]\u001b[A\n",
            "Epoch 1:  84%|████████▍ | 1067/1271 [05:34<01:05,  3.12it/s, training_loss=0.967]\u001b[A\n",
            "Epoch 1:  84%|████████▍ | 1067/1271 [05:34<01:05,  3.12it/s, training_loss=1.004]\u001b[A\n",
            "Epoch 1:  84%|████████▍ | 1068/1271 [05:34<01:05,  3.11it/s, training_loss=1.004]\u001b[A\n",
            "Epoch 1:  84%|████████▍ | 1068/1271 [05:35<01:05,  3.11it/s, training_loss=0.948]\u001b[A\n",
            "Epoch 1:  84%|████████▍ | 1069/1271 [05:35<01:05,  3.10it/s, training_loss=0.948]\u001b[A\n",
            "Epoch 1:  84%|████████▍ | 1069/1271 [05:35<01:05,  3.10it/s, training_loss=0.202]\u001b[A\n",
            "Epoch 1:  84%|████████▍ | 1070/1271 [05:35<01:04,  3.11it/s, training_loss=0.202]\u001b[A\n",
            "Epoch 1:  84%|████████▍ | 1070/1271 [05:35<01:04,  3.11it/s, training_loss=0.030]\u001b[A\n",
            "Epoch 1:  84%|████████▍ | 1071/1271 [05:35<01:04,  3.11it/s, training_loss=0.030]\u001b[A\n",
            "Epoch 1:  84%|████████▍ | 1071/1271 [05:36<01:04,  3.11it/s, training_loss=0.461]\u001b[A\n",
            "Epoch 1:  84%|████████▍ | 1072/1271 [05:36<01:03,  3.13it/s, training_loss=0.461]\u001b[A\n",
            "Epoch 1:  84%|████████▍ | 1072/1271 [05:36<01:03,  3.13it/s, training_loss=0.669]\u001b[A\n",
            "Epoch 1:  84%|████████▍ | 1073/1271 [05:36<01:03,  3.14it/s, training_loss=0.669]\u001b[A\n",
            "Epoch 1:  84%|████████▍ | 1073/1271 [05:36<01:03,  3.14it/s, training_loss=0.494]\u001b[A\n",
            "Epoch 1:  85%|████████▍ | 1074/1271 [05:36<01:02,  3.14it/s, training_loss=0.494]\u001b[A\n",
            "Epoch 1:  85%|████████▍ | 1074/1271 [05:37<01:02,  3.14it/s, training_loss=0.495]\u001b[A\n",
            "Epoch 1:  85%|████████▍ | 1075/1271 [05:37<01:02,  3.12it/s, training_loss=0.495]\u001b[A\n",
            "Epoch 1:  85%|████████▍ | 1075/1271 [05:37<01:02,  3.12it/s, training_loss=0.707]\u001b[A\n",
            "Epoch 1:  85%|████████▍ | 1076/1271 [05:37<01:02,  3.12it/s, training_loss=0.707]\u001b[A\n",
            "Epoch 1:  85%|████████▍ | 1076/1271 [05:37<01:02,  3.12it/s, training_loss=0.496]\u001b[A\n",
            "Epoch 1:  85%|████████▍ | 1077/1271 [05:37<01:02,  3.13it/s, training_loss=0.496]\u001b[A\n",
            "Epoch 1:  85%|████████▍ | 1077/1271 [05:37<01:02,  3.13it/s, training_loss=0.411]\u001b[A\n",
            "Epoch 1:  85%|████████▍ | 1078/1271 [05:37<01:01,  3.13it/s, training_loss=0.411]\u001b[A\n",
            "Epoch 1:  85%|████████▍ | 1078/1271 [05:38<01:01,  3.13it/s, training_loss=1.033]\u001b[A\n",
            "Epoch 1:  85%|████████▍ | 1079/1271 [05:38<01:01,  3.13it/s, training_loss=1.033]\u001b[A\n",
            "Epoch 1:  85%|████████▍ | 1079/1271 [05:38<01:01,  3.13it/s, training_loss=0.179]\u001b[A\n",
            "Epoch 1:  85%|████████▍ | 1080/1271 [05:38<01:01,  3.12it/s, training_loss=0.179]\u001b[A\n",
            "Epoch 1:  85%|████████▍ | 1080/1271 [05:38<01:01,  3.12it/s, training_loss=0.597]\u001b[A\n",
            "Epoch 1:  85%|████████▌ | 1081/1271 [05:38<01:00,  3.13it/s, training_loss=0.597]\u001b[A\n",
            "Epoch 1:  85%|████████▌ | 1081/1271 [05:39<01:00,  3.13it/s, training_loss=0.197]\u001b[A\n",
            "Epoch 1:  85%|████████▌ | 1082/1271 [05:39<01:00,  3.13it/s, training_loss=0.197]\u001b[A\n",
            "Epoch 1:  85%|████████▌ | 1082/1271 [05:39<01:00,  3.13it/s, training_loss=0.780]\u001b[A\n",
            "Epoch 1:  85%|████████▌ | 1083/1271 [05:39<00:59,  3.13it/s, training_loss=0.780]\u001b[A\n",
            "Epoch 1:  85%|████████▌ | 1083/1271 [05:39<00:59,  3.13it/s, training_loss=0.577]\u001b[A\n",
            "Epoch 1:  85%|████████▌ | 1084/1271 [05:39<01:00,  3.11it/s, training_loss=0.577]\u001b[A\n",
            "Epoch 1:  85%|████████▌ | 1084/1271 [05:40<01:00,  3.11it/s, training_loss=0.920]\u001b[A\n",
            "Epoch 1:  85%|████████▌ | 1085/1271 [05:40<00:59,  3.11it/s, training_loss=0.920]\u001b[A\n",
            "Epoch 1:  85%|████████▌ | 1085/1271 [05:40<00:59,  3.11it/s, training_loss=0.350]\u001b[A\n",
            "Epoch 1:  85%|████████▌ | 1086/1271 [05:40<00:59,  3.11it/s, training_loss=0.350]\u001b[A\n",
            "Epoch 1:  85%|████████▌ | 1086/1271 [05:40<00:59,  3.11it/s, training_loss=0.951]\u001b[A\n",
            "Epoch 1:  86%|████████▌ | 1087/1271 [05:40<00:59,  3.11it/s, training_loss=0.951]\u001b[A\n",
            "Epoch 1:  86%|████████▌ | 1087/1271 [05:41<00:59,  3.11it/s, training_loss=0.642]\u001b[A\n",
            "Epoch 1:  86%|████████▌ | 1088/1271 [05:41<00:58,  3.11it/s, training_loss=0.642]\u001b[A\n",
            "Epoch 1:  86%|████████▌ | 1088/1271 [05:41<00:58,  3.11it/s, training_loss=0.864]\u001b[A\n",
            "Epoch 1:  86%|████████▌ | 1089/1271 [05:41<00:58,  3.12it/s, training_loss=0.864]\u001b[A\n",
            "Epoch 1:  86%|████████▌ | 1089/1271 [05:41<00:58,  3.12it/s, training_loss=0.392]\u001b[A\n",
            "Epoch 1:  86%|████████▌ | 1090/1271 [05:41<00:58,  3.12it/s, training_loss=0.392]\u001b[A\n",
            "Epoch 1:  86%|████████▌ | 1090/1271 [05:42<00:58,  3.12it/s, training_loss=0.524]\u001b[A\n",
            "Epoch 1:  86%|████████▌ | 1091/1271 [05:42<00:57,  3.12it/s, training_loss=0.524]\u001b[A\n",
            "Epoch 1:  86%|████████▌ | 1091/1271 [05:42<00:57,  3.12it/s, training_loss=0.201]\u001b[A\n",
            "Epoch 1:  86%|████████▌ | 1092/1271 [05:42<00:57,  3.11it/s, training_loss=0.201]\u001b[A\n",
            "Epoch 1:  86%|████████▌ | 1092/1271 [05:42<00:57,  3.11it/s, training_loss=0.278]\u001b[A\n",
            "Epoch 1:  86%|████████▌ | 1093/1271 [05:42<00:57,  3.12it/s, training_loss=0.278]\u001b[A\n",
            "Epoch 1:  86%|████████▌ | 1093/1271 [05:43<00:57,  3.12it/s, training_loss=0.403]\u001b[A\n",
            "Epoch 1:  86%|████████▌ | 1094/1271 [05:43<00:56,  3.12it/s, training_loss=0.403]\u001b[A\n",
            "Epoch 1:  86%|████████▌ | 1094/1271 [05:43<00:56,  3.12it/s, training_loss=0.654]\u001b[A\n",
            "Epoch 1:  86%|████████▌ | 1095/1271 [05:43<00:56,  3.12it/s, training_loss=0.654]\u001b[A\n",
            "Epoch 1:  86%|████████▌ | 1095/1271 [05:43<00:56,  3.12it/s, training_loss=0.412]\u001b[A\n",
            "Epoch 1:  86%|████████▌ | 1096/1271 [05:43<00:56,  3.12it/s, training_loss=0.412]\u001b[A\n",
            "Epoch 1:  86%|████████▌ | 1096/1271 [05:44<00:56,  3.12it/s, training_loss=0.264]\u001b[A\n",
            "Epoch 1:  86%|████████▋ | 1097/1271 [05:44<00:55,  3.12it/s, training_loss=0.264]\u001b[A\n",
            "Epoch 1:  86%|████████▋ | 1097/1271 [05:44<00:55,  3.12it/s, training_loss=0.880]\u001b[A\n",
            "Epoch 1:  86%|████████▋ | 1098/1271 [05:44<00:55,  3.11it/s, training_loss=0.880]\u001b[A\n",
            "Epoch 1:  86%|████████▋ | 1098/1271 [05:44<00:55,  3.11it/s, training_loss=0.022]\u001b[A\n",
            "Epoch 1:  86%|████████▋ | 1099/1271 [05:44<00:55,  3.12it/s, training_loss=0.022]\u001b[A\n",
            "Epoch 1:  86%|████████▋ | 1099/1271 [05:45<00:55,  3.12it/s, training_loss=0.492]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 1100/1271 [05:45<00:54,  3.13it/s, training_loss=0.492]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 1100/1271 [05:45<00:54,  3.13it/s, training_loss=1.004]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 1101/1271 [05:45<00:54,  3.12it/s, training_loss=1.004]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 1101/1271 [05:45<00:54,  3.12it/s, training_loss=0.561]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 1102/1271 [05:45<00:53,  3.14it/s, training_loss=0.561]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 1102/1271 [05:45<00:53,  3.14it/s, training_loss=0.327]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 1103/1271 [05:46<00:53,  3.14it/s, training_loss=0.327]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 1103/1271 [05:46<00:53,  3.14it/s, training_loss=0.933]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 1104/1271 [05:46<00:53,  3.14it/s, training_loss=0.933]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 1104/1271 [05:46<00:53,  3.14it/s, training_loss=0.309]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 1105/1271 [05:46<00:52,  3.14it/s, training_loss=0.309]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 1105/1271 [05:46<00:52,  3.14it/s, training_loss=0.865]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 1106/1271 [05:46<00:52,  3.13it/s, training_loss=0.865]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 1106/1271 [05:47<00:52,  3.13it/s, training_loss=0.390]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 1107/1271 [05:47<00:52,  3.13it/s, training_loss=0.390]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 1107/1271 [05:47<00:52,  3.13it/s, training_loss=0.882]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 1108/1271 [05:47<00:52,  3.11it/s, training_loss=0.882]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 1108/1271 [05:47<00:52,  3.11it/s, training_loss=0.528]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 1109/1271 [05:47<00:52,  3.11it/s, training_loss=0.528]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 1109/1271 [05:48<00:52,  3.11it/s, training_loss=0.668]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 1110/1271 [05:48<00:51,  3.12it/s, training_loss=0.668]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 1110/1271 [05:48<00:51,  3.12it/s, training_loss=0.179]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 1111/1271 [05:48<00:51,  3.10it/s, training_loss=0.179]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 1111/1271 [05:48<00:51,  3.10it/s, training_loss=0.073]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 1112/1271 [05:48<00:51,  3.10it/s, training_loss=0.073]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 1112/1271 [05:49<00:51,  3.10it/s, training_loss=0.261]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 1113/1271 [05:49<00:50,  3.11it/s, training_loss=0.261]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 1113/1271 [05:49<00:50,  3.11it/s, training_loss=0.446]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 1114/1271 [05:49<00:50,  3.12it/s, training_loss=0.446]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 1114/1271 [05:49<00:50,  3.12it/s, training_loss=0.734]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 1115/1271 [05:49<00:49,  3.13it/s, training_loss=0.734]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 1115/1271 [05:50<00:49,  3.13it/s, training_loss=0.409]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 1116/1271 [05:50<00:49,  3.13it/s, training_loss=0.409]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 1116/1271 [05:50<00:49,  3.13it/s, training_loss=0.231]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 1117/1271 [05:50<00:49,  3.12it/s, training_loss=0.231]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 1117/1271 [05:50<00:49,  3.12it/s, training_loss=1.257]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 1118/1271 [05:50<00:48,  3.12it/s, training_loss=1.257]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 1118/1271 [05:51<00:48,  3.12it/s, training_loss=0.244]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 1119/1271 [05:51<00:48,  3.12it/s, training_loss=0.244]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 1119/1271 [05:51<00:48,  3.12it/s, training_loss=0.161]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 1120/1271 [05:51<00:48,  3.12it/s, training_loss=0.161]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 1120/1271 [05:51<00:48,  3.12it/s, training_loss=0.236]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 1121/1271 [05:51<00:48,  3.12it/s, training_loss=0.236]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 1121/1271 [05:52<00:48,  3.12it/s, training_loss=0.521]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 1122/1271 [05:52<00:47,  3.13it/s, training_loss=0.521]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 1122/1271 [05:52<00:47,  3.13it/s, training_loss=0.490]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 1123/1271 [05:52<00:47,  3.12it/s, training_loss=0.490]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 1123/1271 [05:52<00:47,  3.12it/s, training_loss=0.699]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 1124/1271 [05:52<00:47,  3.11it/s, training_loss=0.699]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 1124/1271 [05:53<00:47,  3.11it/s, training_loss=1.027]\u001b[A\n",
            "Epoch 1:  89%|████████▊ | 1125/1271 [05:53<00:46,  3.12it/s, training_loss=1.027]\u001b[A\n",
            "Epoch 1:  89%|████████▊ | 1125/1271 [05:53<00:46,  3.12it/s, training_loss=0.354]\u001b[A\n",
            "Epoch 1:  89%|████████▊ | 1126/1271 [05:53<00:46,  3.10it/s, training_loss=0.354]\u001b[A\n",
            "Epoch 1:  89%|████████▊ | 1126/1271 [05:53<00:46,  3.10it/s, training_loss=0.451]\u001b[A\n",
            "Epoch 1:  89%|████████▊ | 1127/1271 [05:53<00:46,  3.11it/s, training_loss=0.451]\u001b[A\n",
            "Epoch 1:  89%|████████▊ | 1127/1271 [05:54<00:46,  3.11it/s, training_loss=0.443]\u001b[A\n",
            "Epoch 1:  89%|████████▊ | 1128/1271 [05:54<00:46,  3.10it/s, training_loss=0.443]\u001b[A\n",
            "Epoch 1:  89%|████████▊ | 1128/1271 [05:54<00:46,  3.10it/s, training_loss=0.681]\u001b[A\n",
            "Epoch 1:  89%|████████▉ | 1129/1271 [05:54<00:45,  3.11it/s, training_loss=0.681]\u001b[A\n",
            "Epoch 1:  89%|████████▉ | 1129/1271 [05:54<00:45,  3.11it/s, training_loss=0.881]\u001b[A\n",
            "Epoch 1:  89%|████████▉ | 1130/1271 [05:54<00:45,  3.08it/s, training_loss=0.881]\u001b[A\n",
            "Epoch 1:  89%|████████▉ | 1130/1271 [05:54<00:45,  3.08it/s, training_loss=0.811]\u001b[A\n",
            "Epoch 1:  89%|████████▉ | 1131/1271 [05:54<00:45,  3.09it/s, training_loss=0.811]\u001b[A\n",
            "Epoch 1:  89%|████████▉ | 1131/1271 [05:55<00:45,  3.09it/s, training_loss=0.353]\u001b[A\n",
            "Epoch 1:  89%|████████▉ | 1132/1271 [05:55<00:45,  3.08it/s, training_loss=0.353]\u001b[A\n",
            "Epoch 1:  89%|████████▉ | 1132/1271 [05:55<00:45,  3.08it/s, training_loss=0.622]\u001b[A\n",
            "Epoch 1:  89%|████████▉ | 1133/1271 [05:55<00:44,  3.09it/s, training_loss=0.622]\u001b[A\n",
            "Epoch 1:  89%|████████▉ | 1133/1271 [05:55<00:44,  3.09it/s, training_loss=0.686]\u001b[A\n",
            "Epoch 1:  89%|████████▉ | 1134/1271 [05:55<00:44,  3.09it/s, training_loss=0.686]\u001b[A\n",
            "Epoch 1:  89%|████████▉ | 1134/1271 [05:56<00:44,  3.09it/s, training_loss=0.651]\u001b[A\n",
            "Epoch 1:  89%|████████▉ | 1135/1271 [05:56<00:43,  3.11it/s, training_loss=0.651]\u001b[A\n",
            "Epoch 1:  89%|████████▉ | 1135/1271 [05:56<00:43,  3.11it/s, training_loss=0.407]\u001b[A\n",
            "Epoch 1:  89%|████████▉ | 1136/1271 [05:56<00:43,  3.12it/s, training_loss=0.407]\u001b[A\n",
            "Epoch 1:  89%|████████▉ | 1136/1271 [05:56<00:43,  3.12it/s, training_loss=0.728]\u001b[A\n",
            "Epoch 1:  89%|████████▉ | 1137/1271 [05:56<00:42,  3.12it/s, training_loss=0.728]\u001b[A\n",
            "Epoch 1:  89%|████████▉ | 1137/1271 [05:57<00:42,  3.12it/s, training_loss=0.417]\u001b[A\n",
            "Epoch 1:  90%|████████▉ | 1138/1271 [05:57<00:42,  3.13it/s, training_loss=0.417]\u001b[A\n",
            "Epoch 1:  90%|████████▉ | 1138/1271 [05:57<00:42,  3.13it/s, training_loss=0.956]\u001b[A\n",
            "Epoch 1:  90%|████████▉ | 1139/1271 [05:57<00:42,  3.14it/s, training_loss=0.956]\u001b[A\n",
            "Epoch 1:  90%|████████▉ | 1139/1271 [05:57<00:42,  3.14it/s, training_loss=0.823]\u001b[A\n",
            "Epoch 1:  90%|████████▉ | 1140/1271 [05:57<00:42,  3.10it/s, training_loss=0.823]\u001b[A\n",
            "Epoch 1:  90%|████████▉ | 1140/1271 [05:58<00:42,  3.10it/s, training_loss=1.008]\u001b[A\n",
            "Epoch 1:  90%|████████▉ | 1141/1271 [05:58<00:41,  3.11it/s, training_loss=1.008]\u001b[A\n",
            "Epoch 1:  90%|████████▉ | 1141/1271 [05:58<00:41,  3.11it/s, training_loss=0.609]\u001b[A\n",
            "Epoch 1:  90%|████████▉ | 1142/1271 [05:58<00:41,  3.12it/s, training_loss=0.609]\u001b[A\n",
            "Epoch 1:  90%|████████▉ | 1142/1271 [05:58<00:41,  3.12it/s, training_loss=0.380]\u001b[A\n",
            "Epoch 1:  90%|████████▉ | 1143/1271 [05:58<00:40,  3.13it/s, training_loss=0.380]\u001b[A\n",
            "Epoch 1:  90%|████████▉ | 1143/1271 [05:59<00:40,  3.13it/s, training_loss=0.782]\u001b[A\n",
            "Epoch 1:  90%|█████████ | 1144/1271 [05:59<00:40,  3.13it/s, training_loss=0.782]\u001b[A\n",
            "Epoch 1:  90%|█████████ | 1144/1271 [05:59<00:40,  3.13it/s, training_loss=0.648]\u001b[A\n",
            "Epoch 1:  90%|█████████ | 1145/1271 [05:59<00:40,  3.13it/s, training_loss=0.648]\u001b[A\n",
            "Epoch 1:  90%|█████████ | 1145/1271 [05:59<00:40,  3.13it/s, training_loss=0.504]\u001b[A\n",
            "Epoch 1:  90%|█████████ | 1146/1271 [05:59<00:40,  3.11it/s, training_loss=0.504]\u001b[A\n",
            "Epoch 1:  90%|█████████ | 1146/1271 [06:00<00:40,  3.11it/s, training_loss=0.679]\u001b[A\n",
            "Epoch 1:  90%|█████████ | 1147/1271 [06:00<00:39,  3.12it/s, training_loss=0.679]\u001b[A\n",
            "Epoch 1:  90%|█████████ | 1147/1271 [06:00<00:39,  3.12it/s, training_loss=0.642]\u001b[A\n",
            "Epoch 1:  90%|█████████ | 1148/1271 [06:00<00:39,  3.12it/s, training_loss=0.642]\u001b[A\n",
            "Epoch 1:  90%|█████████ | 1148/1271 [06:00<00:39,  3.12it/s, training_loss=0.497]\u001b[A\n",
            "Epoch 1:  90%|█████████ | 1149/1271 [06:00<00:38,  3.13it/s, training_loss=0.497]\u001b[A\n",
            "Epoch 1:  90%|█████████ | 1149/1271 [06:01<00:38,  3.13it/s, training_loss=0.292]\u001b[A\n",
            "Epoch 1:  90%|█████████ | 1150/1271 [06:01<00:38,  3.14it/s, training_loss=0.292]\u001b[A\n",
            "Epoch 1:  90%|█████████ | 1150/1271 [06:01<00:38,  3.14it/s, training_loss=0.513]\u001b[A\n",
            "Epoch 1:  91%|█████████ | 1151/1271 [06:01<00:38,  3.14it/s, training_loss=0.513]\u001b[A\n",
            "Epoch 1:  91%|█████████ | 1151/1271 [06:01<00:38,  3.14it/s, training_loss=0.482]\u001b[A\n",
            "Epoch 1:  91%|█████████ | 1152/1271 [06:01<00:37,  3.13it/s, training_loss=0.482]\u001b[A\n",
            "Epoch 1:  91%|█████████ | 1152/1271 [06:02<00:37,  3.13it/s, training_loss=0.389]\u001b[A\n",
            "Epoch 1:  91%|█████████ | 1153/1271 [06:02<00:37,  3.13it/s, training_loss=0.389]\u001b[A\n",
            "Epoch 1:  91%|█████████ | 1153/1271 [06:02<00:37,  3.13it/s, training_loss=0.254]\u001b[A\n",
            "Epoch 1:  91%|█████████ | 1154/1271 [06:02<00:37,  3.14it/s, training_loss=0.254]\u001b[A\n",
            "Epoch 1:  91%|█████████ | 1154/1271 [06:02<00:37,  3.14it/s, training_loss=0.208]\u001b[A\n",
            "Epoch 1:  91%|█████████ | 1155/1271 [06:02<00:36,  3.14it/s, training_loss=0.208]\u001b[A\n",
            "Epoch 1:  91%|█████████ | 1155/1271 [06:02<00:36,  3.14it/s, training_loss=0.191]\u001b[A\n",
            "Epoch 1:  91%|█████████ | 1156/1271 [06:02<00:36,  3.14it/s, training_loss=0.191]\u001b[A\n",
            "Epoch 1:  91%|█████████ | 1156/1271 [06:03<00:36,  3.14it/s, training_loss=0.586]\u001b[A\n",
            "Epoch 1:  91%|█████████ | 1157/1271 [06:03<00:36,  3.14it/s, training_loss=0.586]\u001b[A\n",
            "Epoch 1:  91%|█████████ | 1157/1271 [06:03<00:36,  3.14it/s, training_loss=0.200]\u001b[A\n",
            "Epoch 1:  91%|█████████ | 1158/1271 [06:03<00:35,  3.15it/s, training_loss=0.200]\u001b[A\n",
            "Epoch 1:  91%|█████████ | 1158/1271 [06:03<00:35,  3.15it/s, training_loss=0.238]\u001b[A\n",
            "Epoch 1:  91%|█████████ | 1159/1271 [06:03<00:35,  3.16it/s, training_loss=0.238]\u001b[A\n",
            "Epoch 1:  91%|█████████ | 1159/1271 [06:04<00:35,  3.16it/s, training_loss=0.870]\u001b[A\n",
            "Epoch 1:  91%|█████████▏| 1160/1271 [06:04<00:35,  3.15it/s, training_loss=0.870]\u001b[A\n",
            "Epoch 1:  91%|█████████▏| 1160/1271 [06:04<00:35,  3.15it/s, training_loss=0.578]\u001b[A\n",
            "Epoch 1:  91%|█████████▏| 1161/1271 [06:04<00:34,  3.15it/s, training_loss=0.578]\u001b[A\n",
            "Epoch 1:  91%|█████████▏| 1161/1271 [06:04<00:34,  3.15it/s, training_loss=0.529]\u001b[A\n",
            "Epoch 1:  91%|█████████▏| 1162/1271 [06:04<00:34,  3.16it/s, training_loss=0.529]\u001b[A\n",
            "Epoch 1:  91%|█████████▏| 1162/1271 [06:05<00:34,  3.16it/s, training_loss=0.139]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 1163/1271 [06:05<00:34,  3.14it/s, training_loss=0.139]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 1163/1271 [06:05<00:34,  3.14it/s, training_loss=0.656]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 1164/1271 [06:05<00:34,  3.11it/s, training_loss=0.656]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 1164/1271 [06:05<00:34,  3.11it/s, training_loss=0.407]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 1165/1271 [06:05<00:34,  3.12it/s, training_loss=0.407]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 1165/1271 [06:06<00:34,  3.12it/s, training_loss=0.805]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 1166/1271 [06:06<00:33,  3.10it/s, training_loss=0.805]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 1166/1271 [06:06<00:33,  3.10it/s, training_loss=0.301]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 1167/1271 [06:06<00:33,  3.09it/s, training_loss=0.301]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 1167/1271 [06:06<00:33,  3.09it/s, training_loss=0.207]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 1168/1271 [06:06<00:33,  3.10it/s, training_loss=0.207]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 1168/1271 [06:07<00:33,  3.10it/s, training_loss=0.230]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 1169/1271 [06:07<00:32,  3.11it/s, training_loss=0.230]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 1169/1271 [06:07<00:32,  3.11it/s, training_loss=0.453]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 1170/1271 [06:07<00:32,  3.12it/s, training_loss=0.453]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 1170/1271 [06:07<00:32,  3.12it/s, training_loss=0.605]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 1171/1271 [06:07<00:31,  3.13it/s, training_loss=0.605]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 1171/1271 [06:08<00:31,  3.13it/s, training_loss=0.779]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 1172/1271 [06:08<00:31,  3.14it/s, training_loss=0.779]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 1172/1271 [06:08<00:31,  3.14it/s, training_loss=0.301]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 1173/1271 [06:08<00:31,  3.13it/s, training_loss=0.301]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 1173/1271 [06:08<00:31,  3.13it/s, training_loss=0.834]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 1174/1271 [06:08<00:31,  3.12it/s, training_loss=0.834]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 1174/1271 [06:09<00:31,  3.12it/s, training_loss=0.236]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 1175/1271 [06:09<00:30,  3.12it/s, training_loss=0.236]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 1175/1271 [06:09<00:30,  3.12it/s, training_loss=0.565]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 1176/1271 [06:09<00:30,  3.10it/s, training_loss=0.565]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 1176/1271 [06:09<00:30,  3.10it/s, training_loss=0.598]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 1177/1271 [06:09<00:30,  3.12it/s, training_loss=0.598]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 1177/1271 [06:10<00:30,  3.12it/s, training_loss=0.764]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 1178/1271 [06:10<00:29,  3.12it/s, training_loss=0.764]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 1178/1271 [06:10<00:29,  3.12it/s, training_loss=1.173]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 1179/1271 [06:10<00:29,  3.11it/s, training_loss=1.173]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 1179/1271 [06:10<00:29,  3.11it/s, training_loss=0.627]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 1180/1271 [06:10<00:29,  3.12it/s, training_loss=0.627]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 1180/1271 [06:10<00:29,  3.12it/s, training_loss=0.289]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 1181/1271 [06:10<00:28,  3.13it/s, training_loss=0.289]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 1181/1271 [06:11<00:28,  3.13it/s, training_loss=0.211]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 1182/1271 [06:11<00:28,  3.14it/s, training_loss=0.211]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 1182/1271 [06:11<00:28,  3.14it/s, training_loss=0.934]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 1183/1271 [06:11<00:28,  3.13it/s, training_loss=0.934]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 1183/1271 [06:11<00:28,  3.13it/s, training_loss=0.498]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 1184/1271 [06:11<00:27,  3.11it/s, training_loss=0.498]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 1184/1271 [06:12<00:27,  3.11it/s, training_loss=0.401]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 1185/1271 [06:12<00:27,  3.09it/s, training_loss=0.401]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 1185/1271 [06:12<00:27,  3.09it/s, training_loss=1.007]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 1186/1271 [06:12<00:27,  3.06it/s, training_loss=1.007]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 1186/1271 [06:12<00:27,  3.06it/s, training_loss=0.744]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 1187/1271 [06:12<00:27,  3.08it/s, training_loss=0.744]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 1187/1271 [06:13<00:27,  3.08it/s, training_loss=0.411]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 1188/1271 [06:13<00:26,  3.09it/s, training_loss=0.411]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 1188/1271 [06:13<00:26,  3.09it/s, training_loss=0.461]\u001b[A\n",
            "Epoch 1:  94%|█████████▎| 1189/1271 [06:13<00:26,  3.10it/s, training_loss=0.461]\u001b[A\n",
            "Epoch 1:  94%|█████████▎| 1189/1271 [06:13<00:26,  3.10it/s, training_loss=0.548]\u001b[A\n",
            "Epoch 1:  94%|█████████▎| 1190/1271 [06:13<00:26,  3.11it/s, training_loss=0.548]\u001b[A\n",
            "Epoch 1:  94%|█████████▎| 1190/1271 [06:14<00:26,  3.11it/s, training_loss=0.471]\u001b[A\n",
            "Epoch 1:  94%|█████████▎| 1191/1271 [06:14<00:25,  3.12it/s, training_loss=0.471]\u001b[A\n",
            "Epoch 1:  94%|█████████▎| 1191/1271 [06:14<00:25,  3.12it/s, training_loss=0.195]\u001b[A\n",
            "Epoch 1:  94%|█████████▍| 1192/1271 [06:14<00:25,  3.13it/s, training_loss=0.195]\u001b[A\n",
            "Epoch 1:  94%|█████████▍| 1192/1271 [06:14<00:25,  3.13it/s, training_loss=0.610]\u001b[A\n",
            "Epoch 1:  94%|█████████▍| 1193/1271 [06:14<00:24,  3.13it/s, training_loss=0.610]\u001b[A\n",
            "Epoch 1:  94%|█████████▍| 1193/1271 [06:15<00:24,  3.13it/s, training_loss=0.340]\u001b[A\n",
            "Epoch 1:  94%|█████████▍| 1194/1271 [06:15<00:24,  3.11it/s, training_loss=0.340]\u001b[A\n",
            "Epoch 1:  94%|█████████▍| 1194/1271 [06:15<00:24,  3.11it/s, training_loss=0.817]\u001b[A\n",
            "Epoch 1:  94%|█████████▍| 1195/1271 [06:15<00:24,  3.12it/s, training_loss=0.817]\u001b[A\n",
            "Epoch 1:  94%|█████████▍| 1195/1271 [06:15<00:24,  3.12it/s, training_loss=0.214]\u001b[A\n",
            "Epoch 1:  94%|█████████▍| 1196/1271 [06:15<00:24,  3.11it/s, training_loss=0.214]\u001b[A\n",
            "Epoch 1:  94%|█████████▍| 1196/1271 [06:16<00:24,  3.11it/s, training_loss=0.824]\u001b[A\n",
            "Epoch 1:  94%|█████████▍| 1197/1271 [06:16<00:23,  3.12it/s, training_loss=0.824]\u001b[A\n",
            "Epoch 1:  94%|█████████▍| 1197/1271 [06:16<00:23,  3.12it/s, training_loss=0.562]\u001b[A\n",
            "Epoch 1:  94%|█████████▍| 1198/1271 [06:16<00:23,  3.13it/s, training_loss=0.562]\u001b[A\n",
            "Epoch 1:  94%|█████████▍| 1198/1271 [06:16<00:23,  3.13it/s, training_loss=0.545]\u001b[A\n",
            "Epoch 1:  94%|█████████▍| 1199/1271 [06:16<00:23,  3.11it/s, training_loss=0.545]\u001b[A\n",
            "Epoch 1:  94%|█████████▍| 1199/1271 [06:17<00:23,  3.11it/s, training_loss=0.699]\u001b[A\n",
            "Epoch 1:  94%|█████████▍| 1200/1271 [06:17<00:22,  3.12it/s, training_loss=0.699]\u001b[A\n",
            "Epoch 1:  94%|█████████▍| 1200/1271 [06:17<00:22,  3.12it/s, training_loss=0.623]\u001b[A\n",
            "Epoch 1:  94%|█████████▍| 1201/1271 [06:17<00:22,  3.12it/s, training_loss=0.623]\u001b[A\n",
            "Epoch 1:  94%|█████████▍| 1201/1271 [06:17<00:22,  3.12it/s, training_loss=0.177]\u001b[A\n",
            "Epoch 1:  95%|█████████▍| 1202/1271 [06:17<00:22,  3.13it/s, training_loss=0.177]\u001b[A\n",
            "Epoch 1:  95%|█████████▍| 1202/1271 [06:18<00:22,  3.13it/s, training_loss=0.754]\u001b[A\n",
            "Epoch 1:  95%|█████████▍| 1203/1271 [06:18<00:21,  3.13it/s, training_loss=0.754]\u001b[A\n",
            "Epoch 1:  95%|█████████▍| 1203/1271 [06:18<00:21,  3.13it/s, training_loss=0.939]\u001b[A\n",
            "Epoch 1:  95%|█████████▍| 1204/1271 [06:18<00:21,  3.12it/s, training_loss=0.939]\u001b[A\n",
            "Epoch 1:  95%|█████████▍| 1204/1271 [06:18<00:21,  3.12it/s, training_loss=0.041]\u001b[A\n",
            "Epoch 1:  95%|█████████▍| 1205/1271 [06:18<00:21,  3.12it/s, training_loss=0.041]\u001b[A\n",
            "Epoch 1:  95%|█████████▍| 1205/1271 [06:19<00:21,  3.12it/s, training_loss=0.305]\u001b[A\n",
            "Epoch 1:  95%|█████████▍| 1206/1271 [06:19<00:20,  3.13it/s, training_loss=0.305]\u001b[A\n",
            "Epoch 1:  95%|█████████▍| 1206/1271 [06:19<00:20,  3.13it/s, training_loss=0.650]\u001b[A\n",
            "Epoch 1:  95%|█████████▍| 1207/1271 [06:19<00:20,  3.14it/s, training_loss=0.650]\u001b[A\n",
            "Epoch 1:  95%|█████████▍| 1207/1271 [06:19<00:20,  3.14it/s, training_loss=1.094]\u001b[A\n",
            "Epoch 1:  95%|█████████▌| 1208/1271 [06:19<00:20,  3.13it/s, training_loss=1.094]\u001b[A\n",
            "Epoch 1:  95%|█████████▌| 1208/1271 [06:19<00:20,  3.13it/s, training_loss=0.489]\u001b[A\n",
            "Epoch 1:  95%|█████████▌| 1209/1271 [06:19<00:19,  3.11it/s, training_loss=0.489]\u001b[A\n",
            "Epoch 1:  95%|█████████▌| 1209/1271 [06:20<00:19,  3.11it/s, training_loss=0.736]\u001b[A\n",
            "Epoch 1:  95%|█████████▌| 1210/1271 [06:20<00:19,  3.10it/s, training_loss=0.736]\u001b[A\n",
            "Epoch 1:  95%|█████████▌| 1210/1271 [06:20<00:19,  3.10it/s, training_loss=0.338]\u001b[A\n",
            "Epoch 1:  95%|█████████▌| 1211/1271 [06:20<00:19,  3.12it/s, training_loss=0.338]\u001b[A\n",
            "Epoch 1:  95%|█████████▌| 1211/1271 [06:20<00:19,  3.12it/s, training_loss=0.219]\u001b[A\n",
            "Epoch 1:  95%|█████████▌| 1212/1271 [06:20<00:18,  3.13it/s, training_loss=0.219]\u001b[A\n",
            "Epoch 1:  95%|█████████▌| 1212/1271 [06:21<00:18,  3.13it/s, training_loss=1.211]\u001b[A\n",
            "Epoch 1:  95%|█████████▌| 1213/1271 [06:21<00:18,  3.13it/s, training_loss=1.211]\u001b[A\n",
            "Epoch 1:  95%|█████████▌| 1213/1271 [06:21<00:18,  3.13it/s, training_loss=0.656]\u001b[A\n",
            "Epoch 1:  96%|█████████▌| 1214/1271 [06:21<00:18,  3.13it/s, training_loss=0.656]\u001b[A\n",
            "Epoch 1:  96%|█████████▌| 1214/1271 [06:21<00:18,  3.13it/s, training_loss=0.967]\u001b[A\n",
            "Epoch 1:  96%|█████████▌| 1215/1271 [06:21<00:17,  3.13it/s, training_loss=0.967]\u001b[A\n",
            "Epoch 1:  96%|█████████▌| 1215/1271 [06:22<00:17,  3.13it/s, training_loss=0.325]\u001b[A\n",
            "Epoch 1:  96%|█████████▌| 1216/1271 [06:22<00:17,  3.12it/s, training_loss=0.325]\u001b[A\n",
            "Epoch 1:  96%|█████████▌| 1216/1271 [06:22<00:17,  3.12it/s, training_loss=0.507]\u001b[A\n",
            "Epoch 1:  96%|█████████▌| 1217/1271 [06:22<00:17,  3.13it/s, training_loss=0.507]\u001b[A\n",
            "Epoch 1:  96%|█████████▌| 1217/1271 [06:22<00:17,  3.13it/s, training_loss=0.658]\u001b[A\n",
            "Epoch 1:  96%|█████████▌| 1218/1271 [06:22<00:16,  3.14it/s, training_loss=0.658]\u001b[A\n",
            "Epoch 1:  96%|█████████▌| 1218/1271 [06:23<00:16,  3.14it/s, training_loss=0.574]\u001b[A\n",
            "Epoch 1:  96%|█████████▌| 1219/1271 [06:23<00:16,  3.14it/s, training_loss=0.574]\u001b[A\n",
            "Epoch 1:  96%|█████████▌| 1219/1271 [06:23<00:16,  3.14it/s, training_loss=1.184]\u001b[A\n",
            "Epoch 1:  96%|█████████▌| 1220/1271 [06:23<00:16,  3.11it/s, training_loss=1.184]\u001b[A\n",
            "Epoch 1:  96%|█████████▌| 1220/1271 [06:23<00:16,  3.11it/s, training_loss=0.680]\u001b[A\n",
            "Epoch 1:  96%|█████████▌| 1221/1271 [06:23<00:16,  3.08it/s, training_loss=0.680]\u001b[A\n",
            "Epoch 1:  96%|█████████▌| 1221/1271 [06:24<00:16,  3.08it/s, training_loss=0.578]\u001b[A\n",
            "Epoch 1:  96%|█████████▌| 1222/1271 [06:24<00:15,  3.09it/s, training_loss=0.578]\u001b[A\n",
            "Epoch 1:  96%|█████████▌| 1222/1271 [06:24<00:15,  3.09it/s, training_loss=0.947]\u001b[A\n",
            "Epoch 1:  96%|█████████▌| 1223/1271 [06:24<00:15,  3.11it/s, training_loss=0.947]\u001b[A\n",
            "Epoch 1:  96%|█████████▌| 1223/1271 [06:24<00:15,  3.11it/s, training_loss=0.334]\u001b[A\n",
            "Epoch 1:  96%|█████████▋| 1224/1271 [06:24<00:15,  3.12it/s, training_loss=0.334]\u001b[A\n",
            "Epoch 1:  96%|█████████▋| 1224/1271 [06:25<00:15,  3.12it/s, training_loss=0.211]\u001b[A\n",
            "Epoch 1:  96%|█████████▋| 1225/1271 [06:25<00:14,  3.12it/s, training_loss=0.211]\u001b[A\n",
            "Epoch 1:  96%|█████████▋| 1225/1271 [06:25<00:14,  3.12it/s, training_loss=0.772]\u001b[A\n",
            "Epoch 1:  96%|█████████▋| 1226/1271 [06:25<00:14,  3.11it/s, training_loss=0.772]\u001b[A\n",
            "Epoch 1:  96%|█████████▋| 1226/1271 [06:25<00:14,  3.11it/s, training_loss=0.163]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 1227/1271 [06:25<00:14,  3.12it/s, training_loss=0.163]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 1227/1271 [06:26<00:14,  3.12it/s, training_loss=0.774]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 1228/1271 [06:26<00:13,  3.08it/s, training_loss=0.774]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 1228/1271 [06:26<00:13,  3.08it/s, training_loss=0.652]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 1229/1271 [06:26<00:13,  3.09it/s, training_loss=0.652]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 1229/1271 [06:26<00:13,  3.09it/s, training_loss=0.559]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 1230/1271 [06:26<00:13,  3.09it/s, training_loss=0.559]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 1230/1271 [06:27<00:13,  3.09it/s, training_loss=0.587]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 1231/1271 [06:27<00:12,  3.09it/s, training_loss=0.587]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 1231/1271 [06:27<00:12,  3.09it/s, training_loss=0.305]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 1232/1271 [06:27<00:12,  3.11it/s, training_loss=0.305]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 1232/1271 [06:27<00:12,  3.11it/s, training_loss=0.598]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 1233/1271 [06:27<00:12,  3.12it/s, training_loss=0.598]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 1233/1271 [06:28<00:12,  3.12it/s, training_loss=0.392]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 1234/1271 [06:28<00:11,  3.13it/s, training_loss=0.392]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 1234/1271 [06:28<00:11,  3.13it/s, training_loss=0.375]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 1235/1271 [06:28<00:11,  3.13it/s, training_loss=0.375]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 1235/1271 [06:28<00:11,  3.13it/s, training_loss=0.668]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 1236/1271 [06:28<00:11,  3.12it/s, training_loss=0.668]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 1236/1271 [06:28<00:11,  3.12it/s, training_loss=0.297]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 1237/1271 [06:28<00:10,  3.11it/s, training_loss=0.297]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 1237/1271 [06:29<00:10,  3.11it/s, training_loss=0.508]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 1238/1271 [06:29<00:10,  3.11it/s, training_loss=0.508]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 1238/1271 [06:29<00:10,  3.11it/s, training_loss=0.292]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 1239/1271 [06:29<00:10,  3.12it/s, training_loss=0.292]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 1239/1271 [06:29<00:10,  3.12it/s, training_loss=1.174]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 1240/1271 [06:29<00:09,  3.13it/s, training_loss=1.174]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 1240/1271 [06:30<00:09,  3.13it/s, training_loss=0.169]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 1241/1271 [06:30<00:09,  3.13it/s, training_loss=0.169]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 1241/1271 [06:30<00:09,  3.13it/s, training_loss=0.685]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 1242/1271 [06:30<00:09,  3.12it/s, training_loss=0.685]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 1242/1271 [06:30<00:09,  3.12it/s, training_loss=0.038]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 1243/1271 [06:30<00:08,  3.12it/s, training_loss=0.038]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 1243/1271 [06:31<00:08,  3.12it/s, training_loss=0.531]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 1244/1271 [06:31<00:08,  3.11it/s, training_loss=0.531]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 1244/1271 [06:31<00:08,  3.11it/s, training_loss=0.595]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 1245/1271 [06:31<00:08,  3.11it/s, training_loss=0.595]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 1245/1271 [06:31<00:08,  3.11it/s, training_loss=0.772]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 1246/1271 [06:31<00:08,  3.10it/s, training_loss=0.772]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 1246/1271 [06:32<00:08,  3.10it/s, training_loss=0.089]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 1247/1271 [06:32<00:07,  3.11it/s, training_loss=0.089]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 1247/1271 [06:32<00:07,  3.11it/s, training_loss=0.558]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 1248/1271 [06:32<00:07,  3.12it/s, training_loss=0.558]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 1248/1271 [06:32<00:07,  3.12it/s, training_loss=0.251]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 1249/1271 [06:32<00:07,  3.11it/s, training_loss=0.251]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 1249/1271 [06:33<00:07,  3.11it/s, training_loss=0.705]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 1250/1271 [06:33<00:06,  3.09it/s, training_loss=0.705]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 1250/1271 [06:33<00:06,  3.09it/s, training_loss=0.531]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 1251/1271 [06:33<00:06,  3.11it/s, training_loss=0.531]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 1251/1271 [06:33<00:06,  3.11it/s, training_loss=0.283]\u001b[A\n",
            "Epoch 1:  99%|█████████▊| 1252/1271 [06:33<00:06,  3.13it/s, training_loss=0.283]\u001b[A\n",
            "Epoch 1:  99%|█████████▊| 1252/1271 [06:34<00:06,  3.13it/s, training_loss=0.216]\u001b[A\n",
            "Epoch 1:  99%|█████████▊| 1253/1271 [06:34<00:05,  3.13it/s, training_loss=0.216]\u001b[A\n",
            "Epoch 1:  99%|█████████▊| 1253/1271 [06:34<00:05,  3.13it/s, training_loss=0.127]\u001b[A\n",
            "Epoch 1:  99%|█████████▊| 1254/1271 [06:34<00:05,  3.14it/s, training_loss=0.127]\u001b[A\n",
            "Epoch 1:  99%|█████████▊| 1254/1271 [06:34<00:05,  3.14it/s, training_loss=0.648]\u001b[A\n",
            "Epoch 1:  99%|█████████▊| 1255/1271 [06:34<00:05,  3.13it/s, training_loss=0.648]\u001b[A\n",
            "Epoch 1:  99%|█████████▊| 1255/1271 [06:35<00:05,  3.13it/s, training_loss=0.561]\u001b[A\n",
            "Epoch 1:  99%|█████████▉| 1256/1271 [06:35<00:04,  3.11it/s, training_loss=0.561]\u001b[A\n",
            "Epoch 1:  99%|█████████▉| 1256/1271 [06:35<00:04,  3.11it/s, training_loss=0.470]\u001b[A\n",
            "Epoch 1:  99%|█████████▉| 1257/1271 [06:35<00:04,  3.10it/s, training_loss=0.470]\u001b[A\n",
            "Epoch 1:  99%|█████████▉| 1257/1271 [06:35<00:04,  3.10it/s, training_loss=0.420]\u001b[A\n",
            "Epoch 1:  99%|█████████▉| 1258/1271 [06:35<00:04,  3.11it/s, training_loss=0.420]\u001b[A\n",
            "Epoch 1:  99%|█████████▉| 1258/1271 [06:36<00:04,  3.11it/s, training_loss=0.726]\u001b[A\n",
            "Epoch 1:  99%|█████████▉| 1259/1271 [06:36<00:03,  3.12it/s, training_loss=0.726]\u001b[A\n",
            "Epoch 1:  99%|█████████▉| 1259/1271 [06:36<00:03,  3.12it/s, training_loss=0.393]\u001b[A\n",
            "Epoch 1:  99%|█████████▉| 1260/1271 [06:36<00:03,  3.13it/s, training_loss=0.393]\u001b[A\n",
            "Epoch 1:  99%|█████████▉| 1260/1271 [06:36<00:03,  3.13it/s, training_loss=0.964]\u001b[A\n",
            "Epoch 1:  99%|█████████▉| 1261/1271 [06:36<00:03,  3.12it/s, training_loss=0.964]\u001b[A\n",
            "Epoch 1:  99%|█████████▉| 1261/1271 [06:36<00:03,  3.12it/s, training_loss=1.131]\u001b[A\n",
            "Epoch 1:  99%|█████████▉| 1262/1271 [06:37<00:02,  3.11it/s, training_loss=1.131]\u001b[A\n",
            "Epoch 1:  99%|█████████▉| 1262/1271 [06:37<00:02,  3.11it/s, training_loss=0.747]\u001b[A\n",
            "Epoch 1:  99%|█████████▉| 1263/1271 [06:37<00:02,  3.11it/s, training_loss=0.747]\u001b[A\n",
            "Epoch 1:  99%|█████████▉| 1263/1271 [06:37<00:02,  3.11it/s, training_loss=0.356]\u001b[A\n",
            "Epoch 1:  99%|█████████▉| 1264/1271 [06:37<00:02,  3.09it/s, training_loss=0.356]\u001b[A\n",
            "Epoch 1:  99%|█████████▉| 1264/1271 [06:37<00:02,  3.09it/s, training_loss=0.251]\u001b[A\n",
            "Epoch 1: 100%|█████████▉| 1265/1271 [06:37<00:01,  3.10it/s, training_loss=0.251]\u001b[A\n",
            "Epoch 1: 100%|█████████▉| 1265/1271 [06:38<00:01,  3.10it/s, training_loss=0.029]\u001b[A\n",
            "Epoch 1: 100%|█████████▉| 1266/1271 [06:38<00:01,  3.12it/s, training_loss=0.029]\u001b[A\n",
            "Epoch 1: 100%|█████████▉| 1266/1271 [06:38<00:01,  3.12it/s, training_loss=0.570]\u001b[A\n",
            "Epoch 1: 100%|█████████▉| 1267/1271 [06:38<00:01,  3.14it/s, training_loss=0.570]\u001b[A\n",
            "Epoch 1: 100%|█████████▉| 1267/1271 [06:38<00:01,  3.14it/s, training_loss=0.609]\u001b[A\n",
            "Epoch 1: 100%|█████████▉| 1268/1271 [06:38<00:00,  3.10it/s, training_loss=0.609]\u001b[A\n",
            "Epoch 1: 100%|█████████▉| 1268/1271 [06:39<00:00,  3.10it/s, training_loss=0.127]\u001b[A\n",
            "Epoch 1: 100%|█████████▉| 1269/1271 [06:39<00:00,  3.11it/s, training_loss=0.127]\u001b[A\n",
            "Epoch 1: 100%|█████████▉| 1269/1271 [06:39<00:00,  3.11it/s, training_loss=0.306]\u001b[A\n",
            "Epoch 1: 100%|█████████▉| 1270/1271 [06:39<00:00,  3.12it/s, training_loss=0.306]\u001b[A\n",
            "Epoch 1: 100%|█████████▉| 1270/1271 [06:39<00:00,  3.12it/s, training_loss=0.639]\u001b[A\n",
            "Epoch 1: 100%|██████████| 1271/1271 [06:39<00:00,  3.66it/s, training_loss=0.639]\u001b[A\n",
            "  0%|          | 0/5 [06:47<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1\n",
            "Training loss: 2.3098087324771845\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 1/5 [07:07<28:30, 427.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 1.4743896122939057\n",
            "F1 Score (Weighted): 0.6063206751832861\n",
            "Recall@5: 0.9019316493313522\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2:   0%|          | 0/1271 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:   0%|          | 0/1271 [00:00<?, ?it/s, training_loss=0.918]\u001b[A\n",
            "Epoch 2:   0%|          | 1/1271 [00:00<06:42,  3.15it/s, training_loss=0.918]\u001b[A\n",
            "Epoch 2:   0%|          | 1/1271 [00:00<06:42,  3.15it/s, training_loss=0.510]\u001b[A\n",
            "Epoch 2:   0%|          | 2/1271 [00:00<06:44,  3.14it/s, training_loss=0.510]\u001b[A\n",
            "Epoch 2:   0%|          | 2/1271 [00:00<06:44,  3.14it/s, training_loss=0.244]\u001b[A\n",
            "Epoch 2:   0%|          | 3/1271 [00:00<06:47,  3.11it/s, training_loss=0.244]\u001b[A\n",
            "Epoch 2:   0%|          | 3/1271 [00:01<06:47,  3.11it/s, training_loss=0.838]\u001b[A\n",
            "Epoch 2:   0%|          | 4/1271 [00:01<06:53,  3.06it/s, training_loss=0.838]\u001b[A\n",
            "Epoch 2:   0%|          | 4/1271 [00:01<06:53,  3.06it/s, training_loss=0.425]\u001b[A\n",
            "Epoch 2:   0%|          | 5/1271 [00:01<06:50,  3.08it/s, training_loss=0.425]\u001b[A\n",
            "Epoch 2:   0%|          | 5/1271 [00:01<06:50,  3.08it/s, training_loss=0.196]\u001b[A\n",
            "Epoch 2:   0%|          | 6/1271 [00:01<06:48,  3.10it/s, training_loss=0.196]\u001b[A\n",
            "Epoch 2:   0%|          | 6/1271 [00:02<06:48,  3.10it/s, training_loss=0.397]\u001b[A\n",
            "Epoch 2:   1%|          | 7/1271 [00:02<06:47,  3.10it/s, training_loss=0.397]\u001b[A\n",
            "Epoch 2:   1%|          | 7/1271 [00:02<06:47,  3.10it/s, training_loss=0.361]\u001b[A\n",
            "Epoch 2:   1%|          | 8/1271 [00:02<06:46,  3.11it/s, training_loss=0.361]\u001b[A\n",
            "Epoch 2:   1%|          | 8/1271 [00:02<06:46,  3.11it/s, training_loss=0.291]\u001b[A\n",
            "Epoch 2:   1%|          | 9/1271 [00:02<06:47,  3.10it/s, training_loss=0.291]\u001b[A\n",
            "Epoch 2:   1%|          | 9/1271 [00:03<06:47,  3.10it/s, training_loss=0.740]\u001b[A\n",
            "Epoch 2:   1%|          | 10/1271 [00:03<06:47,  3.09it/s, training_loss=0.740]\u001b[A\n",
            "Epoch 2:   1%|          | 10/1271 [00:03<06:47,  3.09it/s, training_loss=0.264]\u001b[A\n",
            "Epoch 2:   1%|          | 11/1271 [00:03<06:49,  3.08it/s, training_loss=0.264]\u001b[A\n",
            "Epoch 2:   1%|          | 11/1271 [00:03<06:49,  3.08it/s, training_loss=0.185]\u001b[A\n",
            "Epoch 2:   1%|          | 12/1271 [00:03<06:46,  3.10it/s, training_loss=0.185]\u001b[A\n",
            "Epoch 2:   1%|          | 12/1271 [00:04<06:46,  3.10it/s, training_loss=0.672]\u001b[A\n",
            "Epoch 2:   1%|          | 13/1271 [00:04<06:48,  3.08it/s, training_loss=0.672]\u001b[A\n",
            "Epoch 2:   1%|          | 13/1271 [00:04<06:48,  3.08it/s, training_loss=0.324]\u001b[A\n",
            "Epoch 2:   1%|          | 14/1271 [00:04<06:49,  3.07it/s, training_loss=0.324]\u001b[A\n",
            "Epoch 2:   1%|          | 14/1271 [00:04<06:49,  3.07it/s, training_loss=0.412]\u001b[A\n",
            "Epoch 2:   1%|          | 15/1271 [00:04<06:49,  3.06it/s, training_loss=0.412]\u001b[A\n",
            "Epoch 2:   1%|          | 15/1271 [00:05<06:49,  3.06it/s, training_loss=0.755]\u001b[A\n",
            "Epoch 2:   1%|▏         | 16/1271 [00:05<06:49,  3.06it/s, training_loss=0.755]\u001b[A\n",
            "Epoch 2:   1%|▏         | 16/1271 [00:05<06:49,  3.06it/s, training_loss=0.351]\u001b[A\n",
            "Epoch 2:   1%|▏         | 17/1271 [00:05<06:47,  3.08it/s, training_loss=0.351]\u001b[A\n",
            "Epoch 2:   1%|▏         | 17/1271 [00:05<06:47,  3.08it/s, training_loss=0.377]\u001b[A\n",
            "Epoch 2:   1%|▏         | 18/1271 [00:05<06:48,  3.07it/s, training_loss=0.377]\u001b[A\n",
            "Epoch 2:   1%|▏         | 18/1271 [00:06<06:48,  3.07it/s, training_loss=0.398]\u001b[A\n",
            "Epoch 2:   1%|▏         | 19/1271 [00:06<06:47,  3.07it/s, training_loss=0.398]\u001b[A\n",
            "Epoch 2:   1%|▏         | 19/1271 [00:06<06:47,  3.07it/s, training_loss=0.075]\u001b[A\n",
            "Epoch 2:   2%|▏         | 20/1271 [00:06<06:49,  3.06it/s, training_loss=0.075]\u001b[A\n",
            "Epoch 2:   2%|▏         | 20/1271 [00:06<06:49,  3.06it/s, training_loss=0.442]\u001b[A\n",
            "Epoch 2:   2%|▏         | 21/1271 [00:06<06:48,  3.06it/s, training_loss=0.442]\u001b[A\n",
            "Epoch 2:   2%|▏         | 21/1271 [00:07<06:48,  3.06it/s, training_loss=0.419]\u001b[A\n",
            "Epoch 2:   2%|▏         | 22/1271 [00:07<06:45,  3.08it/s, training_loss=0.419]\u001b[A\n",
            "Epoch 2:   2%|▏         | 22/1271 [00:07<06:45,  3.08it/s, training_loss=0.502]\u001b[A\n",
            "Epoch 2:   2%|▏         | 23/1271 [00:07<06:44,  3.08it/s, training_loss=0.502]\u001b[A\n",
            "Epoch 2:   2%|▏         | 23/1271 [00:07<06:44,  3.08it/s, training_loss=0.845]\u001b[A\n",
            "Epoch 2:   2%|▏         | 24/1271 [00:07<06:48,  3.05it/s, training_loss=0.845]\u001b[A\n",
            "Epoch 2:   2%|▏         | 24/1271 [00:08<06:48,  3.05it/s, training_loss=0.408]\u001b[A\n",
            "Epoch 2:   2%|▏         | 25/1271 [00:08<06:49,  3.04it/s, training_loss=0.408]\u001b[A\n",
            "Epoch 2:   2%|▏         | 25/1271 [00:08<06:49,  3.04it/s, training_loss=0.322]\u001b[A\n",
            "Epoch 2:   2%|▏         | 26/1271 [00:08<06:47,  3.05it/s, training_loss=0.322]\u001b[A\n",
            "Epoch 2:   2%|▏         | 26/1271 [00:08<06:47,  3.05it/s, training_loss=0.251]\u001b[A\n",
            "Epoch 2:   2%|▏         | 27/1271 [00:08<06:47,  3.05it/s, training_loss=0.251]\u001b[A\n",
            "Epoch 2:   2%|▏         | 27/1271 [00:09<06:47,  3.05it/s, training_loss=0.032]\u001b[A\n",
            "Epoch 2:   2%|▏         | 28/1271 [00:09<06:47,  3.05it/s, training_loss=0.032]\u001b[A\n",
            "Epoch 2:   2%|▏         | 28/1271 [00:09<06:47,  3.05it/s, training_loss=0.312]\u001b[A\n",
            "Epoch 2:   2%|▏         | 29/1271 [00:09<06:45,  3.06it/s, training_loss=0.312]\u001b[A\n",
            "Epoch 2:   2%|▏         | 29/1271 [00:09<06:45,  3.06it/s, training_loss=0.555]\u001b[A\n",
            "Epoch 2:   2%|▏         | 30/1271 [00:09<06:45,  3.06it/s, training_loss=0.555]\u001b[A\n",
            "Epoch 2:   2%|▏         | 30/1271 [00:10<06:45,  3.06it/s, training_loss=0.247]\u001b[A\n",
            "Epoch 2:   2%|▏         | 31/1271 [00:10<06:43,  3.07it/s, training_loss=0.247]\u001b[A\n",
            "Epoch 2:   2%|▏         | 31/1271 [00:10<06:43,  3.07it/s, training_loss=0.973]\u001b[A\n",
            "Epoch 2:   3%|▎         | 32/1271 [00:10<06:42,  3.07it/s, training_loss=0.973]\u001b[A\n",
            "Epoch 2:   3%|▎         | 32/1271 [00:10<06:42,  3.07it/s, training_loss=0.218]\u001b[A\n",
            "Epoch 2:   3%|▎         | 33/1271 [00:10<06:41,  3.09it/s, training_loss=0.218]\u001b[A\n",
            "Epoch 2:   3%|▎         | 33/1271 [00:11<06:41,  3.09it/s, training_loss=0.549]\u001b[A\n",
            "Epoch 2:   3%|▎         | 34/1271 [00:11<06:43,  3.07it/s, training_loss=0.549]\u001b[A\n",
            "Epoch 2:   3%|▎         | 34/1271 [00:11<06:43,  3.07it/s, training_loss=0.182]\u001b[A\n",
            "Epoch 2:   3%|▎         | 35/1271 [00:11<06:41,  3.08it/s, training_loss=0.182]\u001b[A\n",
            "Epoch 2:   3%|▎         | 35/1271 [00:11<06:41,  3.08it/s, training_loss=0.472]\u001b[A\n",
            "Epoch 2:   3%|▎         | 36/1271 [00:11<06:42,  3.07it/s, training_loss=0.472]\u001b[A\n",
            "Epoch 2:   3%|▎         | 36/1271 [00:12<06:42,  3.07it/s, training_loss=0.478]\u001b[A\n",
            "Epoch 2:   3%|▎         | 37/1271 [00:12<06:42,  3.07it/s, training_loss=0.478]\u001b[A\n",
            "Epoch 2:   3%|▎         | 37/1271 [00:12<06:42,  3.07it/s, training_loss=0.168]\u001b[A\n",
            "Epoch 2:   3%|▎         | 38/1271 [00:12<06:39,  3.09it/s, training_loss=0.168]\u001b[A\n",
            "Epoch 2:   3%|▎         | 38/1271 [00:12<06:39,  3.09it/s, training_loss=0.585]\u001b[A\n",
            "Epoch 2:   3%|▎         | 39/1271 [00:12<06:39,  3.09it/s, training_loss=0.585]\u001b[A\n",
            "Epoch 2:   3%|▎         | 39/1271 [00:12<06:39,  3.09it/s, training_loss=0.330]\u001b[A\n",
            "Epoch 2:   3%|▎         | 40/1271 [00:13<06:38,  3.09it/s, training_loss=0.330]\u001b[A\n",
            "Epoch 2:   3%|▎         | 40/1271 [00:13<06:38,  3.09it/s, training_loss=0.640]\u001b[A\n",
            "Epoch 2:   3%|▎         | 41/1271 [00:13<06:40,  3.07it/s, training_loss=0.640]\u001b[A\n",
            "Epoch 2:   3%|▎         | 41/1271 [00:13<06:40,  3.07it/s, training_loss=0.140]\u001b[A\n",
            "Epoch 2:   3%|▎         | 42/1271 [00:13<06:41,  3.06it/s, training_loss=0.140]\u001b[A\n",
            "Epoch 2:   3%|▎         | 42/1271 [00:13<06:41,  3.06it/s, training_loss=1.130]\u001b[A\n",
            "Epoch 2:   3%|▎         | 43/1271 [00:13<06:39,  3.07it/s, training_loss=1.130]\u001b[A\n",
            "Epoch 2:   3%|▎         | 43/1271 [00:14<06:39,  3.07it/s, training_loss=0.241]\u001b[A\n",
            "Epoch 2:   3%|▎         | 44/1271 [00:14<06:41,  3.06it/s, training_loss=0.241]\u001b[A\n",
            "Epoch 2:   3%|▎         | 44/1271 [00:14<06:41,  3.06it/s, training_loss=0.677]\u001b[A\n",
            "Epoch 2:   4%|▎         | 45/1271 [00:14<06:41,  3.05it/s, training_loss=0.677]\u001b[A\n",
            "Epoch 2:   4%|▎         | 45/1271 [00:14<06:41,  3.05it/s, training_loss=0.394]\u001b[A\n",
            "Epoch 2:   4%|▎         | 46/1271 [00:14<06:40,  3.06it/s, training_loss=0.394]\u001b[A\n",
            "Epoch 2:   4%|▎         | 46/1271 [00:15<06:40,  3.06it/s, training_loss=0.426]\u001b[A\n",
            "Epoch 2:   4%|▎         | 47/1271 [00:15<06:37,  3.08it/s, training_loss=0.426]\u001b[A\n",
            "Epoch 2:   4%|▎         | 47/1271 [00:15<06:37,  3.08it/s, training_loss=0.680]\u001b[A\n",
            "Epoch 2:   4%|▍         | 48/1271 [00:15<06:36,  3.08it/s, training_loss=0.680]\u001b[A\n",
            "Epoch 2:   4%|▍         | 48/1271 [00:15<06:36,  3.08it/s, training_loss=0.132]\u001b[A\n",
            "Epoch 2:   4%|▍         | 49/1271 [00:15<06:34,  3.10it/s, training_loss=0.132]\u001b[A\n",
            "Epoch 2:   4%|▍         | 49/1271 [00:16<06:34,  3.10it/s, training_loss=0.633]\u001b[A\n",
            "Epoch 2:   4%|▍         | 50/1271 [00:16<06:34,  3.09it/s, training_loss=0.633]\u001b[A\n",
            "Epoch 2:   4%|▍         | 50/1271 [00:16<06:34,  3.09it/s, training_loss=0.404]\u001b[A\n",
            "Epoch 2:   4%|▍         | 51/1271 [00:16<06:35,  3.09it/s, training_loss=0.404]\u001b[A\n",
            "Epoch 2:   4%|▍         | 51/1271 [00:16<06:35,  3.09it/s, training_loss=0.780]\u001b[A\n",
            "Epoch 2:   4%|▍         | 52/1271 [00:16<06:36,  3.07it/s, training_loss=0.780]\u001b[A\n",
            "Epoch 2:   4%|▍         | 52/1271 [00:17<06:36,  3.07it/s, training_loss=0.604]\u001b[A\n",
            "Epoch 2:   4%|▍         | 53/1271 [00:17<06:35,  3.08it/s, training_loss=0.604]\u001b[A\n",
            "Epoch 2:   4%|▍         | 53/1271 [00:17<06:35,  3.08it/s, training_loss=0.776]\u001b[A\n",
            "Epoch 2:   4%|▍         | 54/1271 [00:17<06:36,  3.07it/s, training_loss=0.776]\u001b[A\n",
            "Epoch 2:   4%|▍         | 54/1271 [00:17<06:36,  3.07it/s, training_loss=0.074]\u001b[A\n",
            "Epoch 2:   4%|▍         | 55/1271 [00:17<06:35,  3.08it/s, training_loss=0.074]\u001b[A\n",
            "Epoch 2:   4%|▍         | 55/1271 [00:18<06:35,  3.08it/s, training_loss=0.688]\u001b[A\n",
            "Epoch 2:   4%|▍         | 56/1271 [00:18<06:34,  3.08it/s, training_loss=0.688]\u001b[A\n",
            "Epoch 2:   4%|▍         | 56/1271 [00:18<06:34,  3.08it/s, training_loss=0.220]\u001b[A\n",
            "Epoch 2:   4%|▍         | 57/1271 [00:18<06:31,  3.10it/s, training_loss=0.220]\u001b[A\n",
            "Epoch 2:   4%|▍         | 57/1271 [00:18<06:31,  3.10it/s, training_loss=0.483]\u001b[A\n",
            "Epoch 2:   5%|▍         | 58/1271 [00:18<06:33,  3.09it/s, training_loss=0.483]\u001b[A\n",
            "Epoch 2:   5%|▍         | 58/1271 [00:19<06:33,  3.09it/s, training_loss=0.447]\u001b[A\n",
            "Epoch 2:   5%|▍         | 59/1271 [00:19<06:33,  3.08it/s, training_loss=0.447]\u001b[A\n",
            "Epoch 2:   5%|▍         | 59/1271 [00:19<06:33,  3.08it/s, training_loss=0.892]\u001b[A\n",
            "Epoch 2:   5%|▍         | 60/1271 [00:19<06:33,  3.08it/s, training_loss=0.892]\u001b[A\n",
            "Epoch 2:   5%|▍         | 60/1271 [00:19<06:33,  3.08it/s, training_loss=0.589]\u001b[A\n",
            "Epoch 2:   5%|▍         | 61/1271 [00:19<06:32,  3.08it/s, training_loss=0.589]\u001b[A\n",
            "Epoch 2:   5%|▍         | 61/1271 [00:20<06:32,  3.08it/s, training_loss=0.596]\u001b[A\n",
            "Epoch 2:   5%|▍         | 62/1271 [00:20<06:32,  3.08it/s, training_loss=0.596]\u001b[A\n",
            "Epoch 2:   5%|▍         | 62/1271 [00:20<06:32,  3.08it/s, training_loss=0.149]\u001b[A\n",
            "Epoch 2:   5%|▍         | 63/1271 [00:20<06:32,  3.08it/s, training_loss=0.149]\u001b[A\n",
            "Epoch 2:   5%|▍         | 63/1271 [00:20<06:32,  3.08it/s, training_loss=0.604]\u001b[A\n",
            "Epoch 2:   5%|▌         | 64/1271 [00:20<06:32,  3.07it/s, training_loss=0.604]\u001b[A\n",
            "Epoch 2:   5%|▌         | 64/1271 [00:21<06:32,  3.07it/s, training_loss=0.580]\u001b[A\n",
            "Epoch 2:   5%|▌         | 65/1271 [00:21<06:31,  3.08it/s, training_loss=0.580]\u001b[A\n",
            "Epoch 2:   5%|▌         | 65/1271 [00:21<06:31,  3.08it/s, training_loss=0.156]\u001b[A\n",
            "Epoch 2:   5%|▌         | 66/1271 [00:21<06:32,  3.07it/s, training_loss=0.156]\u001b[A\n",
            "Epoch 2:   5%|▌         | 66/1271 [00:21<06:32,  3.07it/s, training_loss=0.035]\u001b[A\n",
            "Epoch 2:   5%|▌         | 67/1271 [00:21<06:33,  3.06it/s, training_loss=0.035]\u001b[A\n",
            "Epoch 2:   5%|▌         | 67/1271 [00:22<06:33,  3.06it/s, training_loss=0.409]\u001b[A\n",
            "Epoch 2:   5%|▌         | 68/1271 [00:22<06:34,  3.05it/s, training_loss=0.409]\u001b[A\n",
            "Epoch 2:   5%|▌         | 68/1271 [00:22<06:34,  3.05it/s, training_loss=0.021]\u001b[A\n",
            "Epoch 2:   5%|▌         | 69/1271 [00:22<06:33,  3.05it/s, training_loss=0.021]\u001b[A\n",
            "Epoch 2:   5%|▌         | 69/1271 [00:22<06:33,  3.05it/s, training_loss=0.404]\u001b[A\n",
            "Epoch 2:   6%|▌         | 70/1271 [00:22<06:32,  3.06it/s, training_loss=0.404]\u001b[A\n",
            "Epoch 2:   6%|▌         | 70/1271 [00:23<06:32,  3.06it/s, training_loss=0.428]\u001b[A\n",
            "Epoch 2:   6%|▌         | 71/1271 [00:23<06:30,  3.08it/s, training_loss=0.428]\u001b[A\n",
            "Epoch 2:   6%|▌         | 71/1271 [00:23<06:30,  3.08it/s, training_loss=0.650]\u001b[A\n",
            "Epoch 2:   6%|▌         | 72/1271 [00:23<06:29,  3.08it/s, training_loss=0.650]\u001b[A\n",
            "Epoch 2:   6%|▌         | 72/1271 [00:23<06:29,  3.08it/s, training_loss=0.613]\u001b[A\n",
            "Epoch 2:   6%|▌         | 73/1271 [00:23<06:26,  3.10it/s, training_loss=0.613]\u001b[A\n",
            "Epoch 2:   6%|▌         | 73/1271 [00:24<06:26,  3.10it/s, training_loss=0.011]\u001b[A\n",
            "Epoch 2:   6%|▌         | 74/1271 [00:24<06:24,  3.11it/s, training_loss=0.011]\u001b[A\n",
            "Epoch 2:   6%|▌         | 74/1271 [00:24<06:24,  3.11it/s, training_loss=0.456]\u001b[A\n",
            "Epoch 2:   6%|▌         | 75/1271 [00:24<06:27,  3.09it/s, training_loss=0.456]\u001b[A\n",
            "Epoch 2:   6%|▌         | 75/1271 [00:24<06:27,  3.09it/s, training_loss=0.751]\u001b[A\n",
            "Epoch 2:   6%|▌         | 76/1271 [00:24<06:28,  3.07it/s, training_loss=0.751]\u001b[A\n",
            "Epoch 2:   6%|▌         | 76/1271 [00:25<06:28,  3.07it/s, training_loss=0.979]\u001b[A\n",
            "Epoch 2:   6%|▌         | 77/1271 [00:25<06:28,  3.07it/s, training_loss=0.979]\u001b[A\n",
            "Epoch 2:   6%|▌         | 77/1271 [00:25<06:28,  3.07it/s, training_loss=0.347]\u001b[A\n",
            "Epoch 2:   6%|▌         | 78/1271 [00:25<06:27,  3.08it/s, training_loss=0.347]\u001b[A\n",
            "Epoch 2:   6%|▌         | 78/1271 [00:25<06:27,  3.08it/s, training_loss=0.872]\u001b[A\n",
            "Epoch 2:   6%|▌         | 79/1271 [00:25<06:28,  3.07it/s, training_loss=0.872]\u001b[A\n",
            "Epoch 2:   6%|▌         | 79/1271 [00:26<06:28,  3.07it/s, training_loss=0.772]\u001b[A\n",
            "Epoch 2:   6%|▋         | 80/1271 [00:26<06:30,  3.05it/s, training_loss=0.772]\u001b[A\n",
            "Epoch 2:   6%|▋         | 80/1271 [00:26<06:30,  3.05it/s, training_loss=0.353]\u001b[A\n",
            "Epoch 2:   6%|▋         | 81/1271 [00:26<06:29,  3.06it/s, training_loss=0.353]\u001b[A\n",
            "Epoch 2:   6%|▋         | 81/1271 [00:26<06:29,  3.06it/s, training_loss=0.848]\u001b[A\n",
            "Epoch 2:   6%|▋         | 82/1271 [00:26<06:25,  3.08it/s, training_loss=0.848]\u001b[A\n",
            "Epoch 2:   6%|▋         | 82/1271 [00:26<06:25,  3.08it/s, training_loss=1.026]\u001b[A\n",
            "Epoch 2:   7%|▋         | 83/1271 [00:26<06:26,  3.07it/s, training_loss=1.026]\u001b[A\n",
            "Epoch 2:   7%|▋         | 83/1271 [00:27<06:26,  3.07it/s, training_loss=0.809]\u001b[A\n",
            "Epoch 2:   7%|▋         | 84/1271 [00:27<06:24,  3.08it/s, training_loss=0.809]\u001b[A\n",
            "Epoch 2:   7%|▋         | 84/1271 [00:27<06:24,  3.08it/s, training_loss=1.027]\u001b[A\n",
            "Epoch 2:   7%|▋         | 85/1271 [00:27<06:23,  3.09it/s, training_loss=1.027]\u001b[A\n",
            "Epoch 2:   7%|▋         | 85/1271 [00:27<06:23,  3.09it/s, training_loss=0.661]\u001b[A\n",
            "Epoch 2:   7%|▋         | 86/1271 [00:27<06:22,  3.10it/s, training_loss=0.661]\u001b[A\n",
            "Epoch 2:   7%|▋         | 86/1271 [00:28<06:22,  3.10it/s, training_loss=0.532]\u001b[A\n",
            "Epoch 2:   7%|▋         | 87/1271 [00:28<06:22,  3.10it/s, training_loss=0.532]\u001b[A\n",
            "Epoch 2:   7%|▋         | 87/1271 [00:28<06:22,  3.10it/s, training_loss=0.297]\u001b[A\n",
            "Epoch 2:   7%|▋         | 88/1271 [00:28<06:22,  3.10it/s, training_loss=0.297]\u001b[A\n",
            "Epoch 2:   7%|▋         | 88/1271 [00:28<06:22,  3.10it/s, training_loss=0.437]\u001b[A\n",
            "Epoch 2:   7%|▋         | 89/1271 [00:28<06:20,  3.10it/s, training_loss=0.437]\u001b[A\n",
            "Epoch 2:   7%|▋         | 89/1271 [00:29<06:20,  3.10it/s, training_loss=0.480]\u001b[A\n",
            "Epoch 2:   7%|▋         | 90/1271 [00:29<06:22,  3.08it/s, training_loss=0.480]\u001b[A\n",
            "Epoch 2:   7%|▋         | 90/1271 [00:29<06:22,  3.08it/s, training_loss=0.949]\u001b[A\n",
            "Epoch 2:   7%|▋         | 91/1271 [00:29<06:21,  3.09it/s, training_loss=0.949]\u001b[A\n",
            "Epoch 2:   7%|▋         | 91/1271 [00:29<06:21,  3.09it/s, training_loss=0.179]\u001b[A\n",
            "Epoch 2:   7%|▋         | 92/1271 [00:29<06:20,  3.10it/s, training_loss=0.179]\u001b[A\n",
            "Epoch 2:   7%|▋         | 92/1271 [00:30<06:20,  3.10it/s, training_loss=0.451]\u001b[A\n",
            "Epoch 2:   7%|▋         | 93/1271 [00:30<06:18,  3.11it/s, training_loss=0.451]\u001b[A\n",
            "Epoch 2:   7%|▋         | 93/1271 [00:30<06:18,  3.11it/s, training_loss=0.389]\u001b[A\n",
            "Epoch 2:   7%|▋         | 94/1271 [00:30<06:17,  3.12it/s, training_loss=0.389]\u001b[A\n",
            "Epoch 2:   7%|▋         | 94/1271 [00:30<06:17,  3.12it/s, training_loss=0.032]\u001b[A\n",
            "Epoch 2:   7%|▋         | 95/1271 [00:30<06:16,  3.12it/s, training_loss=0.032]\u001b[A\n",
            "Epoch 2:   7%|▋         | 95/1271 [00:31<06:16,  3.12it/s, training_loss=0.410]\u001b[A\n",
            "Epoch 2:   8%|▊         | 96/1271 [00:31<06:16,  3.12it/s, training_loss=0.410]\u001b[A\n",
            "Epoch 2:   8%|▊         | 96/1271 [00:31<06:16,  3.12it/s, training_loss=0.485]\u001b[A\n",
            "Epoch 2:   8%|▊         | 97/1271 [00:31<06:16,  3.12it/s, training_loss=0.485]\u001b[A\n",
            "Epoch 2:   8%|▊         | 97/1271 [00:31<06:16,  3.12it/s, training_loss=0.467]\u001b[A\n",
            "Epoch 2:   8%|▊         | 98/1271 [00:31<06:15,  3.12it/s, training_loss=0.467]\u001b[A\n",
            "Epoch 2:   8%|▊         | 98/1271 [00:32<06:15,  3.12it/s, training_loss=0.398]\u001b[A\n",
            "Epoch 2:   8%|▊         | 99/1271 [00:32<06:15,  3.12it/s, training_loss=0.398]\u001b[A\n",
            "Epoch 2:   8%|▊         | 99/1271 [00:32<06:15,  3.12it/s, training_loss=0.456]\u001b[A\n",
            "Epoch 2:   8%|▊         | 100/1271 [00:32<06:16,  3.11it/s, training_loss=0.456]\u001b[A\n",
            "Epoch 2:   8%|▊         | 100/1271 [00:32<06:16,  3.11it/s, training_loss=0.698]\u001b[A\n",
            "Epoch 2:   8%|▊         | 101/1271 [00:32<06:17,  3.10it/s, training_loss=0.698]\u001b[A\n",
            "Epoch 2:   8%|▊         | 101/1271 [00:33<06:17,  3.10it/s, training_loss=0.393]\u001b[A\n",
            "Epoch 2:   8%|▊         | 102/1271 [00:33<06:18,  3.09it/s, training_loss=0.393]\u001b[A\n",
            "Epoch 2:   8%|▊         | 102/1271 [00:33<06:18,  3.09it/s, training_loss=0.201]\u001b[A\n",
            "Epoch 2:   8%|▊         | 103/1271 [00:33<06:17,  3.10it/s, training_loss=0.201]\u001b[A\n",
            "Epoch 2:   8%|▊         | 103/1271 [00:33<06:17,  3.10it/s, training_loss=1.085]\u001b[A\n",
            "Epoch 2:   8%|▊         | 104/1271 [00:33<06:15,  3.11it/s, training_loss=1.085]\u001b[A\n",
            "Epoch 2:   8%|▊         | 104/1271 [00:34<06:15,  3.11it/s, training_loss=0.201]\u001b[A\n",
            "Epoch 2:   8%|▊         | 105/1271 [00:34<06:14,  3.12it/s, training_loss=0.201]\u001b[A\n",
            "Epoch 2:   8%|▊         | 105/1271 [00:34<06:14,  3.12it/s, training_loss=0.852]\u001b[A\n",
            "Epoch 2:   8%|▊         | 106/1271 [00:34<06:14,  3.11it/s, training_loss=0.852]\u001b[A\n",
            "Epoch 2:   8%|▊         | 106/1271 [00:34<06:14,  3.11it/s, training_loss=0.805]\u001b[A\n",
            "Epoch 2:   8%|▊         | 107/1271 [00:34<06:15,  3.10it/s, training_loss=0.805]\u001b[A\n",
            "Epoch 2:   8%|▊         | 107/1271 [00:35<06:15,  3.10it/s, training_loss=0.489]\u001b[A\n",
            "Epoch 2:   8%|▊         | 108/1271 [00:35<06:13,  3.11it/s, training_loss=0.489]\u001b[A\n",
            "Epoch 2:   8%|▊         | 108/1271 [00:35<06:13,  3.11it/s, training_loss=0.105]\u001b[A\n",
            "Epoch 2:   9%|▊         | 109/1271 [00:35<06:13,  3.11it/s, training_loss=0.105]\u001b[A\n",
            "Epoch 2:   9%|▊         | 109/1271 [00:35<06:13,  3.11it/s, training_loss=0.309]\u001b[A\n",
            "Epoch 2:   9%|▊         | 110/1271 [00:35<06:12,  3.11it/s, training_loss=0.309]\u001b[A\n",
            "Epoch 2:   9%|▊         | 110/1271 [00:35<06:12,  3.11it/s, training_loss=0.144]\u001b[A\n",
            "Epoch 2:   9%|▊         | 111/1271 [00:36<06:13,  3.11it/s, training_loss=0.144]\u001b[A\n",
            "Epoch 2:   9%|▊         | 111/1271 [00:36<06:13,  3.11it/s, training_loss=0.371]\u001b[A\n",
            "Epoch 2:   9%|▉         | 112/1271 [00:36<06:15,  3.08it/s, training_loss=0.371]\u001b[A\n",
            "Epoch 2:   9%|▉         | 112/1271 [00:36<06:15,  3.08it/s, training_loss=0.510]\u001b[A\n",
            "Epoch 2:   9%|▉         | 113/1271 [00:36<06:14,  3.09it/s, training_loss=0.510]\u001b[A\n",
            "Epoch 2:   9%|▉         | 113/1271 [00:36<06:14,  3.09it/s, training_loss=0.267]\u001b[A\n",
            "Epoch 2:   9%|▉         | 114/1271 [00:36<06:14,  3.09it/s, training_loss=0.267]\u001b[A\n",
            "Epoch 2:   9%|▉         | 114/1271 [00:37<06:14,  3.09it/s, training_loss=0.213]\u001b[A\n",
            "Epoch 2:   9%|▉         | 115/1271 [00:37<06:12,  3.10it/s, training_loss=0.213]\u001b[A\n",
            "Epoch 2:   9%|▉         | 115/1271 [00:37<06:12,  3.10it/s, training_loss=0.826]\u001b[A\n",
            "Epoch 2:   9%|▉         | 116/1271 [00:37<06:12,  3.10it/s, training_loss=0.826]\u001b[A\n",
            "Epoch 2:   9%|▉         | 116/1271 [00:37<06:12,  3.10it/s, training_loss=0.332]\u001b[A\n",
            "Epoch 2:   9%|▉         | 117/1271 [00:37<06:11,  3.10it/s, training_loss=0.332]\u001b[A\n",
            "Epoch 2:   9%|▉         | 117/1271 [00:38<06:11,  3.10it/s, training_loss=0.394]\u001b[A\n",
            "Epoch 2:   9%|▉         | 118/1271 [00:38<06:12,  3.10it/s, training_loss=0.394]\u001b[A\n",
            "Epoch 2:   9%|▉         | 118/1271 [00:38<06:12,  3.10it/s, training_loss=0.257]\u001b[A\n",
            "Epoch 2:   9%|▉         | 119/1271 [00:38<06:10,  3.11it/s, training_loss=0.257]\u001b[A\n",
            "Epoch 2:   9%|▉         | 119/1271 [00:38<06:10,  3.11it/s, training_loss=0.425]\u001b[A\n",
            "Epoch 2:   9%|▉         | 120/1271 [00:38<06:12,  3.09it/s, training_loss=0.425]\u001b[A\n",
            "Epoch 2:   9%|▉         | 120/1271 [00:39<06:12,  3.09it/s, training_loss=0.775]\u001b[A\n",
            "Epoch 2:  10%|▉         | 121/1271 [00:39<06:10,  3.10it/s, training_loss=0.775]\u001b[A\n",
            "Epoch 2:  10%|▉         | 121/1271 [00:39<06:10,  3.10it/s, training_loss=0.634]\u001b[A\n",
            "Epoch 2:  10%|▉         | 122/1271 [00:39<06:08,  3.12it/s, training_loss=0.634]\u001b[A\n",
            "Epoch 2:  10%|▉         | 122/1271 [00:39<06:08,  3.12it/s, training_loss=0.413]\u001b[A\n",
            "Epoch 2:  10%|▉         | 123/1271 [00:39<06:06,  3.13it/s, training_loss=0.413]\u001b[A\n",
            "Epoch 2:  10%|▉         | 123/1271 [00:40<06:06,  3.13it/s, training_loss=0.196]\u001b[A\n",
            "Epoch 2:  10%|▉         | 124/1271 [00:40<06:06,  3.13it/s, training_loss=0.196]\u001b[A\n",
            "Epoch 2:  10%|▉         | 124/1271 [00:40<06:06,  3.13it/s, training_loss=0.399]\u001b[A\n",
            "Epoch 2:  10%|▉         | 125/1271 [00:40<06:07,  3.12it/s, training_loss=0.399]\u001b[A\n",
            "Epoch 2:  10%|▉         | 125/1271 [00:40<06:07,  3.12it/s, training_loss=0.537]\u001b[A\n",
            "Epoch 2:  10%|▉         | 126/1271 [00:40<06:06,  3.13it/s, training_loss=0.537]\u001b[A\n",
            "Epoch 2:  10%|▉         | 126/1271 [00:41<06:06,  3.13it/s, training_loss=0.269]\u001b[A\n",
            "Epoch 2:  10%|▉         | 127/1271 [00:41<06:06,  3.12it/s, training_loss=0.269]\u001b[A\n",
            "Epoch 2:  10%|▉         | 127/1271 [00:41<06:06,  3.12it/s, training_loss=0.576]\u001b[A\n",
            "Epoch 2:  10%|█         | 128/1271 [00:41<06:06,  3.12it/s, training_loss=0.576]\u001b[A\n",
            "Epoch 2:  10%|█         | 128/1271 [00:41<06:06,  3.12it/s, training_loss=0.686]\u001b[A\n",
            "Epoch 2:  10%|█         | 129/1271 [00:41<06:05,  3.13it/s, training_loss=0.686]\u001b[A\n",
            "Epoch 2:  10%|█         | 129/1271 [00:42<06:05,  3.13it/s, training_loss=0.749]\u001b[A\n",
            "Epoch 2:  10%|█         | 130/1271 [00:42<06:06,  3.11it/s, training_loss=0.749]\u001b[A\n",
            "Epoch 2:  10%|█         | 130/1271 [00:42<06:06,  3.11it/s, training_loss=0.273]\u001b[A\n",
            "Epoch 2:  10%|█         | 131/1271 [00:42<06:06,  3.11it/s, training_loss=0.273]\u001b[A\n",
            "Epoch 2:  10%|█         | 131/1271 [00:42<06:06,  3.11it/s, training_loss=0.691]\u001b[A\n",
            "Epoch 2:  10%|█         | 132/1271 [00:42<06:04,  3.12it/s, training_loss=0.691]\u001b[A\n",
            "Epoch 2:  10%|█         | 132/1271 [00:43<06:04,  3.12it/s, training_loss=0.381]\u001b[A\n",
            "Epoch 2:  10%|█         | 133/1271 [00:43<06:03,  3.13it/s, training_loss=0.381]\u001b[A\n",
            "Epoch 2:  10%|█         | 133/1271 [00:43<06:03,  3.13it/s, training_loss=0.251]\u001b[A\n",
            "Epoch 2:  11%|█         | 134/1271 [00:43<06:02,  3.14it/s, training_loss=0.251]\u001b[A\n",
            "Epoch 2:  11%|█         | 134/1271 [00:43<06:02,  3.14it/s, training_loss=0.212]\u001b[A\n",
            "Epoch 2:  11%|█         | 135/1271 [00:43<06:02,  3.14it/s, training_loss=0.212]\u001b[A\n",
            "Epoch 2:  11%|█         | 135/1271 [00:44<06:02,  3.14it/s, training_loss=0.653]\u001b[A\n",
            "Epoch 2:  11%|█         | 136/1271 [00:44<06:02,  3.13it/s, training_loss=0.653]\u001b[A\n",
            "Epoch 2:  11%|█         | 136/1271 [00:44<06:02,  3.13it/s, training_loss=0.423]\u001b[A\n",
            "Epoch 2:  11%|█         | 137/1271 [00:44<06:02,  3.13it/s, training_loss=0.423]\u001b[A\n",
            "Epoch 2:  11%|█         | 137/1271 [00:44<06:02,  3.13it/s, training_loss=0.564]\u001b[A\n",
            "Epoch 2:  11%|█         | 138/1271 [00:44<06:02,  3.12it/s, training_loss=0.564]\u001b[A\n",
            "Epoch 2:  11%|█         | 138/1271 [00:44<06:02,  3.12it/s, training_loss=0.652]\u001b[A\n",
            "Epoch 2:  11%|█         | 139/1271 [00:44<06:02,  3.12it/s, training_loss=0.652]\u001b[A\n",
            "Epoch 2:  11%|█         | 139/1271 [00:45<06:02,  3.12it/s, training_loss=0.331]\u001b[A\n",
            "Epoch 2:  11%|█         | 140/1271 [00:45<06:01,  3.13it/s, training_loss=0.331]\u001b[A\n",
            "Epoch 2:  11%|█         | 140/1271 [00:45<06:01,  3.13it/s, training_loss=0.605]\u001b[A\n",
            "Epoch 2:  11%|█         | 141/1271 [00:45<06:00,  3.14it/s, training_loss=0.605]\u001b[A\n",
            "Epoch 2:  11%|█         | 141/1271 [00:45<06:00,  3.14it/s, training_loss=0.666]\u001b[A\n",
            "Epoch 2:  11%|█         | 142/1271 [00:45<06:02,  3.12it/s, training_loss=0.666]\u001b[A\n",
            "Epoch 2:  11%|█         | 142/1271 [00:46<06:02,  3.12it/s, training_loss=0.639]\u001b[A\n",
            "Epoch 2:  11%|█▏        | 143/1271 [00:46<06:03,  3.11it/s, training_loss=0.639]\u001b[A\n",
            "Epoch 2:  11%|█▏        | 143/1271 [00:46<06:03,  3.11it/s, training_loss=0.211]\u001b[A\n",
            "Epoch 2:  11%|█▏        | 144/1271 [00:46<06:00,  3.13it/s, training_loss=0.211]\u001b[A\n",
            "Epoch 2:  11%|█▏        | 144/1271 [00:46<06:00,  3.13it/s, training_loss=0.400]\u001b[A\n",
            "Epoch 2:  11%|█▏        | 145/1271 [00:46<06:01,  3.11it/s, training_loss=0.400]\u001b[A\n",
            "Epoch 2:  11%|█▏        | 145/1271 [00:47<06:01,  3.11it/s, training_loss=0.571]\u001b[A\n",
            "Epoch 2:  11%|█▏        | 146/1271 [00:47<06:00,  3.12it/s, training_loss=0.571]\u001b[A\n",
            "Epoch 2:  11%|█▏        | 146/1271 [00:47<06:00,  3.12it/s, training_loss=0.256]\u001b[A\n",
            "Epoch 2:  12%|█▏        | 147/1271 [00:47<06:01,  3.11it/s, training_loss=0.256]\u001b[A\n",
            "Epoch 2:  12%|█▏        | 147/1271 [00:47<06:01,  3.11it/s, training_loss=1.113]\u001b[A\n",
            "Epoch 2:  12%|█▏        | 148/1271 [00:47<06:00,  3.11it/s, training_loss=1.113]\u001b[A\n",
            "Epoch 2:  12%|█▏        | 148/1271 [00:48<06:00,  3.11it/s, training_loss=0.170]\u001b[A\n",
            "Epoch 2:  12%|█▏        | 149/1271 [00:48<05:58,  3.13it/s, training_loss=0.170]\u001b[A\n",
            "Epoch 2:  12%|█▏        | 149/1271 [00:48<05:58,  3.13it/s, training_loss=0.937]\u001b[A\n",
            "Epoch 2:  12%|█▏        | 150/1271 [00:48<05:58,  3.13it/s, training_loss=0.937]\u001b[A\n",
            "Epoch 2:  12%|█▏        | 150/1271 [00:48<05:58,  3.13it/s, training_loss=0.200]\u001b[A\n",
            "Epoch 2:  12%|█▏        | 151/1271 [00:48<05:55,  3.15it/s, training_loss=0.200]\u001b[A\n",
            "Epoch 2:  12%|█▏        | 151/1271 [00:49<05:55,  3.15it/s, training_loss=0.355]\u001b[A\n",
            "Epoch 2:  12%|█▏        | 152/1271 [00:49<05:54,  3.16it/s, training_loss=0.355]\u001b[A\n",
            "Epoch 2:  12%|█▏        | 152/1271 [00:49<05:54,  3.16it/s, training_loss=0.954]\u001b[A\n",
            "Epoch 2:  12%|█▏        | 153/1271 [00:49<05:53,  3.16it/s, training_loss=0.954]\u001b[A\n",
            "Epoch 2:  12%|█▏        | 153/1271 [00:49<05:53,  3.16it/s, training_loss=0.381]\u001b[A\n",
            "Epoch 2:  12%|█▏        | 154/1271 [00:49<05:53,  3.16it/s, training_loss=0.381]\u001b[A\n",
            "Epoch 2:  12%|█▏        | 154/1271 [00:50<05:53,  3.16it/s, training_loss=0.318]\u001b[A\n",
            "Epoch 2:  12%|█▏        | 155/1271 [00:50<05:54,  3.14it/s, training_loss=0.318]\u001b[A\n",
            "Epoch 2:  12%|█▏        | 155/1271 [00:50<05:54,  3.14it/s, training_loss=0.712]\u001b[A\n",
            "Epoch 2:  12%|█▏        | 156/1271 [00:50<05:54,  3.15it/s, training_loss=0.712]\u001b[A\n",
            "Epoch 2:  12%|█▏        | 156/1271 [00:50<05:54,  3.15it/s, training_loss=0.526]\u001b[A\n",
            "Epoch 2:  12%|█▏        | 157/1271 [00:50<05:56,  3.12it/s, training_loss=0.526]\u001b[A\n",
            "Epoch 2:  12%|█▏        | 157/1271 [00:51<05:56,  3.12it/s, training_loss=0.957]\u001b[A\n",
            "Epoch 2:  12%|█▏        | 158/1271 [00:51<05:58,  3.11it/s, training_loss=0.957]\u001b[A\n",
            "Epoch 2:  12%|█▏        | 158/1271 [00:51<05:58,  3.11it/s, training_loss=0.394]\u001b[A\n",
            "Epoch 2:  13%|█▎        | 159/1271 [00:51<05:56,  3.12it/s, training_loss=0.394]\u001b[A\n",
            "Epoch 2:  13%|█▎        | 159/1271 [00:51<05:56,  3.12it/s, training_loss=0.312]\u001b[A\n",
            "Epoch 2:  13%|█▎        | 160/1271 [00:51<05:55,  3.13it/s, training_loss=0.312]\u001b[A\n",
            "Epoch 2:  13%|█▎        | 160/1271 [00:52<05:55,  3.13it/s, training_loss=0.410]\u001b[A\n",
            "Epoch 2:  13%|█▎        | 161/1271 [00:52<05:55,  3.12it/s, training_loss=0.410]\u001b[A\n",
            "Epoch 2:  13%|█▎        | 161/1271 [00:52<05:55,  3.12it/s, training_loss=0.422]\u001b[A\n",
            "Epoch 2:  13%|█▎        | 162/1271 [00:52<05:55,  3.12it/s, training_loss=0.422]\u001b[A\n",
            "Epoch 2:  13%|█▎        | 162/1271 [00:52<05:55,  3.12it/s, training_loss=0.335]\u001b[A\n",
            "Epoch 2:  13%|█▎        | 163/1271 [00:52<05:55,  3.11it/s, training_loss=0.335]\u001b[A\n",
            "Epoch 2:  13%|█▎        | 163/1271 [00:52<05:55,  3.11it/s, training_loss=0.410]\u001b[A\n",
            "Epoch 2:  13%|█▎        | 164/1271 [00:52<05:55,  3.11it/s, training_loss=0.410]\u001b[A\n",
            "Epoch 2:  13%|█▎        | 164/1271 [00:53<05:55,  3.11it/s, training_loss=0.496]\u001b[A\n",
            "Epoch 2:  13%|█▎        | 165/1271 [00:53<05:56,  3.10it/s, training_loss=0.496]\u001b[A\n",
            "Epoch 2:  13%|█▎        | 165/1271 [00:53<05:56,  3.10it/s, training_loss=0.466]\u001b[A\n",
            "Epoch 2:  13%|█▎        | 166/1271 [00:53<05:56,  3.10it/s, training_loss=0.466]\u001b[A\n",
            "Epoch 2:  13%|█▎        | 166/1271 [00:53<05:56,  3.10it/s, training_loss=0.773]\u001b[A\n",
            "Epoch 2:  13%|█▎        | 167/1271 [00:53<05:56,  3.10it/s, training_loss=0.773]\u001b[A\n",
            "Epoch 2:  13%|█▎        | 167/1271 [00:54<05:56,  3.10it/s, training_loss=0.466]\u001b[A\n",
            "Epoch 2:  13%|█▎        | 168/1271 [00:54<05:54,  3.11it/s, training_loss=0.466]\u001b[A\n",
            "Epoch 2:  13%|█▎        | 168/1271 [00:54<05:54,  3.11it/s, training_loss=0.401]\u001b[A\n",
            "Epoch 2:  13%|█▎        | 169/1271 [00:54<05:53,  3.12it/s, training_loss=0.401]\u001b[A\n",
            "Epoch 2:  13%|█▎        | 169/1271 [00:54<05:53,  3.12it/s, training_loss=0.203]\u001b[A\n",
            "Epoch 2:  13%|█▎        | 170/1271 [00:54<05:52,  3.13it/s, training_loss=0.203]\u001b[A\n",
            "Epoch 2:  13%|█▎        | 170/1271 [00:55<05:52,  3.13it/s, training_loss=0.361]\u001b[A\n",
            "Epoch 2:  13%|█▎        | 171/1271 [00:55<05:52,  3.12it/s, training_loss=0.361]\u001b[A\n",
            "Epoch 2:  13%|█▎        | 171/1271 [00:55<05:52,  3.12it/s, training_loss=0.253]\u001b[A\n",
            "Epoch 2:  14%|█▎        | 172/1271 [00:55<05:52,  3.12it/s, training_loss=0.253]\u001b[A\n",
            "Epoch 2:  14%|█▎        | 172/1271 [00:55<05:52,  3.12it/s, training_loss=0.392]\u001b[A\n",
            "Epoch 2:  14%|█▎        | 173/1271 [00:55<05:51,  3.12it/s, training_loss=0.392]\u001b[A\n",
            "Epoch 2:  14%|█▎        | 173/1271 [00:56<05:51,  3.12it/s, training_loss=0.573]\u001b[A\n",
            "Epoch 2:  14%|█▎        | 174/1271 [00:56<05:55,  3.08it/s, training_loss=0.573]\u001b[A\n",
            "Epoch 2:  14%|█▎        | 174/1271 [00:56<05:55,  3.08it/s, training_loss=0.437]\u001b[A\n",
            "Epoch 2:  14%|█▍        | 175/1271 [00:56<05:53,  3.10it/s, training_loss=0.437]\u001b[A\n",
            "Epoch 2:  14%|█▍        | 175/1271 [00:56<05:53,  3.10it/s, training_loss=0.875]\u001b[A\n",
            "Epoch 2:  14%|█▍        | 176/1271 [00:56<05:53,  3.10it/s, training_loss=0.875]\u001b[A\n",
            "Epoch 2:  14%|█▍        | 176/1271 [00:57<05:53,  3.10it/s, training_loss=0.294]\u001b[A\n",
            "Epoch 2:  14%|█▍        | 177/1271 [00:57<05:53,  3.09it/s, training_loss=0.294]\u001b[A\n",
            "Epoch 2:  14%|█▍        | 177/1271 [00:57<05:53,  3.09it/s, training_loss=0.258]\u001b[A\n",
            "Epoch 2:  14%|█▍        | 178/1271 [00:57<05:52,  3.10it/s, training_loss=0.258]\u001b[A\n",
            "Epoch 2:  14%|█▍        | 178/1271 [00:57<05:52,  3.10it/s, training_loss=0.504]\u001b[A\n",
            "Epoch 2:  14%|█▍        | 179/1271 [00:57<05:50,  3.11it/s, training_loss=0.504]\u001b[A\n",
            "Epoch 2:  14%|█▍        | 179/1271 [00:58<05:50,  3.11it/s, training_loss=0.641]\u001b[A\n",
            "Epoch 2:  14%|█▍        | 180/1271 [00:58<05:49,  3.13it/s, training_loss=0.641]\u001b[A\n",
            "Epoch 2:  14%|█▍        | 180/1271 [00:58<05:49,  3.13it/s, training_loss=0.758]\u001b[A\n",
            "Epoch 2:  14%|█▍        | 181/1271 [00:58<05:50,  3.11it/s, training_loss=0.758]\u001b[A\n",
            "Epoch 2:  14%|█▍        | 181/1271 [00:58<05:50,  3.11it/s, training_loss=0.614]\u001b[A\n",
            "Epoch 2:  14%|█▍        | 182/1271 [00:58<05:49,  3.12it/s, training_loss=0.614]\u001b[A\n",
            "Epoch 2:  14%|█▍        | 182/1271 [00:59<05:49,  3.12it/s, training_loss=0.435]\u001b[A\n",
            "Epoch 2:  14%|█▍        | 183/1271 [00:59<05:47,  3.13it/s, training_loss=0.435]\u001b[A\n",
            "Epoch 2:  14%|█▍        | 183/1271 [00:59<05:47,  3.13it/s, training_loss=0.501]\u001b[A\n",
            "Epoch 2:  14%|█▍        | 184/1271 [00:59<05:47,  3.12it/s, training_loss=0.501]\u001b[A\n",
            "Epoch 2:  14%|█▍        | 184/1271 [00:59<05:47,  3.12it/s, training_loss=0.616]\u001b[A\n",
            "Epoch 2:  15%|█▍        | 185/1271 [00:59<05:48,  3.12it/s, training_loss=0.616]\u001b[A\n",
            "Epoch 2:  15%|█▍        | 185/1271 [01:00<05:48,  3.12it/s, training_loss=0.416]\u001b[A\n",
            "Epoch 2:  15%|█▍        | 186/1271 [01:00<05:49,  3.11it/s, training_loss=0.416]\u001b[A\n",
            "Epoch 2:  15%|█▍        | 186/1271 [01:00<05:49,  3.11it/s, training_loss=0.886]\u001b[A\n",
            "Epoch 2:  15%|█▍        | 187/1271 [01:00<05:48,  3.11it/s, training_loss=0.886]\u001b[A\n",
            "Epoch 2:  15%|█▍        | 187/1271 [01:00<05:48,  3.11it/s, training_loss=0.810]\u001b[A\n",
            "Epoch 2:  15%|█▍        | 188/1271 [01:00<05:48,  3.11it/s, training_loss=0.810]\u001b[A\n",
            "Epoch 2:  15%|█▍        | 188/1271 [01:01<05:48,  3.11it/s, training_loss=0.351]\u001b[A\n",
            "Epoch 2:  15%|█▍        | 189/1271 [01:01<05:46,  3.12it/s, training_loss=0.351]\u001b[A\n",
            "Epoch 2:  15%|█▍        | 189/1271 [01:01<05:46,  3.12it/s, training_loss=0.363]\u001b[A\n",
            "Epoch 2:  15%|█▍        | 190/1271 [01:01<05:47,  3.11it/s, training_loss=0.363]\u001b[A\n",
            "Epoch 2:  15%|█▍        | 190/1271 [01:01<05:47,  3.11it/s, training_loss=0.656]\u001b[A\n",
            "Epoch 2:  15%|█▌        | 191/1271 [01:01<05:46,  3.12it/s, training_loss=0.656]\u001b[A\n",
            "Epoch 2:  15%|█▌        | 191/1271 [01:01<05:46,  3.12it/s, training_loss=0.177]\u001b[A\n",
            "Epoch 2:  15%|█▌        | 192/1271 [01:01<05:48,  3.10it/s, training_loss=0.177]\u001b[A\n",
            "Epoch 2:  15%|█▌        | 192/1271 [01:02<05:48,  3.10it/s, training_loss=0.910]\u001b[A\n",
            "Epoch 2:  15%|█▌        | 193/1271 [01:02<05:47,  3.10it/s, training_loss=0.910]\u001b[A\n",
            "Epoch 2:  15%|█▌        | 193/1271 [01:02<05:47,  3.10it/s, training_loss=0.385]\u001b[A\n",
            "Epoch 2:  15%|█▌        | 194/1271 [01:02<05:46,  3.11it/s, training_loss=0.385]\u001b[A\n",
            "Epoch 2:  15%|█▌        | 194/1271 [01:02<05:46,  3.11it/s, training_loss=0.467]\u001b[A\n",
            "Epoch 2:  15%|█▌        | 195/1271 [01:02<05:44,  3.13it/s, training_loss=0.467]\u001b[A\n",
            "Epoch 2:  15%|█▌        | 195/1271 [01:03<05:44,  3.13it/s, training_loss=0.415]\u001b[A\n",
            "Epoch 2:  15%|█▌        | 196/1271 [01:03<05:44,  3.12it/s, training_loss=0.415]\u001b[A\n",
            "Epoch 2:  15%|█▌        | 196/1271 [01:03<05:44,  3.12it/s, training_loss=0.710]\u001b[A\n",
            "Epoch 2:  15%|█▌        | 197/1271 [01:03<05:44,  3.12it/s, training_loss=0.710]\u001b[A\n",
            "Epoch 2:  15%|█▌        | 197/1271 [01:03<05:44,  3.12it/s, training_loss=0.962]\u001b[A\n",
            "Epoch 2:  16%|█▌        | 198/1271 [01:03<05:43,  3.12it/s, training_loss=0.962]\u001b[A\n",
            "Epoch 2:  16%|█▌        | 198/1271 [01:04<05:43,  3.12it/s, training_loss=0.627]\u001b[A\n",
            "Epoch 2:  16%|█▌        | 199/1271 [01:04<05:42,  3.13it/s, training_loss=0.627]\u001b[A\n",
            "Epoch 2:  16%|█▌        | 199/1271 [01:04<05:42,  3.13it/s, training_loss=0.757]\u001b[A\n",
            "Epoch 2:  16%|█▌        | 200/1271 [01:04<05:44,  3.11it/s, training_loss=0.757]\u001b[A\n",
            "Epoch 2:  16%|█▌        | 200/1271 [01:04<05:44,  3.11it/s, training_loss=0.728]\u001b[A\n",
            "Epoch 2:  16%|█▌        | 201/1271 [01:04<05:43,  3.11it/s, training_loss=0.728]\u001b[A\n",
            "Epoch 2:  16%|█▌        | 201/1271 [01:05<05:43,  3.11it/s, training_loss=0.501]\u001b[A\n",
            "Epoch 2:  16%|█▌        | 202/1271 [01:05<05:43,  3.11it/s, training_loss=0.501]\u001b[A\n",
            "Epoch 2:  16%|█▌        | 202/1271 [01:05<05:43,  3.11it/s, training_loss=0.544]\u001b[A\n",
            "Epoch 2:  16%|█▌        | 203/1271 [01:05<05:44,  3.10it/s, training_loss=0.544]\u001b[A\n",
            "Epoch 2:  16%|█▌        | 203/1271 [01:05<05:44,  3.10it/s, training_loss=0.372]\u001b[A\n",
            "Epoch 2:  16%|█▌        | 204/1271 [01:05<05:43,  3.11it/s, training_loss=0.372]\u001b[A\n",
            "Epoch 2:  16%|█▌        | 204/1271 [01:06<05:43,  3.11it/s, training_loss=0.164]\u001b[A\n",
            "Epoch 2:  16%|█▌        | 205/1271 [01:06<05:42,  3.11it/s, training_loss=0.164]\u001b[A\n",
            "Epoch 2:  16%|█▌        | 205/1271 [01:06<05:42,  3.11it/s, training_loss=0.432]\u001b[A\n",
            "Epoch 2:  16%|█▌        | 206/1271 [01:06<05:42,  3.11it/s, training_loss=0.432]\u001b[A\n",
            "Epoch 2:  16%|█▌        | 206/1271 [01:06<05:42,  3.11it/s, training_loss=0.968]\u001b[A\n",
            "Epoch 2:  16%|█▋        | 207/1271 [01:06<05:41,  3.12it/s, training_loss=0.968]\u001b[A\n",
            "Epoch 2:  16%|█▋        | 207/1271 [01:07<05:41,  3.12it/s, training_loss=0.390]\u001b[A\n",
            "Epoch 2:  16%|█▋        | 208/1271 [01:07<05:39,  3.13it/s, training_loss=0.390]\u001b[A\n",
            "Epoch 2:  16%|█▋        | 208/1271 [01:07<05:39,  3.13it/s, training_loss=0.957]\u001b[A\n",
            "Epoch 2:  16%|█▋        | 209/1271 [01:07<05:38,  3.14it/s, training_loss=0.957]\u001b[A\n",
            "Epoch 2:  16%|█▋        | 209/1271 [01:07<05:38,  3.14it/s, training_loss=0.687]\u001b[A\n",
            "Epoch 2:  17%|█▋        | 210/1271 [01:07<05:40,  3.11it/s, training_loss=0.687]\u001b[A\n",
            "Epoch 2:  17%|█▋        | 210/1271 [01:08<05:40,  3.11it/s, training_loss=0.152]\u001b[A\n",
            "Epoch 2:  17%|█▋        | 211/1271 [01:08<05:37,  3.14it/s, training_loss=0.152]\u001b[A\n",
            "Epoch 2:  17%|█▋        | 211/1271 [01:08<05:37,  3.14it/s, training_loss=0.755]\u001b[A\n",
            "Epoch 2:  17%|█▋        | 212/1271 [01:08<05:40,  3.11it/s, training_loss=0.755]\u001b[A\n",
            "Epoch 2:  17%|█▋        | 212/1271 [01:08<05:40,  3.11it/s, training_loss=0.255]\u001b[A\n",
            "Epoch 2:  17%|█▋        | 213/1271 [01:08<05:38,  3.12it/s, training_loss=0.255]\u001b[A\n",
            "Epoch 2:  17%|█▋        | 213/1271 [01:09<05:38,  3.12it/s, training_loss=0.459]\u001b[A\n",
            "Epoch 2:  17%|█▋        | 214/1271 [01:09<05:36,  3.14it/s, training_loss=0.459]\u001b[A\n",
            "Epoch 2:  17%|█▋        | 214/1271 [01:09<05:36,  3.14it/s, training_loss=0.189]\u001b[A\n",
            "Epoch 2:  17%|█▋        | 215/1271 [01:09<05:35,  3.15it/s, training_loss=0.189]\u001b[A\n",
            "Epoch 2:  17%|█▋        | 215/1271 [01:09<05:35,  3.15it/s, training_loss=0.534]\u001b[A\n",
            "Epoch 2:  17%|█▋        | 216/1271 [01:09<05:36,  3.14it/s, training_loss=0.534]\u001b[A\n",
            "Epoch 2:  17%|█▋        | 216/1271 [01:09<05:36,  3.14it/s, training_loss=0.252]\u001b[A\n",
            "Epoch 2:  17%|█▋        | 217/1271 [01:09<05:37,  3.13it/s, training_loss=0.252]\u001b[A\n",
            "Epoch 2:  17%|█▋        | 217/1271 [01:10<05:37,  3.13it/s, training_loss=0.361]\u001b[A\n",
            "Epoch 2:  17%|█▋        | 218/1271 [01:10<05:35,  3.14it/s, training_loss=0.361]\u001b[A\n",
            "Epoch 2:  17%|█▋        | 218/1271 [01:10<05:35,  3.14it/s, training_loss=0.437]\u001b[A\n",
            "Epoch 2:  17%|█▋        | 219/1271 [01:10<05:36,  3.12it/s, training_loss=0.437]\u001b[A\n",
            "Epoch 2:  17%|█▋        | 219/1271 [01:10<05:36,  3.12it/s, training_loss=0.021]\u001b[A\n",
            "Epoch 2:  17%|█▋        | 220/1271 [01:10<05:35,  3.13it/s, training_loss=0.021]\u001b[A\n",
            "Epoch 2:  17%|█▋        | 220/1271 [01:11<05:35,  3.13it/s, training_loss=0.256]\u001b[A\n",
            "Epoch 2:  17%|█▋        | 221/1271 [01:11<05:35,  3.13it/s, training_loss=0.256]\u001b[A\n",
            "Epoch 2:  17%|█▋        | 221/1271 [01:11<05:35,  3.13it/s, training_loss=0.481]\u001b[A\n",
            "Epoch 2:  17%|█▋        | 222/1271 [01:11<05:36,  3.12it/s, training_loss=0.481]\u001b[A\n",
            "Epoch 2:  17%|█▋        | 222/1271 [01:11<05:36,  3.12it/s, training_loss=0.248]\u001b[A\n",
            "Epoch 2:  18%|█▊        | 223/1271 [01:11<05:35,  3.12it/s, training_loss=0.248]\u001b[A\n",
            "Epoch 2:  18%|█▊        | 223/1271 [01:12<05:35,  3.12it/s, training_loss=0.355]\u001b[A\n",
            "Epoch 2:  18%|█▊        | 224/1271 [01:12<05:36,  3.11it/s, training_loss=0.355]\u001b[A\n",
            "Epoch 2:  18%|█▊        | 224/1271 [01:12<05:36,  3.11it/s, training_loss=0.253]\u001b[A\n",
            "Epoch 2:  18%|█▊        | 225/1271 [01:12<05:35,  3.11it/s, training_loss=0.253]\u001b[A\n",
            "Epoch 2:  18%|█▊        | 225/1271 [01:12<05:35,  3.11it/s, training_loss=0.294]\u001b[A\n",
            "Epoch 2:  18%|█▊        | 226/1271 [01:12<05:34,  3.13it/s, training_loss=0.294]\u001b[A\n",
            "Epoch 2:  18%|█▊        | 226/1271 [01:13<05:34,  3.13it/s, training_loss=0.403]\u001b[A\n",
            "Epoch 2:  18%|█▊        | 227/1271 [01:13<05:33,  3.13it/s, training_loss=0.403]\u001b[A\n",
            "Epoch 2:  18%|█▊        | 227/1271 [01:13<05:33,  3.13it/s, training_loss=0.420]\u001b[A\n",
            "Epoch 2:  18%|█▊        | 228/1271 [01:13<05:32,  3.14it/s, training_loss=0.420]\u001b[A\n",
            "Epoch 2:  18%|█▊        | 228/1271 [01:13<05:32,  3.14it/s, training_loss=0.502]\u001b[A\n",
            "Epoch 2:  18%|█▊        | 229/1271 [01:13<05:32,  3.14it/s, training_loss=0.502]\u001b[A\n",
            "Epoch 2:  18%|█▊        | 229/1271 [01:14<05:32,  3.14it/s, training_loss=1.029]\u001b[A\n",
            "Epoch 2:  18%|█▊        | 230/1271 [01:14<05:32,  3.13it/s, training_loss=1.029]\u001b[A\n",
            "Epoch 2:  18%|█▊        | 230/1271 [01:14<05:32,  3.13it/s, training_loss=0.769]\u001b[A\n",
            "Epoch 2:  18%|█▊        | 231/1271 [01:14<05:31,  3.13it/s, training_loss=0.769]\u001b[A\n",
            "Epoch 2:  18%|█▊        | 231/1271 [01:14<05:31,  3.13it/s, training_loss=0.669]\u001b[A\n",
            "Epoch 2:  18%|█▊        | 232/1271 [01:14<05:31,  3.13it/s, training_loss=0.669]\u001b[A\n",
            "Epoch 2:  18%|█▊        | 232/1271 [01:15<05:31,  3.13it/s, training_loss=0.449]\u001b[A\n",
            "Epoch 2:  18%|█▊        | 233/1271 [01:15<05:30,  3.14it/s, training_loss=0.449]\u001b[A\n",
            "Epoch 2:  18%|█▊        | 233/1271 [01:15<05:30,  3.14it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 2:  18%|█▊        | 234/1271 [01:15<05:31,  3.13it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 2:  18%|█▊        | 234/1271 [01:15<05:31,  3.13it/s, training_loss=0.096]\u001b[A\n",
            "Epoch 2:  18%|█▊        | 235/1271 [01:15<05:32,  3.11it/s, training_loss=0.096]\u001b[A\n",
            "Epoch 2:  18%|█▊        | 235/1271 [01:16<05:32,  3.11it/s, training_loss=0.854]\u001b[A\n",
            "Epoch 2:  19%|█▊        | 236/1271 [01:16<05:31,  3.12it/s, training_loss=0.854]\u001b[A\n",
            "Epoch 2:  19%|█▊        | 236/1271 [01:16<05:31,  3.12it/s, training_loss=0.105]\u001b[A\n",
            "Epoch 2:  19%|█▊        | 237/1271 [01:16<05:30,  3.13it/s, training_loss=0.105]\u001b[A\n",
            "Epoch 2:  19%|█▊        | 237/1271 [01:16<05:30,  3.13it/s, training_loss=0.787]\u001b[A\n",
            "Epoch 2:  19%|█▊        | 238/1271 [01:16<05:31,  3.12it/s, training_loss=0.787]\u001b[A\n",
            "Epoch 2:  19%|█▊        | 238/1271 [01:17<05:31,  3.12it/s, training_loss=0.212]\u001b[A\n",
            "Epoch 2:  19%|█▉        | 239/1271 [01:17<05:32,  3.11it/s, training_loss=0.212]\u001b[A\n",
            "Epoch 2:  19%|█▉        | 239/1271 [01:17<05:32,  3.11it/s, training_loss=0.989]\u001b[A\n",
            "Epoch 2:  19%|█▉        | 240/1271 [01:17<05:35,  3.08it/s, training_loss=0.989]\u001b[A\n",
            "Epoch 2:  19%|█▉        | 240/1271 [01:17<05:35,  3.08it/s, training_loss=0.618]\u001b[A\n",
            "Epoch 2:  19%|█▉        | 241/1271 [01:17<05:34,  3.08it/s, training_loss=0.618]\u001b[A\n",
            "Epoch 2:  19%|█▉        | 241/1271 [01:18<05:34,  3.08it/s, training_loss=0.403]\u001b[A\n",
            "Epoch 2:  19%|█▉        | 242/1271 [01:18<05:32,  3.09it/s, training_loss=0.403]\u001b[A\n",
            "Epoch 2:  19%|█▉        | 242/1271 [01:18<05:32,  3.09it/s, training_loss=0.345]\u001b[A\n",
            "Epoch 2:  19%|█▉        | 243/1271 [01:18<05:30,  3.11it/s, training_loss=0.345]\u001b[A\n",
            "Epoch 2:  19%|█▉        | 243/1271 [01:18<05:30,  3.11it/s, training_loss=0.409]\u001b[A\n",
            "Epoch 2:  19%|█▉        | 244/1271 [01:18<05:28,  3.12it/s, training_loss=0.409]\u001b[A\n",
            "Epoch 2:  19%|█▉        | 244/1271 [01:18<05:28,  3.12it/s, training_loss=0.726]\u001b[A\n",
            "Epoch 2:  19%|█▉        | 245/1271 [01:18<05:29,  3.11it/s, training_loss=0.726]\u001b[A\n",
            "Epoch 2:  19%|█▉        | 245/1271 [01:19<05:29,  3.11it/s, training_loss=0.593]\u001b[A\n",
            "Epoch 2:  19%|█▉        | 246/1271 [01:19<05:29,  3.11it/s, training_loss=0.593]\u001b[A\n",
            "Epoch 2:  19%|█▉        | 246/1271 [01:19<05:29,  3.11it/s, training_loss=0.818]\u001b[A\n",
            "Epoch 2:  19%|█▉        | 247/1271 [01:19<05:27,  3.13it/s, training_loss=0.818]\u001b[A\n",
            "Epoch 2:  19%|█▉        | 247/1271 [01:19<05:27,  3.13it/s, training_loss=0.499]\u001b[A\n",
            "Epoch 2:  20%|█▉        | 248/1271 [01:19<05:28,  3.12it/s, training_loss=0.499]\u001b[A\n",
            "Epoch 2:  20%|█▉        | 248/1271 [01:20<05:28,  3.12it/s, training_loss=0.477]\u001b[A\n",
            "Epoch 2:  20%|█▉        | 249/1271 [01:20<05:28,  3.11it/s, training_loss=0.477]\u001b[A\n",
            "Epoch 2:  20%|█▉        | 249/1271 [01:20<05:28,  3.11it/s, training_loss=0.998]\u001b[A\n",
            "Epoch 2:  20%|█▉        | 250/1271 [01:20<05:30,  3.09it/s, training_loss=0.998]\u001b[A\n",
            "Epoch 2:  20%|█▉        | 250/1271 [01:20<05:30,  3.09it/s, training_loss=0.210]\u001b[A\n",
            "Epoch 2:  20%|█▉        | 251/1271 [01:20<05:29,  3.10it/s, training_loss=0.210]\u001b[A\n",
            "Epoch 2:  20%|█▉        | 251/1271 [01:21<05:29,  3.10it/s, training_loss=0.422]\u001b[A\n",
            "Epoch 2:  20%|█▉        | 252/1271 [01:21<05:27,  3.11it/s, training_loss=0.422]\u001b[A\n",
            "Epoch 2:  20%|█▉        | 252/1271 [01:21<05:27,  3.11it/s, training_loss=0.814]\u001b[A\n",
            "Epoch 2:  20%|█▉        | 253/1271 [01:21<05:27,  3.11it/s, training_loss=0.814]\u001b[A\n",
            "Epoch 2:  20%|█▉        | 253/1271 [01:21<05:27,  3.11it/s, training_loss=0.680]\u001b[A\n",
            "Epoch 2:  20%|█▉        | 254/1271 [01:21<05:26,  3.11it/s, training_loss=0.680]\u001b[A\n",
            "Epoch 2:  20%|█▉        | 254/1271 [01:22<05:26,  3.11it/s, training_loss=0.403]\u001b[A\n",
            "Epoch 2:  20%|██        | 255/1271 [01:22<05:26,  3.11it/s, training_loss=0.403]\u001b[A\n",
            "Epoch 2:  20%|██        | 255/1271 [01:22<05:26,  3.11it/s, training_loss=0.309]\u001b[A\n",
            "Epoch 2:  20%|██        | 256/1271 [01:22<05:25,  3.12it/s, training_loss=0.309]\u001b[A\n",
            "Epoch 2:  20%|██        | 256/1271 [01:22<05:25,  3.12it/s, training_loss=0.287]\u001b[A\n",
            "Epoch 2:  20%|██        | 257/1271 [01:22<05:24,  3.13it/s, training_loss=0.287]\u001b[A\n",
            "Epoch 2:  20%|██        | 257/1271 [01:23<05:24,  3.13it/s, training_loss=0.192]\u001b[A\n",
            "Epoch 2:  20%|██        | 258/1271 [01:23<05:23,  3.13it/s, training_loss=0.192]\u001b[A\n",
            "Epoch 2:  20%|██        | 258/1271 [01:23<05:23,  3.13it/s, training_loss=0.149]\u001b[A\n",
            "Epoch 2:  20%|██        | 259/1271 [01:23<05:25,  3.11it/s, training_loss=0.149]\u001b[A\n",
            "Epoch 2:  20%|██        | 259/1271 [01:23<05:25,  3.11it/s, training_loss=0.056]\u001b[A\n",
            "Epoch 2:  20%|██        | 260/1271 [01:23<05:24,  3.11it/s, training_loss=0.056]\u001b[A\n",
            "Epoch 2:  20%|██        | 260/1271 [01:24<05:24,  3.11it/s, training_loss=0.455]\u001b[A\n",
            "Epoch 2:  21%|██        | 261/1271 [01:24<05:23,  3.12it/s, training_loss=0.455]\u001b[A\n",
            "Epoch 2:  21%|██        | 261/1271 [01:24<05:23,  3.12it/s, training_loss=0.700]\u001b[A\n",
            "Epoch 2:  21%|██        | 262/1271 [01:24<05:22,  3.13it/s, training_loss=0.700]\u001b[A\n",
            "Epoch 2:  21%|██        | 262/1271 [01:24<05:22,  3.13it/s, training_loss=0.281]\u001b[A\n",
            "Epoch 2:  21%|██        | 263/1271 [01:24<05:23,  3.11it/s, training_loss=0.281]\u001b[A\n",
            "Epoch 2:  21%|██        | 263/1271 [01:25<05:23,  3.11it/s, training_loss=0.548]\u001b[A\n",
            "Epoch 2:  21%|██        | 264/1271 [01:25<05:24,  3.10it/s, training_loss=0.548]\u001b[A\n",
            "Epoch 2:  21%|██        | 264/1271 [01:25<05:24,  3.10it/s, training_loss=0.274]\u001b[A\n",
            "Epoch 2:  21%|██        | 265/1271 [01:25<05:24,  3.10it/s, training_loss=0.274]\u001b[A\n",
            "Epoch 2:  21%|██        | 265/1271 [01:25<05:24,  3.10it/s, training_loss=0.293]\u001b[A\n",
            "Epoch 2:  21%|██        | 266/1271 [01:25<05:24,  3.10it/s, training_loss=0.293]\u001b[A\n",
            "Epoch 2:  21%|██        | 266/1271 [01:26<05:24,  3.10it/s, training_loss=0.276]\u001b[A\n",
            "Epoch 2:  21%|██        | 267/1271 [01:26<05:23,  3.11it/s, training_loss=0.276]\u001b[A\n",
            "Epoch 2:  21%|██        | 267/1271 [01:26<05:23,  3.11it/s, training_loss=0.576]\u001b[A\n",
            "Epoch 2:  21%|██        | 268/1271 [01:26<05:24,  3.09it/s, training_loss=0.576]\u001b[A\n",
            "Epoch 2:  21%|██        | 268/1271 [01:26<05:24,  3.09it/s, training_loss=0.017]\u001b[A\n",
            "Epoch 2:  21%|██        | 269/1271 [01:26<05:22,  3.11it/s, training_loss=0.017]\u001b[A\n",
            "Epoch 2:  21%|██        | 269/1271 [01:27<05:22,  3.11it/s, training_loss=0.884]\u001b[A\n",
            "Epoch 2:  21%|██        | 270/1271 [01:27<05:23,  3.09it/s, training_loss=0.884]\u001b[A\n",
            "Epoch 2:  21%|██        | 270/1271 [01:27<05:23,  3.09it/s, training_loss=1.052]\u001b[A\n",
            "Epoch 2:  21%|██▏       | 271/1271 [01:27<05:23,  3.09it/s, training_loss=1.052]\u001b[A\n",
            "Epoch 2:  21%|██▏       | 271/1271 [01:27<05:23,  3.09it/s, training_loss=0.317]\u001b[A\n",
            "Epoch 2:  21%|██▏       | 272/1271 [01:27<05:21,  3.11it/s, training_loss=0.317]\u001b[A\n",
            "Epoch 2:  21%|██▏       | 272/1271 [01:27<05:21,  3.11it/s, training_loss=0.638]\u001b[A\n",
            "Epoch 2:  21%|██▏       | 273/1271 [01:27<05:23,  3.08it/s, training_loss=0.638]\u001b[A\n",
            "Epoch 2:  21%|██▏       | 273/1271 [01:28<05:23,  3.08it/s, training_loss=0.471]\u001b[A\n",
            "Epoch 2:  22%|██▏       | 274/1271 [01:28<05:21,  3.10it/s, training_loss=0.471]\u001b[A\n",
            "Epoch 2:  22%|██▏       | 274/1271 [01:28<05:21,  3.10it/s, training_loss=0.526]\u001b[A\n",
            "Epoch 2:  22%|██▏       | 275/1271 [01:28<05:21,  3.10it/s, training_loss=0.526]\u001b[A\n",
            "Epoch 2:  22%|██▏       | 275/1271 [01:28<05:21,  3.10it/s, training_loss=0.493]\u001b[A\n",
            "Epoch 2:  22%|██▏       | 276/1271 [01:28<05:21,  3.09it/s, training_loss=0.493]\u001b[A\n",
            "Epoch 2:  22%|██▏       | 276/1271 [01:29<05:21,  3.09it/s, training_loss=0.524]\u001b[A\n",
            "Epoch 2:  22%|██▏       | 277/1271 [01:29<05:20,  3.10it/s, training_loss=0.524]\u001b[A\n",
            "Epoch 2:  22%|██▏       | 277/1271 [01:29<05:20,  3.10it/s, training_loss=0.476]\u001b[A\n",
            "Epoch 2:  22%|██▏       | 278/1271 [01:29<05:21,  3.09it/s, training_loss=0.476]\u001b[A\n",
            "Epoch 2:  22%|██▏       | 278/1271 [01:29<05:21,  3.09it/s, training_loss=0.738]\u001b[A\n",
            "Epoch 2:  22%|██▏       | 279/1271 [01:29<05:20,  3.09it/s, training_loss=0.738]\u001b[A\n",
            "Epoch 2:  22%|██▏       | 279/1271 [01:30<05:20,  3.09it/s, training_loss=0.342]\u001b[A\n",
            "Epoch 2:  22%|██▏       | 280/1271 [01:30<05:21,  3.08it/s, training_loss=0.342]\u001b[A\n",
            "Epoch 2:  22%|██▏       | 280/1271 [01:30<05:21,  3.08it/s, training_loss=0.169]\u001b[A\n",
            "Epoch 2:  22%|██▏       | 281/1271 [01:30<05:20,  3.09it/s, training_loss=0.169]\u001b[A\n",
            "Epoch 2:  22%|██▏       | 281/1271 [01:30<05:20,  3.09it/s, training_loss=0.209]\u001b[A\n",
            "Epoch 2:  22%|██▏       | 282/1271 [01:30<05:19,  3.10it/s, training_loss=0.209]\u001b[A\n",
            "Epoch 2:  22%|██▏       | 282/1271 [01:31<05:19,  3.10it/s, training_loss=0.788]\u001b[A\n",
            "Epoch 2:  22%|██▏       | 283/1271 [01:31<05:20,  3.08it/s, training_loss=0.788]\u001b[A\n",
            "Epoch 2:  22%|██▏       | 283/1271 [01:31<05:20,  3.08it/s, training_loss=0.574]\u001b[A\n",
            "Epoch 2:  22%|██▏       | 284/1271 [01:31<05:19,  3.09it/s, training_loss=0.574]\u001b[A\n",
            "Epoch 2:  22%|██▏       | 284/1271 [01:31<05:19,  3.09it/s, training_loss=0.376]\u001b[A\n",
            "Epoch 2:  22%|██▏       | 285/1271 [01:31<05:18,  3.10it/s, training_loss=0.376]\u001b[A\n",
            "Epoch 2:  22%|██▏       | 285/1271 [01:32<05:18,  3.10it/s, training_loss=0.448]\u001b[A\n",
            "Epoch 2:  23%|██▎       | 286/1271 [01:32<05:18,  3.10it/s, training_loss=0.448]\u001b[A\n",
            "Epoch 2:  23%|██▎       | 286/1271 [01:32<05:18,  3.10it/s, training_loss=0.031]\u001b[A\n",
            "Epoch 2:  23%|██▎       | 287/1271 [01:32<05:18,  3.09it/s, training_loss=0.031]\u001b[A\n",
            "Epoch 2:  23%|██▎       | 287/1271 [01:32<05:18,  3.09it/s, training_loss=0.511]\u001b[A\n",
            "Epoch 2:  23%|██▎       | 288/1271 [01:32<05:18,  3.09it/s, training_loss=0.511]\u001b[A\n",
            "Epoch 2:  23%|██▎       | 288/1271 [01:33<05:18,  3.09it/s, training_loss=0.225]\u001b[A\n",
            "Epoch 2:  23%|██▎       | 289/1271 [01:33<05:17,  3.09it/s, training_loss=0.225]\u001b[A\n",
            "Epoch 2:  23%|██▎       | 289/1271 [01:33<05:17,  3.09it/s, training_loss=0.969]\u001b[A\n",
            "Epoch 2:  23%|██▎       | 290/1271 [01:33<05:16,  3.10it/s, training_loss=0.969]\u001b[A\n",
            "Epoch 2:  23%|██▎       | 290/1271 [01:33<05:16,  3.10it/s, training_loss=0.392]\u001b[A\n",
            "Epoch 2:  23%|██▎       | 291/1271 [01:33<05:16,  3.10it/s, training_loss=0.392]\u001b[A\n",
            "Epoch 2:  23%|██▎       | 291/1271 [01:34<05:16,  3.10it/s, training_loss=0.679]\u001b[A\n",
            "Epoch 2:  23%|██▎       | 292/1271 [01:34<05:15,  3.10it/s, training_loss=0.679]\u001b[A\n",
            "Epoch 2:  23%|██▎       | 292/1271 [01:34<05:15,  3.10it/s, training_loss=0.598]\u001b[A\n",
            "Epoch 2:  23%|██▎       | 293/1271 [01:34<05:14,  3.11it/s, training_loss=0.598]\u001b[A\n",
            "Epoch 2:  23%|██▎       | 293/1271 [01:34<05:14,  3.11it/s, training_loss=0.172]\u001b[A\n",
            "Epoch 2:  23%|██▎       | 294/1271 [01:34<05:14,  3.11it/s, training_loss=0.172]\u001b[A\n",
            "Epoch 2:  23%|██▎       | 294/1271 [01:35<05:14,  3.11it/s, training_loss=0.759]\u001b[A\n",
            "Epoch 2:  23%|██▎       | 295/1271 [01:35<05:14,  3.10it/s, training_loss=0.759]\u001b[A\n",
            "Epoch 2:  23%|██▎       | 295/1271 [01:35<05:14,  3.10it/s, training_loss=0.025]\u001b[A\n",
            "Epoch 2:  23%|██▎       | 296/1271 [01:35<05:13,  3.11it/s, training_loss=0.025]\u001b[A\n",
            "Epoch 2:  23%|██▎       | 296/1271 [01:35<05:13,  3.11it/s, training_loss=0.021]\u001b[A\n",
            "Epoch 2:  23%|██▎       | 297/1271 [01:35<05:12,  3.11it/s, training_loss=0.021]\u001b[A\n",
            "Epoch 2:  23%|██▎       | 297/1271 [01:36<05:12,  3.11it/s, training_loss=0.860]\u001b[A\n",
            "Epoch 2:  23%|██▎       | 298/1271 [01:36<05:12,  3.11it/s, training_loss=0.860]\u001b[A\n",
            "Epoch 2:  23%|██▎       | 298/1271 [01:36<05:12,  3.11it/s, training_loss=0.478]\u001b[A\n",
            "Epoch 2:  24%|██▎       | 299/1271 [01:36<05:13,  3.10it/s, training_loss=0.478]\u001b[A\n",
            "Epoch 2:  24%|██▎       | 299/1271 [01:36<05:13,  3.10it/s, training_loss=0.869]\u001b[A\n",
            "Epoch 2:  24%|██▎       | 300/1271 [01:36<05:11,  3.12it/s, training_loss=0.869]\u001b[A\n",
            "Epoch 2:  24%|██▎       | 300/1271 [01:37<05:11,  3.12it/s, training_loss=0.511]\u001b[A\n",
            "Epoch 2:  24%|██▎       | 301/1271 [01:37<05:11,  3.11it/s, training_loss=0.511]\u001b[A\n",
            "Epoch 2:  24%|██▎       | 301/1271 [01:37<05:11,  3.11it/s, training_loss=0.327]\u001b[A\n",
            "Epoch 2:  24%|██▍       | 302/1271 [01:37<05:13,  3.09it/s, training_loss=0.327]\u001b[A\n",
            "Epoch 2:  24%|██▍       | 302/1271 [01:37<05:13,  3.09it/s, training_loss=0.556]\u001b[A\n",
            "Epoch 2:  24%|██▍       | 303/1271 [01:37<05:12,  3.10it/s, training_loss=0.556]\u001b[A\n",
            "Epoch 2:  24%|██▍       | 303/1271 [01:37<05:12,  3.10it/s, training_loss=0.388]\u001b[A\n",
            "Epoch 2:  24%|██▍       | 304/1271 [01:37<05:11,  3.10it/s, training_loss=0.388]\u001b[A\n",
            "Epoch 2:  24%|██▍       | 304/1271 [01:38<05:11,  3.10it/s, training_loss=0.712]\u001b[A\n",
            "Epoch 2:  24%|██▍       | 305/1271 [01:38<05:12,  3.09it/s, training_loss=0.712]\u001b[A\n",
            "Epoch 2:  24%|██▍       | 305/1271 [01:38<05:12,  3.09it/s, training_loss=0.970]\u001b[A\n",
            "Epoch 2:  24%|██▍       | 306/1271 [01:38<05:12,  3.09it/s, training_loss=0.970]\u001b[A\n",
            "Epoch 2:  24%|██▍       | 306/1271 [01:38<05:12,  3.09it/s, training_loss=0.580]\u001b[A\n",
            "Epoch 2:  24%|██▍       | 307/1271 [01:38<05:12,  3.09it/s, training_loss=0.580]\u001b[A\n",
            "Epoch 2:  24%|██▍       | 307/1271 [01:39<05:12,  3.09it/s, training_loss=0.245]\u001b[A\n",
            "Epoch 2:  24%|██▍       | 308/1271 [01:39<05:11,  3.09it/s, training_loss=0.245]\u001b[A\n",
            "Epoch 2:  24%|██▍       | 308/1271 [01:39<05:11,  3.09it/s, training_loss=0.088]\u001b[A\n",
            "Epoch 2:  24%|██▍       | 309/1271 [01:39<05:11,  3.09it/s, training_loss=0.088]\u001b[A\n",
            "Epoch 2:  24%|██▍       | 309/1271 [01:39<05:11,  3.09it/s, training_loss=0.614]\u001b[A\n",
            "Epoch 2:  24%|██▍       | 310/1271 [01:39<05:10,  3.10it/s, training_loss=0.614]\u001b[A\n",
            "Epoch 2:  24%|██▍       | 310/1271 [01:40<05:10,  3.10it/s, training_loss=0.408]\u001b[A\n",
            "Epoch 2:  24%|██▍       | 311/1271 [01:40<05:08,  3.11it/s, training_loss=0.408]\u001b[A\n",
            "Epoch 2:  24%|██▍       | 311/1271 [01:40<05:08,  3.11it/s, training_loss=0.399]\u001b[A\n",
            "Epoch 2:  25%|██▍       | 312/1271 [01:40<05:09,  3.10it/s, training_loss=0.399]\u001b[A\n",
            "Epoch 2:  25%|██▍       | 312/1271 [01:40<05:09,  3.10it/s, training_loss=0.434]\u001b[A\n",
            "Epoch 2:  25%|██▍       | 313/1271 [01:40<05:10,  3.09it/s, training_loss=0.434]\u001b[A\n",
            "Epoch 2:  25%|██▍       | 313/1271 [01:41<05:10,  3.09it/s, training_loss=0.598]\u001b[A\n",
            "Epoch 2:  25%|██▍       | 314/1271 [01:41<05:09,  3.09it/s, training_loss=0.598]\u001b[A\n",
            "Epoch 2:  25%|██▍       | 314/1271 [01:41<05:09,  3.09it/s, training_loss=0.703]\u001b[A\n",
            "Epoch 2:  25%|██▍       | 315/1271 [01:41<05:09,  3.09it/s, training_loss=0.703]\u001b[A\n",
            "Epoch 2:  25%|██▍       | 315/1271 [01:41<05:09,  3.09it/s, training_loss=0.641]\u001b[A\n",
            "Epoch 2:  25%|██▍       | 316/1271 [01:41<05:10,  3.07it/s, training_loss=0.641]\u001b[A\n",
            "Epoch 2:  25%|██▍       | 316/1271 [01:42<05:10,  3.07it/s, training_loss=0.447]\u001b[A\n",
            "Epoch 2:  25%|██▍       | 317/1271 [01:42<05:09,  3.08it/s, training_loss=0.447]\u001b[A\n",
            "Epoch 2:  25%|██▍       | 317/1271 [01:42<05:09,  3.08it/s, training_loss=0.041]\u001b[A\n",
            "Epoch 2:  25%|██▌       | 318/1271 [01:42<05:10,  3.06it/s, training_loss=0.041]\u001b[A\n",
            "Epoch 2:  25%|██▌       | 318/1271 [01:42<05:10,  3.06it/s, training_loss=0.056]\u001b[A\n",
            "Epoch 2:  25%|██▌       | 319/1271 [01:42<05:11,  3.05it/s, training_loss=0.056]\u001b[A\n",
            "Epoch 2:  25%|██▌       | 319/1271 [01:43<05:11,  3.05it/s, training_loss=0.736]\u001b[A\n",
            "Epoch 2:  25%|██▌       | 320/1271 [01:43<05:10,  3.06it/s, training_loss=0.736]\u001b[A\n",
            "Epoch 2:  25%|██▌       | 320/1271 [01:43<05:10,  3.06it/s, training_loss=0.447]\u001b[A\n",
            "Epoch 2:  25%|██▌       | 321/1271 [01:43<05:09,  3.07it/s, training_loss=0.447]\u001b[A\n",
            "Epoch 2:  25%|██▌       | 321/1271 [01:43<05:09,  3.07it/s, training_loss=0.519]\u001b[A\n",
            "Epoch 2:  25%|██▌       | 322/1271 [01:43<05:07,  3.08it/s, training_loss=0.519]\u001b[A\n",
            "Epoch 2:  25%|██▌       | 322/1271 [01:44<05:07,  3.08it/s, training_loss=0.227]\u001b[A\n",
            "Epoch 2:  25%|██▌       | 323/1271 [01:44<05:06,  3.10it/s, training_loss=0.227]\u001b[A\n",
            "Epoch 2:  25%|██▌       | 323/1271 [01:44<05:06,  3.10it/s, training_loss=0.786]\u001b[A\n",
            "Epoch 2:  25%|██▌       | 324/1271 [01:44<05:04,  3.11it/s, training_loss=0.786]\u001b[A\n",
            "Epoch 2:  25%|██▌       | 324/1271 [01:44<05:04,  3.11it/s, training_loss=0.055]\u001b[A\n",
            "Epoch 2:  26%|██▌       | 325/1271 [01:44<05:03,  3.12it/s, training_loss=0.055]\u001b[A\n",
            "Epoch 2:  26%|██▌       | 325/1271 [01:45<05:03,  3.12it/s, training_loss=0.507]\u001b[A\n",
            "Epoch 2:  26%|██▌       | 326/1271 [01:45<05:05,  3.10it/s, training_loss=0.507]\u001b[A\n",
            "Epoch 2:  26%|██▌       | 326/1271 [01:45<05:05,  3.10it/s, training_loss=0.361]\u001b[A\n",
            "Epoch 2:  26%|██▌       | 327/1271 [01:45<05:05,  3.09it/s, training_loss=0.361]\u001b[A\n",
            "Epoch 2:  26%|██▌       | 327/1271 [01:45<05:05,  3.09it/s, training_loss=0.164]\u001b[A\n",
            "Epoch 2:  26%|██▌       | 328/1271 [01:45<05:04,  3.10it/s, training_loss=0.164]\u001b[A\n",
            "Epoch 2:  26%|██▌       | 328/1271 [01:46<05:04,  3.10it/s, training_loss=0.265]\u001b[A\n",
            "Epoch 2:  26%|██▌       | 329/1271 [01:46<05:03,  3.11it/s, training_loss=0.265]\u001b[A\n",
            "Epoch 2:  26%|██▌       | 329/1271 [01:46<05:03,  3.11it/s, training_loss=0.376]\u001b[A\n",
            "Epoch 2:  26%|██▌       | 330/1271 [01:46<05:03,  3.10it/s, training_loss=0.376]\u001b[A\n",
            "Epoch 2:  26%|██▌       | 330/1271 [01:46<05:03,  3.10it/s, training_loss=0.317]\u001b[A\n",
            "Epoch 2:  26%|██▌       | 331/1271 [01:46<05:02,  3.11it/s, training_loss=0.317]\u001b[A\n",
            "Epoch 2:  26%|██▌       | 331/1271 [01:47<05:02,  3.11it/s, training_loss=0.466]\u001b[A\n",
            "Epoch 2:  26%|██▌       | 332/1271 [01:47<05:00,  3.12it/s, training_loss=0.466]\u001b[A\n",
            "Epoch 2:  26%|██▌       | 332/1271 [01:47<05:00,  3.12it/s, training_loss=0.018]\u001b[A\n",
            "Epoch 2:  26%|██▌       | 333/1271 [01:47<04:59,  3.13it/s, training_loss=0.018]\u001b[A\n",
            "Epoch 2:  26%|██▌       | 333/1271 [01:47<04:59,  3.13it/s, training_loss=0.069]\u001b[A\n",
            "Epoch 2:  26%|██▋       | 334/1271 [01:47<04:59,  3.13it/s, training_loss=0.069]\u001b[A\n",
            "Epoch 2:  26%|██▋       | 334/1271 [01:47<04:59,  3.13it/s, training_loss=0.584]\u001b[A\n",
            "Epoch 2:  26%|██▋       | 335/1271 [01:48<05:00,  3.12it/s, training_loss=0.584]\u001b[A\n",
            "Epoch 2:  26%|██▋       | 335/1271 [01:48<05:00,  3.12it/s, training_loss=0.449]\u001b[A\n",
            "Epoch 2:  26%|██▋       | 336/1271 [01:48<05:01,  3.10it/s, training_loss=0.449]\u001b[A\n",
            "Epoch 2:  26%|██▋       | 336/1271 [01:48<05:01,  3.10it/s, training_loss=0.589]\u001b[A\n",
            "Epoch 2:  27%|██▋       | 337/1271 [01:48<05:01,  3.10it/s, training_loss=0.589]\u001b[A\n",
            "Epoch 2:  27%|██▋       | 337/1271 [01:48<05:01,  3.10it/s, training_loss=0.127]\u001b[A\n",
            "Epoch 2:  27%|██▋       | 338/1271 [01:48<05:00,  3.10it/s, training_loss=0.127]\u001b[A\n",
            "Epoch 2:  27%|██▋       | 338/1271 [01:49<05:00,  3.10it/s, training_loss=0.573]\u001b[A\n",
            "Epoch 2:  27%|██▋       | 339/1271 [01:49<05:01,  3.10it/s, training_loss=0.573]\u001b[A\n",
            "Epoch 2:  27%|██▋       | 339/1271 [01:49<05:01,  3.10it/s, training_loss=0.545]\u001b[A\n",
            "Epoch 2:  27%|██▋       | 340/1271 [01:49<05:00,  3.10it/s, training_loss=0.545]\u001b[A\n",
            "Epoch 2:  27%|██▋       | 340/1271 [01:49<05:00,  3.10it/s, training_loss=0.488]\u001b[A\n",
            "Epoch 2:  27%|██▋       | 341/1271 [01:49<05:00,  3.09it/s, training_loss=0.488]\u001b[A\n",
            "Epoch 2:  27%|██▋       | 341/1271 [01:50<05:00,  3.09it/s, training_loss=0.466]\u001b[A\n",
            "Epoch 2:  27%|██▋       | 342/1271 [01:50<04:59,  3.10it/s, training_loss=0.466]\u001b[A\n",
            "Epoch 2:  27%|██▋       | 342/1271 [01:50<04:59,  3.10it/s, training_loss=0.381]\u001b[A\n",
            "Epoch 2:  27%|██▋       | 343/1271 [01:50<04:59,  3.10it/s, training_loss=0.381]\u001b[A\n",
            "Epoch 2:  27%|██▋       | 343/1271 [01:50<04:59,  3.10it/s, training_loss=0.791]\u001b[A\n",
            "Epoch 2:  27%|██▋       | 344/1271 [01:50<04:59,  3.10it/s, training_loss=0.791]\u001b[A\n",
            "Epoch 2:  27%|██▋       | 344/1271 [01:51<04:59,  3.10it/s, training_loss=0.765]\u001b[A\n",
            "Epoch 2:  27%|██▋       | 345/1271 [01:51<05:00,  3.09it/s, training_loss=0.765]\u001b[A\n",
            "Epoch 2:  27%|██▋       | 345/1271 [01:51<05:00,  3.09it/s, training_loss=0.511]\u001b[A\n",
            "Epoch 2:  27%|██▋       | 346/1271 [01:51<04:59,  3.09it/s, training_loss=0.511]\u001b[A\n",
            "Epoch 2:  27%|██▋       | 346/1271 [01:51<04:59,  3.09it/s, training_loss=0.711]\u001b[A\n",
            "Epoch 2:  27%|██▋       | 347/1271 [01:51<04:58,  3.10it/s, training_loss=0.711]\u001b[A\n",
            "Epoch 2:  27%|██▋       | 347/1271 [01:52<04:58,  3.10it/s, training_loss=0.804]\u001b[A\n",
            "Epoch 2:  27%|██▋       | 348/1271 [01:52<04:57,  3.10it/s, training_loss=0.804]\u001b[A\n",
            "Epoch 2:  27%|██▋       | 348/1271 [01:52<04:57,  3.10it/s, training_loss=0.531]\u001b[A\n",
            "Epoch 2:  27%|██▋       | 349/1271 [01:52<04:59,  3.08it/s, training_loss=0.531]\u001b[A\n",
            "Epoch 2:  27%|██▋       | 349/1271 [01:52<04:59,  3.08it/s, training_loss=0.499]\u001b[A\n",
            "Epoch 2:  28%|██▊       | 350/1271 [01:52<04:59,  3.08it/s, training_loss=0.499]\u001b[A\n",
            "Epoch 2:  28%|██▊       | 350/1271 [01:53<04:59,  3.08it/s, training_loss=0.392]\u001b[A\n",
            "Epoch 2:  28%|██▊       | 351/1271 [01:53<05:00,  3.07it/s, training_loss=0.392]\u001b[A\n",
            "Epoch 2:  28%|██▊       | 351/1271 [01:53<05:00,  3.07it/s, training_loss=0.141]\u001b[A\n",
            "Epoch 2:  28%|██▊       | 352/1271 [01:53<04:59,  3.07it/s, training_loss=0.141]\u001b[A\n",
            "Epoch 2:  28%|██▊       | 352/1271 [01:53<04:59,  3.07it/s, training_loss=0.700]\u001b[A\n",
            "Epoch 2:  28%|██▊       | 353/1271 [01:53<04:57,  3.08it/s, training_loss=0.700]\u001b[A\n",
            "Epoch 2:  28%|██▊       | 353/1271 [01:54<04:57,  3.08it/s, training_loss=0.856]\u001b[A\n",
            "Epoch 2:  28%|██▊       | 354/1271 [01:54<04:57,  3.08it/s, training_loss=0.856]\u001b[A\n",
            "Epoch 2:  28%|██▊       | 354/1271 [01:54<04:57,  3.08it/s, training_loss=0.256]\u001b[A\n",
            "Epoch 2:  28%|██▊       | 355/1271 [01:54<04:57,  3.07it/s, training_loss=0.256]\u001b[A\n",
            "Epoch 2:  28%|██▊       | 355/1271 [01:54<04:57,  3.07it/s, training_loss=0.443]\u001b[A\n",
            "Epoch 2:  28%|██▊       | 356/1271 [01:54<04:56,  3.08it/s, training_loss=0.443]\u001b[A\n",
            "Epoch 2:  28%|██▊       | 356/1271 [01:55<04:56,  3.08it/s, training_loss=0.441]\u001b[A\n",
            "Epoch 2:  28%|██▊       | 357/1271 [01:55<04:56,  3.09it/s, training_loss=0.441]\u001b[A\n",
            "Epoch 2:  28%|██▊       | 357/1271 [01:55<04:56,  3.09it/s, training_loss=0.568]\u001b[A\n",
            "Epoch 2:  28%|██▊       | 358/1271 [01:55<04:55,  3.09it/s, training_loss=0.568]\u001b[A\n",
            "Epoch 2:  28%|██▊       | 358/1271 [01:55<04:55,  3.09it/s, training_loss=0.307]\u001b[A\n",
            "Epoch 2:  28%|██▊       | 359/1271 [01:55<04:54,  3.10it/s, training_loss=0.307]\u001b[A\n",
            "Epoch 2:  28%|██▊       | 359/1271 [01:56<04:54,  3.10it/s, training_loss=0.628]\u001b[A\n",
            "Epoch 2:  28%|██▊       | 360/1271 [01:56<04:53,  3.10it/s, training_loss=0.628]\u001b[A\n",
            "Epoch 2:  28%|██▊       | 360/1271 [01:56<04:53,  3.10it/s, training_loss=0.312]\u001b[A\n",
            "Epoch 2:  28%|██▊       | 361/1271 [01:56<04:54,  3.09it/s, training_loss=0.312]\u001b[A\n",
            "Epoch 2:  28%|██▊       | 361/1271 [01:56<04:54,  3.09it/s, training_loss=0.589]\u001b[A\n",
            "Epoch 2:  28%|██▊       | 362/1271 [01:56<04:52,  3.11it/s, training_loss=0.589]\u001b[A\n",
            "Epoch 2:  28%|██▊       | 362/1271 [01:57<04:52,  3.11it/s, training_loss=0.264]\u001b[A\n",
            "Epoch 2:  29%|██▊       | 363/1271 [01:57<04:52,  3.11it/s, training_loss=0.264]\u001b[A\n",
            "Epoch 2:  29%|██▊       | 363/1271 [01:57<04:52,  3.11it/s, training_loss=0.474]\u001b[A\n",
            "Epoch 2:  29%|██▊       | 364/1271 [01:57<04:51,  3.12it/s, training_loss=0.474]\u001b[A\n",
            "Epoch 2:  29%|██▊       | 364/1271 [01:57<04:51,  3.12it/s, training_loss=0.150]\u001b[A\n",
            "Epoch 2:  29%|██▊       | 365/1271 [01:57<04:50,  3.12it/s, training_loss=0.150]\u001b[A\n",
            "Epoch 2:  29%|██▊       | 365/1271 [01:58<04:50,  3.12it/s, training_loss=0.590]\u001b[A\n",
            "Epoch 2:  29%|██▉       | 366/1271 [01:58<04:50,  3.11it/s, training_loss=0.590]\u001b[A\n",
            "Epoch 2:  29%|██▉       | 366/1271 [01:58<04:50,  3.11it/s, training_loss=0.925]\u001b[A\n",
            "Epoch 2:  29%|██▉       | 367/1271 [01:58<04:51,  3.10it/s, training_loss=0.925]\u001b[A\n",
            "Epoch 2:  29%|██▉       | 367/1271 [01:58<04:51,  3.10it/s, training_loss=0.015]\u001b[A\n",
            "Epoch 2:  29%|██▉       | 368/1271 [01:58<04:51,  3.09it/s, training_loss=0.015]\u001b[A\n",
            "Epoch 2:  29%|██▉       | 368/1271 [01:58<04:51,  3.09it/s, training_loss=0.592]\u001b[A\n",
            "Epoch 2:  29%|██▉       | 369/1271 [01:58<04:50,  3.10it/s, training_loss=0.592]\u001b[A\n",
            "Epoch 2:  29%|██▉       | 369/1271 [01:59<04:50,  3.10it/s, training_loss=0.727]\u001b[A\n",
            "Epoch 2:  29%|██▉       | 370/1271 [01:59<04:50,  3.10it/s, training_loss=0.727]\u001b[A\n",
            "Epoch 2:  29%|██▉       | 370/1271 [01:59<04:50,  3.10it/s, training_loss=0.155]\u001b[A\n",
            "Epoch 2:  29%|██▉       | 371/1271 [01:59<04:49,  3.11it/s, training_loss=0.155]\u001b[A\n",
            "Epoch 2:  29%|██▉       | 371/1271 [01:59<04:49,  3.11it/s, training_loss=0.274]\u001b[A\n",
            "Epoch 2:  29%|██▉       | 372/1271 [01:59<04:48,  3.12it/s, training_loss=0.274]\u001b[A\n",
            "Epoch 2:  29%|██▉       | 372/1271 [02:00<04:48,  3.12it/s, training_loss=0.130]\u001b[A\n",
            "Epoch 2:  29%|██▉       | 373/1271 [02:00<04:47,  3.12it/s, training_loss=0.130]\u001b[A\n",
            "Epoch 2:  29%|██▉       | 373/1271 [02:00<04:47,  3.12it/s, training_loss=0.388]\u001b[A\n",
            "Epoch 2:  29%|██▉       | 374/1271 [02:00<04:48,  3.11it/s, training_loss=0.388]\u001b[A\n",
            "Epoch 2:  29%|██▉       | 374/1271 [02:00<04:48,  3.11it/s, training_loss=0.597]\u001b[A\n",
            "Epoch 2:  30%|██▉       | 375/1271 [02:00<04:48,  3.11it/s, training_loss=0.597]\u001b[A\n",
            "Epoch 2:  30%|██▉       | 375/1271 [02:01<04:48,  3.11it/s, training_loss=0.182]\u001b[A\n",
            "Epoch 2:  30%|██▉       | 376/1271 [02:01<04:47,  3.11it/s, training_loss=0.182]\u001b[A\n",
            "Epoch 2:  30%|██▉       | 376/1271 [02:01<04:47,  3.11it/s, training_loss=0.779]\u001b[A\n",
            "Epoch 2:  30%|██▉       | 377/1271 [02:01<04:48,  3.10it/s, training_loss=0.779]\u001b[A\n",
            "Epoch 2:  30%|██▉       | 377/1271 [02:01<04:48,  3.10it/s, training_loss=1.015]\u001b[A\n",
            "Epoch 2:  30%|██▉       | 378/1271 [02:01<04:47,  3.11it/s, training_loss=1.015]\u001b[A\n",
            "Epoch 2:  30%|██▉       | 378/1271 [02:02<04:47,  3.11it/s, training_loss=0.493]\u001b[A\n",
            "Epoch 2:  30%|██▉       | 379/1271 [02:02<04:45,  3.12it/s, training_loss=0.493]\u001b[A\n",
            "Epoch 2:  30%|██▉       | 379/1271 [02:02<04:45,  3.12it/s, training_loss=0.353]\u001b[A\n",
            "Epoch 2:  30%|██▉       | 380/1271 [02:02<04:45,  3.12it/s, training_loss=0.353]\u001b[A\n",
            "Epoch 2:  30%|██▉       | 380/1271 [02:02<04:45,  3.12it/s, training_loss=0.318]\u001b[A\n",
            "Epoch 2:  30%|██▉       | 381/1271 [02:02<04:47,  3.09it/s, training_loss=0.318]\u001b[A\n",
            "Epoch 2:  30%|██▉       | 381/1271 [02:03<04:47,  3.09it/s, training_loss=0.509]\u001b[A\n",
            "Epoch 2:  30%|███       | 382/1271 [02:03<04:47,  3.10it/s, training_loss=0.509]\u001b[A\n",
            "Epoch 2:  30%|███       | 382/1271 [02:03<04:47,  3.10it/s, training_loss=0.486]\u001b[A\n",
            "Epoch 2:  30%|███       | 383/1271 [02:03<04:47,  3.09it/s, training_loss=0.486]\u001b[A\n",
            "Epoch 2:  30%|███       | 383/1271 [02:03<04:47,  3.09it/s, training_loss=0.747]\u001b[A\n",
            "Epoch 2:  30%|███       | 384/1271 [02:03<04:47,  3.09it/s, training_loss=0.747]\u001b[A\n",
            "Epoch 2:  30%|███       | 384/1271 [02:04<04:47,  3.09it/s, training_loss=0.185]\u001b[A\n",
            "Epoch 2:  30%|███       | 385/1271 [02:04<04:46,  3.09it/s, training_loss=0.185]\u001b[A\n",
            "Epoch 2:  30%|███       | 385/1271 [02:04<04:46,  3.09it/s, training_loss=0.013]\u001b[A\n",
            "Epoch 2:  30%|███       | 386/1271 [02:04<04:45,  3.10it/s, training_loss=0.013]\u001b[A\n",
            "Epoch 2:  30%|███       | 386/1271 [02:04<04:45,  3.10it/s, training_loss=0.804]\u001b[A\n",
            "Epoch 2:  30%|███       | 387/1271 [02:04<04:44,  3.11it/s, training_loss=0.804]\u001b[A\n",
            "Epoch 2:  30%|███       | 387/1271 [02:05<04:44,  3.11it/s, training_loss=0.364]\u001b[A\n",
            "Epoch 2:  31%|███       | 388/1271 [02:05<04:43,  3.11it/s, training_loss=0.364]\u001b[A\n",
            "Epoch 2:  31%|███       | 388/1271 [02:05<04:43,  3.11it/s, training_loss=0.546]\u001b[A\n",
            "Epoch 2:  31%|███       | 389/1271 [02:05<04:43,  3.11it/s, training_loss=0.546]\u001b[A\n",
            "Epoch 2:  31%|███       | 389/1271 [02:05<04:43,  3.11it/s, training_loss=0.160]\u001b[A\n",
            "Epoch 2:  31%|███       | 390/1271 [02:05<04:44,  3.10it/s, training_loss=0.160]\u001b[A\n",
            "Epoch 2:  31%|███       | 390/1271 [02:06<04:44,  3.10it/s, training_loss=0.370]\u001b[A\n",
            "Epoch 2:  31%|███       | 391/1271 [02:06<04:42,  3.11it/s, training_loss=0.370]\u001b[A\n",
            "Epoch 2:  31%|███       | 391/1271 [02:06<04:42,  3.11it/s, training_loss=0.390]\u001b[A\n",
            "Epoch 2:  31%|███       | 392/1271 [02:06<04:46,  3.07it/s, training_loss=0.390]\u001b[A\n",
            "Epoch 2:  31%|███       | 392/1271 [02:06<04:46,  3.07it/s, training_loss=0.406]\u001b[A\n",
            "Epoch 2:  31%|███       | 393/1271 [02:06<04:46,  3.06it/s, training_loss=0.406]\u001b[A\n",
            "Epoch 2:  31%|███       | 393/1271 [02:07<04:46,  3.06it/s, training_loss=0.295]\u001b[A\n",
            "Epoch 2:  31%|███       | 394/1271 [02:07<04:44,  3.08it/s, training_loss=0.295]\u001b[A\n",
            "Epoch 2:  31%|███       | 394/1271 [02:07<04:44,  3.08it/s, training_loss=0.641]\u001b[A\n",
            "Epoch 2:  31%|███       | 395/1271 [02:07<04:42,  3.10it/s, training_loss=0.641]\u001b[A\n",
            "Epoch 2:  31%|███       | 395/1271 [02:07<04:42,  3.10it/s, training_loss=0.549]\u001b[A\n",
            "Epoch 2:  31%|███       | 396/1271 [02:07<04:42,  3.10it/s, training_loss=0.549]\u001b[A\n",
            "Epoch 2:  31%|███       | 396/1271 [02:08<04:42,  3.10it/s, training_loss=0.018]\u001b[A\n",
            "Epoch 2:  31%|███       | 397/1271 [02:08<04:40,  3.11it/s, training_loss=0.018]\u001b[A\n",
            "Epoch 2:  31%|███       | 397/1271 [02:08<04:40,  3.11it/s, training_loss=0.470]\u001b[A\n",
            "Epoch 2:  31%|███▏      | 398/1271 [02:08<04:39,  3.12it/s, training_loss=0.470]\u001b[A\n",
            "Epoch 2:  31%|███▏      | 398/1271 [02:08<04:39,  3.12it/s, training_loss=0.483]\u001b[A\n",
            "Epoch 2:  31%|███▏      | 399/1271 [02:08<04:39,  3.12it/s, training_loss=0.483]\u001b[A\n",
            "Epoch 2:  31%|███▏      | 399/1271 [02:08<04:39,  3.12it/s, training_loss=0.641]\u001b[A\n",
            "Epoch 2:  31%|███▏      | 400/1271 [02:08<04:38,  3.13it/s, training_loss=0.641]\u001b[A\n",
            "Epoch 2:  31%|███▏      | 400/1271 [02:09<04:38,  3.13it/s, training_loss=0.027]\u001b[A\n",
            "Epoch 2:  32%|███▏      | 401/1271 [02:09<04:36,  3.15it/s, training_loss=0.027]\u001b[A\n",
            "Epoch 2:  32%|███▏      | 401/1271 [02:09<04:36,  3.15it/s, training_loss=0.307]\u001b[A\n",
            "Epoch 2:  32%|███▏      | 402/1271 [02:09<04:38,  3.12it/s, training_loss=0.307]\u001b[A\n",
            "Epoch 2:  32%|███▏      | 402/1271 [02:09<04:38,  3.12it/s, training_loss=0.792]\u001b[A\n",
            "Epoch 2:  32%|███▏      | 403/1271 [02:09<04:39,  3.10it/s, training_loss=0.792]\u001b[A\n",
            "Epoch 2:  32%|███▏      | 403/1271 [02:10<04:39,  3.10it/s, training_loss=0.023]\u001b[A\n",
            "Epoch 2:  32%|███▏      | 404/1271 [02:10<04:38,  3.11it/s, training_loss=0.023]\u001b[A\n",
            "Epoch 2:  32%|███▏      | 404/1271 [02:10<04:38,  3.11it/s, training_loss=0.151]\u001b[A\n",
            "Epoch 2:  32%|███▏      | 405/1271 [02:10<04:38,  3.10it/s, training_loss=0.151]\u001b[A\n",
            "Epoch 2:  32%|███▏      | 405/1271 [02:10<04:38,  3.10it/s, training_loss=0.591]\u001b[A\n",
            "Epoch 2:  32%|███▏      | 406/1271 [02:10<04:37,  3.11it/s, training_loss=0.591]\u001b[A\n",
            "Epoch 2:  32%|███▏      | 406/1271 [02:11<04:37,  3.11it/s, training_loss=0.480]\u001b[A\n",
            "Epoch 2:  32%|███▏      | 407/1271 [02:11<04:37,  3.12it/s, training_loss=0.480]\u001b[A\n",
            "Epoch 2:  32%|███▏      | 407/1271 [02:11<04:37,  3.12it/s, training_loss=0.388]\u001b[A\n",
            "Epoch 2:  32%|███▏      | 408/1271 [02:11<04:35,  3.13it/s, training_loss=0.388]\u001b[A\n",
            "Epoch 2:  32%|███▏      | 408/1271 [02:11<04:35,  3.13it/s, training_loss=0.013]\u001b[A\n",
            "Epoch 2:  32%|███▏      | 409/1271 [02:11<04:35,  3.13it/s, training_loss=0.013]\u001b[A\n",
            "Epoch 2:  32%|███▏      | 409/1271 [02:12<04:35,  3.13it/s, training_loss=0.484]\u001b[A\n",
            "Epoch 2:  32%|███▏      | 410/1271 [02:12<04:36,  3.12it/s, training_loss=0.484]\u001b[A\n",
            "Epoch 2:  32%|███▏      | 410/1271 [02:12<04:36,  3.12it/s, training_loss=0.400]\u001b[A\n",
            "Epoch 2:  32%|███▏      | 411/1271 [02:12<04:36,  3.11it/s, training_loss=0.400]\u001b[A\n",
            "Epoch 2:  32%|███▏      | 411/1271 [02:12<04:36,  3.11it/s, training_loss=0.367]\u001b[A\n",
            "Epoch 2:  32%|███▏      | 412/1271 [02:12<04:36,  3.11it/s, training_loss=0.367]\u001b[A\n",
            "Epoch 2:  32%|███▏      | 412/1271 [02:13<04:36,  3.11it/s, training_loss=0.867]\u001b[A\n",
            "Epoch 2:  32%|███▏      | 413/1271 [02:13<04:37,  3.09it/s, training_loss=0.867]\u001b[A\n",
            "Epoch 2:  32%|███▏      | 413/1271 [02:13<04:37,  3.09it/s, training_loss=0.820]\u001b[A\n",
            "Epoch 2:  33%|███▎      | 414/1271 [02:13<04:37,  3.09it/s, training_loss=0.820]\u001b[A\n",
            "Epoch 2:  33%|███▎      | 414/1271 [02:13<04:37,  3.09it/s, training_loss=0.129]\u001b[A\n",
            "Epoch 2:  33%|███▎      | 415/1271 [02:13<04:36,  3.09it/s, training_loss=0.129]\u001b[A\n",
            "Epoch 2:  33%|███▎      | 415/1271 [02:14<04:36,  3.09it/s, training_loss=0.292]\u001b[A\n",
            "Epoch 2:  33%|███▎      | 416/1271 [02:14<04:36,  3.10it/s, training_loss=0.292]\u001b[A\n",
            "Epoch 2:  33%|███▎      | 416/1271 [02:14<04:36,  3.10it/s, training_loss=0.447]\u001b[A\n",
            "Epoch 2:  33%|███▎      | 417/1271 [02:14<04:36,  3.09it/s, training_loss=0.447]\u001b[A\n",
            "Epoch 2:  33%|███▎      | 417/1271 [02:14<04:36,  3.09it/s, training_loss=0.387]\u001b[A\n",
            "Epoch 2:  33%|███▎      | 418/1271 [02:14<04:33,  3.12it/s, training_loss=0.387]\u001b[A\n",
            "Epoch 2:  33%|███▎      | 418/1271 [02:15<04:33,  3.12it/s, training_loss=0.924]\u001b[A\n",
            "Epoch 2:  33%|███▎      | 419/1271 [02:15<04:33,  3.12it/s, training_loss=0.924]\u001b[A\n",
            "Epoch 2:  33%|███▎      | 419/1271 [02:15<04:33,  3.12it/s, training_loss=0.017]\u001b[A\n",
            "Epoch 2:  33%|███▎      | 420/1271 [02:15<04:33,  3.11it/s, training_loss=0.017]\u001b[A\n",
            "Epoch 2:  33%|███▎      | 420/1271 [02:15<04:33,  3.11it/s, training_loss=0.295]\u001b[A\n",
            "Epoch 2:  33%|███▎      | 421/1271 [02:15<04:33,  3.11it/s, training_loss=0.295]\u001b[A\n",
            "Epoch 2:  33%|███▎      | 421/1271 [02:16<04:33,  3.11it/s, training_loss=0.283]\u001b[A\n",
            "Epoch 2:  33%|███▎      | 422/1271 [02:16<04:32,  3.11it/s, training_loss=0.283]\u001b[A\n",
            "Epoch 2:  33%|███▎      | 422/1271 [02:16<04:32,  3.11it/s, training_loss=0.354]\u001b[A\n",
            "Epoch 2:  33%|███▎      | 423/1271 [02:16<04:33,  3.10it/s, training_loss=0.354]\u001b[A\n",
            "Epoch 2:  33%|███▎      | 423/1271 [02:16<04:33,  3.10it/s, training_loss=0.468]\u001b[A\n",
            "Epoch 2:  33%|███▎      | 424/1271 [02:16<04:31,  3.12it/s, training_loss=0.468]\u001b[A\n",
            "Epoch 2:  33%|███▎      | 424/1271 [02:17<04:31,  3.12it/s, training_loss=0.673]\u001b[A\n",
            "Epoch 2:  33%|███▎      | 425/1271 [02:17<04:31,  3.11it/s, training_loss=0.673]\u001b[A\n",
            "Epoch 2:  33%|███▎      | 425/1271 [02:17<04:31,  3.11it/s, training_loss=0.651]\u001b[A\n",
            "Epoch 2:  34%|███▎      | 426/1271 [02:17<04:32,  3.10it/s, training_loss=0.651]\u001b[A\n",
            "Epoch 2:  34%|███▎      | 426/1271 [02:17<04:32,  3.10it/s, training_loss=0.371]\u001b[A\n",
            "Epoch 2:  34%|███▎      | 427/1271 [02:17<04:31,  3.11it/s, training_loss=0.371]\u001b[A\n",
            "Epoch 2:  34%|███▎      | 427/1271 [02:17<04:31,  3.11it/s, training_loss=1.281]\u001b[A\n",
            "Epoch 2:  34%|███▎      | 428/1271 [02:17<04:31,  3.10it/s, training_loss=1.281]\u001b[A\n",
            "Epoch 2:  34%|███▎      | 428/1271 [02:18<04:31,  3.10it/s, training_loss=1.156]\u001b[A\n",
            "Epoch 2:  34%|███▍      | 429/1271 [02:18<04:31,  3.10it/s, training_loss=1.156]\u001b[A\n",
            "Epoch 2:  34%|███▍      | 429/1271 [02:18<04:31,  3.10it/s, training_loss=0.380]\u001b[A\n",
            "Epoch 2:  34%|███▍      | 430/1271 [02:18<04:32,  3.09it/s, training_loss=0.380]\u001b[A\n",
            "Epoch 2:  34%|███▍      | 430/1271 [02:18<04:32,  3.09it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 2:  34%|███▍      | 431/1271 [02:18<04:31,  3.09it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 2:  34%|███▍      | 431/1271 [02:19<04:31,  3.09it/s, training_loss=0.220]\u001b[A\n",
            "Epoch 2:  34%|███▍      | 432/1271 [02:19<04:31,  3.09it/s, training_loss=0.220]\u001b[A\n",
            "Epoch 2:  34%|███▍      | 432/1271 [02:19<04:31,  3.09it/s, training_loss=0.254]\u001b[A\n",
            "Epoch 2:  34%|███▍      | 433/1271 [02:19<04:30,  3.10it/s, training_loss=0.254]\u001b[A\n",
            "Epoch 2:  34%|███▍      | 433/1271 [02:19<04:30,  3.10it/s, training_loss=0.228]\u001b[A\n",
            "Epoch 2:  34%|███▍      | 434/1271 [02:19<04:28,  3.12it/s, training_loss=0.228]\u001b[A\n",
            "Epoch 2:  34%|███▍      | 434/1271 [02:20<04:28,  3.12it/s, training_loss=0.199]\u001b[A\n",
            "Epoch 2:  34%|███▍      | 435/1271 [02:20<04:27,  3.12it/s, training_loss=0.199]\u001b[A\n",
            "Epoch 2:  34%|███▍      | 435/1271 [02:20<04:27,  3.12it/s, training_loss=0.316]\u001b[A\n",
            "Epoch 2:  34%|███▍      | 436/1271 [02:20<04:28,  3.11it/s, training_loss=0.316]\u001b[A\n",
            "Epoch 2:  34%|███▍      | 436/1271 [02:20<04:28,  3.11it/s, training_loss=0.768]\u001b[A\n",
            "Epoch 2:  34%|███▍      | 437/1271 [02:20<04:28,  3.10it/s, training_loss=0.768]\u001b[A\n",
            "Epoch 2:  34%|███▍      | 437/1271 [02:21<04:28,  3.10it/s, training_loss=0.650]\u001b[A\n",
            "Epoch 2:  34%|███▍      | 438/1271 [02:21<04:28,  3.10it/s, training_loss=0.650]\u001b[A\n",
            "Epoch 2:  34%|███▍      | 438/1271 [02:21<04:28,  3.10it/s, training_loss=0.649]\u001b[A\n",
            "Epoch 2:  35%|███▍      | 439/1271 [02:21<04:26,  3.12it/s, training_loss=0.649]\u001b[A\n",
            "Epoch 2:  35%|███▍      | 439/1271 [02:21<04:26,  3.12it/s, training_loss=0.409]\u001b[A\n",
            "Epoch 2:  35%|███▍      | 440/1271 [02:21<04:27,  3.11it/s, training_loss=0.409]\u001b[A\n",
            "Epoch 2:  35%|███▍      | 440/1271 [02:22<04:27,  3.11it/s, training_loss=0.332]\u001b[A\n",
            "Epoch 2:  35%|███▍      | 441/1271 [02:22<04:25,  3.13it/s, training_loss=0.332]\u001b[A\n",
            "Epoch 2:  35%|███▍      | 441/1271 [02:22<04:25,  3.13it/s, training_loss=0.418]\u001b[A\n",
            "Epoch 2:  35%|███▍      | 442/1271 [02:22<04:24,  3.13it/s, training_loss=0.418]\u001b[A\n",
            "Epoch 2:  35%|███▍      | 442/1271 [02:22<04:24,  3.13it/s, training_loss=0.305]\u001b[A\n",
            "Epoch 2:  35%|███▍      | 443/1271 [02:22<04:24,  3.14it/s, training_loss=0.305]\u001b[A\n",
            "Epoch 2:  35%|███▍      | 443/1271 [02:23<04:24,  3.14it/s, training_loss=0.368]\u001b[A\n",
            "Epoch 2:  35%|███▍      | 444/1271 [02:23<04:23,  3.14it/s, training_loss=0.368]\u001b[A\n",
            "Epoch 2:  35%|███▍      | 444/1271 [02:23<04:23,  3.14it/s, training_loss=0.494]\u001b[A\n",
            "Epoch 2:  35%|███▌      | 445/1271 [02:23<04:22,  3.14it/s, training_loss=0.494]\u001b[A\n",
            "Epoch 2:  35%|███▌      | 445/1271 [02:23<04:22,  3.14it/s, training_loss=0.189]\u001b[A\n",
            "Epoch 2:  35%|███▌      | 446/1271 [02:23<04:22,  3.14it/s, training_loss=0.189]\u001b[A\n",
            "Epoch 2:  35%|███▌      | 446/1271 [02:24<04:22,  3.14it/s, training_loss=0.501]\u001b[A\n",
            "Epoch 2:  35%|███▌      | 447/1271 [02:24<04:22,  3.14it/s, training_loss=0.501]\u001b[A\n",
            "Epoch 2:  35%|███▌      | 447/1271 [02:24<04:22,  3.14it/s, training_loss=0.637]\u001b[A\n",
            "Epoch 2:  35%|███▌      | 448/1271 [02:24<04:21,  3.15it/s, training_loss=0.637]\u001b[A\n",
            "Epoch 2:  35%|███▌      | 448/1271 [02:24<04:21,  3.15it/s, training_loss=0.415]\u001b[A\n",
            "Epoch 2:  35%|███▌      | 449/1271 [02:24<04:22,  3.13it/s, training_loss=0.415]\u001b[A\n",
            "Epoch 2:  35%|███▌      | 449/1271 [02:25<04:22,  3.13it/s, training_loss=0.601]\u001b[A\n",
            "Epoch 2:  35%|███▌      | 450/1271 [02:25<04:24,  3.11it/s, training_loss=0.601]\u001b[A\n",
            "Epoch 2:  35%|███▌      | 450/1271 [02:25<04:24,  3.11it/s, training_loss=0.646]\u001b[A\n",
            "Epoch 2:  35%|███▌      | 451/1271 [02:25<04:22,  3.12it/s, training_loss=0.646]\u001b[A\n",
            "Epoch 2:  35%|███▌      | 451/1271 [02:25<04:22,  3.12it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 2:  36%|███▌      | 452/1271 [02:25<04:21,  3.13it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 2:  36%|███▌      | 452/1271 [02:25<04:21,  3.13it/s, training_loss=0.438]\u001b[A\n",
            "Epoch 2:  36%|███▌      | 453/1271 [02:25<04:20,  3.14it/s, training_loss=0.438]\u001b[A\n",
            "Epoch 2:  36%|███▌      | 453/1271 [02:26<04:20,  3.14it/s, training_loss=1.073]\u001b[A\n",
            "Epoch 2:  36%|███▌      | 454/1271 [02:26<04:20,  3.14it/s, training_loss=1.073]\u001b[A\n",
            "Epoch 2:  36%|███▌      | 454/1271 [02:26<04:20,  3.14it/s, training_loss=0.592]\u001b[A\n",
            "Epoch 2:  36%|███▌      | 455/1271 [02:26<04:20,  3.13it/s, training_loss=0.592]\u001b[A\n",
            "Epoch 2:  36%|███▌      | 455/1271 [02:26<04:20,  3.13it/s, training_loss=0.655]\u001b[A\n",
            "Epoch 2:  36%|███▌      | 456/1271 [02:26<04:20,  3.12it/s, training_loss=0.655]\u001b[A\n",
            "Epoch 2:  36%|███▌      | 456/1271 [02:27<04:20,  3.12it/s, training_loss=0.651]\u001b[A\n",
            "Epoch 2:  36%|███▌      | 457/1271 [02:27<04:19,  3.14it/s, training_loss=0.651]\u001b[A\n",
            "Epoch 2:  36%|███▌      | 457/1271 [02:27<04:19,  3.14it/s, training_loss=0.400]\u001b[A\n",
            "Epoch 2:  36%|███▌      | 458/1271 [02:27<04:19,  3.13it/s, training_loss=0.400]\u001b[A\n",
            "Epoch 2:  36%|███▌      | 458/1271 [02:27<04:19,  3.13it/s, training_loss=0.127]\u001b[A\n",
            "Epoch 2:  36%|███▌      | 459/1271 [02:27<04:20,  3.12it/s, training_loss=0.127]\u001b[A\n",
            "Epoch 2:  36%|███▌      | 459/1271 [02:28<04:20,  3.12it/s, training_loss=0.193]\u001b[A\n",
            "Epoch 2:  36%|███▌      | 460/1271 [02:28<04:19,  3.12it/s, training_loss=0.193]\u001b[A\n",
            "Epoch 2:  36%|███▌      | 460/1271 [02:28<04:19,  3.12it/s, training_loss=0.180]\u001b[A\n",
            "Epoch 2:  36%|███▋      | 461/1271 [02:28<04:19,  3.12it/s, training_loss=0.180]\u001b[A\n",
            "Epoch 2:  36%|███▋      | 461/1271 [02:28<04:19,  3.12it/s, training_loss=0.976]\u001b[A\n",
            "Epoch 2:  36%|███▋      | 462/1271 [02:28<04:20,  3.10it/s, training_loss=0.976]\u001b[A\n",
            "Epoch 2:  36%|███▋      | 462/1271 [02:29<04:20,  3.10it/s, training_loss=0.420]\u001b[A\n",
            "Epoch 2:  36%|███▋      | 463/1271 [02:29<04:19,  3.12it/s, training_loss=0.420]\u001b[A\n",
            "Epoch 2:  36%|███▋      | 463/1271 [02:29<04:19,  3.12it/s, training_loss=0.358]\u001b[A\n",
            "Epoch 2:  37%|███▋      | 464/1271 [02:29<04:19,  3.11it/s, training_loss=0.358]\u001b[A\n",
            "Epoch 2:  37%|███▋      | 464/1271 [02:29<04:19,  3.11it/s, training_loss=0.500]\u001b[A\n",
            "Epoch 2:  37%|███▋      | 465/1271 [02:29<04:18,  3.12it/s, training_loss=0.500]\u001b[A\n",
            "Epoch 2:  37%|███▋      | 465/1271 [02:30<04:18,  3.12it/s, training_loss=0.642]\u001b[A\n",
            "Epoch 2:  37%|███▋      | 466/1271 [02:30<04:17,  3.12it/s, training_loss=0.642]\u001b[A\n",
            "Epoch 2:  37%|███▋      | 466/1271 [02:30<04:17,  3.12it/s, training_loss=0.235]\u001b[A\n",
            "Epoch 2:  37%|███▋      | 467/1271 [02:30<04:18,  3.11it/s, training_loss=0.235]\u001b[A\n",
            "Epoch 2:  37%|███▋      | 467/1271 [02:30<04:18,  3.11it/s, training_loss=0.198]\u001b[A\n",
            "Epoch 2:  37%|███▋      | 468/1271 [02:30<04:18,  3.10it/s, training_loss=0.198]\u001b[A\n",
            "Epoch 2:  37%|███▋      | 468/1271 [02:31<04:18,  3.10it/s, training_loss=0.850]\u001b[A\n",
            "Epoch 2:  37%|███▋      | 469/1271 [02:31<04:17,  3.11it/s, training_loss=0.850]\u001b[A\n",
            "Epoch 2:  37%|███▋      | 469/1271 [02:31<04:17,  3.11it/s, training_loss=0.335]\u001b[A\n",
            "Epoch 2:  37%|███▋      | 470/1271 [02:31<04:18,  3.10it/s, training_loss=0.335]\u001b[A\n",
            "Epoch 2:  37%|███▋      | 470/1271 [02:31<04:18,  3.10it/s, training_loss=0.359]\u001b[A\n",
            "Epoch 2:  37%|███▋      | 471/1271 [02:31<04:16,  3.12it/s, training_loss=0.359]\u001b[A\n",
            "Epoch 2:  37%|███▋      | 471/1271 [02:32<04:16,  3.12it/s, training_loss=0.612]\u001b[A\n",
            "Epoch 2:  37%|███▋      | 472/1271 [02:32<04:16,  3.11it/s, training_loss=0.612]\u001b[A\n",
            "Epoch 2:  37%|███▋      | 472/1271 [02:32<04:16,  3.11it/s, training_loss=0.152]\u001b[A\n",
            "Epoch 2:  37%|███▋      | 473/1271 [02:32<04:15,  3.13it/s, training_loss=0.152]\u001b[A\n",
            "Epoch 2:  37%|███▋      | 473/1271 [02:32<04:15,  3.13it/s, training_loss=0.571]\u001b[A\n",
            "Epoch 2:  37%|███▋      | 474/1271 [02:32<04:14,  3.13it/s, training_loss=0.571]\u001b[A\n",
            "Epoch 2:  37%|███▋      | 474/1271 [02:33<04:14,  3.13it/s, training_loss=0.740]\u001b[A\n",
            "Epoch 2:  37%|███▋      | 475/1271 [02:33<04:16,  3.10it/s, training_loss=0.740]\u001b[A\n",
            "Epoch 2:  37%|███▋      | 475/1271 [02:33<04:16,  3.10it/s, training_loss=0.479]\u001b[A\n",
            "Epoch 2:  37%|███▋      | 476/1271 [02:33<04:15,  3.11it/s, training_loss=0.479]\u001b[A\n",
            "Epoch 2:  37%|███▋      | 476/1271 [02:33<04:15,  3.11it/s, training_loss=0.430]\u001b[A\n",
            "Epoch 2:  38%|███▊      | 477/1271 [02:33<04:17,  3.09it/s, training_loss=0.430]\u001b[A\n",
            "Epoch 2:  38%|███▊      | 477/1271 [02:34<04:17,  3.09it/s, training_loss=0.381]\u001b[A\n",
            "Epoch 2:  38%|███▊      | 478/1271 [02:34<04:15,  3.10it/s, training_loss=0.381]\u001b[A\n",
            "Epoch 2:  38%|███▊      | 478/1271 [02:34<04:15,  3.10it/s, training_loss=0.151]\u001b[A\n",
            "Epoch 2:  38%|███▊      | 479/1271 [02:34<04:14,  3.11it/s, training_loss=0.151]\u001b[A\n",
            "Epoch 2:  38%|███▊      | 479/1271 [02:34<04:14,  3.11it/s, training_loss=0.311]\u001b[A\n",
            "Epoch 2:  38%|███▊      | 480/1271 [02:34<04:13,  3.12it/s, training_loss=0.311]\u001b[A\n",
            "Epoch 2:  38%|███▊      | 480/1271 [02:34<04:13,  3.12it/s, training_loss=0.165]\u001b[A\n",
            "Epoch 2:  38%|███▊      | 481/1271 [02:34<04:13,  3.12it/s, training_loss=0.165]\u001b[A\n",
            "Epoch 2:  38%|███▊      | 481/1271 [02:35<04:13,  3.12it/s, training_loss=0.580]\u001b[A\n",
            "Epoch 2:  38%|███▊      | 482/1271 [02:35<04:13,  3.11it/s, training_loss=0.580]\u001b[A\n",
            "Epoch 2:  38%|███▊      | 482/1271 [02:35<04:13,  3.11it/s, training_loss=0.314]\u001b[A\n",
            "Epoch 2:  38%|███▊      | 483/1271 [02:35<04:12,  3.12it/s, training_loss=0.314]\u001b[A\n",
            "Epoch 2:  38%|███▊      | 483/1271 [02:35<04:12,  3.12it/s, training_loss=0.349]\u001b[A\n",
            "Epoch 2:  38%|███▊      | 484/1271 [02:35<04:12,  3.11it/s, training_loss=0.349]\u001b[A\n",
            "Epoch 2:  38%|███▊      | 484/1271 [02:36<04:12,  3.11it/s, training_loss=0.354]\u001b[A\n",
            "Epoch 2:  38%|███▊      | 485/1271 [02:36<04:11,  3.12it/s, training_loss=0.354]\u001b[A\n",
            "Epoch 2:  38%|███▊      | 485/1271 [02:36<04:11,  3.12it/s, training_loss=0.450]\u001b[A\n",
            "Epoch 2:  38%|███▊      | 486/1271 [02:36<04:10,  3.13it/s, training_loss=0.450]\u001b[A\n",
            "Epoch 2:  38%|███▊      | 486/1271 [02:36<04:10,  3.13it/s, training_loss=0.233]\u001b[A\n",
            "Epoch 2:  38%|███▊      | 487/1271 [02:36<04:10,  3.13it/s, training_loss=0.233]\u001b[A\n",
            "Epoch 2:  38%|███▊      | 487/1271 [02:37<04:10,  3.13it/s, training_loss=0.335]\u001b[A\n",
            "Epoch 2:  38%|███▊      | 488/1271 [02:37<04:12,  3.11it/s, training_loss=0.335]\u001b[A\n",
            "Epoch 2:  38%|███▊      | 488/1271 [02:37<04:12,  3.11it/s, training_loss=0.409]\u001b[A\n",
            "Epoch 2:  38%|███▊      | 489/1271 [02:37<04:10,  3.12it/s, training_loss=0.409]\u001b[A\n",
            "Epoch 2:  38%|███▊      | 489/1271 [02:37<04:10,  3.12it/s, training_loss=0.477]\u001b[A\n",
            "Epoch 2:  39%|███▊      | 490/1271 [02:37<04:10,  3.12it/s, training_loss=0.477]\u001b[A\n",
            "Epoch 2:  39%|███▊      | 490/1271 [02:38<04:10,  3.12it/s, training_loss=0.622]\u001b[A\n",
            "Epoch 2:  39%|███▊      | 491/1271 [02:38<04:09,  3.13it/s, training_loss=0.622]\u001b[A\n",
            "Epoch 2:  39%|███▊      | 491/1271 [02:38<04:09,  3.13it/s, training_loss=0.893]\u001b[A\n",
            "Epoch 2:  39%|███▊      | 492/1271 [02:38<04:09,  3.12it/s, training_loss=0.893]\u001b[A\n",
            "Epoch 2:  39%|███▊      | 492/1271 [02:38<04:09,  3.12it/s, training_loss=0.641]\u001b[A\n",
            "Epoch 2:  39%|███▉      | 493/1271 [02:38<04:09,  3.12it/s, training_loss=0.641]\u001b[A\n",
            "Epoch 2:  39%|███▉      | 493/1271 [02:39<04:09,  3.12it/s, training_loss=0.339]\u001b[A\n",
            "Epoch 2:  39%|███▉      | 494/1271 [02:39<04:08,  3.13it/s, training_loss=0.339]\u001b[A\n",
            "Epoch 2:  39%|███▉      | 494/1271 [02:39<04:08,  3.13it/s, training_loss=0.372]\u001b[A\n",
            "Epoch 2:  39%|███▉      | 495/1271 [02:39<04:06,  3.15it/s, training_loss=0.372]\u001b[A\n",
            "Epoch 2:  39%|███▉      | 495/1271 [02:39<04:06,  3.15it/s, training_loss=0.467]\u001b[A\n",
            "Epoch 2:  39%|███▉      | 496/1271 [02:39<04:07,  3.13it/s, training_loss=0.467]\u001b[A\n",
            "Epoch 2:  39%|███▉      | 496/1271 [02:40<04:07,  3.13it/s, training_loss=0.559]\u001b[A\n",
            "Epoch 2:  39%|███▉      | 497/1271 [02:40<04:07,  3.13it/s, training_loss=0.559]\u001b[A\n",
            "Epoch 2:  39%|███▉      | 497/1271 [02:40<04:07,  3.13it/s, training_loss=0.694]\u001b[A\n",
            "Epoch 2:  39%|███▉      | 498/1271 [02:40<04:06,  3.13it/s, training_loss=0.694]\u001b[A\n",
            "Epoch 2:  39%|███▉      | 498/1271 [02:40<04:06,  3.13it/s, training_loss=0.794]\u001b[A\n",
            "Epoch 2:  39%|███▉      | 499/1271 [02:40<04:06,  3.14it/s, training_loss=0.794]\u001b[A\n",
            "Epoch 2:  39%|███▉      | 499/1271 [02:41<04:06,  3.14it/s, training_loss=0.570]\u001b[A\n",
            "Epoch 2:  39%|███▉      | 500/1271 [02:41<04:05,  3.14it/s, training_loss=0.570]\u001b[A\n",
            "Epoch 2:  39%|███▉      | 500/1271 [02:41<04:05,  3.14it/s, training_loss=0.210]\u001b[A\n",
            "Epoch 2:  39%|███▉      | 501/1271 [02:41<04:05,  3.14it/s, training_loss=0.210]\u001b[A\n",
            "Epoch 2:  39%|███▉      | 501/1271 [02:41<04:05,  3.14it/s, training_loss=0.255]\u001b[A\n",
            "Epoch 2:  39%|███▉      | 502/1271 [02:41<04:06,  3.12it/s, training_loss=0.255]\u001b[A\n",
            "Epoch 2:  39%|███▉      | 502/1271 [02:42<04:06,  3.12it/s, training_loss=0.165]\u001b[A\n",
            "Epoch 2:  40%|███▉      | 503/1271 [02:42<04:07,  3.11it/s, training_loss=0.165]\u001b[A\n",
            "Epoch 2:  40%|███▉      | 503/1271 [02:42<04:07,  3.11it/s, training_loss=0.552]\u001b[A\n",
            "Epoch 2:  40%|███▉      | 504/1271 [02:42<04:07,  3.10it/s, training_loss=0.552]\u001b[A\n",
            "Epoch 2:  40%|███▉      | 504/1271 [02:42<04:07,  3.10it/s, training_loss=0.157]\u001b[A\n",
            "Epoch 2:  40%|███▉      | 505/1271 [02:42<04:07,  3.09it/s, training_loss=0.157]\u001b[A\n",
            "Epoch 2:  40%|███▉      | 505/1271 [02:42<04:07,  3.09it/s, training_loss=0.027]\u001b[A\n",
            "Epoch 2:  40%|███▉      | 506/1271 [02:42<04:08,  3.08it/s, training_loss=0.027]\u001b[A\n",
            "Epoch 2:  40%|███▉      | 506/1271 [02:43<04:08,  3.08it/s, training_loss=0.017]\u001b[A\n",
            "Epoch 2:  40%|███▉      | 507/1271 [02:43<04:05,  3.11it/s, training_loss=0.017]\u001b[A\n",
            "Epoch 2:  40%|███▉      | 507/1271 [02:43<04:05,  3.11it/s, training_loss=0.849]\u001b[A\n",
            "Epoch 2:  40%|███▉      | 508/1271 [02:43<04:05,  3.11it/s, training_loss=0.849]\u001b[A\n",
            "Epoch 2:  40%|███▉      | 508/1271 [02:43<04:05,  3.11it/s, training_loss=0.621]\u001b[A\n",
            "Epoch 2:  40%|████      | 509/1271 [02:43<04:05,  3.10it/s, training_loss=0.621]\u001b[A\n",
            "Epoch 2:  40%|████      | 509/1271 [02:44<04:05,  3.10it/s, training_loss=0.497]\u001b[A\n",
            "Epoch 2:  40%|████      | 510/1271 [02:44<04:05,  3.11it/s, training_loss=0.497]\u001b[A\n",
            "Epoch 2:  40%|████      | 510/1271 [02:44<04:05,  3.11it/s, training_loss=0.682]\u001b[A\n",
            "Epoch 2:  40%|████      | 511/1271 [02:44<04:04,  3.11it/s, training_loss=0.682]\u001b[A\n",
            "Epoch 2:  40%|████      | 511/1271 [02:44<04:04,  3.11it/s, training_loss=0.786]\u001b[A\n",
            "Epoch 2:  40%|████      | 512/1271 [02:44<04:05,  3.09it/s, training_loss=0.786]\u001b[A\n",
            "Epoch 2:  40%|████      | 512/1271 [02:45<04:05,  3.09it/s, training_loss=0.440]\u001b[A\n",
            "Epoch 2:  40%|████      | 513/1271 [02:45<04:05,  3.08it/s, training_loss=0.440]\u001b[A\n",
            "Epoch 2:  40%|████      | 513/1271 [02:45<04:05,  3.08it/s, training_loss=0.205]\u001b[A\n",
            "Epoch 2:  40%|████      | 514/1271 [02:45<04:04,  3.09it/s, training_loss=0.205]\u001b[A\n",
            "Epoch 2:  40%|████      | 514/1271 [02:45<04:04,  3.09it/s, training_loss=0.366]\u001b[A\n",
            "Epoch 2:  41%|████      | 515/1271 [02:45<04:04,  3.09it/s, training_loss=0.366]\u001b[A\n",
            "Epoch 2:  41%|████      | 515/1271 [02:46<04:04,  3.09it/s, training_loss=0.444]\u001b[A\n",
            "Epoch 2:  41%|████      | 516/1271 [02:46<04:04,  3.09it/s, training_loss=0.444]\u001b[A\n",
            "Epoch 2:  41%|████      | 516/1271 [02:46<04:04,  3.09it/s, training_loss=0.419]\u001b[A\n",
            "Epoch 2:  41%|████      | 517/1271 [02:46<04:03,  3.09it/s, training_loss=0.419]\u001b[A\n",
            "Epoch 2:  41%|████      | 517/1271 [02:46<04:03,  3.09it/s, training_loss=0.723]\u001b[A\n",
            "Epoch 2:  41%|████      | 518/1271 [02:46<04:03,  3.10it/s, training_loss=0.723]\u001b[A\n",
            "Epoch 2:  41%|████      | 518/1271 [02:47<04:03,  3.10it/s, training_loss=0.527]\u001b[A\n",
            "Epoch 2:  41%|████      | 519/1271 [02:47<04:01,  3.11it/s, training_loss=0.527]\u001b[A\n",
            "Epoch 2:  41%|████      | 519/1271 [02:47<04:01,  3.11it/s, training_loss=0.867]\u001b[A\n",
            "Epoch 2:  41%|████      | 520/1271 [02:47<04:01,  3.11it/s, training_loss=0.867]\u001b[A\n",
            "Epoch 2:  41%|████      | 520/1271 [02:47<04:01,  3.11it/s, training_loss=0.011]\u001b[A\n",
            "Epoch 2:  41%|████      | 521/1271 [02:47<04:01,  3.10it/s, training_loss=0.011]\u001b[A\n",
            "Epoch 2:  41%|████      | 521/1271 [02:48<04:01,  3.10it/s, training_loss=0.500]\u001b[A\n",
            "Epoch 2:  41%|████      | 522/1271 [02:48<04:01,  3.11it/s, training_loss=0.500]\u001b[A\n",
            "Epoch 2:  41%|████      | 522/1271 [02:48<04:01,  3.11it/s, training_loss=0.551]\u001b[A\n",
            "Epoch 2:  41%|████      | 523/1271 [02:48<04:00,  3.11it/s, training_loss=0.551]\u001b[A\n",
            "Epoch 2:  41%|████      | 523/1271 [02:48<04:00,  3.11it/s, training_loss=0.726]\u001b[A\n",
            "Epoch 2:  41%|████      | 524/1271 [02:48<04:00,  3.11it/s, training_loss=0.726]\u001b[A\n",
            "Epoch 2:  41%|████      | 524/1271 [02:49<04:00,  3.11it/s, training_loss=0.259]\u001b[A\n",
            "Epoch 2:  41%|████▏     | 525/1271 [02:49<03:58,  3.12it/s, training_loss=0.259]\u001b[A\n",
            "Epoch 2:  41%|████▏     | 525/1271 [02:49<03:58,  3.12it/s, training_loss=0.144]\u001b[A\n",
            "Epoch 2:  41%|████▏     | 526/1271 [02:49<03:58,  3.12it/s, training_loss=0.144]\u001b[A\n",
            "Epoch 2:  41%|████▏     | 526/1271 [02:49<03:58,  3.12it/s, training_loss=0.187]\u001b[A\n",
            "Epoch 2:  41%|████▏     | 527/1271 [02:49<03:59,  3.11it/s, training_loss=0.187]\u001b[A\n",
            "Epoch 2:  41%|████▏     | 527/1271 [02:50<03:59,  3.11it/s, training_loss=0.289]\u001b[A\n",
            "Epoch 2:  42%|████▏     | 528/1271 [02:50<03:59,  3.10it/s, training_loss=0.289]\u001b[A\n",
            "Epoch 2:  42%|████▏     | 528/1271 [02:50<03:59,  3.10it/s, training_loss=0.454]\u001b[A\n",
            "Epoch 2:  42%|████▏     | 529/1271 [02:50<03:58,  3.11it/s, training_loss=0.454]\u001b[A\n",
            "Epoch 2:  42%|████▏     | 529/1271 [02:50<03:58,  3.11it/s, training_loss=0.697]\u001b[A\n",
            "Epoch 2:  42%|████▏     | 530/1271 [02:50<03:59,  3.10it/s, training_loss=0.697]\u001b[A\n",
            "Epoch 2:  42%|████▏     | 530/1271 [02:51<03:59,  3.10it/s, training_loss=0.415]\u001b[A\n",
            "Epoch 2:  42%|████▏     | 531/1271 [02:51<03:58,  3.10it/s, training_loss=0.415]\u001b[A\n",
            "Epoch 2:  42%|████▏     | 531/1271 [02:51<03:58,  3.10it/s, training_loss=0.318]\u001b[A\n",
            "Epoch 2:  42%|████▏     | 532/1271 [02:51<03:57,  3.11it/s, training_loss=0.318]\u001b[A\n",
            "Epoch 2:  42%|████▏     | 532/1271 [02:51<03:57,  3.11it/s, training_loss=0.213]\u001b[A\n",
            "Epoch 2:  42%|████▏     | 533/1271 [02:51<03:56,  3.12it/s, training_loss=0.213]\u001b[A\n",
            "Epoch 2:  42%|████▏     | 533/1271 [02:51<03:56,  3.12it/s, training_loss=0.571]\u001b[A\n",
            "Epoch 2:  42%|████▏     | 534/1271 [02:51<03:55,  3.13it/s, training_loss=0.571]\u001b[A\n",
            "Epoch 2:  42%|████▏     | 534/1271 [02:52<03:55,  3.13it/s, training_loss=0.581]\u001b[A\n",
            "Epoch 2:  42%|████▏     | 535/1271 [02:52<03:55,  3.13it/s, training_loss=0.581]\u001b[A\n",
            "Epoch 2:  42%|████▏     | 535/1271 [02:52<03:55,  3.13it/s, training_loss=0.670]\u001b[A\n",
            "Epoch 2:  42%|████▏     | 536/1271 [02:52<03:55,  3.12it/s, training_loss=0.670]\u001b[A\n",
            "Epoch 2:  42%|████▏     | 536/1271 [02:52<03:55,  3.12it/s, training_loss=0.491]\u001b[A\n",
            "Epoch 2:  42%|████▏     | 537/1271 [02:52<03:55,  3.12it/s, training_loss=0.491]\u001b[A\n",
            "Epoch 2:  42%|████▏     | 537/1271 [02:53<03:55,  3.12it/s, training_loss=0.540]\u001b[A\n",
            "Epoch 2:  42%|████▏     | 538/1271 [02:53<03:55,  3.11it/s, training_loss=0.540]\u001b[A\n",
            "Epoch 2:  42%|████▏     | 538/1271 [02:53<03:55,  3.11it/s, training_loss=0.785]\u001b[A\n",
            "Epoch 2:  42%|████▏     | 539/1271 [02:53<03:53,  3.13it/s, training_loss=0.785]\u001b[A\n",
            "Epoch 2:  42%|████▏     | 539/1271 [02:53<03:53,  3.13it/s, training_loss=0.169]\u001b[A\n",
            "Epoch 2:  42%|████▏     | 540/1271 [02:53<03:55,  3.10it/s, training_loss=0.169]\u001b[A\n",
            "Epoch 2:  42%|████▏     | 540/1271 [02:54<03:55,  3.10it/s, training_loss=0.540]\u001b[A\n",
            "Epoch 2:  43%|████▎     | 541/1271 [02:54<03:55,  3.10it/s, training_loss=0.540]\u001b[A\n",
            "Epoch 2:  43%|████▎     | 541/1271 [02:54<03:55,  3.10it/s, training_loss=0.471]\u001b[A\n",
            "Epoch 2:  43%|████▎     | 542/1271 [02:54<03:53,  3.12it/s, training_loss=0.471]\u001b[A\n",
            "Epoch 2:  43%|████▎     | 542/1271 [02:54<03:53,  3.12it/s, training_loss=0.807]\u001b[A\n",
            "Epoch 2:  43%|████▎     | 543/1271 [02:54<03:54,  3.11it/s, training_loss=0.807]\u001b[A\n",
            "Epoch 2:  43%|████▎     | 543/1271 [02:55<03:54,  3.11it/s, training_loss=0.475]\u001b[A\n",
            "Epoch 2:  43%|████▎     | 544/1271 [02:55<03:53,  3.11it/s, training_loss=0.475]\u001b[A\n",
            "Epoch 2:  43%|████▎     | 544/1271 [02:55<03:53,  3.11it/s, training_loss=0.283]\u001b[A\n",
            "Epoch 2:  43%|████▎     | 545/1271 [02:55<03:52,  3.12it/s, training_loss=0.283]\u001b[A\n",
            "Epoch 2:  43%|████▎     | 545/1271 [02:55<03:52,  3.12it/s, training_loss=0.112]\u001b[A\n",
            "Epoch 2:  43%|████▎     | 546/1271 [02:55<03:52,  3.12it/s, training_loss=0.112]\u001b[A\n",
            "Epoch 2:  43%|████▎     | 546/1271 [02:56<03:52,  3.12it/s, training_loss=0.215]\u001b[A\n",
            "Epoch 2:  43%|████▎     | 547/1271 [02:56<03:53,  3.10it/s, training_loss=0.215]\u001b[A\n",
            "Epoch 2:  43%|████▎     | 547/1271 [02:56<03:53,  3.10it/s, training_loss=0.412]\u001b[A\n",
            "Epoch 2:  43%|████▎     | 548/1271 [02:56<03:54,  3.09it/s, training_loss=0.412]\u001b[A\n",
            "Epoch 2:  43%|████▎     | 548/1271 [02:56<03:54,  3.09it/s, training_loss=0.498]\u001b[A\n",
            "Epoch 2:  43%|████▎     | 549/1271 [02:56<03:53,  3.09it/s, training_loss=0.498]\u001b[A\n",
            "Epoch 2:  43%|████▎     | 549/1271 [02:57<03:53,  3.09it/s, training_loss=0.588]\u001b[A\n",
            "Epoch 2:  43%|████▎     | 550/1271 [02:57<03:51,  3.11it/s, training_loss=0.588]\u001b[A\n",
            "Epoch 2:  43%|████▎     | 550/1271 [02:57<03:51,  3.11it/s, training_loss=0.255]\u001b[A\n",
            "Epoch 2:  43%|████▎     | 551/1271 [02:57<03:50,  3.13it/s, training_loss=0.255]\u001b[A\n",
            "Epoch 2:  43%|████▎     | 551/1271 [02:57<03:50,  3.13it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 2:  43%|████▎     | 552/1271 [02:57<03:49,  3.14it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 2:  43%|████▎     | 552/1271 [02:58<03:49,  3.14it/s, training_loss=0.163]\u001b[A\n",
            "Epoch 2:  44%|████▎     | 553/1271 [02:58<03:51,  3.10it/s, training_loss=0.163]\u001b[A\n",
            "Epoch 2:  44%|████▎     | 553/1271 [02:58<03:51,  3.10it/s, training_loss=0.373]\u001b[A\n",
            "Epoch 2:  44%|████▎     | 554/1271 [02:58<03:51,  3.10it/s, training_loss=0.373]\u001b[A\n",
            "Epoch 2:  44%|████▎     | 554/1271 [02:58<03:51,  3.10it/s, training_loss=0.504]\u001b[A\n",
            "Epoch 2:  44%|████▎     | 555/1271 [02:58<03:51,  3.09it/s, training_loss=0.504]\u001b[A\n",
            "Epoch 2:  44%|████▎     | 555/1271 [02:59<03:51,  3.09it/s, training_loss=0.240]\u001b[A\n",
            "Epoch 2:  44%|████▎     | 556/1271 [02:59<03:52,  3.08it/s, training_loss=0.240]\u001b[A\n",
            "Epoch 2:  44%|████▎     | 556/1271 [02:59<03:52,  3.08it/s, training_loss=0.015]\u001b[A\n",
            "Epoch 2:  44%|████▍     | 557/1271 [02:59<03:51,  3.08it/s, training_loss=0.015]\u001b[A\n",
            "Epoch 2:  44%|████▍     | 557/1271 [02:59<03:51,  3.08it/s, training_loss=0.727]\u001b[A\n",
            "Epoch 2:  44%|████▍     | 558/1271 [02:59<03:50,  3.09it/s, training_loss=0.727]\u001b[A\n",
            "Epoch 2:  44%|████▍     | 558/1271 [03:00<03:50,  3.09it/s, training_loss=0.375]\u001b[A\n",
            "Epoch 2:  44%|████▍     | 559/1271 [03:00<03:49,  3.10it/s, training_loss=0.375]\u001b[A\n",
            "Epoch 2:  44%|████▍     | 559/1271 [03:00<03:49,  3.10it/s, training_loss=0.661]\u001b[A\n",
            "Epoch 2:  44%|████▍     | 560/1271 [03:00<03:50,  3.09it/s, training_loss=0.661]\u001b[A\n",
            "Epoch 2:  44%|████▍     | 560/1271 [03:00<03:50,  3.09it/s, training_loss=0.382]\u001b[A\n",
            "Epoch 2:  44%|████▍     | 561/1271 [03:00<03:50,  3.09it/s, training_loss=0.382]\u001b[A\n",
            "Epoch 2:  44%|████▍     | 561/1271 [03:01<03:50,  3.09it/s, training_loss=0.200]\u001b[A\n",
            "Epoch 2:  44%|████▍     | 562/1271 [03:01<03:49,  3.09it/s, training_loss=0.200]\u001b[A\n",
            "Epoch 2:  44%|████▍     | 562/1271 [03:01<03:49,  3.09it/s, training_loss=0.509]\u001b[A\n",
            "Epoch 2:  44%|████▍     | 563/1271 [03:01<03:48,  3.10it/s, training_loss=0.509]\u001b[A\n",
            "Epoch 2:  44%|████▍     | 563/1271 [03:01<03:48,  3.10it/s, training_loss=0.676]\u001b[A\n",
            "Epoch 2:  44%|████▍     | 564/1271 [03:01<03:47,  3.11it/s, training_loss=0.676]\u001b[A\n",
            "Epoch 2:  44%|████▍     | 564/1271 [03:01<03:47,  3.11it/s, training_loss=0.528]\u001b[A\n",
            "Epoch 2:  44%|████▍     | 565/1271 [03:01<03:46,  3.12it/s, training_loss=0.528]\u001b[A\n",
            "Epoch 2:  44%|████▍     | 565/1271 [03:02<03:46,  3.12it/s, training_loss=0.483]\u001b[A\n",
            "Epoch 2:  45%|████▍     | 566/1271 [03:02<03:46,  3.11it/s, training_loss=0.483]\u001b[A\n",
            "Epoch 2:  45%|████▍     | 566/1271 [03:02<03:46,  3.11it/s, training_loss=0.602]\u001b[A\n",
            "Epoch 2:  45%|████▍     | 567/1271 [03:02<03:47,  3.09it/s, training_loss=0.602]\u001b[A\n",
            "Epoch 2:  45%|████▍     | 567/1271 [03:02<03:47,  3.09it/s, training_loss=0.181]\u001b[A\n",
            "Epoch 2:  45%|████▍     | 568/1271 [03:02<03:46,  3.10it/s, training_loss=0.181]\u001b[A\n",
            "Epoch 2:  45%|████▍     | 568/1271 [03:03<03:46,  3.10it/s, training_loss=0.015]\u001b[A\n",
            "Epoch 2:  45%|████▍     | 569/1271 [03:03<03:47,  3.08it/s, training_loss=0.015]\u001b[A\n",
            "Epoch 2:  45%|████▍     | 569/1271 [03:03<03:47,  3.08it/s, training_loss=0.571]\u001b[A\n",
            "Epoch 2:  45%|████▍     | 570/1271 [03:03<03:48,  3.07it/s, training_loss=0.571]\u001b[A\n",
            "Epoch 2:  45%|████▍     | 570/1271 [03:03<03:48,  3.07it/s, training_loss=0.841]\u001b[A\n",
            "Epoch 2:  45%|████▍     | 571/1271 [03:03<03:48,  3.06it/s, training_loss=0.841]\u001b[A\n",
            "Epoch 2:  45%|████▍     | 571/1271 [03:04<03:48,  3.06it/s, training_loss=0.356]\u001b[A\n",
            "Epoch 2:  45%|████▌     | 572/1271 [03:04<03:48,  3.06it/s, training_loss=0.356]\u001b[A\n",
            "Epoch 2:  45%|████▌     | 572/1271 [03:04<03:48,  3.06it/s, training_loss=0.279]\u001b[A\n",
            "Epoch 2:  45%|████▌     | 573/1271 [03:04<03:46,  3.08it/s, training_loss=0.279]\u001b[A\n",
            "Epoch 2:  45%|████▌     | 573/1271 [03:04<03:46,  3.08it/s, training_loss=0.089]\u001b[A\n",
            "Epoch 2:  45%|████▌     | 574/1271 [03:04<03:44,  3.11it/s, training_loss=0.089]\u001b[A\n",
            "Epoch 2:  45%|████▌     | 574/1271 [03:05<03:44,  3.11it/s, training_loss=0.258]\u001b[A\n",
            "Epoch 2:  45%|████▌     | 575/1271 [03:05<03:42,  3.13it/s, training_loss=0.258]\u001b[A\n",
            "Epoch 2:  45%|████▌     | 575/1271 [03:05<03:42,  3.13it/s, training_loss=0.803]\u001b[A\n",
            "Epoch 2:  45%|████▌     | 576/1271 [03:05<03:42,  3.12it/s, training_loss=0.803]\u001b[A\n",
            "Epoch 2:  45%|████▌     | 576/1271 [03:05<03:42,  3.12it/s, training_loss=0.702]\u001b[A\n",
            "Epoch 2:  45%|████▌     | 577/1271 [03:05<03:42,  3.12it/s, training_loss=0.702]\u001b[A\n",
            "Epoch 2:  45%|████▌     | 577/1271 [03:06<03:42,  3.12it/s, training_loss=0.665]\u001b[A\n",
            "Epoch 2:  45%|████▌     | 578/1271 [03:06<03:43,  3.10it/s, training_loss=0.665]\u001b[A\n",
            "Epoch 2:  45%|████▌     | 578/1271 [03:06<03:43,  3.10it/s, training_loss=0.206]\u001b[A\n",
            "Epoch 2:  46%|████▌     | 579/1271 [03:06<03:41,  3.12it/s, training_loss=0.206]\u001b[A\n",
            "Epoch 2:  46%|████▌     | 579/1271 [03:06<03:41,  3.12it/s, training_loss=0.527]\u001b[A\n",
            "Epoch 2:  46%|████▌     | 580/1271 [03:06<03:41,  3.12it/s, training_loss=0.527]\u001b[A\n",
            "Epoch 2:  46%|████▌     | 580/1271 [03:07<03:41,  3.12it/s, training_loss=0.589]\u001b[A\n",
            "Epoch 2:  46%|████▌     | 581/1271 [03:07<03:42,  3.10it/s, training_loss=0.589]\u001b[A\n",
            "Epoch 2:  46%|████▌     | 581/1271 [03:07<03:42,  3.10it/s, training_loss=0.261]\u001b[A\n",
            "Epoch 2:  46%|████▌     | 582/1271 [03:07<03:44,  3.07it/s, training_loss=0.261]\u001b[A\n",
            "Epoch 2:  46%|████▌     | 582/1271 [03:07<03:44,  3.07it/s, training_loss=0.534]\u001b[A\n",
            "Epoch 2:  46%|████▌     | 583/1271 [03:07<03:44,  3.06it/s, training_loss=0.534]\u001b[A\n",
            "Epoch 2:  46%|████▌     | 583/1271 [03:08<03:44,  3.06it/s, training_loss=0.422]\u001b[A\n",
            "Epoch 2:  46%|████▌     | 584/1271 [03:08<03:43,  3.08it/s, training_loss=0.422]\u001b[A\n",
            "Epoch 2:  46%|████▌     | 584/1271 [03:08<03:43,  3.08it/s, training_loss=0.306]\u001b[A\n",
            "Epoch 2:  46%|████▌     | 585/1271 [03:08<03:43,  3.07it/s, training_loss=0.306]\u001b[A\n",
            "Epoch 2:  46%|████▌     | 585/1271 [03:08<03:43,  3.07it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 2:  46%|████▌     | 586/1271 [03:08<03:42,  3.08it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 2:  46%|████▌     | 586/1271 [03:09<03:42,  3.08it/s, training_loss=0.458]\u001b[A\n",
            "Epoch 2:  46%|████▌     | 587/1271 [03:09<03:42,  3.08it/s, training_loss=0.458]\u001b[A\n",
            "Epoch 2:  46%|████▌     | 587/1271 [03:09<03:42,  3.08it/s, training_loss=0.660]\u001b[A\n",
            "Epoch 2:  46%|████▋     | 588/1271 [03:09<03:42,  3.07it/s, training_loss=0.660]\u001b[A\n",
            "Epoch 2:  46%|████▋     | 588/1271 [03:09<03:42,  3.07it/s, training_loss=0.689]\u001b[A\n",
            "Epoch 2:  46%|████▋     | 589/1271 [03:09<03:40,  3.09it/s, training_loss=0.689]\u001b[A\n",
            "Epoch 2:  46%|████▋     | 589/1271 [03:10<03:40,  3.09it/s, training_loss=0.879]\u001b[A\n",
            "Epoch 2:  46%|████▋     | 590/1271 [03:10<03:39,  3.10it/s, training_loss=0.879]\u001b[A\n",
            "Epoch 2:  46%|████▋     | 590/1271 [03:10<03:39,  3.10it/s, training_loss=0.193]\u001b[A\n",
            "Epoch 2:  46%|████▋     | 591/1271 [03:10<03:38,  3.11it/s, training_loss=0.193]\u001b[A\n",
            "Epoch 2:  46%|████▋     | 591/1271 [03:10<03:38,  3.11it/s, training_loss=0.313]\u001b[A\n",
            "Epoch 2:  47%|████▋     | 592/1271 [03:10<03:38,  3.11it/s, training_loss=0.313]\u001b[A\n",
            "Epoch 2:  47%|████▋     | 592/1271 [03:11<03:38,  3.11it/s, training_loss=0.189]\u001b[A\n",
            "Epoch 2:  47%|████▋     | 593/1271 [03:11<03:38,  3.10it/s, training_loss=0.189]\u001b[A\n",
            "Epoch 2:  47%|████▋     | 593/1271 [03:11<03:38,  3.10it/s, training_loss=0.205]\u001b[A\n",
            "Epoch 2:  47%|████▋     | 594/1271 [03:11<03:36,  3.12it/s, training_loss=0.205]\u001b[A\n",
            "Epoch 2:  47%|████▋     | 594/1271 [03:11<03:36,  3.12it/s, training_loss=0.806]\u001b[A\n",
            "Epoch 2:  47%|████▋     | 595/1271 [03:11<03:36,  3.12it/s, training_loss=0.806]\u001b[A\n",
            "Epoch 2:  47%|████▋     | 595/1271 [03:11<03:36,  3.12it/s, training_loss=0.246]\u001b[A\n",
            "Epoch 2:  47%|████▋     | 596/1271 [03:11<03:37,  3.11it/s, training_loss=0.246]\u001b[A\n",
            "Epoch 2:  47%|████▋     | 596/1271 [03:12<03:37,  3.11it/s, training_loss=0.514]\u001b[A\n",
            "Epoch 2:  47%|████▋     | 597/1271 [03:12<03:35,  3.12it/s, training_loss=0.514]\u001b[A\n",
            "Epoch 2:  47%|████▋     | 597/1271 [03:12<03:35,  3.12it/s, training_loss=0.410]\u001b[A\n",
            "Epoch 2:  47%|████▋     | 598/1271 [03:12<03:35,  3.12it/s, training_loss=0.410]\u001b[A\n",
            "Epoch 2:  47%|████▋     | 598/1271 [03:12<03:35,  3.12it/s, training_loss=0.348]\u001b[A\n",
            "Epoch 2:  47%|████▋     | 599/1271 [03:12<03:35,  3.11it/s, training_loss=0.348]\u001b[A\n",
            "Epoch 2:  47%|████▋     | 599/1271 [03:13<03:35,  3.11it/s, training_loss=0.588]\u001b[A\n",
            "Epoch 2:  47%|████▋     | 600/1271 [03:13<03:35,  3.12it/s, training_loss=0.588]\u001b[A\n",
            "Epoch 2:  47%|████▋     | 600/1271 [03:13<03:35,  3.12it/s, training_loss=1.088]\u001b[A\n",
            "Epoch 2:  47%|████▋     | 601/1271 [03:13<03:35,  3.12it/s, training_loss=1.088]\u001b[A\n",
            "Epoch 2:  47%|████▋     | 601/1271 [03:13<03:35,  3.12it/s, training_loss=0.490]\u001b[A\n",
            "Epoch 2:  47%|████▋     | 602/1271 [03:13<03:34,  3.12it/s, training_loss=0.490]\u001b[A\n",
            "Epoch 2:  47%|████▋     | 602/1271 [03:14<03:34,  3.12it/s, training_loss=0.370]\u001b[A\n",
            "Epoch 2:  47%|████▋     | 603/1271 [03:14<03:34,  3.11it/s, training_loss=0.370]\u001b[A\n",
            "Epoch 2:  47%|████▋     | 603/1271 [03:14<03:34,  3.11it/s, training_loss=0.011]\u001b[A\n",
            "Epoch 2:  48%|████▊     | 604/1271 [03:14<03:33,  3.12it/s, training_loss=0.011]\u001b[A\n",
            "Epoch 2:  48%|████▊     | 604/1271 [03:14<03:33,  3.12it/s, training_loss=0.213]\u001b[A\n",
            "Epoch 2:  48%|████▊     | 605/1271 [03:14<03:33,  3.12it/s, training_loss=0.213]\u001b[A\n",
            "Epoch 2:  48%|████▊     | 605/1271 [03:15<03:33,  3.12it/s, training_loss=0.633]\u001b[A\n",
            "Epoch 2:  48%|████▊     | 606/1271 [03:15<03:34,  3.09it/s, training_loss=0.633]\u001b[A\n",
            "Epoch 2:  48%|████▊     | 606/1271 [03:15<03:34,  3.09it/s, training_loss=0.224]\u001b[A\n",
            "Epoch 2:  48%|████▊     | 607/1271 [03:15<03:33,  3.11it/s, training_loss=0.224]\u001b[A\n",
            "Epoch 2:  48%|████▊     | 607/1271 [03:15<03:33,  3.11it/s, training_loss=0.249]\u001b[A\n",
            "Epoch 2:  48%|████▊     | 608/1271 [03:15<03:32,  3.12it/s, training_loss=0.249]\u001b[A\n",
            "Epoch 2:  48%|████▊     | 608/1271 [03:16<03:32,  3.12it/s, training_loss=0.014]\u001b[A\n",
            "Epoch 2:  48%|████▊     | 609/1271 [03:16<03:30,  3.14it/s, training_loss=0.014]\u001b[A\n",
            "Epoch 2:  48%|████▊     | 609/1271 [03:16<03:30,  3.14it/s, training_loss=0.087]\u001b[A\n",
            "Epoch 2:  48%|████▊     | 610/1271 [03:16<03:31,  3.12it/s, training_loss=0.087]\u001b[A\n",
            "Epoch 2:  48%|████▊     | 610/1271 [03:16<03:31,  3.12it/s, training_loss=0.159]\u001b[A\n",
            "Epoch 2:  48%|████▊     | 611/1271 [03:16<03:30,  3.13it/s, training_loss=0.159]\u001b[A\n",
            "Epoch 2:  48%|████▊     | 611/1271 [03:17<03:30,  3.13it/s, training_loss=0.450]\u001b[A\n",
            "Epoch 2:  48%|████▊     | 612/1271 [03:17<03:30,  3.13it/s, training_loss=0.450]\u001b[A\n",
            "Epoch 2:  48%|████▊     | 612/1271 [03:17<03:30,  3.13it/s, training_loss=0.233]\u001b[A\n",
            "Epoch 2:  48%|████▊     | 613/1271 [03:17<03:30,  3.12it/s, training_loss=0.233]\u001b[A\n",
            "Epoch 2:  48%|████▊     | 613/1271 [03:17<03:30,  3.12it/s, training_loss=0.546]\u001b[A\n",
            "Epoch 2:  48%|████▊     | 614/1271 [03:17<03:31,  3.10it/s, training_loss=0.546]\u001b[A\n",
            "Epoch 2:  48%|████▊     | 614/1271 [03:18<03:31,  3.10it/s, training_loss=0.411]\u001b[A\n",
            "Epoch 2:  48%|████▊     | 615/1271 [03:18<03:31,  3.10it/s, training_loss=0.411]\u001b[A\n",
            "Epoch 2:  48%|████▊     | 615/1271 [03:18<03:31,  3.10it/s, training_loss=0.799]\u001b[A\n",
            "Epoch 2:  48%|████▊     | 616/1271 [03:18<03:32,  3.08it/s, training_loss=0.799]\u001b[A\n",
            "Epoch 2:  48%|████▊     | 616/1271 [03:18<03:32,  3.08it/s, training_loss=0.452]\u001b[A\n",
            "Epoch 2:  49%|████▊     | 617/1271 [03:18<03:31,  3.09it/s, training_loss=0.452]\u001b[A\n",
            "Epoch 2:  49%|████▊     | 617/1271 [03:19<03:31,  3.09it/s, training_loss=0.543]\u001b[A\n",
            "Epoch 2:  49%|████▊     | 618/1271 [03:19<03:31,  3.09it/s, training_loss=0.543]\u001b[A\n",
            "Epoch 2:  49%|████▊     | 618/1271 [03:19<03:31,  3.09it/s, training_loss=0.450]\u001b[A\n",
            "Epoch 2:  49%|████▊     | 619/1271 [03:19<03:30,  3.10it/s, training_loss=0.450]\u001b[A\n",
            "Epoch 2:  49%|████▊     | 619/1271 [03:19<03:30,  3.10it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 2:  49%|████▉     | 620/1271 [03:19<03:29,  3.11it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 2:  49%|████▉     | 620/1271 [03:20<03:29,  3.11it/s, training_loss=0.708]\u001b[A\n",
            "Epoch 2:  49%|████▉     | 621/1271 [03:20<03:30,  3.09it/s, training_loss=0.708]\u001b[A\n",
            "Epoch 2:  49%|████▉     | 621/1271 [03:20<03:30,  3.09it/s, training_loss=0.882]\u001b[A\n",
            "Epoch 2:  49%|████▉     | 622/1271 [03:20<03:30,  3.09it/s, training_loss=0.882]\u001b[A\n",
            "Epoch 2:  49%|████▉     | 622/1271 [03:20<03:30,  3.09it/s, training_loss=0.569]\u001b[A\n",
            "Epoch 2:  49%|████▉     | 623/1271 [03:20<03:29,  3.10it/s, training_loss=0.569]\u001b[A\n",
            "Epoch 2:  49%|████▉     | 623/1271 [03:20<03:29,  3.10it/s, training_loss=0.366]\u001b[A\n",
            "Epoch 2:  49%|████▉     | 624/1271 [03:21<03:27,  3.11it/s, training_loss=0.366]\u001b[A\n",
            "Epoch 2:  49%|████▉     | 624/1271 [03:21<03:27,  3.11it/s, training_loss=0.336]\u001b[A\n",
            "Epoch 2:  49%|████▉     | 625/1271 [03:21<03:28,  3.09it/s, training_loss=0.336]\u001b[A\n",
            "Epoch 2:  49%|████▉     | 625/1271 [03:21<03:28,  3.09it/s, training_loss=0.122]\u001b[A\n",
            "Epoch 2:  49%|████▉     | 626/1271 [03:21<03:28,  3.10it/s, training_loss=0.122]\u001b[A\n",
            "Epoch 2:  49%|████▉     | 626/1271 [03:21<03:28,  3.10it/s, training_loss=0.579]\u001b[A\n",
            "Epoch 2:  49%|████▉     | 627/1271 [03:21<03:26,  3.11it/s, training_loss=0.579]\u001b[A\n",
            "Epoch 2:  49%|████▉     | 627/1271 [03:22<03:26,  3.11it/s, training_loss=0.562]\u001b[A\n",
            "Epoch 2:  49%|████▉     | 628/1271 [03:22<03:26,  3.12it/s, training_loss=0.562]\u001b[A\n",
            "Epoch 2:  49%|████▉     | 628/1271 [03:22<03:26,  3.12it/s, training_loss=0.441]\u001b[A\n",
            "Epoch 2:  49%|████▉     | 629/1271 [03:22<03:26,  3.11it/s, training_loss=0.441]\u001b[A\n",
            "Epoch 2:  49%|████▉     | 629/1271 [03:22<03:26,  3.11it/s, training_loss=0.755]\u001b[A\n",
            "Epoch 2:  50%|████▉     | 630/1271 [03:22<03:26,  3.11it/s, training_loss=0.755]\u001b[A\n",
            "Epoch 2:  50%|████▉     | 630/1271 [03:23<03:26,  3.11it/s, training_loss=0.194]\u001b[A\n",
            "Epoch 2:  50%|████▉     | 631/1271 [03:23<03:25,  3.11it/s, training_loss=0.194]\u001b[A\n",
            "Epoch 2:  50%|████▉     | 631/1271 [03:23<03:25,  3.11it/s, training_loss=0.484]\u001b[A\n",
            "Epoch 2:  50%|████▉     | 632/1271 [03:23<03:25,  3.10it/s, training_loss=0.484]\u001b[A\n",
            "Epoch 2:  50%|████▉     | 632/1271 [03:23<03:25,  3.10it/s, training_loss=0.611]\u001b[A\n",
            "Epoch 2:  50%|████▉     | 633/1271 [03:23<03:25,  3.10it/s, training_loss=0.611]\u001b[A\n",
            "Epoch 2:  50%|████▉     | 633/1271 [03:24<03:25,  3.10it/s, training_loss=0.330]\u001b[A\n",
            "Epoch 2:  50%|████▉     | 634/1271 [03:24<03:24,  3.12it/s, training_loss=0.330]\u001b[A\n",
            "Epoch 2:  50%|████▉     | 634/1271 [03:24<03:24,  3.12it/s, training_loss=0.630]\u001b[A\n",
            "Epoch 2:  50%|████▉     | 635/1271 [03:24<03:22,  3.13it/s, training_loss=0.630]\u001b[A\n",
            "Epoch 2:  50%|████▉     | 635/1271 [03:24<03:22,  3.13it/s, training_loss=1.089]\u001b[A\n",
            "Epoch 2:  50%|█████     | 636/1271 [03:24<03:22,  3.14it/s, training_loss=1.089]\u001b[A\n",
            "Epoch 2:  50%|█████     | 636/1271 [03:25<03:22,  3.14it/s, training_loss=0.608]\u001b[A\n",
            "Epoch 2:  50%|█████     | 637/1271 [03:25<03:22,  3.14it/s, training_loss=0.608]\u001b[A\n",
            "Epoch 2:  50%|█████     | 637/1271 [03:25<03:22,  3.14it/s, training_loss=0.173]\u001b[A\n",
            "Epoch 2:  50%|█████     | 638/1271 [03:25<03:21,  3.14it/s, training_loss=0.173]\u001b[A\n",
            "Epoch 2:  50%|█████     | 638/1271 [03:25<03:21,  3.14it/s, training_loss=0.454]\u001b[A\n",
            "Epoch 2:  50%|█████     | 639/1271 [03:25<03:21,  3.13it/s, training_loss=0.454]\u001b[A\n",
            "Epoch 2:  50%|█████     | 639/1271 [03:26<03:21,  3.13it/s, training_loss=0.495]\u001b[A\n",
            "Epoch 2:  50%|█████     | 640/1271 [03:26<03:23,  3.10it/s, training_loss=0.495]\u001b[A\n",
            "Epoch 2:  50%|█████     | 640/1271 [03:26<03:23,  3.10it/s, training_loss=0.348]\u001b[A\n",
            "Epoch 2:  50%|█████     | 641/1271 [03:26<03:25,  3.07it/s, training_loss=0.348]\u001b[A\n",
            "Epoch 2:  50%|█████     | 641/1271 [03:26<03:25,  3.07it/s, training_loss=0.401]\u001b[A\n",
            "Epoch 2:  51%|█████     | 642/1271 [03:26<03:23,  3.09it/s, training_loss=0.401]\u001b[A\n",
            "Epoch 2:  51%|█████     | 642/1271 [03:27<03:23,  3.09it/s, training_loss=0.010]\u001b[A\n",
            "Epoch 2:  51%|█████     | 643/1271 [03:27<03:23,  3.08it/s, training_loss=0.010]\u001b[A\n",
            "Epoch 2:  51%|█████     | 643/1271 [03:27<03:23,  3.08it/s, training_loss=0.523]\u001b[A\n",
            "Epoch 2:  51%|█████     | 644/1271 [03:27<03:23,  3.08it/s, training_loss=0.523]\u001b[A\n",
            "Epoch 2:  51%|█████     | 644/1271 [03:27<03:23,  3.08it/s, training_loss=0.290]\u001b[A\n",
            "Epoch 2:  51%|█████     | 645/1271 [03:27<03:22,  3.09it/s, training_loss=0.290]\u001b[A\n",
            "Epoch 2:  51%|█████     | 645/1271 [03:28<03:22,  3.09it/s, training_loss=0.998]\u001b[A\n",
            "Epoch 2:  51%|█████     | 646/1271 [03:28<03:21,  3.10it/s, training_loss=0.998]\u001b[A\n",
            "Epoch 2:  51%|█████     | 646/1271 [03:28<03:21,  3.10it/s, training_loss=0.476]\u001b[A\n",
            "Epoch 2:  51%|█████     | 647/1271 [03:28<03:21,  3.10it/s, training_loss=0.476]\u001b[A\n",
            "Epoch 2:  51%|█████     | 647/1271 [03:28<03:21,  3.10it/s, training_loss=1.011]\u001b[A\n",
            "Epoch 2:  51%|█████     | 648/1271 [03:28<03:20,  3.11it/s, training_loss=1.011]\u001b[A\n",
            "Epoch 2:  51%|█████     | 648/1271 [03:29<03:20,  3.11it/s, training_loss=0.420]\u001b[A\n",
            "Epoch 2:  51%|█████     | 649/1271 [03:29<03:20,  3.09it/s, training_loss=0.420]\u001b[A\n",
            "Epoch 2:  51%|█████     | 649/1271 [03:29<03:20,  3.09it/s, training_loss=0.843]\u001b[A\n",
            "Epoch 2:  51%|█████     | 650/1271 [03:29<03:21,  3.08it/s, training_loss=0.843]\u001b[A\n",
            "Epoch 2:  51%|█████     | 650/1271 [03:29<03:21,  3.08it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 2:  51%|█████     | 651/1271 [03:29<03:20,  3.10it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 2:  51%|█████     | 651/1271 [03:30<03:20,  3.10it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 2:  51%|█████▏    | 652/1271 [03:30<03:20,  3.09it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 2:  51%|█████▏    | 652/1271 [03:30<03:20,  3.09it/s, training_loss=0.575]\u001b[A\n",
            "Epoch 2:  51%|█████▏    | 653/1271 [03:30<03:20,  3.08it/s, training_loss=0.575]\u001b[A\n",
            "Epoch 2:  51%|█████▏    | 653/1271 [03:30<03:20,  3.08it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 2:  51%|█████▏    | 654/1271 [03:30<03:19,  3.10it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 2:  51%|█████▏    | 654/1271 [03:30<03:19,  3.10it/s, training_loss=0.585]\u001b[A\n",
            "Epoch 2:  52%|█████▏    | 655/1271 [03:30<03:18,  3.10it/s, training_loss=0.585]\u001b[A\n",
            "Epoch 2:  52%|█████▏    | 655/1271 [03:31<03:18,  3.10it/s, training_loss=0.083]\u001b[A\n",
            "Epoch 2:  52%|█████▏    | 656/1271 [03:31<03:17,  3.11it/s, training_loss=0.083]\u001b[A\n",
            "Epoch 2:  52%|█████▏    | 656/1271 [03:31<03:17,  3.11it/s, training_loss=0.094]\u001b[A\n",
            "Epoch 2:  52%|█████▏    | 657/1271 [03:31<03:16,  3.13it/s, training_loss=0.094]\u001b[A\n",
            "Epoch 2:  52%|█████▏    | 657/1271 [03:31<03:16,  3.13it/s, training_loss=0.788]\u001b[A\n",
            "Epoch 2:  52%|█████▏    | 658/1271 [03:31<03:16,  3.12it/s, training_loss=0.788]\u001b[A\n",
            "Epoch 2:  52%|█████▏    | 658/1271 [03:32<03:16,  3.12it/s, training_loss=0.433]\u001b[A\n",
            "Epoch 2:  52%|█████▏    | 659/1271 [03:32<03:16,  3.11it/s, training_loss=0.433]\u001b[A\n",
            "Epoch 2:  52%|█████▏    | 659/1271 [03:32<03:16,  3.11it/s, training_loss=0.468]\u001b[A\n",
            "Epoch 2:  52%|█████▏    | 660/1271 [03:32<03:15,  3.12it/s, training_loss=0.468]\u001b[A\n",
            "Epoch 2:  52%|█████▏    | 660/1271 [03:32<03:15,  3.12it/s, training_loss=0.187]\u001b[A\n",
            "Epoch 2:  52%|█████▏    | 661/1271 [03:32<03:13,  3.15it/s, training_loss=0.187]\u001b[A\n",
            "Epoch 2:  52%|█████▏    | 661/1271 [03:33<03:13,  3.15it/s, training_loss=0.045]\u001b[A\n",
            "Epoch 2:  52%|█████▏    | 662/1271 [03:33<03:14,  3.14it/s, training_loss=0.045]\u001b[A\n",
            "Epoch 2:  52%|█████▏    | 662/1271 [03:33<03:14,  3.14it/s, training_loss=0.816]\u001b[A\n",
            "Epoch 2:  52%|█████▏    | 663/1271 [03:33<03:14,  3.12it/s, training_loss=0.816]\u001b[A\n",
            "Epoch 2:  52%|█████▏    | 663/1271 [03:33<03:14,  3.12it/s, training_loss=0.081]\u001b[A\n",
            "Epoch 2:  52%|█████▏    | 664/1271 [03:33<03:15,  3.10it/s, training_loss=0.081]\u001b[A\n",
            "Epoch 2:  52%|█████▏    | 664/1271 [03:34<03:15,  3.10it/s, training_loss=1.126]\u001b[A\n",
            "Epoch 2:  52%|█████▏    | 665/1271 [03:34<03:14,  3.11it/s, training_loss=1.126]\u001b[A\n",
            "Epoch 2:  52%|█████▏    | 665/1271 [03:34<03:14,  3.11it/s, training_loss=0.195]\u001b[A\n",
            "Epoch 2:  52%|█████▏    | 666/1271 [03:34<03:14,  3.11it/s, training_loss=0.195]\u001b[A\n",
            "Epoch 2:  52%|█████▏    | 666/1271 [03:34<03:14,  3.11it/s, training_loss=0.925]\u001b[A\n",
            "Epoch 2:  52%|█████▏    | 667/1271 [03:34<03:14,  3.11it/s, training_loss=0.925]\u001b[A\n",
            "Epoch 2:  52%|█████▏    | 667/1271 [03:35<03:14,  3.11it/s, training_loss=0.792]\u001b[A\n",
            "Epoch 2:  53%|█████▎    | 668/1271 [03:35<03:14,  3.10it/s, training_loss=0.792]\u001b[A\n",
            "Epoch 2:  53%|█████▎    | 668/1271 [03:35<03:14,  3.10it/s, training_loss=0.546]\u001b[A\n",
            "Epoch 2:  53%|█████▎    | 669/1271 [03:35<03:13,  3.11it/s, training_loss=0.546]\u001b[A\n",
            "Epoch 2:  53%|█████▎    | 669/1271 [03:35<03:13,  3.11it/s, training_loss=0.160]\u001b[A\n",
            "Epoch 2:  53%|█████▎    | 670/1271 [03:35<03:13,  3.10it/s, training_loss=0.160]\u001b[A\n",
            "Epoch 2:  53%|█████▎    | 670/1271 [03:36<03:13,  3.10it/s, training_loss=0.166]\u001b[A\n",
            "Epoch 2:  53%|█████▎    | 671/1271 [03:36<03:13,  3.10it/s, training_loss=0.166]\u001b[A\n",
            "Epoch 2:  53%|█████▎    | 671/1271 [03:36<03:13,  3.10it/s, training_loss=0.523]\u001b[A\n",
            "Epoch 2:  53%|█████▎    | 672/1271 [03:36<03:13,  3.10it/s, training_loss=0.523]\u001b[A\n",
            "Epoch 2:  53%|█████▎    | 672/1271 [03:36<03:13,  3.10it/s, training_loss=0.151]\u001b[A\n",
            "Epoch 2:  53%|█████▎    | 673/1271 [03:36<03:11,  3.12it/s, training_loss=0.151]\u001b[A\n",
            "Epoch 2:  53%|█████▎    | 673/1271 [03:37<03:11,  3.12it/s, training_loss=0.834]\u001b[A\n",
            "Epoch 2:  53%|█████▎    | 674/1271 [03:37<03:11,  3.12it/s, training_loss=0.834]\u001b[A\n",
            "Epoch 2:  53%|█████▎    | 674/1271 [03:37<03:11,  3.12it/s, training_loss=0.096]\u001b[A\n",
            "Epoch 2:  53%|█████▎    | 675/1271 [03:37<03:10,  3.13it/s, training_loss=0.096]\u001b[A\n",
            "Epoch 2:  53%|█████▎    | 675/1271 [03:37<03:10,  3.13it/s, training_loss=0.544]\u001b[A\n",
            "Epoch 2:  53%|█████▎    | 676/1271 [03:37<03:11,  3.11it/s, training_loss=0.544]\u001b[A\n",
            "Epoch 2:  53%|█████▎    | 676/1271 [03:38<03:11,  3.11it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 2:  53%|█████▎    | 677/1271 [03:38<03:10,  3.12it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 2:  53%|█████▎    | 677/1271 [03:38<03:10,  3.12it/s, training_loss=0.324]\u001b[A\n",
            "Epoch 2:  53%|█████▎    | 678/1271 [03:38<03:10,  3.11it/s, training_loss=0.324]\u001b[A\n",
            "Epoch 2:  53%|█████▎    | 678/1271 [03:38<03:10,  3.11it/s, training_loss=0.214]\u001b[A\n",
            "Epoch 2:  53%|█████▎    | 679/1271 [03:38<03:11,  3.09it/s, training_loss=0.214]\u001b[A\n",
            "Epoch 2:  53%|█████▎    | 679/1271 [03:39<03:11,  3.09it/s, training_loss=0.704]\u001b[A\n",
            "Epoch 2:  54%|█████▎    | 680/1271 [03:39<03:10,  3.10it/s, training_loss=0.704]\u001b[A\n",
            "Epoch 2:  54%|█████▎    | 680/1271 [03:39<03:10,  3.10it/s, training_loss=0.303]\u001b[A\n",
            "Epoch 2:  54%|█████▎    | 681/1271 [03:39<03:09,  3.11it/s, training_loss=0.303]\u001b[A\n",
            "Epoch 2:  54%|█████▎    | 681/1271 [03:39<03:09,  3.11it/s, training_loss=0.529]\u001b[A\n",
            "Epoch 2:  54%|█████▎    | 682/1271 [03:39<03:09,  3.11it/s, training_loss=0.529]\u001b[A\n",
            "Epoch 2:  54%|█████▎    | 682/1271 [03:39<03:09,  3.11it/s, training_loss=0.741]\u001b[A\n",
            "Epoch 2:  54%|█████▎    | 683/1271 [03:39<03:10,  3.09it/s, training_loss=0.741]\u001b[A\n",
            "Epoch 2:  54%|█████▎    | 683/1271 [03:40<03:10,  3.09it/s, training_loss=0.540]\u001b[A\n",
            "Epoch 2:  54%|█████▍    | 684/1271 [03:40<03:09,  3.09it/s, training_loss=0.540]\u001b[A\n",
            "Epoch 2:  54%|█████▍    | 684/1271 [03:40<03:09,  3.09it/s, training_loss=1.152]\u001b[A\n",
            "Epoch 2:  54%|█████▍    | 685/1271 [03:40<03:09,  3.09it/s, training_loss=1.152]\u001b[A\n",
            "Epoch 2:  54%|█████▍    | 685/1271 [03:40<03:09,  3.09it/s, training_loss=0.538]\u001b[A\n",
            "Epoch 2:  54%|█████▍    | 686/1271 [03:40<03:09,  3.09it/s, training_loss=0.538]\u001b[A\n",
            "Epoch 2:  54%|█████▍    | 686/1271 [03:41<03:09,  3.09it/s, training_loss=0.056]\u001b[A\n",
            "Epoch 2:  54%|█████▍    | 687/1271 [03:41<03:08,  3.10it/s, training_loss=0.056]\u001b[A\n",
            "Epoch 2:  54%|█████▍    | 687/1271 [03:41<03:08,  3.10it/s, training_loss=0.391]\u001b[A\n",
            "Epoch 2:  54%|█████▍    | 688/1271 [03:41<03:07,  3.11it/s, training_loss=0.391]\u001b[A\n",
            "Epoch 2:  54%|█████▍    | 688/1271 [03:41<03:07,  3.11it/s, training_loss=0.244]\u001b[A\n",
            "Epoch 2:  54%|█████▍    | 689/1271 [03:41<03:06,  3.11it/s, training_loss=0.244]\u001b[A\n",
            "Epoch 2:  54%|█████▍    | 689/1271 [03:42<03:06,  3.11it/s, training_loss=0.588]\u001b[A\n",
            "Epoch 2:  54%|█████▍    | 690/1271 [03:42<03:06,  3.12it/s, training_loss=0.588]\u001b[A\n",
            "Epoch 2:  54%|█████▍    | 690/1271 [03:42<03:06,  3.12it/s, training_loss=0.449]\u001b[A\n",
            "Epoch 2:  54%|█████▍    | 691/1271 [03:42<03:05,  3.13it/s, training_loss=0.449]\u001b[A\n",
            "Epoch 2:  54%|█████▍    | 691/1271 [03:42<03:05,  3.13it/s, training_loss=0.789]\u001b[A\n",
            "Epoch 2:  54%|█████▍    | 692/1271 [03:42<03:06,  3.11it/s, training_loss=0.789]\u001b[A\n",
            "Epoch 2:  54%|█████▍    | 692/1271 [03:43<03:06,  3.11it/s, training_loss=0.619]\u001b[A\n",
            "Epoch 2:  55%|█████▍    | 693/1271 [03:43<03:06,  3.09it/s, training_loss=0.619]\u001b[A\n",
            "Epoch 2:  55%|█████▍    | 693/1271 [03:43<03:06,  3.09it/s, training_loss=0.395]\u001b[A\n",
            "Epoch 2:  55%|█████▍    | 694/1271 [03:43<03:06,  3.10it/s, training_loss=0.395]\u001b[A\n",
            "Epoch 2:  55%|█████▍    | 694/1271 [03:43<03:06,  3.10it/s, training_loss=0.456]\u001b[A\n",
            "Epoch 2:  55%|█████▍    | 695/1271 [03:43<03:05,  3.10it/s, training_loss=0.456]\u001b[A\n",
            "Epoch 2:  55%|█████▍    | 695/1271 [03:44<03:05,  3.10it/s, training_loss=0.476]\u001b[A\n",
            "Epoch 2:  55%|█████▍    | 696/1271 [03:44<03:05,  3.10it/s, training_loss=0.476]\u001b[A\n",
            "Epoch 2:  55%|█████▍    | 696/1271 [03:44<03:05,  3.10it/s, training_loss=0.485]\u001b[A\n",
            "Epoch 2:  55%|█████▍    | 697/1271 [03:44<03:05,  3.09it/s, training_loss=0.485]\u001b[A\n",
            "Epoch 2:  55%|█████▍    | 697/1271 [03:44<03:05,  3.09it/s, training_loss=0.198]\u001b[A\n",
            "Epoch 2:  55%|█████▍    | 698/1271 [03:44<03:05,  3.09it/s, training_loss=0.198]\u001b[A\n",
            "Epoch 2:  55%|█████▍    | 698/1271 [03:45<03:05,  3.09it/s, training_loss=0.081]\u001b[A\n",
            "Epoch 2:  55%|█████▍    | 699/1271 [03:45<03:05,  3.08it/s, training_loss=0.081]\u001b[A\n",
            "Epoch 2:  55%|█████▍    | 699/1271 [03:45<03:05,  3.08it/s, training_loss=0.222]\u001b[A\n",
            "Epoch 2:  55%|█████▌    | 700/1271 [03:45<03:05,  3.09it/s, training_loss=0.222]\u001b[A\n",
            "Epoch 2:  55%|█████▌    | 700/1271 [03:45<03:05,  3.09it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 2:  55%|█████▌    | 701/1271 [03:45<03:03,  3.10it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 2:  55%|█████▌    | 701/1271 [03:46<03:03,  3.10it/s, training_loss=0.377]\u001b[A\n",
            "Epoch 2:  55%|█████▌    | 702/1271 [03:46<03:02,  3.12it/s, training_loss=0.377]\u001b[A\n",
            "Epoch 2:  55%|█████▌    | 702/1271 [03:46<03:02,  3.12it/s, training_loss=0.677]\u001b[A\n",
            "Epoch 2:  55%|█████▌    | 703/1271 [03:46<03:02,  3.11it/s, training_loss=0.677]\u001b[A\n",
            "Epoch 2:  55%|█████▌    | 703/1271 [03:46<03:02,  3.11it/s, training_loss=0.177]\u001b[A\n",
            "Epoch 2:  55%|█████▌    | 704/1271 [03:46<03:02,  3.10it/s, training_loss=0.177]\u001b[A\n",
            "Epoch 2:  55%|█████▌    | 704/1271 [03:47<03:02,  3.10it/s, training_loss=0.518]\u001b[A\n",
            "Epoch 2:  55%|█████▌    | 705/1271 [03:47<03:03,  3.08it/s, training_loss=0.518]\u001b[A\n",
            "Epoch 2:  55%|█████▌    | 705/1271 [03:47<03:03,  3.08it/s, training_loss=0.489]\u001b[A\n",
            "Epoch 2:  56%|█████▌    | 706/1271 [03:47<03:03,  3.08it/s, training_loss=0.489]\u001b[A\n",
            "Epoch 2:  56%|█████▌    | 706/1271 [03:47<03:03,  3.08it/s, training_loss=0.319]\u001b[A\n",
            "Epoch 2:  56%|█████▌    | 707/1271 [03:47<03:04,  3.05it/s, training_loss=0.319]\u001b[A\n",
            "Epoch 2:  56%|█████▌    | 707/1271 [03:48<03:04,  3.05it/s, training_loss=0.650]\u001b[A\n",
            "Epoch 2:  56%|█████▌    | 708/1271 [03:48<03:04,  3.05it/s, training_loss=0.650]\u001b[A\n",
            "Epoch 2:  56%|█████▌    | 708/1271 [03:48<03:04,  3.05it/s, training_loss=0.906]\u001b[A\n",
            "Epoch 2:  56%|█████▌    | 709/1271 [03:48<03:02,  3.08it/s, training_loss=0.906]\u001b[A\n",
            "Epoch 2:  56%|█████▌    | 709/1271 [03:48<03:02,  3.08it/s, training_loss=0.517]\u001b[A\n",
            "Epoch 2:  56%|█████▌    | 710/1271 [03:48<03:01,  3.09it/s, training_loss=0.517]\u001b[A\n",
            "Epoch 2:  56%|█████▌    | 710/1271 [03:49<03:01,  3.09it/s, training_loss=0.513]\u001b[A\n",
            "Epoch 2:  56%|█████▌    | 711/1271 [03:49<03:00,  3.10it/s, training_loss=0.513]\u001b[A\n",
            "Epoch 2:  56%|█████▌    | 711/1271 [03:49<03:00,  3.10it/s, training_loss=0.761]\u001b[A\n",
            "Epoch 2:  56%|█████▌    | 712/1271 [03:49<03:00,  3.10it/s, training_loss=0.761]\u001b[A\n",
            "Epoch 2:  56%|█████▌    | 712/1271 [03:49<03:00,  3.10it/s, training_loss=0.163]\u001b[A\n",
            "Epoch 2:  56%|█████▌    | 713/1271 [03:49<03:00,  3.10it/s, training_loss=0.163]\u001b[A\n",
            "Epoch 2:  56%|█████▌    | 713/1271 [03:49<03:00,  3.10it/s, training_loss=0.178]\u001b[A\n",
            "Epoch 2:  56%|█████▌    | 714/1271 [03:49<02:59,  3.10it/s, training_loss=0.178]\u001b[A\n",
            "Epoch 2:  56%|█████▌    | 714/1271 [03:50<02:59,  3.10it/s, training_loss=0.166]\u001b[A\n",
            "Epoch 2:  56%|█████▋    | 715/1271 [03:50<02:58,  3.11it/s, training_loss=0.166]\u001b[A\n",
            "Epoch 2:  56%|█████▋    | 715/1271 [03:50<02:58,  3.11it/s, training_loss=0.751]\u001b[A\n",
            "Epoch 2:  56%|█████▋    | 716/1271 [03:50<02:58,  3.10it/s, training_loss=0.751]\u001b[A\n",
            "Epoch 2:  56%|█████▋    | 716/1271 [03:50<02:58,  3.10it/s, training_loss=0.688]\u001b[A\n",
            "Epoch 2:  56%|█████▋    | 717/1271 [03:50<02:59,  3.09it/s, training_loss=0.688]\u001b[A\n",
            "Epoch 2:  56%|█████▋    | 717/1271 [03:51<02:59,  3.09it/s, training_loss=0.093]\u001b[A\n",
            "Epoch 2:  56%|█████▋    | 718/1271 [03:51<02:59,  3.08it/s, training_loss=0.093]\u001b[A\n",
            "Epoch 2:  56%|█████▋    | 718/1271 [03:51<02:59,  3.08it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 2:  57%|█████▋    | 719/1271 [03:51<02:58,  3.10it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 2:  57%|█████▋    | 719/1271 [03:51<02:58,  3.10it/s, training_loss=0.237]\u001b[A\n",
            "Epoch 2:  57%|█████▋    | 720/1271 [03:51<02:58,  3.09it/s, training_loss=0.237]\u001b[A\n",
            "Epoch 2:  57%|█████▋    | 720/1271 [03:52<02:58,  3.09it/s, training_loss=0.453]\u001b[A\n",
            "Epoch 2:  57%|█████▋    | 721/1271 [03:52<02:57,  3.11it/s, training_loss=0.453]\u001b[A\n",
            "Epoch 2:  57%|█████▋    | 721/1271 [03:52<02:57,  3.11it/s, training_loss=0.424]\u001b[A\n",
            "Epoch 2:  57%|█████▋    | 722/1271 [03:52<02:56,  3.12it/s, training_loss=0.424]\u001b[A\n",
            "Epoch 2:  57%|█████▋    | 722/1271 [03:52<02:56,  3.12it/s, training_loss=0.671]\u001b[A\n",
            "Epoch 2:  57%|█████▋    | 723/1271 [03:52<02:56,  3.11it/s, training_loss=0.671]\u001b[A\n",
            "Epoch 2:  57%|█████▋    | 723/1271 [03:53<02:56,  3.11it/s, training_loss=0.299]\u001b[A\n",
            "Epoch 2:  57%|█████▋    | 724/1271 [03:53<02:56,  3.10it/s, training_loss=0.299]\u001b[A\n",
            "Epoch 2:  57%|█████▋    | 724/1271 [03:53<02:56,  3.10it/s, training_loss=0.556]\u001b[A\n",
            "Epoch 2:  57%|█████▋    | 725/1271 [03:53<02:56,  3.09it/s, training_loss=0.556]\u001b[A\n",
            "Epoch 2:  57%|█████▋    | 725/1271 [03:53<02:56,  3.09it/s, training_loss=0.363]\u001b[A\n",
            "Epoch 2:  57%|█████▋    | 726/1271 [03:53<02:55,  3.10it/s, training_loss=0.363]\u001b[A\n",
            "Epoch 2:  57%|█████▋    | 726/1271 [03:54<02:55,  3.10it/s, training_loss=0.853]\u001b[A\n",
            "Epoch 2:  57%|█████▋    | 727/1271 [03:54<02:55,  3.10it/s, training_loss=0.853]\u001b[A\n",
            "Epoch 2:  57%|█████▋    | 727/1271 [03:54<02:55,  3.10it/s, training_loss=0.349]\u001b[A\n",
            "Epoch 2:  57%|█████▋    | 728/1271 [03:54<02:55,  3.10it/s, training_loss=0.349]\u001b[A\n",
            "Epoch 2:  57%|█████▋    | 728/1271 [03:54<02:55,  3.10it/s, training_loss=0.056]\u001b[A\n",
            "Epoch 2:  57%|█████▋    | 729/1271 [03:54<02:54,  3.10it/s, training_loss=0.056]\u001b[A\n",
            "Epoch 2:  57%|█████▋    | 729/1271 [03:55<02:54,  3.10it/s, training_loss=0.382]\u001b[A\n",
            "Epoch 2:  57%|█████▋    | 730/1271 [03:55<02:55,  3.09it/s, training_loss=0.382]\u001b[A\n",
            "Epoch 2:  57%|█████▋    | 730/1271 [03:55<02:55,  3.09it/s, training_loss=0.560]\u001b[A\n",
            "Epoch 2:  58%|█████▊    | 731/1271 [03:55<02:54,  3.10it/s, training_loss=0.560]\u001b[A\n",
            "Epoch 2:  58%|█████▊    | 731/1271 [03:55<02:54,  3.10it/s, training_loss=0.658]\u001b[A\n",
            "Epoch 2:  58%|█████▊    | 732/1271 [03:55<02:53,  3.10it/s, training_loss=0.658]\u001b[A\n",
            "Epoch 2:  58%|█████▊    | 732/1271 [03:56<02:53,  3.10it/s, training_loss=0.505]\u001b[A\n",
            "Epoch 2:  58%|█████▊    | 733/1271 [03:56<02:53,  3.10it/s, training_loss=0.505]\u001b[A\n",
            "Epoch 2:  58%|█████▊    | 733/1271 [03:56<02:53,  3.10it/s, training_loss=0.161]\u001b[A\n",
            "Epoch 2:  58%|█████▊    | 734/1271 [03:56<02:53,  3.09it/s, training_loss=0.161]\u001b[A\n",
            "Epoch 2:  58%|█████▊    | 734/1271 [03:56<02:53,  3.09it/s, training_loss=0.309]\u001b[A\n",
            "Epoch 2:  58%|█████▊    | 735/1271 [03:56<02:53,  3.09it/s, training_loss=0.309]\u001b[A\n",
            "Epoch 2:  58%|█████▊    | 735/1271 [03:57<02:53,  3.09it/s, training_loss=0.011]\u001b[A\n",
            "Epoch 2:  58%|█████▊    | 736/1271 [03:57<02:52,  3.10it/s, training_loss=0.011]\u001b[A\n",
            "Epoch 2:  58%|█████▊    | 736/1271 [03:57<02:52,  3.10it/s, training_loss=0.578]\u001b[A\n",
            "Epoch 2:  58%|█████▊    | 737/1271 [03:57<02:52,  3.09it/s, training_loss=0.578]\u001b[A\n",
            "Epoch 2:  58%|█████▊    | 737/1271 [03:57<02:52,  3.09it/s, training_loss=0.015]\u001b[A\n",
            "Epoch 2:  58%|█████▊    | 738/1271 [03:57<02:52,  3.09it/s, training_loss=0.015]\u001b[A\n",
            "Epoch 2:  58%|█████▊    | 738/1271 [03:58<02:52,  3.09it/s, training_loss=0.323]\u001b[A\n",
            "Epoch 2:  58%|█████▊    | 739/1271 [03:58<02:51,  3.11it/s, training_loss=0.323]\u001b[A\n",
            "Epoch 2:  58%|█████▊    | 739/1271 [03:58<02:51,  3.11it/s, training_loss=0.391]\u001b[A\n",
            "Epoch 2:  58%|█████▊    | 740/1271 [03:58<02:50,  3.11it/s, training_loss=0.391]\u001b[A\n",
            "Epoch 2:  58%|█████▊    | 740/1271 [03:58<02:50,  3.11it/s, training_loss=0.361]\u001b[A\n",
            "Epoch 2:  58%|█████▊    | 741/1271 [03:58<02:49,  3.12it/s, training_loss=0.361]\u001b[A\n",
            "Epoch 2:  58%|█████▊    | 741/1271 [03:59<02:49,  3.12it/s, training_loss=0.668]\u001b[A\n",
            "Epoch 2:  58%|█████▊    | 742/1271 [03:59<02:49,  3.13it/s, training_loss=0.668]\u001b[A\n",
            "Epoch 2:  58%|█████▊    | 742/1271 [03:59<02:49,  3.13it/s, training_loss=0.304]\u001b[A\n",
            "Epoch 2:  58%|█████▊    | 743/1271 [03:59<02:48,  3.14it/s, training_loss=0.304]\u001b[A\n",
            "Epoch 2:  58%|█████▊    | 743/1271 [03:59<02:48,  3.14it/s, training_loss=0.270]\u001b[A\n",
            "Epoch 2:  59%|█████▊    | 744/1271 [03:59<02:48,  3.13it/s, training_loss=0.270]\u001b[A\n",
            "Epoch 2:  59%|█████▊    | 744/1271 [03:59<02:48,  3.13it/s, training_loss=0.419]\u001b[A\n",
            "Epoch 2:  59%|█████▊    | 745/1271 [03:59<02:48,  3.13it/s, training_loss=0.419]\u001b[A\n",
            "Epoch 2:  59%|█████▊    | 745/1271 [04:00<02:48,  3.13it/s, training_loss=0.329]\u001b[A\n",
            "Epoch 2:  59%|█████▊    | 746/1271 [04:00<02:48,  3.11it/s, training_loss=0.329]\u001b[A\n",
            "Epoch 2:  59%|█████▊    | 746/1271 [04:00<02:48,  3.11it/s, training_loss=0.216]\u001b[A\n",
            "Epoch 2:  59%|█████▉    | 747/1271 [04:00<02:48,  3.12it/s, training_loss=0.216]\u001b[A\n",
            "Epoch 2:  59%|█████▉    | 747/1271 [04:00<02:48,  3.12it/s, training_loss=0.464]\u001b[A\n",
            "Epoch 2:  59%|█████▉    | 748/1271 [04:00<02:47,  3.12it/s, training_loss=0.464]\u001b[A\n",
            "Epoch 2:  59%|█████▉    | 748/1271 [04:01<02:47,  3.12it/s, training_loss=0.571]\u001b[A\n",
            "Epoch 2:  59%|█████▉    | 749/1271 [04:01<02:47,  3.11it/s, training_loss=0.571]\u001b[A\n",
            "Epoch 2:  59%|█████▉    | 749/1271 [04:01<02:47,  3.11it/s, training_loss=0.778]\u001b[A\n",
            "Epoch 2:  59%|█████▉    | 750/1271 [04:01<02:47,  3.12it/s, training_loss=0.778]\u001b[A\n",
            "Epoch 2:  59%|█████▉    | 750/1271 [04:01<02:47,  3.12it/s, training_loss=0.314]\u001b[A\n",
            "Epoch 2:  59%|█████▉    | 751/1271 [04:01<02:46,  3.13it/s, training_loss=0.314]\u001b[A\n",
            "Epoch 2:  59%|█████▉    | 751/1271 [04:02<02:46,  3.13it/s, training_loss=0.475]\u001b[A\n",
            "Epoch 2:  59%|█████▉    | 752/1271 [04:02<02:46,  3.12it/s, training_loss=0.475]\u001b[A\n",
            "Epoch 2:  59%|█████▉    | 752/1271 [04:02<02:46,  3.12it/s, training_loss=0.218]\u001b[A\n",
            "Epoch 2:  59%|█████▉    | 753/1271 [04:02<02:45,  3.12it/s, training_loss=0.218]\u001b[A\n",
            "Epoch 2:  59%|█████▉    | 753/1271 [04:02<02:45,  3.12it/s, training_loss=0.631]\u001b[A\n",
            "Epoch 2:  59%|█████▉    | 754/1271 [04:02<02:45,  3.13it/s, training_loss=0.631]\u001b[A\n",
            "Epoch 2:  59%|█████▉    | 754/1271 [04:03<02:45,  3.13it/s, training_loss=0.655]\u001b[A\n",
            "Epoch 2:  59%|█████▉    | 755/1271 [04:03<02:46,  3.10it/s, training_loss=0.655]\u001b[A\n",
            "Epoch 2:  59%|█████▉    | 755/1271 [04:03<02:46,  3.10it/s, training_loss=0.442]\u001b[A\n",
            "Epoch 2:  59%|█████▉    | 756/1271 [04:03<02:45,  3.11it/s, training_loss=0.442]\u001b[A\n",
            "Epoch 2:  59%|█████▉    | 756/1271 [04:03<02:45,  3.11it/s, training_loss=0.458]\u001b[A\n",
            "Epoch 2:  60%|█████▉    | 757/1271 [04:03<02:45,  3.10it/s, training_loss=0.458]\u001b[A\n",
            "Epoch 2:  60%|█████▉    | 757/1271 [04:04<02:45,  3.10it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 2:  60%|█████▉    | 758/1271 [04:04<02:45,  3.10it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 2:  60%|█████▉    | 758/1271 [04:04<02:45,  3.10it/s, training_loss=0.316]\u001b[A\n",
            "Epoch 2:  60%|█████▉    | 759/1271 [04:04<02:45,  3.10it/s, training_loss=0.316]\u001b[A\n",
            "Epoch 2:  60%|█████▉    | 759/1271 [04:04<02:45,  3.10it/s, training_loss=0.987]\u001b[A\n",
            "Epoch 2:  60%|█████▉    | 760/1271 [04:04<02:44,  3.11it/s, training_loss=0.987]\u001b[A\n",
            "Epoch 2:  60%|█████▉    | 760/1271 [04:05<02:44,  3.11it/s, training_loss=1.062]\u001b[A\n",
            "Epoch 2:  60%|█████▉    | 761/1271 [04:05<02:44,  3.10it/s, training_loss=1.062]\u001b[A\n",
            "Epoch 2:  60%|█████▉    | 761/1271 [04:05<02:44,  3.10it/s, training_loss=0.682]\u001b[A\n",
            "Epoch 2:  60%|█████▉    | 762/1271 [04:05<02:43,  3.10it/s, training_loss=0.682]\u001b[A\n",
            "Epoch 2:  60%|█████▉    | 762/1271 [04:05<02:43,  3.10it/s, training_loss=0.666]\u001b[A\n",
            "Epoch 2:  60%|██████    | 763/1271 [04:05<02:43,  3.11it/s, training_loss=0.666]\u001b[A\n",
            "Epoch 2:  60%|██████    | 763/1271 [04:06<02:43,  3.11it/s, training_loss=0.045]\u001b[A\n",
            "Epoch 2:  60%|██████    | 764/1271 [04:06<02:43,  3.11it/s, training_loss=0.045]\u001b[A\n",
            "Epoch 2:  60%|██████    | 764/1271 [04:06<02:43,  3.11it/s, training_loss=0.060]\u001b[A\n",
            "Epoch 2:  60%|██████    | 765/1271 [04:06<02:41,  3.13it/s, training_loss=0.060]\u001b[A\n",
            "Epoch 2:  60%|██████    | 765/1271 [04:06<02:41,  3.13it/s, training_loss=0.299]\u001b[A\n",
            "Epoch 2:  60%|██████    | 766/1271 [04:06<02:41,  3.12it/s, training_loss=0.299]\u001b[A\n",
            "Epoch 2:  60%|██████    | 766/1271 [04:07<02:41,  3.12it/s, training_loss=0.117]\u001b[A\n",
            "Epoch 2:  60%|██████    | 767/1271 [04:07<02:41,  3.12it/s, training_loss=0.117]\u001b[A\n",
            "Epoch 2:  60%|██████    | 767/1271 [04:07<02:41,  3.12it/s, training_loss=0.256]\u001b[A\n",
            "Epoch 2:  60%|██████    | 768/1271 [04:07<02:41,  3.12it/s, training_loss=0.256]\u001b[A\n",
            "Epoch 2:  60%|██████    | 768/1271 [04:07<02:41,  3.12it/s, training_loss=0.253]\u001b[A\n",
            "Epoch 2:  61%|██████    | 769/1271 [04:07<02:40,  3.13it/s, training_loss=0.253]\u001b[A\n",
            "Epoch 2:  61%|██████    | 769/1271 [04:07<02:40,  3.13it/s, training_loss=0.334]\u001b[A\n",
            "Epoch 2:  61%|██████    | 770/1271 [04:08<02:39,  3.14it/s, training_loss=0.334]\u001b[A\n",
            "Epoch 2:  61%|██████    | 770/1271 [04:08<02:39,  3.14it/s, training_loss=1.017]\u001b[A\n",
            "Epoch 2:  61%|██████    | 771/1271 [04:08<02:40,  3.12it/s, training_loss=1.017]\u001b[A\n",
            "Epoch 2:  61%|██████    | 771/1271 [04:08<02:40,  3.12it/s, training_loss=0.323]\u001b[A\n",
            "Epoch 2:  61%|██████    | 772/1271 [04:08<02:40,  3.11it/s, training_loss=0.323]\u001b[A\n",
            "Epoch 2:  61%|██████    | 772/1271 [04:08<02:40,  3.11it/s, training_loss=0.183]\u001b[A\n",
            "Epoch 2:  61%|██████    | 773/1271 [04:08<02:40,  3.10it/s, training_loss=0.183]\u001b[A\n",
            "Epoch 2:  61%|██████    | 773/1271 [04:09<02:40,  3.10it/s, training_loss=0.201]\u001b[A\n",
            "Epoch 2:  61%|██████    | 774/1271 [04:09<02:40,  3.10it/s, training_loss=0.201]\u001b[A\n",
            "Epoch 2:  61%|██████    | 774/1271 [04:09<02:40,  3.10it/s, training_loss=0.244]\u001b[A\n",
            "Epoch 2:  61%|██████    | 775/1271 [04:09<02:39,  3.11it/s, training_loss=0.244]\u001b[A\n",
            "Epoch 2:  61%|██████    | 775/1271 [04:09<02:39,  3.11it/s, training_loss=0.076]\u001b[A\n",
            "Epoch 2:  61%|██████    | 776/1271 [04:09<02:39,  3.10it/s, training_loss=0.076]\u001b[A\n",
            "Epoch 2:  61%|██████    | 776/1271 [04:10<02:39,  3.10it/s, training_loss=0.575]\u001b[A\n",
            "Epoch 2:  61%|██████    | 777/1271 [04:10<02:39,  3.10it/s, training_loss=0.575]\u001b[A\n",
            "Epoch 2:  61%|██████    | 777/1271 [04:10<02:39,  3.10it/s, training_loss=0.589]\u001b[A\n",
            "Epoch 2:  61%|██████    | 778/1271 [04:10<02:39,  3.10it/s, training_loss=0.589]\u001b[A\n",
            "Epoch 2:  61%|██████    | 778/1271 [04:10<02:39,  3.10it/s, training_loss=0.149]\u001b[A\n",
            "Epoch 2:  61%|██████▏   | 779/1271 [04:10<02:39,  3.08it/s, training_loss=0.149]\u001b[A\n",
            "Epoch 2:  61%|██████▏   | 779/1271 [04:11<02:39,  3.08it/s, training_loss=0.447]\u001b[A\n",
            "Epoch 2:  61%|██████▏   | 780/1271 [04:11<02:39,  3.08it/s, training_loss=0.447]\u001b[A\n",
            "Epoch 2:  61%|██████▏   | 780/1271 [04:11<02:39,  3.08it/s, training_loss=0.413]\u001b[A\n",
            "Epoch 2:  61%|██████▏   | 781/1271 [04:11<02:38,  3.08it/s, training_loss=0.413]\u001b[A\n",
            "Epoch 2:  61%|██████▏   | 781/1271 [04:11<02:38,  3.08it/s, training_loss=0.391]\u001b[A\n",
            "Epoch 2:  62%|██████▏   | 782/1271 [04:11<02:38,  3.09it/s, training_loss=0.391]\u001b[A\n",
            "Epoch 2:  62%|██████▏   | 782/1271 [04:12<02:38,  3.09it/s, training_loss=0.444]\u001b[A\n",
            "Epoch 2:  62%|██████▏   | 783/1271 [04:12<02:37,  3.10it/s, training_loss=0.444]\u001b[A\n",
            "Epoch 2:  62%|██████▏   | 783/1271 [04:12<02:37,  3.10it/s, training_loss=0.603]\u001b[A\n",
            "Epoch 2:  62%|██████▏   | 784/1271 [04:12<02:38,  3.07it/s, training_loss=0.603]\u001b[A\n",
            "Epoch 2:  62%|██████▏   | 784/1271 [04:12<02:38,  3.07it/s, training_loss=0.508]\u001b[A\n",
            "Epoch 2:  62%|██████▏   | 785/1271 [04:12<02:37,  3.09it/s, training_loss=0.508]\u001b[A\n",
            "Epoch 2:  62%|██████▏   | 785/1271 [04:13<02:37,  3.09it/s, training_loss=0.626]\u001b[A\n",
            "Epoch 2:  62%|██████▏   | 786/1271 [04:13<02:36,  3.11it/s, training_loss=0.626]\u001b[A\n",
            "Epoch 2:  62%|██████▏   | 786/1271 [04:13<02:36,  3.11it/s, training_loss=0.158]\u001b[A\n",
            "Epoch 2:  62%|██████▏   | 787/1271 [04:13<02:35,  3.12it/s, training_loss=0.158]\u001b[A\n",
            "Epoch 2:  62%|██████▏   | 787/1271 [04:13<02:35,  3.12it/s, training_loss=0.361]\u001b[A\n",
            "Epoch 2:  62%|██████▏   | 788/1271 [04:13<02:34,  3.12it/s, training_loss=0.361]\u001b[A\n",
            "Epoch 2:  62%|██████▏   | 788/1271 [04:14<02:34,  3.12it/s, training_loss=0.395]\u001b[A\n",
            "Epoch 2:  62%|██████▏   | 789/1271 [04:14<02:35,  3.10it/s, training_loss=0.395]\u001b[A\n",
            "Epoch 2:  62%|██████▏   | 789/1271 [04:14<02:35,  3.10it/s, training_loss=0.183]\u001b[A\n",
            "Epoch 2:  62%|██████▏   | 790/1271 [04:14<02:35,  3.09it/s, training_loss=0.183]\u001b[A\n",
            "Epoch 2:  62%|██████▏   | 790/1271 [04:14<02:35,  3.09it/s, training_loss=0.428]\u001b[A\n",
            "Epoch 2:  62%|██████▏   | 791/1271 [04:14<02:34,  3.10it/s, training_loss=0.428]\u001b[A\n",
            "Epoch 2:  62%|██████▏   | 791/1271 [04:15<02:34,  3.10it/s, training_loss=0.296]\u001b[A\n",
            "Epoch 2:  62%|██████▏   | 792/1271 [04:15<02:33,  3.11it/s, training_loss=0.296]\u001b[A\n",
            "Epoch 2:  62%|██████▏   | 792/1271 [04:15<02:33,  3.11it/s, training_loss=0.252]\u001b[A\n",
            "Epoch 2:  62%|██████▏   | 793/1271 [04:15<02:33,  3.11it/s, training_loss=0.252]\u001b[A\n",
            "Epoch 2:  62%|██████▏   | 793/1271 [04:15<02:33,  3.11it/s, training_loss=0.735]\u001b[A\n",
            "Epoch 2:  62%|██████▏   | 794/1271 [04:15<02:33,  3.11it/s, training_loss=0.735]\u001b[A\n",
            "Epoch 2:  62%|██████▏   | 794/1271 [04:16<02:33,  3.11it/s, training_loss=0.261]\u001b[A\n",
            "Epoch 2:  63%|██████▎   | 795/1271 [04:16<02:32,  3.11it/s, training_loss=0.261]\u001b[A\n",
            "Epoch 2:  63%|██████▎   | 795/1271 [04:16<02:32,  3.11it/s, training_loss=0.518]\u001b[A\n",
            "Epoch 2:  63%|██████▎   | 796/1271 [04:16<02:32,  3.11it/s, training_loss=0.518]\u001b[A\n",
            "Epoch 2:  63%|██████▎   | 796/1271 [04:16<02:32,  3.11it/s, training_loss=0.011]\u001b[A\n",
            "Epoch 2:  63%|██████▎   | 797/1271 [04:16<02:33,  3.09it/s, training_loss=0.011]\u001b[A\n",
            "Epoch 2:  63%|██████▎   | 797/1271 [04:17<02:33,  3.09it/s, training_loss=0.278]\u001b[A\n",
            "Epoch 2:  63%|██████▎   | 798/1271 [04:17<02:32,  3.11it/s, training_loss=0.278]\u001b[A\n",
            "Epoch 2:  63%|██████▎   | 798/1271 [04:17<02:32,  3.11it/s, training_loss=0.241]\u001b[A\n",
            "Epoch 2:  63%|██████▎   | 799/1271 [04:17<02:31,  3.11it/s, training_loss=0.241]\u001b[A\n",
            "Epoch 2:  63%|██████▎   | 799/1271 [04:17<02:31,  3.11it/s, training_loss=0.594]\u001b[A\n",
            "Epoch 2:  63%|██████▎   | 800/1271 [04:17<02:31,  3.10it/s, training_loss=0.594]\u001b[A\n",
            "Epoch 2:  63%|██████▎   | 800/1271 [04:17<02:31,  3.10it/s, training_loss=0.433]\u001b[A\n",
            "Epoch 2:  63%|██████▎   | 801/1271 [04:18<02:31,  3.11it/s, training_loss=0.433]\u001b[A\n",
            "Epoch 2:  63%|██████▎   | 801/1271 [04:18<02:31,  3.11it/s, training_loss=0.194]\u001b[A\n",
            "Epoch 2:  63%|██████▎   | 802/1271 [04:18<02:30,  3.11it/s, training_loss=0.194]\u001b[A\n",
            "Epoch 2:  63%|██████▎   | 802/1271 [04:18<02:30,  3.11it/s, training_loss=0.265]\u001b[A\n",
            "Epoch 2:  63%|██████▎   | 803/1271 [04:18<02:30,  3.11it/s, training_loss=0.265]\u001b[A\n",
            "Epoch 2:  63%|██████▎   | 803/1271 [04:18<02:30,  3.11it/s, training_loss=0.137]\u001b[A\n",
            "Epoch 2:  63%|██████▎   | 804/1271 [04:18<02:29,  3.13it/s, training_loss=0.137]\u001b[A\n",
            "Epoch 2:  63%|██████▎   | 804/1271 [04:19<02:29,  3.13it/s, training_loss=0.696]\u001b[A\n",
            "Epoch 2:  63%|██████▎   | 805/1271 [04:19<02:28,  3.13it/s, training_loss=0.696]\u001b[A\n",
            "Epoch 2:  63%|██████▎   | 805/1271 [04:19<02:28,  3.13it/s, training_loss=0.063]\u001b[A\n",
            "Epoch 2:  63%|██████▎   | 806/1271 [04:19<02:29,  3.12it/s, training_loss=0.063]\u001b[A\n",
            "Epoch 2:  63%|██████▎   | 806/1271 [04:19<02:29,  3.12it/s, training_loss=0.032]\u001b[A\n",
            "Epoch 2:  63%|██████▎   | 807/1271 [04:19<02:29,  3.11it/s, training_loss=0.032]\u001b[A\n",
            "Epoch 2:  63%|██████▎   | 807/1271 [04:20<02:29,  3.11it/s, training_loss=0.382]\u001b[A\n",
            "Epoch 2:  64%|██████▎   | 808/1271 [04:20<02:29,  3.11it/s, training_loss=0.382]\u001b[A\n",
            "Epoch 2:  64%|██████▎   | 808/1271 [04:20<02:29,  3.11it/s, training_loss=0.235]\u001b[A\n",
            "Epoch 2:  64%|██████▎   | 809/1271 [04:20<02:28,  3.10it/s, training_loss=0.235]\u001b[A\n",
            "Epoch 2:  64%|██████▎   | 809/1271 [04:20<02:28,  3.10it/s, training_loss=0.554]\u001b[A\n",
            "Epoch 2:  64%|██████▎   | 810/1271 [04:20<02:28,  3.10it/s, training_loss=0.554]\u001b[A\n",
            "Epoch 2:  64%|██████▎   | 810/1271 [04:21<02:28,  3.10it/s, training_loss=0.531]\u001b[A\n",
            "Epoch 2:  64%|██████▍   | 811/1271 [04:21<02:29,  3.08it/s, training_loss=0.531]\u001b[A\n",
            "Epoch 2:  64%|██████▍   | 811/1271 [04:21<02:29,  3.08it/s, training_loss=0.622]\u001b[A\n",
            "Epoch 2:  64%|██████▍   | 812/1271 [04:21<02:28,  3.09it/s, training_loss=0.622]\u001b[A\n",
            "Epoch 2:  64%|██████▍   | 812/1271 [04:21<02:28,  3.09it/s, training_loss=0.405]\u001b[A\n",
            "Epoch 2:  64%|██████▍   | 813/1271 [04:21<02:27,  3.11it/s, training_loss=0.405]\u001b[A\n",
            "Epoch 2:  64%|██████▍   | 813/1271 [04:22<02:27,  3.11it/s, training_loss=0.257]\u001b[A\n",
            "Epoch 2:  64%|██████▍   | 814/1271 [04:22<02:26,  3.12it/s, training_loss=0.257]\u001b[A\n",
            "Epoch 2:  64%|██████▍   | 814/1271 [04:22<02:26,  3.12it/s, training_loss=0.434]\u001b[A\n",
            "Epoch 2:  64%|██████▍   | 815/1271 [04:22<02:25,  3.13it/s, training_loss=0.434]\u001b[A\n",
            "Epoch 2:  64%|██████▍   | 815/1271 [04:22<02:25,  3.13it/s, training_loss=0.321]\u001b[A\n",
            "Epoch 2:  64%|██████▍   | 816/1271 [04:22<02:25,  3.14it/s, training_loss=0.321]\u001b[A\n",
            "Epoch 2:  64%|██████▍   | 816/1271 [04:23<02:25,  3.14it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 2:  64%|██████▍   | 817/1271 [04:23<02:24,  3.14it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 2:  64%|██████▍   | 817/1271 [04:23<02:24,  3.14it/s, training_loss=0.271]\u001b[A\n",
            "Epoch 2:  64%|██████▍   | 818/1271 [04:23<02:24,  3.14it/s, training_loss=0.271]\u001b[A\n",
            "Epoch 2:  64%|██████▍   | 818/1271 [04:23<02:24,  3.14it/s, training_loss=0.283]\u001b[A\n",
            "Epoch 2:  64%|██████▍   | 819/1271 [04:23<02:24,  3.12it/s, training_loss=0.283]\u001b[A\n",
            "Epoch 2:  64%|██████▍   | 819/1271 [04:24<02:24,  3.12it/s, training_loss=0.293]\u001b[A\n",
            "Epoch 2:  65%|██████▍   | 820/1271 [04:24<02:25,  3.10it/s, training_loss=0.293]\u001b[A\n",
            "Epoch 2:  65%|██████▍   | 820/1271 [04:24<02:25,  3.10it/s, training_loss=0.787]\u001b[A\n",
            "Epoch 2:  65%|██████▍   | 821/1271 [04:24<02:25,  3.09it/s, training_loss=0.787]\u001b[A\n",
            "Epoch 2:  65%|██████▍   | 821/1271 [04:24<02:25,  3.09it/s, training_loss=0.479]\u001b[A\n",
            "Epoch 2:  65%|██████▍   | 822/1271 [04:24<02:24,  3.11it/s, training_loss=0.479]\u001b[A\n",
            "Epoch 2:  65%|██████▍   | 822/1271 [04:25<02:24,  3.11it/s, training_loss=0.412]\u001b[A\n",
            "Epoch 2:  65%|██████▍   | 823/1271 [04:25<02:23,  3.12it/s, training_loss=0.412]\u001b[A\n",
            "Epoch 2:  65%|██████▍   | 823/1271 [04:25<02:23,  3.12it/s, training_loss=1.101]\u001b[A\n",
            "Epoch 2:  65%|██████▍   | 824/1271 [04:25<02:23,  3.11it/s, training_loss=1.101]\u001b[A\n",
            "Epoch 2:  65%|██████▍   | 824/1271 [04:25<02:23,  3.11it/s, training_loss=0.404]\u001b[A\n",
            "Epoch 2:  65%|██████▍   | 825/1271 [04:25<02:23,  3.12it/s, training_loss=0.404]\u001b[A\n",
            "Epoch 2:  65%|██████▍   | 825/1271 [04:26<02:23,  3.12it/s, training_loss=0.207]\u001b[A\n",
            "Epoch 2:  65%|██████▍   | 826/1271 [04:26<02:23,  3.11it/s, training_loss=0.207]\u001b[A\n",
            "Epoch 2:  65%|██████▍   | 826/1271 [04:26<02:23,  3.11it/s, training_loss=0.244]\u001b[A\n",
            "Epoch 2:  65%|██████▌   | 827/1271 [04:26<02:22,  3.11it/s, training_loss=0.244]\u001b[A\n",
            "Epoch 2:  65%|██████▌   | 827/1271 [04:26<02:22,  3.11it/s, training_loss=0.205]\u001b[A\n",
            "Epoch 2:  65%|██████▌   | 828/1271 [04:26<02:23,  3.09it/s, training_loss=0.205]\u001b[A\n",
            "Epoch 2:  65%|██████▌   | 828/1271 [04:27<02:23,  3.09it/s, training_loss=0.206]\u001b[A\n",
            "Epoch 2:  65%|██████▌   | 829/1271 [04:27<02:22,  3.09it/s, training_loss=0.206]\u001b[A\n",
            "Epoch 2:  65%|██████▌   | 829/1271 [04:27<02:22,  3.09it/s, training_loss=0.641]\u001b[A\n",
            "Epoch 2:  65%|██████▌   | 830/1271 [04:27<02:21,  3.11it/s, training_loss=0.641]\u001b[A\n",
            "Epoch 2:  65%|██████▌   | 830/1271 [04:27<02:21,  3.11it/s, training_loss=0.023]\u001b[A\n",
            "Epoch 2:  65%|██████▌   | 831/1271 [04:27<02:21,  3.11it/s, training_loss=0.023]\u001b[A\n",
            "Epoch 2:  65%|██████▌   | 831/1271 [04:27<02:21,  3.11it/s, training_loss=0.295]\u001b[A\n",
            "Epoch 2:  65%|██████▌   | 832/1271 [04:27<02:20,  3.12it/s, training_loss=0.295]\u001b[A\n",
            "Epoch 2:  65%|██████▌   | 832/1271 [04:28<02:20,  3.12it/s, training_loss=0.095]\u001b[A\n",
            "Epoch 2:  66%|██████▌   | 833/1271 [04:28<02:21,  3.11it/s, training_loss=0.095]\u001b[A\n",
            "Epoch 2:  66%|██████▌   | 833/1271 [04:28<02:21,  3.11it/s, training_loss=0.265]\u001b[A\n",
            "Epoch 2:  66%|██████▌   | 834/1271 [04:28<02:20,  3.11it/s, training_loss=0.265]\u001b[A\n",
            "Epoch 2:  66%|██████▌   | 834/1271 [04:28<02:20,  3.11it/s, training_loss=0.295]\u001b[A\n",
            "Epoch 2:  66%|██████▌   | 835/1271 [04:28<02:19,  3.12it/s, training_loss=0.295]\u001b[A\n",
            "Epoch 2:  66%|██████▌   | 835/1271 [04:29<02:19,  3.12it/s, training_loss=0.400]\u001b[A\n",
            "Epoch 2:  66%|██████▌   | 836/1271 [04:29<02:19,  3.12it/s, training_loss=0.400]\u001b[A\n",
            "Epoch 2:  66%|██████▌   | 836/1271 [04:29<02:19,  3.12it/s, training_loss=0.129]\u001b[A\n",
            "Epoch 2:  66%|██████▌   | 837/1271 [04:29<02:19,  3.12it/s, training_loss=0.129]\u001b[A\n",
            "Epoch 2:  66%|██████▌   | 837/1271 [04:29<02:19,  3.12it/s, training_loss=0.007]\u001b[A\n",
            "Epoch 2:  66%|██████▌   | 838/1271 [04:29<02:18,  3.12it/s, training_loss=0.007]\u001b[A\n",
            "Epoch 2:  66%|██████▌   | 838/1271 [04:30<02:18,  3.12it/s, training_loss=0.418]\u001b[A\n",
            "Epoch 2:  66%|██████▌   | 839/1271 [04:30<02:18,  3.13it/s, training_loss=0.418]\u001b[A\n",
            "Epoch 2:  66%|██████▌   | 839/1271 [04:30<02:18,  3.13it/s, training_loss=0.268]\u001b[A\n",
            "Epoch 2:  66%|██████▌   | 840/1271 [04:30<02:18,  3.11it/s, training_loss=0.268]\u001b[A\n",
            "Epoch 2:  66%|██████▌   | 840/1271 [04:30<02:18,  3.11it/s, training_loss=0.242]\u001b[A\n",
            "Epoch 2:  66%|██████▌   | 841/1271 [04:30<02:18,  3.11it/s, training_loss=0.242]\u001b[A\n",
            "Epoch 2:  66%|██████▌   | 841/1271 [04:31<02:18,  3.11it/s, training_loss=0.294]\u001b[A\n",
            "Epoch 2:  66%|██████▌   | 842/1271 [04:31<02:17,  3.11it/s, training_loss=0.294]\u001b[A\n",
            "Epoch 2:  66%|██████▌   | 842/1271 [04:31<02:17,  3.11it/s, training_loss=0.862]\u001b[A\n",
            "Epoch 2:  66%|██████▋   | 843/1271 [04:31<02:17,  3.12it/s, training_loss=0.862]\u001b[A\n",
            "Epoch 2:  66%|██████▋   | 843/1271 [04:31<02:17,  3.12it/s, training_loss=0.365]\u001b[A\n",
            "Epoch 2:  66%|██████▋   | 844/1271 [04:31<02:17,  3.10it/s, training_loss=0.365]\u001b[A\n",
            "Epoch 2:  66%|██████▋   | 844/1271 [04:32<02:17,  3.10it/s, training_loss=0.327]\u001b[A\n",
            "Epoch 2:  66%|██████▋   | 845/1271 [04:32<02:18,  3.09it/s, training_loss=0.327]\u001b[A\n",
            "Epoch 2:  66%|██████▋   | 845/1271 [04:32<02:18,  3.09it/s, training_loss=0.183]\u001b[A\n",
            "Epoch 2:  67%|██████▋   | 846/1271 [04:32<02:18,  3.06it/s, training_loss=0.183]\u001b[A\n",
            "Epoch 2:  67%|██████▋   | 846/1271 [04:32<02:18,  3.06it/s, training_loss=0.141]\u001b[A\n",
            "Epoch 2:  67%|██████▋   | 847/1271 [04:32<02:17,  3.09it/s, training_loss=0.141]\u001b[A\n",
            "Epoch 2:  67%|██████▋   | 847/1271 [04:33<02:17,  3.09it/s, training_loss=0.071]\u001b[A\n",
            "Epoch 2:  67%|██████▋   | 848/1271 [04:33<02:16,  3.11it/s, training_loss=0.071]\u001b[A\n",
            "Epoch 2:  67%|██████▋   | 848/1271 [04:33<02:16,  3.11it/s, training_loss=0.264]\u001b[A\n",
            "Epoch 2:  67%|██████▋   | 849/1271 [04:33<02:15,  3.12it/s, training_loss=0.264]\u001b[A\n",
            "Epoch 2:  67%|██████▋   | 849/1271 [04:33<02:15,  3.12it/s, training_loss=0.157]\u001b[A\n",
            "Epoch 2:  67%|██████▋   | 850/1271 [04:33<02:14,  3.13it/s, training_loss=0.157]\u001b[A\n",
            "Epoch 2:  67%|██████▋   | 850/1271 [04:34<02:14,  3.13it/s, training_loss=0.319]\u001b[A\n",
            "Epoch 2:  67%|██████▋   | 851/1271 [04:34<02:14,  3.13it/s, training_loss=0.319]\u001b[A\n",
            "Epoch 2:  67%|██████▋   | 851/1271 [04:34<02:14,  3.13it/s, training_loss=0.143]\u001b[A\n",
            "Epoch 2:  67%|██████▋   | 852/1271 [04:34<02:13,  3.14it/s, training_loss=0.143]\u001b[A\n",
            "Epoch 2:  67%|██████▋   | 852/1271 [04:34<02:13,  3.14it/s, training_loss=0.661]\u001b[A\n",
            "Epoch 2:  67%|██████▋   | 853/1271 [04:34<02:13,  3.14it/s, training_loss=0.661]\u001b[A\n",
            "Epoch 2:  67%|██████▋   | 853/1271 [04:35<02:13,  3.14it/s, training_loss=0.320]\u001b[A\n",
            "Epoch 2:  67%|██████▋   | 854/1271 [04:35<02:12,  3.14it/s, training_loss=0.320]\u001b[A\n",
            "Epoch 2:  67%|██████▋   | 854/1271 [04:35<02:12,  3.14it/s, training_loss=0.501]\u001b[A\n",
            "Epoch 2:  67%|██████▋   | 855/1271 [04:35<02:13,  3.12it/s, training_loss=0.501]\u001b[A\n",
            "Epoch 2:  67%|██████▋   | 855/1271 [04:35<02:13,  3.12it/s, training_loss=0.364]\u001b[A\n",
            "Epoch 2:  67%|██████▋   | 856/1271 [04:35<02:13,  3.11it/s, training_loss=0.364]\u001b[A\n",
            "Epoch 2:  67%|██████▋   | 856/1271 [04:35<02:13,  3.11it/s, training_loss=0.237]\u001b[A\n",
            "Epoch 2:  67%|██████▋   | 857/1271 [04:35<02:12,  3.12it/s, training_loss=0.237]\u001b[A\n",
            "Epoch 2:  67%|██████▋   | 857/1271 [04:36<02:12,  3.12it/s, training_loss=0.305]\u001b[A\n",
            "Epoch 2:  68%|██████▊   | 858/1271 [04:36<02:12,  3.11it/s, training_loss=0.305]\u001b[A\n",
            "Epoch 2:  68%|██████▊   | 858/1271 [04:36<02:12,  3.11it/s, training_loss=0.242]\u001b[A\n",
            "Epoch 2:  68%|██████▊   | 859/1271 [04:36<02:12,  3.10it/s, training_loss=0.242]\u001b[A\n",
            "Epoch 2:  68%|██████▊   | 859/1271 [04:36<02:12,  3.10it/s, training_loss=0.171]\u001b[A\n",
            "Epoch 2:  68%|██████▊   | 860/1271 [04:36<02:11,  3.12it/s, training_loss=0.171]\u001b[A\n",
            "Epoch 2:  68%|██████▊   | 860/1271 [04:37<02:11,  3.12it/s, training_loss=0.507]\u001b[A\n",
            "Epoch 2:  68%|██████▊   | 861/1271 [04:37<02:10,  3.13it/s, training_loss=0.507]\u001b[A\n",
            "Epoch 2:  68%|██████▊   | 861/1271 [04:37<02:10,  3.13it/s, training_loss=0.404]\u001b[A\n",
            "Epoch 2:  68%|██████▊   | 862/1271 [04:37<02:11,  3.11it/s, training_loss=0.404]\u001b[A\n",
            "Epoch 2:  68%|██████▊   | 862/1271 [04:37<02:11,  3.11it/s, training_loss=0.123]\u001b[A\n",
            "Epoch 2:  68%|██████▊   | 863/1271 [04:37<02:11,  3.10it/s, training_loss=0.123]\u001b[A\n",
            "Epoch 2:  68%|██████▊   | 863/1271 [04:38<02:11,  3.10it/s, training_loss=0.416]\u001b[A\n",
            "Epoch 2:  68%|██████▊   | 864/1271 [04:38<02:11,  3.10it/s, training_loss=0.416]\u001b[A\n",
            "Epoch 2:  68%|██████▊   | 864/1271 [04:38<02:11,  3.10it/s, training_loss=0.196]\u001b[A\n",
            "Epoch 2:  68%|██████▊   | 865/1271 [04:38<02:10,  3.11it/s, training_loss=0.196]\u001b[A\n",
            "Epoch 2:  68%|██████▊   | 865/1271 [04:38<02:10,  3.11it/s, training_loss=0.166]\u001b[A\n",
            "Epoch 2:  68%|██████▊   | 866/1271 [04:38<02:11,  3.09it/s, training_loss=0.166]\u001b[A\n",
            "Epoch 2:  68%|██████▊   | 866/1271 [04:39<02:11,  3.09it/s, training_loss=0.309]\u001b[A\n",
            "Epoch 2:  68%|██████▊   | 867/1271 [04:39<02:10,  3.09it/s, training_loss=0.309]\u001b[A\n",
            "Epoch 2:  68%|██████▊   | 867/1271 [04:39<02:10,  3.09it/s, training_loss=0.167]\u001b[A\n",
            "Epoch 2:  68%|██████▊   | 868/1271 [04:39<02:10,  3.09it/s, training_loss=0.167]\u001b[A\n",
            "Epoch 2:  68%|██████▊   | 868/1271 [04:39<02:10,  3.09it/s, training_loss=0.369]\u001b[A\n",
            "Epoch 2:  68%|██████▊   | 869/1271 [04:39<02:09,  3.11it/s, training_loss=0.369]\u001b[A\n",
            "Epoch 2:  68%|██████▊   | 869/1271 [04:40<02:09,  3.11it/s, training_loss=0.712]\u001b[A\n",
            "Epoch 2:  68%|██████▊   | 870/1271 [04:40<02:08,  3.11it/s, training_loss=0.712]\u001b[A\n",
            "Epoch 2:  68%|██████▊   | 870/1271 [04:40<02:08,  3.11it/s, training_loss=0.327]\u001b[A\n",
            "Epoch 2:  69%|██████▊   | 871/1271 [04:40<02:09,  3.09it/s, training_loss=0.327]\u001b[A\n",
            "Epoch 2:  69%|██████▊   | 871/1271 [04:40<02:09,  3.09it/s, training_loss=0.520]\u001b[A\n",
            "Epoch 2:  69%|██████▊   | 872/1271 [04:40<02:08,  3.09it/s, training_loss=0.520]\u001b[A\n",
            "Epoch 2:  69%|██████▊   | 872/1271 [04:41<02:08,  3.09it/s, training_loss=0.145]\u001b[A\n",
            "Epoch 2:  69%|██████▊   | 873/1271 [04:41<02:08,  3.11it/s, training_loss=0.145]\u001b[A\n",
            "Epoch 2:  69%|██████▊   | 873/1271 [04:41<02:08,  3.11it/s, training_loss=0.711]\u001b[A\n",
            "Epoch 2:  69%|██████▉   | 874/1271 [04:41<02:08,  3.08it/s, training_loss=0.711]\u001b[A\n",
            "Epoch 2:  69%|██████▉   | 874/1271 [04:41<02:08,  3.08it/s, training_loss=0.169]\u001b[A\n",
            "Epoch 2:  69%|██████▉   | 875/1271 [04:41<02:08,  3.09it/s, training_loss=0.169]\u001b[A\n",
            "Epoch 2:  69%|██████▉   | 875/1271 [04:42<02:08,  3.09it/s, training_loss=0.376]\u001b[A\n",
            "Epoch 2:  69%|██████▉   | 876/1271 [04:42<02:07,  3.10it/s, training_loss=0.376]\u001b[A\n",
            "Epoch 2:  69%|██████▉   | 876/1271 [04:42<02:07,  3.10it/s, training_loss=0.458]\u001b[A\n",
            "Epoch 2:  69%|██████▉   | 877/1271 [04:42<02:06,  3.11it/s, training_loss=0.458]\u001b[A\n",
            "Epoch 2:  69%|██████▉   | 877/1271 [04:42<02:06,  3.11it/s, training_loss=0.217]\u001b[A\n",
            "Epoch 2:  69%|██████▉   | 878/1271 [04:42<02:06,  3.10it/s, training_loss=0.217]\u001b[A\n",
            "Epoch 2:  69%|██████▉   | 878/1271 [04:43<02:06,  3.10it/s, training_loss=0.700]\u001b[A\n",
            "Epoch 2:  69%|██████▉   | 879/1271 [04:43<02:06,  3.10it/s, training_loss=0.700]\u001b[A\n",
            "Epoch 2:  69%|██████▉   | 879/1271 [04:43<02:06,  3.10it/s, training_loss=0.186]\u001b[A\n",
            "Epoch 2:  69%|██████▉   | 880/1271 [04:43<02:06,  3.10it/s, training_loss=0.186]\u001b[A\n",
            "Epoch 2:  69%|██████▉   | 880/1271 [04:43<02:06,  3.10it/s, training_loss=0.369]\u001b[A\n",
            "Epoch 2:  69%|██████▉   | 881/1271 [04:43<02:04,  3.13it/s, training_loss=0.369]\u001b[A\n",
            "Epoch 2:  69%|██████▉   | 881/1271 [04:44<02:04,  3.13it/s, training_loss=1.209]\u001b[A\n",
            "Epoch 2:  69%|██████▉   | 882/1271 [04:44<02:05,  3.11it/s, training_loss=1.209]\u001b[A\n",
            "Epoch 2:  69%|██████▉   | 882/1271 [04:44<02:05,  3.11it/s, training_loss=0.287]\u001b[A\n",
            "Epoch 2:  69%|██████▉   | 883/1271 [04:44<02:04,  3.12it/s, training_loss=0.287]\u001b[A\n",
            "Epoch 2:  69%|██████▉   | 883/1271 [04:44<02:04,  3.12it/s, training_loss=0.663]\u001b[A\n",
            "Epoch 2:  70%|██████▉   | 884/1271 [04:44<02:04,  3.11it/s, training_loss=0.663]\u001b[A\n",
            "Epoch 2:  70%|██████▉   | 884/1271 [04:45<02:04,  3.11it/s, training_loss=0.147]\u001b[A\n",
            "Epoch 2:  70%|██████▉   | 885/1271 [04:45<02:03,  3.13it/s, training_loss=0.147]\u001b[A\n",
            "Epoch 2:  70%|██████▉   | 885/1271 [04:45<02:03,  3.13it/s, training_loss=0.389]\u001b[A\n",
            "Epoch 2:  70%|██████▉   | 886/1271 [04:45<02:02,  3.14it/s, training_loss=0.389]\u001b[A\n",
            "Epoch 2:  70%|██████▉   | 886/1271 [04:45<02:02,  3.14it/s, training_loss=0.338]\u001b[A\n",
            "Epoch 2:  70%|██████▉   | 887/1271 [04:45<02:02,  3.13it/s, training_loss=0.338]\u001b[A\n",
            "Epoch 2:  70%|██████▉   | 887/1271 [04:45<02:02,  3.13it/s, training_loss=0.429]\u001b[A\n",
            "Epoch 2:  70%|██████▉   | 888/1271 [04:45<02:02,  3.13it/s, training_loss=0.429]\u001b[A\n",
            "Epoch 2:  70%|██████▉   | 888/1271 [04:46<02:02,  3.13it/s, training_loss=0.803]\u001b[A\n",
            "Epoch 2:  70%|██████▉   | 889/1271 [04:46<02:03,  3.10it/s, training_loss=0.803]\u001b[A\n",
            "Epoch 2:  70%|██████▉   | 889/1271 [04:46<02:03,  3.10it/s, training_loss=0.735]\u001b[A\n",
            "Epoch 2:  70%|███████   | 890/1271 [04:46<02:02,  3.11it/s, training_loss=0.735]\u001b[A\n",
            "Epoch 2:  70%|███████   | 890/1271 [04:46<02:02,  3.11it/s, training_loss=0.629]\u001b[A\n",
            "Epoch 2:  70%|███████   | 891/1271 [04:46<02:02,  3.10it/s, training_loss=0.629]\u001b[A\n",
            "Epoch 2:  70%|███████   | 891/1271 [04:47<02:02,  3.10it/s, training_loss=0.188]\u001b[A\n",
            "Epoch 2:  70%|███████   | 892/1271 [04:47<02:02,  3.09it/s, training_loss=0.188]\u001b[A\n",
            "Epoch 2:  70%|███████   | 892/1271 [04:47<02:02,  3.09it/s, training_loss=0.618]\u001b[A\n",
            "Epoch 2:  70%|███████   | 893/1271 [04:47<02:02,  3.08it/s, training_loss=0.618]\u001b[A\n",
            "Epoch 2:  70%|███████   | 893/1271 [04:47<02:02,  3.08it/s, training_loss=0.222]\u001b[A\n",
            "Epoch 2:  70%|███████   | 894/1271 [04:47<02:01,  3.10it/s, training_loss=0.222]\u001b[A\n",
            "Epoch 2:  70%|███████   | 894/1271 [04:48<02:01,  3.10it/s, training_loss=0.572]\u001b[A\n",
            "Epoch 2:  70%|███████   | 895/1271 [04:48<02:01,  3.10it/s, training_loss=0.572]\u001b[A\n",
            "Epoch 2:  70%|███████   | 895/1271 [04:48<02:01,  3.10it/s, training_loss=0.581]\u001b[A\n",
            "Epoch 2:  70%|███████   | 896/1271 [04:48<02:00,  3.11it/s, training_loss=0.581]\u001b[A\n",
            "Epoch 2:  70%|███████   | 896/1271 [04:48<02:00,  3.11it/s, training_loss=0.548]\u001b[A\n",
            "Epoch 2:  71%|███████   | 897/1271 [04:48<01:59,  3.12it/s, training_loss=0.548]\u001b[A\n",
            "Epoch 2:  71%|███████   | 897/1271 [04:49<01:59,  3.12it/s, training_loss=0.299]\u001b[A\n",
            "Epoch 2:  71%|███████   | 898/1271 [04:49<01:59,  3.11it/s, training_loss=0.299]\u001b[A\n",
            "Epoch 2:  71%|███████   | 898/1271 [04:49<01:59,  3.11it/s, training_loss=0.361]\u001b[A\n",
            "Epoch 2:  71%|███████   | 899/1271 [04:49<02:00,  3.08it/s, training_loss=0.361]\u001b[A\n",
            "Epoch 2:  71%|███████   | 899/1271 [04:49<02:00,  3.08it/s, training_loss=0.742]\u001b[A\n",
            "Epoch 2:  71%|███████   | 900/1271 [04:49<01:59,  3.10it/s, training_loss=0.742]\u001b[A\n",
            "Epoch 2:  71%|███████   | 900/1271 [04:50<01:59,  3.10it/s, training_loss=0.177]\u001b[A\n",
            "Epoch 2:  71%|███████   | 901/1271 [04:50<01:58,  3.11it/s, training_loss=0.177]\u001b[A\n",
            "Epoch 2:  71%|███████   | 901/1271 [04:50<01:58,  3.11it/s, training_loss=0.011]\u001b[A\n",
            "Epoch 2:  71%|███████   | 902/1271 [04:50<01:58,  3.11it/s, training_loss=0.011]\u001b[A\n",
            "Epoch 2:  71%|███████   | 902/1271 [04:50<01:58,  3.11it/s, training_loss=0.373]\u001b[A\n",
            "Epoch 2:  71%|███████   | 903/1271 [04:50<01:57,  3.12it/s, training_loss=0.373]\u001b[A\n",
            "Epoch 2:  71%|███████   | 903/1271 [04:51<01:57,  3.12it/s, training_loss=0.593]\u001b[A\n",
            "Epoch 2:  71%|███████   | 904/1271 [04:51<01:59,  3.08it/s, training_loss=0.593]\u001b[A\n",
            "Epoch 2:  71%|███████   | 904/1271 [04:51<01:59,  3.08it/s, training_loss=0.291]\u001b[A\n",
            "Epoch 2:  71%|███████   | 905/1271 [04:51<01:59,  3.07it/s, training_loss=0.291]\u001b[A\n",
            "Epoch 2:  71%|███████   | 905/1271 [04:51<01:59,  3.07it/s, training_loss=0.047]\u001b[A\n",
            "Epoch 2:  71%|███████▏  | 906/1271 [04:51<01:58,  3.07it/s, training_loss=0.047]\u001b[A\n",
            "Epoch 2:  71%|███████▏  | 906/1271 [04:52<01:58,  3.07it/s, training_loss=0.300]\u001b[A\n",
            "Epoch 2:  71%|███████▏  | 907/1271 [04:52<01:57,  3.10it/s, training_loss=0.300]\u001b[A\n",
            "Epoch 2:  71%|███████▏  | 907/1271 [04:52<01:57,  3.10it/s, training_loss=0.201]\u001b[A\n",
            "Epoch 2:  71%|███████▏  | 908/1271 [04:52<01:57,  3.09it/s, training_loss=0.201]\u001b[A\n",
            "Epoch 2:  71%|███████▏  | 908/1271 [04:52<01:57,  3.09it/s, training_loss=0.165]\u001b[A\n",
            "Epoch 2:  72%|███████▏  | 909/1271 [04:52<01:56,  3.10it/s, training_loss=0.165]\u001b[A\n",
            "Epoch 2:  72%|███████▏  | 909/1271 [04:53<01:56,  3.10it/s, training_loss=0.394]\u001b[A\n",
            "Epoch 2:  72%|███████▏  | 910/1271 [04:53<01:56,  3.11it/s, training_loss=0.394]\u001b[A\n",
            "Epoch 2:  72%|███████▏  | 910/1271 [04:53<01:56,  3.11it/s, training_loss=0.490]\u001b[A\n",
            "Epoch 2:  72%|███████▏  | 911/1271 [04:53<01:55,  3.11it/s, training_loss=0.490]\u001b[A\n",
            "Epoch 2:  72%|███████▏  | 911/1271 [04:53<01:55,  3.11it/s, training_loss=0.196]\u001b[A\n",
            "Epoch 2:  72%|███████▏  | 912/1271 [04:53<01:55,  3.11it/s, training_loss=0.196]\u001b[A\n",
            "Epoch 2:  72%|███████▏  | 912/1271 [04:54<01:55,  3.11it/s, training_loss=0.295]\u001b[A\n",
            "Epoch 2:  72%|███████▏  | 913/1271 [04:54<01:54,  3.12it/s, training_loss=0.295]\u001b[A\n",
            "Epoch 2:  72%|███████▏  | 913/1271 [04:54<01:54,  3.12it/s, training_loss=0.185]\u001b[A\n",
            "Epoch 2:  72%|███████▏  | 914/1271 [04:54<01:54,  3.11it/s, training_loss=0.185]\u001b[A\n",
            "Epoch 2:  72%|███████▏  | 914/1271 [04:54<01:54,  3.11it/s, training_loss=0.408]\u001b[A\n",
            "Epoch 2:  72%|███████▏  | 915/1271 [04:54<01:54,  3.10it/s, training_loss=0.408]\u001b[A\n",
            "Epoch 2:  72%|███████▏  | 915/1271 [04:54<01:54,  3.10it/s, training_loss=0.888]\u001b[A\n",
            "Epoch 2:  72%|███████▏  | 916/1271 [04:55<01:55,  3.08it/s, training_loss=0.888]\u001b[A\n",
            "Epoch 2:  72%|███████▏  | 916/1271 [04:55<01:55,  3.08it/s, training_loss=0.471]\u001b[A\n",
            "Epoch 2:  72%|███████▏  | 917/1271 [04:55<01:54,  3.10it/s, training_loss=0.471]\u001b[A\n",
            "Epoch 2:  72%|███████▏  | 917/1271 [04:55<01:54,  3.10it/s, training_loss=0.508]\u001b[A\n",
            "Epoch 2:  72%|███████▏  | 918/1271 [04:55<01:53,  3.10it/s, training_loss=0.508]\u001b[A\n",
            "Epoch 2:  72%|███████▏  | 918/1271 [04:55<01:53,  3.10it/s, training_loss=0.615]\u001b[A\n",
            "Epoch 2:  72%|███████▏  | 919/1271 [04:55<01:53,  3.11it/s, training_loss=0.615]\u001b[A\n",
            "Epoch 2:  72%|███████▏  | 919/1271 [04:56<01:53,  3.11it/s, training_loss=0.345]\u001b[A\n",
            "Epoch 2:  72%|███████▏  | 920/1271 [04:56<01:52,  3.11it/s, training_loss=0.345]\u001b[A\n",
            "Epoch 2:  72%|███████▏  | 920/1271 [04:56<01:52,  3.11it/s, training_loss=0.429]\u001b[A\n",
            "Epoch 2:  72%|███████▏  | 921/1271 [04:56<01:52,  3.12it/s, training_loss=0.429]\u001b[A\n",
            "Epoch 2:  72%|███████▏  | 921/1271 [04:56<01:52,  3.12it/s, training_loss=0.109]\u001b[A\n",
            "Epoch 2:  73%|███████▎  | 922/1271 [04:56<01:52,  3.11it/s, training_loss=0.109]\u001b[A\n",
            "Epoch 2:  73%|███████▎  | 922/1271 [04:57<01:52,  3.11it/s, training_loss=0.369]\u001b[A\n",
            "Epoch 2:  73%|███████▎  | 923/1271 [04:57<01:51,  3.12it/s, training_loss=0.369]\u001b[A\n",
            "Epoch 2:  73%|███████▎  | 923/1271 [04:57<01:51,  3.12it/s, training_loss=0.099]\u001b[A\n",
            "Epoch 2:  73%|███████▎  | 924/1271 [04:57<01:51,  3.12it/s, training_loss=0.099]\u001b[A\n",
            "Epoch 2:  73%|███████▎  | 924/1271 [04:57<01:51,  3.12it/s, training_loss=0.716]\u001b[A\n",
            "Epoch 2:  73%|███████▎  | 925/1271 [04:57<01:50,  3.12it/s, training_loss=0.716]\u001b[A\n",
            "Epoch 2:  73%|███████▎  | 925/1271 [04:58<01:50,  3.12it/s, training_loss=0.346]\u001b[A\n",
            "Epoch 2:  73%|███████▎  | 926/1271 [04:58<01:51,  3.10it/s, training_loss=0.346]\u001b[A\n",
            "Epoch 2:  73%|███████▎  | 926/1271 [04:58<01:51,  3.10it/s, training_loss=0.613]\u001b[A\n",
            "Epoch 2:  73%|███████▎  | 927/1271 [04:58<01:51,  3.09it/s, training_loss=0.613]\u001b[A\n",
            "Epoch 2:  73%|███████▎  | 927/1271 [04:58<01:51,  3.09it/s, training_loss=0.673]\u001b[A\n",
            "Epoch 2:  73%|███████▎  | 928/1271 [04:58<01:50,  3.09it/s, training_loss=0.673]\u001b[A\n",
            "Epoch 2:  73%|███████▎  | 928/1271 [04:59<01:50,  3.09it/s, training_loss=0.396]\u001b[A\n",
            "Epoch 2:  73%|███████▎  | 929/1271 [04:59<01:50,  3.10it/s, training_loss=0.396]\u001b[A\n",
            "Epoch 2:  73%|███████▎  | 929/1271 [04:59<01:50,  3.10it/s, training_loss=0.925]\u001b[A\n",
            "Epoch 2:  73%|███████▎  | 930/1271 [04:59<01:50,  3.08it/s, training_loss=0.925]\u001b[A\n",
            "Epoch 2:  73%|███████▎  | 930/1271 [04:59<01:50,  3.08it/s, training_loss=0.629]\u001b[A\n",
            "Epoch 2:  73%|███████▎  | 931/1271 [04:59<01:49,  3.10it/s, training_loss=0.629]\u001b[A\n",
            "Epoch 2:  73%|███████▎  | 931/1271 [05:00<01:49,  3.10it/s, training_loss=0.373]\u001b[A\n",
            "Epoch 2:  73%|███████▎  | 932/1271 [05:00<01:49,  3.11it/s, training_loss=0.373]\u001b[A\n",
            "Epoch 2:  73%|███████▎  | 932/1271 [05:00<01:49,  3.11it/s, training_loss=0.456]\u001b[A\n",
            "Epoch 2:  73%|███████▎  | 933/1271 [05:00<01:48,  3.13it/s, training_loss=0.456]\u001b[A\n",
            "Epoch 2:  73%|███████▎  | 933/1271 [05:00<01:48,  3.13it/s, training_loss=0.151]\u001b[A\n",
            "Epoch 2:  73%|███████▎  | 934/1271 [05:00<01:47,  3.12it/s, training_loss=0.151]\u001b[A\n",
            "Epoch 2:  73%|███████▎  | 934/1271 [05:01<01:47,  3.12it/s, training_loss=0.781]\u001b[A\n",
            "Epoch 2:  74%|███████▎  | 935/1271 [05:01<01:47,  3.12it/s, training_loss=0.781]\u001b[A\n",
            "Epoch 2:  74%|███████▎  | 935/1271 [05:01<01:47,  3.12it/s, training_loss=0.283]\u001b[A\n",
            "Epoch 2:  74%|███████▎  | 936/1271 [05:01<01:47,  3.11it/s, training_loss=0.283]\u001b[A\n",
            "Epoch 2:  74%|███████▎  | 936/1271 [05:01<01:47,  3.11it/s, training_loss=0.534]\u001b[A\n",
            "Epoch 2:  74%|███████▎  | 937/1271 [05:01<01:47,  3.11it/s, training_loss=0.534]\u001b[A\n",
            "Epoch 2:  74%|███████▎  | 937/1271 [05:02<01:47,  3.11it/s, training_loss=0.198]\u001b[A\n",
            "Epoch 2:  74%|███████▍  | 938/1271 [05:02<01:46,  3.11it/s, training_loss=0.198]\u001b[A\n",
            "Epoch 2:  74%|███████▍  | 938/1271 [05:02<01:46,  3.11it/s, training_loss=0.014]\u001b[A\n",
            "Epoch 2:  74%|███████▍  | 939/1271 [05:02<01:46,  3.12it/s, training_loss=0.014]\u001b[A\n",
            "Epoch 2:  74%|███████▍  | 939/1271 [05:02<01:46,  3.12it/s, training_loss=0.201]\u001b[A\n",
            "Epoch 2:  74%|███████▍  | 940/1271 [05:02<01:45,  3.14it/s, training_loss=0.201]\u001b[A\n",
            "Epoch 2:  74%|███████▍  | 940/1271 [05:03<01:45,  3.14it/s, training_loss=0.312]\u001b[A\n",
            "Epoch 2:  74%|███████▍  | 941/1271 [05:03<01:45,  3.13it/s, training_loss=0.312]\u001b[A\n",
            "Epoch 2:  74%|███████▍  | 941/1271 [05:03<01:45,  3.13it/s, training_loss=0.176]\u001b[A\n",
            "Epoch 2:  74%|███████▍  | 942/1271 [05:03<01:45,  3.13it/s, training_loss=0.176]\u001b[A\n",
            "Epoch 2:  74%|███████▍  | 942/1271 [05:03<01:45,  3.13it/s, training_loss=0.050]\u001b[A\n",
            "Epoch 2:  74%|███████▍  | 943/1271 [05:03<01:45,  3.11it/s, training_loss=0.050]\u001b[A\n",
            "Epoch 2:  74%|███████▍  | 943/1271 [05:03<01:45,  3.11it/s, training_loss=0.375]\u001b[A\n",
            "Epoch 2:  74%|███████▍  | 944/1271 [05:04<01:45,  3.11it/s, training_loss=0.375]\u001b[A\n",
            "Epoch 2:  74%|███████▍  | 944/1271 [05:04<01:45,  3.11it/s, training_loss=0.007]\u001b[A\n",
            "Epoch 2:  74%|███████▍  | 945/1271 [05:04<01:45,  3.10it/s, training_loss=0.007]\u001b[A\n",
            "Epoch 2:  74%|███████▍  | 945/1271 [05:04<01:45,  3.10it/s, training_loss=0.007]\u001b[A\n",
            "Epoch 2:  74%|███████▍  | 946/1271 [05:04<01:44,  3.10it/s, training_loss=0.007]\u001b[A\n",
            "Epoch 2:  74%|███████▍  | 946/1271 [05:04<01:44,  3.10it/s, training_loss=0.677]\u001b[A\n",
            "Epoch 2:  75%|███████▍  | 947/1271 [05:04<01:44,  3.10it/s, training_loss=0.677]\u001b[A\n",
            "Epoch 2:  75%|███████▍  | 947/1271 [05:05<01:44,  3.10it/s, training_loss=0.329]\u001b[A\n",
            "Epoch 2:  75%|███████▍  | 948/1271 [05:05<01:43,  3.11it/s, training_loss=0.329]\u001b[A\n",
            "Epoch 2:  75%|███████▍  | 948/1271 [05:05<01:43,  3.11it/s, training_loss=0.476]\u001b[A\n",
            "Epoch 2:  75%|███████▍  | 949/1271 [05:05<01:43,  3.11it/s, training_loss=0.476]\u001b[A\n",
            "Epoch 2:  75%|███████▍  | 949/1271 [05:05<01:43,  3.11it/s, training_loss=0.017]\u001b[A\n",
            "Epoch 2:  75%|███████▍  | 950/1271 [05:05<01:42,  3.13it/s, training_loss=0.017]\u001b[A\n",
            "Epoch 2:  75%|███████▍  | 950/1271 [05:06<01:42,  3.13it/s, training_loss=0.443]\u001b[A\n",
            "Epoch 2:  75%|███████▍  | 951/1271 [05:06<01:42,  3.13it/s, training_loss=0.443]\u001b[A\n",
            "Epoch 2:  75%|███████▍  | 951/1271 [05:06<01:42,  3.13it/s, training_loss=0.223]\u001b[A\n",
            "Epoch 2:  75%|███████▍  | 952/1271 [05:06<01:42,  3.13it/s, training_loss=0.223]\u001b[A\n",
            "Epoch 2:  75%|███████▍  | 952/1271 [05:06<01:42,  3.13it/s, training_loss=0.191]\u001b[A\n",
            "Epoch 2:  75%|███████▍  | 953/1271 [05:06<01:41,  3.14it/s, training_loss=0.191]\u001b[A\n",
            "Epoch 2:  75%|███████▍  | 953/1271 [05:07<01:41,  3.14it/s, training_loss=0.682]\u001b[A\n",
            "Epoch 2:  75%|███████▌  | 954/1271 [05:07<01:40,  3.14it/s, training_loss=0.682]\u001b[A\n",
            "Epoch 2:  75%|███████▌  | 954/1271 [05:07<01:40,  3.14it/s, training_loss=0.122]\u001b[A\n",
            "Epoch 2:  75%|███████▌  | 955/1271 [05:07<01:40,  3.13it/s, training_loss=0.122]\u001b[A\n",
            "Epoch 2:  75%|███████▌  | 955/1271 [05:07<01:40,  3.13it/s, training_loss=0.040]\u001b[A\n",
            "Epoch 2:  75%|███████▌  | 956/1271 [05:07<01:40,  3.13it/s, training_loss=0.040]\u001b[A\n",
            "Epoch 2:  75%|███████▌  | 956/1271 [05:08<01:40,  3.13it/s, training_loss=0.312]\u001b[A\n",
            "Epoch 2:  75%|███████▌  | 957/1271 [05:08<01:40,  3.14it/s, training_loss=0.312]\u001b[A\n",
            "Epoch 2:  75%|███████▌  | 957/1271 [05:08<01:40,  3.14it/s, training_loss=0.020]\u001b[A\n",
            "Epoch 2:  75%|███████▌  | 958/1271 [05:08<01:39,  3.13it/s, training_loss=0.020]\u001b[A\n",
            "Epoch 2:  75%|███████▌  | 958/1271 [05:08<01:39,  3.13it/s, training_loss=0.436]\u001b[A\n",
            "Epoch 2:  75%|███████▌  | 959/1271 [05:08<01:39,  3.13it/s, training_loss=0.436]\u001b[A\n",
            "Epoch 2:  75%|███████▌  | 959/1271 [05:09<01:39,  3.13it/s, training_loss=0.561]\u001b[A\n",
            "Epoch 2:  76%|███████▌  | 960/1271 [05:09<01:39,  3.13it/s, training_loss=0.561]\u001b[A\n",
            "Epoch 2:  76%|███████▌  | 960/1271 [05:09<01:39,  3.13it/s, training_loss=0.390]\u001b[A\n",
            "Epoch 2:  76%|███████▌  | 961/1271 [05:09<01:38,  3.14it/s, training_loss=0.390]\u001b[A\n",
            "Epoch 2:  76%|███████▌  | 961/1271 [05:09<01:38,  3.14it/s, training_loss=0.319]\u001b[A\n",
            "Epoch 2:  76%|███████▌  | 962/1271 [05:09<01:38,  3.12it/s, training_loss=0.319]\u001b[A\n",
            "Epoch 2:  76%|███████▌  | 962/1271 [05:10<01:38,  3.12it/s, training_loss=0.706]\u001b[A\n",
            "Epoch 2:  76%|███████▌  | 963/1271 [05:10<01:39,  3.11it/s, training_loss=0.706]\u001b[A\n",
            "Epoch 2:  76%|███████▌  | 963/1271 [05:10<01:39,  3.11it/s, training_loss=0.209]\u001b[A\n",
            "Epoch 2:  76%|███████▌  | 964/1271 [05:10<01:38,  3.12it/s, training_loss=0.209]\u001b[A\n",
            "Epoch 2:  76%|███████▌  | 964/1271 [05:10<01:38,  3.12it/s, training_loss=0.989]\u001b[A\n",
            "Epoch 2:  76%|███████▌  | 965/1271 [05:10<01:37,  3.12it/s, training_loss=0.989]\u001b[A\n",
            "Epoch 2:  76%|███████▌  | 965/1271 [05:11<01:37,  3.12it/s, training_loss=0.293]\u001b[A\n",
            "Epoch 2:  76%|███████▌  | 966/1271 [05:11<01:37,  3.12it/s, training_loss=0.293]\u001b[A\n",
            "Epoch 2:  76%|███████▌  | 966/1271 [05:11<01:37,  3.12it/s, training_loss=0.081]\u001b[A\n",
            "Epoch 2:  76%|███████▌  | 967/1271 [05:11<01:37,  3.11it/s, training_loss=0.081]\u001b[A\n",
            "Epoch 2:  76%|███████▌  | 967/1271 [05:11<01:37,  3.11it/s, training_loss=0.461]\u001b[A\n",
            "Epoch 2:  76%|███████▌  | 968/1271 [05:11<01:37,  3.12it/s, training_loss=0.461]\u001b[A\n",
            "Epoch 2:  76%|███████▌  | 968/1271 [05:12<01:37,  3.12it/s, training_loss=0.437]\u001b[A\n",
            "Epoch 2:  76%|███████▌  | 969/1271 [05:12<01:37,  3.09it/s, training_loss=0.437]\u001b[A\n",
            "Epoch 2:  76%|███████▌  | 969/1271 [05:12<01:37,  3.09it/s, training_loss=0.156]\u001b[A\n",
            "Epoch 2:  76%|███████▋  | 970/1271 [05:12<01:37,  3.09it/s, training_loss=0.156]\u001b[A\n",
            "Epoch 2:  76%|███████▋  | 970/1271 [05:12<01:37,  3.09it/s, training_loss=0.660]\u001b[A\n",
            "Epoch 2:  76%|███████▋  | 971/1271 [05:12<01:36,  3.10it/s, training_loss=0.660]\u001b[A\n",
            "Epoch 2:  76%|███████▋  | 971/1271 [05:12<01:36,  3.10it/s, training_loss=0.315]\u001b[A\n",
            "Epoch 2:  76%|███████▋  | 972/1271 [05:12<01:36,  3.11it/s, training_loss=0.315]\u001b[A\n",
            "Epoch 2:  76%|███████▋  | 972/1271 [05:13<01:36,  3.11it/s, training_loss=0.098]\u001b[A\n",
            "Epoch 2:  77%|███████▋  | 973/1271 [05:13<01:35,  3.12it/s, training_loss=0.098]\u001b[A\n",
            "Epoch 2:  77%|███████▋  | 973/1271 [05:13<01:35,  3.12it/s, training_loss=0.584]\u001b[A\n",
            "Epoch 2:  77%|███████▋  | 974/1271 [05:13<01:34,  3.13it/s, training_loss=0.584]\u001b[A\n",
            "Epoch 2:  77%|███████▋  | 974/1271 [05:13<01:34,  3.13it/s, training_loss=0.488]\u001b[A\n",
            "Epoch 2:  77%|███████▋  | 975/1271 [05:13<01:35,  3.11it/s, training_loss=0.488]\u001b[A\n",
            "Epoch 2:  77%|███████▋  | 975/1271 [05:14<01:35,  3.11it/s, training_loss=0.436]\u001b[A\n",
            "Epoch 2:  77%|███████▋  | 976/1271 [05:14<01:34,  3.11it/s, training_loss=0.436]\u001b[A\n",
            "Epoch 2:  77%|███████▋  | 976/1271 [05:14<01:34,  3.11it/s, training_loss=0.342]\u001b[A\n",
            "Epoch 2:  77%|███████▋  | 977/1271 [05:14<01:34,  3.11it/s, training_loss=0.342]\u001b[A\n",
            "Epoch 2:  77%|███████▋  | 977/1271 [05:14<01:34,  3.11it/s, training_loss=0.328]\u001b[A\n",
            "Epoch 2:  77%|███████▋  | 978/1271 [05:14<01:33,  3.12it/s, training_loss=0.328]\u001b[A\n",
            "Epoch 2:  77%|███████▋  | 978/1271 [05:15<01:33,  3.12it/s, training_loss=0.442]\u001b[A\n",
            "Epoch 2:  77%|███████▋  | 979/1271 [05:15<01:33,  3.13it/s, training_loss=0.442]\u001b[A\n",
            "Epoch 2:  77%|███████▋  | 979/1271 [05:15<01:33,  3.13it/s, training_loss=0.604]\u001b[A\n",
            "Epoch 2:  77%|███████▋  | 980/1271 [05:15<01:33,  3.12it/s, training_loss=0.604]\u001b[A\n",
            "Epoch 2:  77%|███████▋  | 980/1271 [05:15<01:33,  3.12it/s, training_loss=0.791]\u001b[A\n",
            "Epoch 2:  77%|███████▋  | 981/1271 [05:15<01:33,  3.12it/s, training_loss=0.791]\u001b[A\n",
            "Epoch 2:  77%|███████▋  | 981/1271 [05:16<01:33,  3.12it/s, training_loss=0.790]\u001b[A\n",
            "Epoch 2:  77%|███████▋  | 982/1271 [05:16<01:33,  3.09it/s, training_loss=0.790]\u001b[A\n",
            "Epoch 2:  77%|███████▋  | 982/1271 [05:16<01:33,  3.09it/s, training_loss=0.642]\u001b[A\n",
            "Epoch 2:  77%|███████▋  | 983/1271 [05:16<01:33,  3.10it/s, training_loss=0.642]\u001b[A\n",
            "Epoch 2:  77%|███████▋  | 983/1271 [05:16<01:33,  3.10it/s, training_loss=0.007]\u001b[A\n",
            "Epoch 2:  77%|███████▋  | 984/1271 [05:16<01:32,  3.11it/s, training_loss=0.007]\u001b[A\n",
            "Epoch 2:  77%|███████▋  | 984/1271 [05:17<01:32,  3.11it/s, training_loss=0.505]\u001b[A\n",
            "Epoch 2:  77%|███████▋  | 985/1271 [05:17<01:31,  3.12it/s, training_loss=0.505]\u001b[A\n",
            "Epoch 2:  77%|███████▋  | 985/1271 [05:17<01:31,  3.12it/s, training_loss=0.355]\u001b[A\n",
            "Epoch 2:  78%|███████▊  | 986/1271 [05:17<01:31,  3.12it/s, training_loss=0.355]\u001b[A\n",
            "Epoch 2:  78%|███████▊  | 986/1271 [05:17<01:31,  3.12it/s, training_loss=0.252]\u001b[A\n",
            "Epoch 2:  78%|███████▊  | 987/1271 [05:17<01:31,  3.11it/s, training_loss=0.252]\u001b[A\n",
            "Epoch 2:  78%|███████▊  | 987/1271 [05:18<01:31,  3.11it/s, training_loss=0.281]\u001b[A\n",
            "Epoch 2:  78%|███████▊  | 988/1271 [05:18<01:31,  3.10it/s, training_loss=0.281]\u001b[A\n",
            "Epoch 2:  78%|███████▊  | 988/1271 [05:18<01:31,  3.10it/s, training_loss=0.465]\u001b[A\n",
            "Epoch 2:  78%|███████▊  | 989/1271 [05:18<01:31,  3.09it/s, training_loss=0.465]\u001b[A\n",
            "Epoch 2:  78%|███████▊  | 989/1271 [05:18<01:31,  3.09it/s, training_loss=0.562]\u001b[A\n",
            "Epoch 2:  78%|███████▊  | 990/1271 [05:18<01:31,  3.07it/s, training_loss=0.562]\u001b[A\n",
            "Epoch 2:  78%|███████▊  | 990/1271 [05:19<01:31,  3.07it/s, training_loss=0.115]\u001b[A\n",
            "Epoch 2:  78%|███████▊  | 991/1271 [05:19<01:30,  3.09it/s, training_loss=0.115]\u001b[A\n",
            "Epoch 2:  78%|███████▊  | 991/1271 [05:19<01:30,  3.09it/s, training_loss=0.472]\u001b[A\n",
            "Epoch 2:  78%|███████▊  | 992/1271 [05:19<01:30,  3.09it/s, training_loss=0.472]\u001b[A\n",
            "Epoch 2:  78%|███████▊  | 992/1271 [05:19<01:30,  3.09it/s, training_loss=0.820]\u001b[A\n",
            "Epoch 2:  78%|███████▊  | 993/1271 [05:19<01:29,  3.10it/s, training_loss=0.820]\u001b[A\n",
            "Epoch 2:  78%|███████▊  | 993/1271 [05:20<01:29,  3.10it/s, training_loss=0.846]\u001b[A\n",
            "Epoch 2:  78%|███████▊  | 994/1271 [05:20<01:29,  3.11it/s, training_loss=0.846]\u001b[A\n",
            "Epoch 2:  78%|███████▊  | 994/1271 [05:20<01:29,  3.11it/s, training_loss=0.560]\u001b[A\n",
            "Epoch 2:  78%|███████▊  | 995/1271 [05:20<01:29,  3.10it/s, training_loss=0.560]\u001b[A\n",
            "Epoch 2:  78%|███████▊  | 995/1271 [05:20<01:29,  3.10it/s, training_loss=0.184]\u001b[A\n",
            "Epoch 2:  78%|███████▊  | 996/1271 [05:20<01:28,  3.11it/s, training_loss=0.184]\u001b[A\n",
            "Epoch 2:  78%|███████▊  | 996/1271 [05:21<01:28,  3.11it/s, training_loss=0.292]\u001b[A\n",
            "Epoch 2:  78%|███████▊  | 997/1271 [05:21<01:27,  3.12it/s, training_loss=0.292]\u001b[A\n",
            "Epoch 2:  78%|███████▊  | 997/1271 [05:21<01:27,  3.12it/s, training_loss=0.630]\u001b[A\n",
            "Epoch 2:  79%|███████▊  | 998/1271 [05:21<01:28,  3.10it/s, training_loss=0.630]\u001b[A\n",
            "Epoch 2:  79%|███████▊  | 998/1271 [05:21<01:28,  3.10it/s, training_loss=0.862]\u001b[A\n",
            "Epoch 2:  79%|███████▊  | 999/1271 [05:21<01:27,  3.10it/s, training_loss=0.862]\u001b[A\n",
            "Epoch 2:  79%|███████▊  | 999/1271 [05:21<01:27,  3.10it/s, training_loss=0.832]\u001b[A\n",
            "Epoch 2:  79%|███████▊  | 1000/1271 [05:21<01:26,  3.12it/s, training_loss=0.832]\u001b[A\n",
            "Epoch 2:  79%|███████▊  | 1000/1271 [05:22<01:26,  3.12it/s, training_loss=0.199]\u001b[A\n",
            "Epoch 2:  79%|███████▉  | 1001/1271 [05:22<01:26,  3.11it/s, training_loss=0.199]\u001b[A\n",
            "Epoch 2:  79%|███████▉  | 1001/1271 [05:22<01:26,  3.11it/s, training_loss=0.065]\u001b[A\n",
            "Epoch 2:  79%|███████▉  | 1002/1271 [05:22<01:26,  3.11it/s, training_loss=0.065]\u001b[A\n",
            "Epoch 2:  79%|███████▉  | 1002/1271 [05:22<01:26,  3.11it/s, training_loss=0.007]\u001b[A\n",
            "Epoch 2:  79%|███████▉  | 1003/1271 [05:22<01:26,  3.10it/s, training_loss=0.007]\u001b[A\n",
            "Epoch 2:  79%|███████▉  | 1003/1271 [05:23<01:26,  3.10it/s, training_loss=0.442]\u001b[A\n",
            "Epoch 2:  79%|███████▉  | 1004/1271 [05:23<01:25,  3.11it/s, training_loss=0.442]\u001b[A\n",
            "Epoch 2:  79%|███████▉  | 1004/1271 [05:23<01:25,  3.11it/s, training_loss=0.175]\u001b[A\n",
            "Epoch 2:  79%|███████▉  | 1005/1271 [05:23<01:25,  3.11it/s, training_loss=0.175]\u001b[A\n",
            "Epoch 2:  79%|███████▉  | 1005/1271 [05:23<01:25,  3.11it/s, training_loss=0.653]\u001b[A\n",
            "Epoch 2:  79%|███████▉  | 1006/1271 [05:23<01:25,  3.11it/s, training_loss=0.653]\u001b[A\n",
            "Epoch 2:  79%|███████▉  | 1006/1271 [05:24<01:25,  3.11it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 2:  79%|███████▉  | 1007/1271 [05:24<01:24,  3.12it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 2:  79%|███████▉  | 1007/1271 [05:24<01:24,  3.12it/s, training_loss=0.304]\u001b[A\n",
            "Epoch 2:  79%|███████▉  | 1008/1271 [05:24<01:24,  3.13it/s, training_loss=0.304]\u001b[A\n",
            "Epoch 2:  79%|███████▉  | 1008/1271 [05:24<01:24,  3.13it/s, training_loss=0.396]\u001b[A\n",
            "Epoch 2:  79%|███████▉  | 1009/1271 [05:24<01:23,  3.12it/s, training_loss=0.396]\u001b[A\n",
            "Epoch 2:  79%|███████▉  | 1009/1271 [05:25<01:23,  3.12it/s, training_loss=0.237]\u001b[A\n",
            "Epoch 2:  79%|███████▉  | 1010/1271 [05:25<01:24,  3.10it/s, training_loss=0.237]\u001b[A\n",
            "Epoch 2:  79%|███████▉  | 1010/1271 [05:25<01:24,  3.10it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 2:  80%|███████▉  | 1011/1271 [05:25<01:23,  3.11it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 2:  80%|███████▉  | 1011/1271 [05:25<01:23,  3.11it/s, training_loss=0.156]\u001b[A\n",
            "Epoch 2:  80%|███████▉  | 1012/1271 [05:25<01:23,  3.09it/s, training_loss=0.156]\u001b[A\n",
            "Epoch 2:  80%|███████▉  | 1012/1271 [05:26<01:23,  3.09it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 2:  80%|███████▉  | 1013/1271 [05:26<01:23,  3.10it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 2:  80%|███████▉  | 1013/1271 [05:26<01:23,  3.10it/s, training_loss=0.445]\u001b[A\n",
            "Epoch 2:  80%|███████▉  | 1014/1271 [05:26<01:22,  3.11it/s, training_loss=0.445]\u001b[A\n",
            "Epoch 2:  80%|███████▉  | 1014/1271 [05:26<01:22,  3.11it/s, training_loss=0.244]\u001b[A\n",
            "Epoch 2:  80%|███████▉  | 1015/1271 [05:26<01:21,  3.12it/s, training_loss=0.244]\u001b[A\n",
            "Epoch 2:  80%|███████▉  | 1015/1271 [05:27<01:21,  3.12it/s, training_loss=0.271]\u001b[A\n",
            "Epoch 2:  80%|███████▉  | 1016/1271 [05:27<01:21,  3.11it/s, training_loss=0.271]\u001b[A\n",
            "Epoch 2:  80%|███████▉  | 1016/1271 [05:27<01:21,  3.11it/s, training_loss=0.476]\u001b[A\n",
            "Epoch 2:  80%|████████  | 1017/1271 [05:27<01:22,  3.10it/s, training_loss=0.476]\u001b[A\n",
            "Epoch 2:  80%|████████  | 1017/1271 [05:27<01:22,  3.10it/s, training_loss=0.061]\u001b[A\n",
            "Epoch 2:  80%|████████  | 1018/1271 [05:27<01:21,  3.10it/s, training_loss=0.061]\u001b[A\n",
            "Epoch 2:  80%|████████  | 1018/1271 [05:28<01:21,  3.10it/s, training_loss=0.420]\u001b[A\n",
            "Epoch 2:  80%|████████  | 1019/1271 [05:28<01:20,  3.12it/s, training_loss=0.420]\u001b[A\n",
            "Epoch 2:  80%|████████  | 1019/1271 [05:28<01:20,  3.12it/s, training_loss=0.429]\u001b[A\n",
            "Epoch 2:  80%|████████  | 1020/1271 [05:28<01:20,  3.10it/s, training_loss=0.429]\u001b[A\n",
            "Epoch 2:  80%|████████  | 1020/1271 [05:28<01:20,  3.10it/s, training_loss=0.055]\u001b[A\n",
            "Epoch 2:  80%|████████  | 1021/1271 [05:28<01:21,  3.08it/s, training_loss=0.055]\u001b[A\n",
            "Epoch 2:  80%|████████  | 1021/1271 [05:29<01:21,  3.08it/s, training_loss=0.394]\u001b[A\n",
            "Epoch 2:  80%|████████  | 1022/1271 [05:29<01:20,  3.09it/s, training_loss=0.394]\u001b[A\n",
            "Epoch 2:  80%|████████  | 1022/1271 [05:29<01:20,  3.09it/s, training_loss=0.402]\u001b[A\n",
            "Epoch 2:  80%|████████  | 1023/1271 [05:29<01:19,  3.10it/s, training_loss=0.402]\u001b[A\n",
            "Epoch 2:  80%|████████  | 1023/1271 [05:29<01:19,  3.10it/s, training_loss=0.333]\u001b[A\n",
            "Epoch 2:  81%|████████  | 1024/1271 [05:29<01:19,  3.11it/s, training_loss=0.333]\u001b[A\n",
            "Epoch 2:  81%|████████  | 1024/1271 [05:30<01:19,  3.11it/s, training_loss=0.189]\u001b[A\n",
            "Epoch 2:  81%|████████  | 1025/1271 [05:30<01:18,  3.12it/s, training_loss=0.189]\u001b[A\n",
            "Epoch 2:  81%|████████  | 1025/1271 [05:30<01:18,  3.12it/s, training_loss=0.500]\u001b[A\n",
            "Epoch 2:  81%|████████  | 1026/1271 [05:30<01:19,  3.10it/s, training_loss=0.500]\u001b[A\n",
            "Epoch 2:  81%|████████  | 1026/1271 [05:30<01:19,  3.10it/s, training_loss=0.492]\u001b[A\n",
            "Epoch 2:  81%|████████  | 1027/1271 [05:30<01:18,  3.09it/s, training_loss=0.492]\u001b[A\n",
            "Epoch 2:  81%|████████  | 1027/1271 [05:31<01:18,  3.09it/s, training_loss=0.151]\u001b[A\n",
            "Epoch 2:  81%|████████  | 1028/1271 [05:31<01:18,  3.09it/s, training_loss=0.151]\u001b[A\n",
            "Epoch 2:  81%|████████  | 1028/1271 [05:31<01:18,  3.09it/s, training_loss=0.125]\u001b[A\n",
            "Epoch 2:  81%|████████  | 1029/1271 [05:31<01:17,  3.11it/s, training_loss=0.125]\u001b[A\n",
            "Epoch 2:  81%|████████  | 1029/1271 [05:31<01:17,  3.11it/s, training_loss=0.642]\u001b[A\n",
            "Epoch 2:  81%|████████  | 1030/1271 [05:31<01:17,  3.11it/s, training_loss=0.642]\u001b[A\n",
            "Epoch 2:  81%|████████  | 1030/1271 [05:31<01:17,  3.11it/s, training_loss=0.338]\u001b[A\n",
            "Epoch 2:  81%|████████  | 1031/1271 [05:31<01:17,  3.11it/s, training_loss=0.338]\u001b[A\n",
            "Epoch 2:  81%|████████  | 1031/1271 [05:32<01:17,  3.11it/s, training_loss=0.279]\u001b[A\n",
            "Epoch 2:  81%|████████  | 1032/1271 [05:32<01:16,  3.12it/s, training_loss=0.279]\u001b[A\n",
            "Epoch 2:  81%|████████  | 1032/1271 [05:32<01:16,  3.12it/s, training_loss=0.406]\u001b[A\n",
            "Epoch 2:  81%|████████▏ | 1033/1271 [05:32<01:16,  3.12it/s, training_loss=0.406]\u001b[A\n",
            "Epoch 2:  81%|████████▏ | 1033/1271 [05:32<01:16,  3.12it/s, training_loss=0.402]\u001b[A\n",
            "Epoch 2:  81%|████████▏ | 1034/1271 [05:32<01:16,  3.10it/s, training_loss=0.402]\u001b[A\n",
            "Epoch 2:  81%|████████▏ | 1034/1271 [05:33<01:16,  3.10it/s, training_loss=0.460]\u001b[A\n",
            "Epoch 2:  81%|████████▏ | 1035/1271 [05:33<01:16,  3.09it/s, training_loss=0.460]\u001b[A\n",
            "Epoch 2:  81%|████████▏ | 1035/1271 [05:33<01:16,  3.09it/s, training_loss=0.382]\u001b[A\n",
            "Epoch 2:  82%|████████▏ | 1036/1271 [05:33<01:15,  3.11it/s, training_loss=0.382]\u001b[A\n",
            "Epoch 2:  82%|████████▏ | 1036/1271 [05:33<01:15,  3.11it/s, training_loss=0.424]\u001b[A\n",
            "Epoch 2:  82%|████████▏ | 1037/1271 [05:33<01:14,  3.13it/s, training_loss=0.424]\u001b[A\n",
            "Epoch 2:  82%|████████▏ | 1037/1271 [05:34<01:14,  3.13it/s, training_loss=0.519]\u001b[A\n",
            "Epoch 2:  82%|████████▏ | 1038/1271 [05:34<01:14,  3.13it/s, training_loss=0.519]\u001b[A\n",
            "Epoch 2:  82%|████████▏ | 1038/1271 [05:34<01:14,  3.13it/s, training_loss=0.304]\u001b[A\n",
            "Epoch 2:  82%|████████▏ | 1039/1271 [05:34<01:14,  3.13it/s, training_loss=0.304]\u001b[A\n",
            "Epoch 2:  82%|████████▏ | 1039/1271 [05:34<01:14,  3.13it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 2:  82%|████████▏ | 1040/1271 [05:34<01:13,  3.12it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 2:  82%|████████▏ | 1040/1271 [05:35<01:13,  3.12it/s, training_loss=0.509]\u001b[A\n",
            "Epoch 2:  82%|████████▏ | 1041/1271 [05:35<01:13,  3.12it/s, training_loss=0.509]\u001b[A\n",
            "Epoch 2:  82%|████████▏ | 1041/1271 [05:35<01:13,  3.12it/s, training_loss=0.148]\u001b[A\n",
            "Epoch 2:  82%|████████▏ | 1042/1271 [05:35<01:13,  3.11it/s, training_loss=0.148]\u001b[A\n",
            "Epoch 2:  82%|████████▏ | 1042/1271 [05:35<01:13,  3.11it/s, training_loss=0.764]\u001b[A\n",
            "Epoch 2:  82%|████████▏ | 1043/1271 [05:35<01:13,  3.11it/s, training_loss=0.764]\u001b[A\n",
            "Epoch 2:  82%|████████▏ | 1043/1271 [05:36<01:13,  3.11it/s, training_loss=0.580]\u001b[A\n",
            "Epoch 2:  82%|████████▏ | 1044/1271 [05:36<01:12,  3.12it/s, training_loss=0.580]\u001b[A\n",
            "Epoch 2:  82%|████████▏ | 1044/1271 [05:36<01:12,  3.12it/s, training_loss=1.080]\u001b[A\n",
            "Epoch 2:  82%|████████▏ | 1045/1271 [05:36<01:12,  3.10it/s, training_loss=1.080]\u001b[A\n",
            "Epoch 2:  82%|████████▏ | 1045/1271 [05:36<01:12,  3.10it/s, training_loss=0.130]\u001b[A\n",
            "Epoch 2:  82%|████████▏ | 1046/1271 [05:36<01:12,  3.09it/s, training_loss=0.130]\u001b[A\n",
            "Epoch 2:  82%|████████▏ | 1046/1271 [05:37<01:12,  3.09it/s, training_loss=0.394]\u001b[A\n",
            "Epoch 2:  82%|████████▏ | 1047/1271 [05:37<01:12,  3.11it/s, training_loss=0.394]\u001b[A\n",
            "Epoch 2:  82%|████████▏ | 1047/1271 [05:37<01:12,  3.11it/s, training_loss=0.183]\u001b[A\n",
            "Epoch 2:  82%|████████▏ | 1048/1271 [05:37<01:12,  3.09it/s, training_loss=0.183]\u001b[A\n",
            "Epoch 2:  82%|████████▏ | 1048/1271 [05:37<01:12,  3.09it/s, training_loss=0.033]\u001b[A\n",
            "Epoch 2:  83%|████████▎ | 1049/1271 [05:37<01:11,  3.10it/s, training_loss=0.033]\u001b[A\n",
            "Epoch 2:  83%|████████▎ | 1049/1271 [05:38<01:11,  3.10it/s, training_loss=0.198]\u001b[A\n",
            "Epoch 2:  83%|████████▎ | 1050/1271 [05:38<01:11,  3.11it/s, training_loss=0.198]\u001b[A\n",
            "Epoch 2:  83%|████████▎ | 1050/1271 [05:38<01:11,  3.11it/s, training_loss=0.342]\u001b[A\n",
            "Epoch 2:  83%|████████▎ | 1051/1271 [05:38<01:10,  3.10it/s, training_loss=0.342]\u001b[A\n",
            "Epoch 2:  83%|████████▎ | 1051/1271 [05:38<01:10,  3.10it/s, training_loss=0.422]\u001b[A\n",
            "Epoch 2:  83%|████████▎ | 1052/1271 [05:38<01:10,  3.11it/s, training_loss=0.422]\u001b[A\n",
            "Epoch 2:  83%|████████▎ | 1052/1271 [05:39<01:10,  3.11it/s, training_loss=0.674]\u001b[A\n",
            "Epoch 2:  83%|████████▎ | 1053/1271 [05:39<01:09,  3.12it/s, training_loss=0.674]\u001b[A\n",
            "Epoch 2:  83%|████████▎ | 1053/1271 [05:39<01:09,  3.12it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 2:  83%|████████▎ | 1054/1271 [05:39<01:09,  3.13it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 2:  83%|████████▎ | 1054/1271 [05:39<01:09,  3.13it/s, training_loss=0.424]\u001b[A\n",
            "Epoch 2:  83%|████████▎ | 1055/1271 [05:39<01:09,  3.12it/s, training_loss=0.424]\u001b[A\n",
            "Epoch 2:  83%|████████▎ | 1055/1271 [05:39<01:09,  3.12it/s, training_loss=0.030]\u001b[A\n",
            "Epoch 2:  83%|████████▎ | 1056/1271 [05:40<01:08,  3.12it/s, training_loss=0.030]\u001b[A\n",
            "Epoch 2:  83%|████████▎ | 1056/1271 [05:40<01:08,  3.12it/s, training_loss=0.467]\u001b[A\n",
            "Epoch 2:  83%|████████▎ | 1057/1271 [05:40<01:08,  3.12it/s, training_loss=0.467]\u001b[A\n",
            "Epoch 2:  83%|████████▎ | 1057/1271 [05:40<01:08,  3.12it/s, training_loss=0.525]\u001b[A\n",
            "Epoch 2:  83%|████████▎ | 1058/1271 [05:40<01:08,  3.11it/s, training_loss=0.525]\u001b[A\n",
            "Epoch 2:  83%|████████▎ | 1058/1271 [05:40<01:08,  3.11it/s, training_loss=0.095]\u001b[A\n",
            "Epoch 2:  83%|████████▎ | 1059/1271 [05:40<01:07,  3.12it/s, training_loss=0.095]\u001b[A\n",
            "Epoch 2:  83%|████████▎ | 1059/1271 [05:41<01:07,  3.12it/s, training_loss=0.667]\u001b[A\n",
            "Epoch 2:  83%|████████▎ | 1060/1271 [05:41<01:07,  3.12it/s, training_loss=0.667]\u001b[A\n",
            "Epoch 2:  83%|████████▎ | 1060/1271 [05:41<01:07,  3.12it/s, training_loss=0.578]\u001b[A\n",
            "Epoch 2:  83%|████████▎ | 1061/1271 [05:41<01:07,  3.12it/s, training_loss=0.578]\u001b[A\n",
            "Epoch 2:  83%|████████▎ | 1061/1271 [05:41<01:07,  3.12it/s, training_loss=0.144]\u001b[A\n",
            "Epoch 2:  84%|████████▎ | 1062/1271 [05:41<01:07,  3.11it/s, training_loss=0.144]\u001b[A\n",
            "Epoch 2:  84%|████████▎ | 1062/1271 [05:42<01:07,  3.11it/s, training_loss=0.528]\u001b[A\n",
            "Epoch 2:  84%|████████▎ | 1063/1271 [05:42<01:06,  3.12it/s, training_loss=0.528]\u001b[A\n",
            "Epoch 2:  84%|████████▎ | 1063/1271 [05:42<01:06,  3.12it/s, training_loss=0.426]\u001b[A\n",
            "Epoch 2:  84%|████████▎ | 1064/1271 [05:42<01:06,  3.11it/s, training_loss=0.426]\u001b[A\n",
            "Epoch 2:  84%|████████▎ | 1064/1271 [05:42<01:06,  3.11it/s, training_loss=0.622]\u001b[A\n",
            "Epoch 2:  84%|████████▍ | 1065/1271 [05:42<01:06,  3.11it/s, training_loss=0.622]\u001b[A\n",
            "Epoch 2:  84%|████████▍ | 1065/1271 [05:43<01:06,  3.11it/s, training_loss=0.323]\u001b[A\n",
            "Epoch 2:  84%|████████▍ | 1066/1271 [05:43<01:05,  3.11it/s, training_loss=0.323]\u001b[A\n",
            "Epoch 2:  84%|████████▍ | 1066/1271 [05:43<01:05,  3.11it/s, training_loss=0.485]\u001b[A\n",
            "Epoch 2:  84%|████████▍ | 1067/1271 [05:43<01:05,  3.11it/s, training_loss=0.485]\u001b[A\n",
            "Epoch 2:  84%|████████▍ | 1067/1271 [05:43<01:05,  3.11it/s, training_loss=0.146]\u001b[A\n",
            "Epoch 2:  84%|████████▍ | 1068/1271 [05:43<01:05,  3.10it/s, training_loss=0.146]\u001b[A\n",
            "Epoch 2:  84%|████████▍ | 1068/1271 [05:44<01:05,  3.10it/s, training_loss=0.060]\u001b[A\n",
            "Epoch 2:  84%|████████▍ | 1069/1271 [05:44<01:05,  3.10it/s, training_loss=0.060]\u001b[A\n",
            "Epoch 2:  84%|████████▍ | 1069/1271 [05:44<01:05,  3.10it/s, training_loss=0.451]\u001b[A\n",
            "Epoch 2:  84%|████████▍ | 1070/1271 [05:44<01:04,  3.10it/s, training_loss=0.451]\u001b[A\n",
            "Epoch 2:  84%|████████▍ | 1070/1271 [05:44<01:04,  3.10it/s, training_loss=0.440]\u001b[A\n",
            "Epoch 2:  84%|████████▍ | 1071/1271 [05:44<01:04,  3.08it/s, training_loss=0.440]\u001b[A\n",
            "Epoch 2:  84%|████████▍ | 1071/1271 [05:45<01:04,  3.08it/s, training_loss=0.137]\u001b[A\n",
            "Epoch 2:  84%|████████▍ | 1072/1271 [05:45<01:04,  3.09it/s, training_loss=0.137]\u001b[A\n",
            "Epoch 2:  84%|████████▍ | 1072/1271 [05:45<01:04,  3.09it/s, training_loss=0.169]\u001b[A\n",
            "Epoch 2:  84%|████████▍ | 1073/1271 [05:45<01:04,  3.08it/s, training_loss=0.169]\u001b[A\n",
            "Epoch 2:  84%|████████▍ | 1073/1271 [05:45<01:04,  3.08it/s, training_loss=0.167]\u001b[A\n",
            "Epoch 2:  85%|████████▍ | 1074/1271 [05:45<01:03,  3.09it/s, training_loss=0.167]\u001b[A\n",
            "Epoch 2:  85%|████████▍ | 1074/1271 [05:46<01:03,  3.09it/s, training_loss=0.404]\u001b[A\n",
            "Epoch 2:  85%|████████▍ | 1075/1271 [05:46<01:03,  3.09it/s, training_loss=0.404]\u001b[A\n",
            "Epoch 2:  85%|████████▍ | 1075/1271 [05:46<01:03,  3.09it/s, training_loss=0.311]\u001b[A\n",
            "Epoch 2:  85%|████████▍ | 1076/1271 [05:46<01:03,  3.09it/s, training_loss=0.311]\u001b[A\n",
            "Epoch 2:  85%|████████▍ | 1076/1271 [05:46<01:03,  3.09it/s, training_loss=0.345]\u001b[A\n",
            "Epoch 2:  85%|████████▍ | 1077/1271 [05:46<01:02,  3.10it/s, training_loss=0.345]\u001b[A\n",
            "Epoch 2:  85%|████████▍ | 1077/1271 [05:47<01:02,  3.10it/s, training_loss=0.331]\u001b[A\n",
            "Epoch 2:  85%|████████▍ | 1078/1271 [05:47<01:02,  3.10it/s, training_loss=0.331]\u001b[A\n",
            "Epoch 2:  85%|████████▍ | 1078/1271 [05:47<01:02,  3.10it/s, training_loss=0.419]\u001b[A\n",
            "Epoch 2:  85%|████████▍ | 1079/1271 [05:47<01:01,  3.12it/s, training_loss=0.419]\u001b[A\n",
            "Epoch 2:  85%|████████▍ | 1079/1271 [05:47<01:01,  3.12it/s, training_loss=0.167]\u001b[A\n",
            "Epoch 2:  85%|████████▍ | 1080/1271 [05:47<01:01,  3.11it/s, training_loss=0.167]\u001b[A\n",
            "Epoch 2:  85%|████████▍ | 1080/1271 [05:48<01:01,  3.11it/s, training_loss=0.552]\u001b[A\n",
            "Epoch 2:  85%|████████▌ | 1081/1271 [05:48<01:00,  3.12it/s, training_loss=0.552]\u001b[A\n",
            "Epoch 2:  85%|████████▌ | 1081/1271 [05:48<01:00,  3.12it/s, training_loss=0.454]\u001b[A\n",
            "Epoch 2:  85%|████████▌ | 1082/1271 [05:48<01:00,  3.12it/s, training_loss=0.454]\u001b[A\n",
            "Epoch 2:  85%|████████▌ | 1082/1271 [05:48<01:00,  3.12it/s, training_loss=0.813]\u001b[A\n",
            "Epoch 2:  85%|████████▌ | 1083/1271 [05:48<01:00,  3.12it/s, training_loss=0.813]\u001b[A\n",
            "Epoch 2:  85%|████████▌ | 1083/1271 [05:49<01:00,  3.12it/s, training_loss=0.456]\u001b[A\n",
            "Epoch 2:  85%|████████▌ | 1084/1271 [05:49<00:59,  3.13it/s, training_loss=0.456]\u001b[A\n",
            "Epoch 2:  85%|████████▌ | 1084/1271 [05:49<00:59,  3.13it/s, training_loss=0.295]\u001b[A\n",
            "Epoch 2:  85%|████████▌ | 1085/1271 [05:49<00:59,  3.13it/s, training_loss=0.295]\u001b[A\n",
            "Epoch 2:  85%|████████▌ | 1085/1271 [05:49<00:59,  3.13it/s, training_loss=0.162]\u001b[A\n",
            "Epoch 2:  85%|████████▌ | 1086/1271 [05:49<00:59,  3.12it/s, training_loss=0.162]\u001b[A\n",
            "Epoch 2:  85%|████████▌ | 1086/1271 [05:49<00:59,  3.12it/s, training_loss=0.420]\u001b[A\n",
            "Epoch 2:  86%|████████▌ | 1087/1271 [05:49<00:59,  3.11it/s, training_loss=0.420]\u001b[A\n",
            "Epoch 2:  86%|████████▌ | 1087/1271 [05:50<00:59,  3.11it/s, training_loss=0.023]\u001b[A\n",
            "Epoch 2:  86%|████████▌ | 1088/1271 [05:50<00:58,  3.12it/s, training_loss=0.023]\u001b[A\n",
            "Epoch 2:  86%|████████▌ | 1088/1271 [05:50<00:58,  3.12it/s, training_loss=0.186]\u001b[A\n",
            "Epoch 2:  86%|████████▌ | 1089/1271 [05:50<00:58,  3.12it/s, training_loss=0.186]\u001b[A\n",
            "Epoch 2:  86%|████████▌ | 1089/1271 [05:50<00:58,  3.12it/s, training_loss=0.283]\u001b[A\n",
            "Epoch 2:  86%|████████▌ | 1090/1271 [05:50<00:57,  3.13it/s, training_loss=0.283]\u001b[A\n",
            "Epoch 2:  86%|████████▌ | 1090/1271 [05:51<00:57,  3.13it/s, training_loss=0.808]\u001b[A\n",
            "Epoch 2:  86%|████████▌ | 1091/1271 [05:51<00:57,  3.11it/s, training_loss=0.808]\u001b[A\n",
            "Epoch 2:  86%|████████▌ | 1091/1271 [05:51<00:57,  3.11it/s, training_loss=0.515]\u001b[A\n",
            "Epoch 2:  86%|████████▌ | 1092/1271 [05:51<00:57,  3.12it/s, training_loss=0.515]\u001b[A\n",
            "Epoch 2:  86%|████████▌ | 1092/1271 [05:51<00:57,  3.12it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 2:  86%|████████▌ | 1093/1271 [05:51<00:56,  3.13it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 2:  86%|████████▌ | 1093/1271 [05:52<00:56,  3.13it/s, training_loss=0.367]\u001b[A\n",
            "Epoch 2:  86%|████████▌ | 1094/1271 [05:52<00:57,  3.10it/s, training_loss=0.367]\u001b[A\n",
            "Epoch 2:  86%|████████▌ | 1094/1271 [05:52<00:57,  3.10it/s, training_loss=0.110]\u001b[A\n",
            "Epoch 2:  86%|████████▌ | 1095/1271 [05:52<00:56,  3.11it/s, training_loss=0.110]\u001b[A\n",
            "Epoch 2:  86%|████████▌ | 1095/1271 [05:52<00:56,  3.11it/s, training_loss=0.403]\u001b[A\n",
            "Epoch 2:  86%|████████▌ | 1096/1271 [05:52<00:56,  3.11it/s, training_loss=0.403]\u001b[A\n",
            "Epoch 2:  86%|████████▌ | 1096/1271 [05:53<00:56,  3.11it/s, training_loss=0.401]\u001b[A\n",
            "Epoch 2:  86%|████████▋ | 1097/1271 [05:53<00:55,  3.12it/s, training_loss=0.401]\u001b[A\n",
            "Epoch 2:  86%|████████▋ | 1097/1271 [05:53<00:55,  3.12it/s, training_loss=0.390]\u001b[A\n",
            "Epoch 2:  86%|████████▋ | 1098/1271 [05:53<00:55,  3.11it/s, training_loss=0.390]\u001b[A\n",
            "Epoch 2:  86%|████████▋ | 1098/1271 [05:53<00:55,  3.11it/s, training_loss=0.510]\u001b[A\n",
            "Epoch 2:  86%|████████▋ | 1099/1271 [05:53<00:55,  3.12it/s, training_loss=0.510]\u001b[A\n",
            "Epoch 2:  86%|████████▋ | 1099/1271 [05:54<00:55,  3.12it/s, training_loss=0.176]\u001b[A\n",
            "Epoch 2:  87%|████████▋ | 1100/1271 [05:54<00:54,  3.11it/s, training_loss=0.176]\u001b[A\n",
            "Epoch 2:  87%|████████▋ | 1100/1271 [05:54<00:54,  3.11it/s, training_loss=0.401]\u001b[A\n",
            "Epoch 2:  87%|████████▋ | 1101/1271 [05:54<00:54,  3.12it/s, training_loss=0.401]\u001b[A\n",
            "Epoch 2:  87%|████████▋ | 1101/1271 [05:54<00:54,  3.12it/s, training_loss=0.242]\u001b[A\n",
            "Epoch 2:  87%|████████▋ | 1102/1271 [05:54<00:54,  3.11it/s, training_loss=0.242]\u001b[A\n",
            "Epoch 2:  87%|████████▋ | 1102/1271 [05:55<00:54,  3.11it/s, training_loss=0.319]\u001b[A\n",
            "Epoch 2:  87%|████████▋ | 1103/1271 [05:55<00:54,  3.09it/s, training_loss=0.319]\u001b[A\n",
            "Epoch 2:  87%|████████▋ | 1103/1271 [05:55<00:54,  3.09it/s, training_loss=0.209]\u001b[A\n",
            "Epoch 2:  87%|████████▋ | 1104/1271 [05:55<00:53,  3.10it/s, training_loss=0.209]\u001b[A\n",
            "Epoch 2:  87%|████████▋ | 1104/1271 [05:55<00:53,  3.10it/s, training_loss=0.167]\u001b[A\n",
            "Epoch 2:  87%|████████▋ | 1105/1271 [05:55<00:53,  3.11it/s, training_loss=0.167]\u001b[A\n",
            "Epoch 2:  87%|████████▋ | 1105/1271 [05:56<00:53,  3.11it/s, training_loss=0.783]\u001b[A\n",
            "Epoch 2:  87%|████████▋ | 1106/1271 [05:56<00:53,  3.11it/s, training_loss=0.783]\u001b[A\n",
            "Epoch 2:  87%|████████▋ | 1106/1271 [05:56<00:53,  3.11it/s, training_loss=0.472]\u001b[A\n",
            "Epoch 2:  87%|████████▋ | 1107/1271 [05:56<00:52,  3.11it/s, training_loss=0.472]\u001b[A\n",
            "Epoch 2:  87%|████████▋ | 1107/1271 [05:56<00:52,  3.11it/s, training_loss=0.327]\u001b[A\n",
            "Epoch 2:  87%|████████▋ | 1108/1271 [05:56<00:52,  3.09it/s, training_loss=0.327]\u001b[A\n",
            "Epoch 2:  87%|████████▋ | 1108/1271 [05:57<00:52,  3.09it/s, training_loss=0.257]\u001b[A\n",
            "Epoch 2:  87%|████████▋ | 1109/1271 [05:57<00:52,  3.11it/s, training_loss=0.257]\u001b[A\n",
            "Epoch 2:  87%|████████▋ | 1109/1271 [05:57<00:52,  3.11it/s, training_loss=0.622]\u001b[A\n",
            "Epoch 2:  87%|████████▋ | 1110/1271 [05:57<00:52,  3.09it/s, training_loss=0.622]\u001b[A\n",
            "Epoch 2:  87%|████████▋ | 1110/1271 [05:57<00:52,  3.09it/s, training_loss=0.186]\u001b[A\n",
            "Epoch 2:  87%|████████▋ | 1111/1271 [05:57<00:51,  3.09it/s, training_loss=0.186]\u001b[A\n",
            "Epoch 2:  87%|████████▋ | 1111/1271 [05:58<00:51,  3.09it/s, training_loss=0.316]\u001b[A\n",
            "Epoch 2:  87%|████████▋ | 1112/1271 [05:58<00:51,  3.11it/s, training_loss=0.316]\u001b[A\n",
            "Epoch 2:  87%|████████▋ | 1112/1271 [05:58<00:51,  3.11it/s, training_loss=0.386]\u001b[A\n",
            "Epoch 2:  88%|████████▊ | 1113/1271 [05:58<00:51,  3.09it/s, training_loss=0.386]\u001b[A\n",
            "Epoch 2:  88%|████████▊ | 1113/1271 [05:58<00:51,  3.09it/s, training_loss=0.377]\u001b[A\n",
            "Epoch 2:  88%|████████▊ | 1114/1271 [05:58<00:50,  3.10it/s, training_loss=0.377]\u001b[A\n",
            "Epoch 2:  88%|████████▊ | 1114/1271 [05:58<00:50,  3.10it/s, training_loss=0.723]\u001b[A\n",
            "Epoch 2:  88%|████████▊ | 1115/1271 [05:58<00:50,  3.11it/s, training_loss=0.723]\u001b[A\n",
            "Epoch 2:  88%|████████▊ | 1115/1271 [05:59<00:50,  3.11it/s, training_loss=0.379]\u001b[A\n",
            "Epoch 2:  88%|████████▊ | 1116/1271 [05:59<00:49,  3.11it/s, training_loss=0.379]\u001b[A\n",
            "Epoch 2:  88%|████████▊ | 1116/1271 [05:59<00:49,  3.11it/s, training_loss=0.500]\u001b[A\n",
            "Epoch 2:  88%|████████▊ | 1117/1271 [05:59<00:49,  3.12it/s, training_loss=0.500]\u001b[A\n",
            "Epoch 2:  88%|████████▊ | 1117/1271 [05:59<00:49,  3.12it/s, training_loss=0.122]\u001b[A\n",
            "Epoch 2:  88%|████████▊ | 1118/1271 [05:59<00:48,  3.12it/s, training_loss=0.122]\u001b[A\n",
            "Epoch 2:  88%|████████▊ | 1118/1271 [06:00<00:48,  3.12it/s, training_loss=0.182]\u001b[A\n",
            "Epoch 2:  88%|████████▊ | 1119/1271 [06:00<00:48,  3.11it/s, training_loss=0.182]\u001b[A\n",
            "Epoch 2:  88%|████████▊ | 1119/1271 [06:00<00:48,  3.11it/s, training_loss=0.309]\u001b[A\n",
            "Epoch 2:  88%|████████▊ | 1120/1271 [06:00<00:48,  3.11it/s, training_loss=0.309]\u001b[A\n",
            "Epoch 2:  88%|████████▊ | 1120/1271 [06:00<00:48,  3.11it/s, training_loss=0.173]\u001b[A\n",
            "Epoch 2:  88%|████████▊ | 1121/1271 [06:00<00:47,  3.13it/s, training_loss=0.173]\u001b[A\n",
            "Epoch 2:  88%|████████▊ | 1121/1271 [06:01<00:47,  3.13it/s, training_loss=0.462]\u001b[A\n",
            "Epoch 2:  88%|████████▊ | 1122/1271 [06:01<00:47,  3.11it/s, training_loss=0.462]\u001b[A\n",
            "Epoch 2:  88%|████████▊ | 1122/1271 [06:01<00:47,  3.11it/s, training_loss=0.434]\u001b[A\n",
            "Epoch 2:  88%|████████▊ | 1123/1271 [06:01<00:47,  3.12it/s, training_loss=0.434]\u001b[A\n",
            "Epoch 2:  88%|████████▊ | 1123/1271 [06:01<00:47,  3.12it/s, training_loss=0.584]\u001b[A\n",
            "Epoch 2:  88%|████████▊ | 1124/1271 [06:01<00:47,  3.12it/s, training_loss=0.584]\u001b[A\n",
            "Epoch 2:  88%|████████▊ | 1124/1271 [06:02<00:47,  3.12it/s, training_loss=0.625]\u001b[A\n",
            "Epoch 2:  89%|████████▊ | 1125/1271 [06:02<00:46,  3.12it/s, training_loss=0.625]\u001b[A\n",
            "Epoch 2:  89%|████████▊ | 1125/1271 [06:02<00:46,  3.12it/s, training_loss=0.244]\u001b[A\n",
            "Epoch 2:  89%|████████▊ | 1126/1271 [06:02<00:46,  3.13it/s, training_loss=0.244]\u001b[A\n",
            "Epoch 2:  89%|████████▊ | 1126/1271 [06:02<00:46,  3.13it/s, training_loss=0.189]\u001b[A\n",
            "Epoch 2:  89%|████████▊ | 1127/1271 [06:02<00:45,  3.14it/s, training_loss=0.189]\u001b[A\n",
            "Epoch 2:  89%|████████▊ | 1127/1271 [06:03<00:45,  3.14it/s, training_loss=0.263]\u001b[A\n",
            "Epoch 2:  89%|████████▊ | 1128/1271 [06:03<00:45,  3.12it/s, training_loss=0.263]\u001b[A\n",
            "Epoch 2:  89%|████████▊ | 1128/1271 [06:03<00:45,  3.12it/s, training_loss=0.921]\u001b[A\n",
            "Epoch 2:  89%|████████▉ | 1129/1271 [06:03<00:45,  3.11it/s, training_loss=0.921]\u001b[A\n",
            "Epoch 2:  89%|████████▉ | 1129/1271 [06:03<00:45,  3.11it/s, training_loss=0.323]\u001b[A\n",
            "Epoch 2:  89%|████████▉ | 1130/1271 [06:03<00:45,  3.13it/s, training_loss=0.323]\u001b[A\n",
            "Epoch 2:  89%|████████▉ | 1130/1271 [06:04<00:45,  3.13it/s, training_loss=0.290]\u001b[A\n",
            "Epoch 2:  89%|████████▉ | 1131/1271 [06:04<00:44,  3.12it/s, training_loss=0.290]\u001b[A\n",
            "Epoch 2:  89%|████████▉ | 1131/1271 [06:04<00:44,  3.12it/s, training_loss=0.154]\u001b[A\n",
            "Epoch 2:  89%|████████▉ | 1132/1271 [06:04<00:44,  3.11it/s, training_loss=0.154]\u001b[A\n",
            "Epoch 2:  89%|████████▉ | 1132/1271 [06:04<00:44,  3.11it/s, training_loss=0.039]\u001b[A\n",
            "Epoch 2:  89%|████████▉ | 1133/1271 [06:04<00:44,  3.11it/s, training_loss=0.039]\u001b[A\n",
            "Epoch 2:  89%|████████▉ | 1133/1271 [06:05<00:44,  3.11it/s, training_loss=0.051]\u001b[A\n",
            "Epoch 2:  89%|████████▉ | 1134/1271 [06:05<00:44,  3.09it/s, training_loss=0.051]\u001b[A\n",
            "Epoch 2:  89%|████████▉ | 1134/1271 [06:05<00:44,  3.09it/s, training_loss=0.669]\u001b[A\n",
            "Epoch 2:  89%|████████▉ | 1135/1271 [06:05<00:44,  3.08it/s, training_loss=0.669]\u001b[A\n",
            "Epoch 2:  89%|████████▉ | 1135/1271 [06:05<00:44,  3.08it/s, training_loss=0.791]\u001b[A\n",
            "Epoch 2:  89%|████████▉ | 1136/1271 [06:05<00:43,  3.09it/s, training_loss=0.791]\u001b[A\n",
            "Epoch 2:  89%|████████▉ | 1136/1271 [06:06<00:43,  3.09it/s, training_loss=0.015]\u001b[A\n",
            "Epoch 2:  89%|████████▉ | 1137/1271 [06:06<00:43,  3.09it/s, training_loss=0.015]\u001b[A\n",
            "Epoch 2:  89%|████████▉ | 1137/1271 [06:06<00:43,  3.09it/s, training_loss=0.538]\u001b[A\n",
            "Epoch 2:  90%|████████▉ | 1138/1271 [06:06<00:43,  3.08it/s, training_loss=0.538]\u001b[A\n",
            "Epoch 2:  90%|████████▉ | 1138/1271 [06:06<00:43,  3.08it/s, training_loss=0.337]\u001b[A\n",
            "Epoch 2:  90%|████████▉ | 1139/1271 [06:06<00:42,  3.10it/s, training_loss=0.337]\u001b[A\n",
            "Epoch 2:  90%|████████▉ | 1139/1271 [06:07<00:42,  3.10it/s, training_loss=0.171]\u001b[A\n",
            "Epoch 2:  90%|████████▉ | 1140/1271 [06:07<00:42,  3.11it/s, training_loss=0.171]\u001b[A\n",
            "Epoch 2:  90%|████████▉ | 1140/1271 [06:07<00:42,  3.11it/s, training_loss=0.260]\u001b[A\n",
            "Epoch 2:  90%|████████▉ | 1141/1271 [06:07<00:41,  3.12it/s, training_loss=0.260]\u001b[A\n",
            "Epoch 2:  90%|████████▉ | 1141/1271 [06:07<00:41,  3.12it/s, training_loss=0.014]\u001b[A\n",
            "Epoch 2:  90%|████████▉ | 1142/1271 [06:07<00:41,  3.12it/s, training_loss=0.014]\u001b[A\n",
            "Epoch 2:  90%|████████▉ | 1142/1271 [06:07<00:41,  3.12it/s, training_loss=0.646]\u001b[A\n",
            "Epoch 2:  90%|████████▉ | 1143/1271 [06:07<00:41,  3.11it/s, training_loss=0.646]\u001b[A\n",
            "Epoch 2:  90%|████████▉ | 1143/1271 [06:08<00:41,  3.11it/s, training_loss=0.165]\u001b[A\n",
            "Epoch 2:  90%|█████████ | 1144/1271 [06:08<00:40,  3.11it/s, training_loss=0.165]\u001b[A\n",
            "Epoch 2:  90%|█████████ | 1144/1271 [06:08<00:40,  3.11it/s, training_loss=0.338]\u001b[A\n",
            "Epoch 2:  90%|█████████ | 1145/1271 [06:08<00:40,  3.12it/s, training_loss=0.338]\u001b[A\n",
            "Epoch 2:  90%|█████████ | 1145/1271 [06:08<00:40,  3.12it/s, training_loss=0.325]\u001b[A\n",
            "Epoch 2:  90%|█████████ | 1146/1271 [06:08<00:40,  3.11it/s, training_loss=0.325]\u001b[A\n",
            "Epoch 2:  90%|█████████ | 1146/1271 [06:09<00:40,  3.11it/s, training_loss=0.666]\u001b[A\n",
            "Epoch 2:  90%|█████████ | 1147/1271 [06:09<00:39,  3.11it/s, training_loss=0.666]\u001b[A\n",
            "Epoch 2:  90%|█████████ | 1147/1271 [06:09<00:39,  3.11it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 2:  90%|█████████ | 1148/1271 [06:09<00:39,  3.12it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 2:  90%|█████████ | 1148/1271 [06:09<00:39,  3.12it/s, training_loss=0.459]\u001b[A\n",
            "Epoch 2:  90%|█████████ | 1149/1271 [06:09<00:39,  3.11it/s, training_loss=0.459]\u001b[A\n",
            "Epoch 2:  90%|█████████ | 1149/1271 [06:10<00:39,  3.11it/s, training_loss=0.617]\u001b[A\n",
            "Epoch 2:  90%|█████████ | 1150/1271 [06:10<00:38,  3.11it/s, training_loss=0.617]\u001b[A\n",
            "Epoch 2:  90%|█████████ | 1150/1271 [06:10<00:38,  3.11it/s, training_loss=0.892]\u001b[A\n",
            "Epoch 2:  91%|█████████ | 1151/1271 [06:10<00:38,  3.12it/s, training_loss=0.892]\u001b[A\n",
            "Epoch 2:  91%|█████████ | 1151/1271 [06:10<00:38,  3.12it/s, training_loss=0.274]\u001b[A\n",
            "Epoch 2:  91%|█████████ | 1152/1271 [06:10<00:38,  3.11it/s, training_loss=0.274]\u001b[A\n",
            "Epoch 2:  91%|█████████ | 1152/1271 [06:11<00:38,  3.11it/s, training_loss=0.939]\u001b[A\n",
            "Epoch 2:  91%|█████████ | 1153/1271 [06:11<00:38,  3.10it/s, training_loss=0.939]\u001b[A\n",
            "Epoch 2:  91%|█████████ | 1153/1271 [06:11<00:38,  3.10it/s, training_loss=0.189]\u001b[A\n",
            "Epoch 2:  91%|█████████ | 1154/1271 [06:11<00:37,  3.11it/s, training_loss=0.189]\u001b[A\n",
            "Epoch 2:  91%|█████████ | 1154/1271 [06:11<00:37,  3.11it/s, training_loss=0.388]\u001b[A\n",
            "Epoch 2:  91%|█████████ | 1155/1271 [06:11<00:37,  3.11it/s, training_loss=0.388]\u001b[A\n",
            "Epoch 2:  91%|█████████ | 1155/1271 [06:12<00:37,  3.11it/s, training_loss=0.395]\u001b[A\n",
            "Epoch 2:  91%|█████████ | 1156/1271 [06:12<00:37,  3.09it/s, training_loss=0.395]\u001b[A\n",
            "Epoch 2:  91%|█████████ | 1156/1271 [06:12<00:37,  3.09it/s, training_loss=0.532]\u001b[A\n",
            "Epoch 2:  91%|█████████ | 1157/1271 [06:12<00:36,  3.10it/s, training_loss=0.532]\u001b[A\n",
            "Epoch 2:  91%|█████████ | 1157/1271 [06:12<00:36,  3.10it/s, training_loss=0.627]\u001b[A\n",
            "Epoch 2:  91%|█████████ | 1158/1271 [06:12<00:36,  3.08it/s, training_loss=0.627]\u001b[A\n",
            "Epoch 2:  91%|█████████ | 1158/1271 [06:13<00:36,  3.08it/s, training_loss=0.382]\u001b[A\n",
            "Epoch 2:  91%|█████████ | 1159/1271 [06:13<00:36,  3.07it/s, training_loss=0.382]\u001b[A\n",
            "Epoch 2:  91%|█████████ | 1159/1271 [06:13<00:36,  3.07it/s, training_loss=0.220]\u001b[A\n",
            "Epoch 2:  91%|█████████▏| 1160/1271 [06:13<00:36,  3.08it/s, training_loss=0.220]\u001b[A\n",
            "Epoch 2:  91%|█████████▏| 1160/1271 [06:13<00:36,  3.08it/s, training_loss=0.584]\u001b[A\n",
            "Epoch 2:  91%|█████████▏| 1161/1271 [06:13<00:35,  3.09it/s, training_loss=0.584]\u001b[A\n",
            "Epoch 2:  91%|█████████▏| 1161/1271 [06:14<00:35,  3.09it/s, training_loss=0.362]\u001b[A\n",
            "Epoch 2:  91%|█████████▏| 1162/1271 [06:14<00:35,  3.09it/s, training_loss=0.362]\u001b[A\n",
            "Epoch 2:  91%|█████████▏| 1162/1271 [06:14<00:35,  3.09it/s, training_loss=0.348]\u001b[A\n",
            "Epoch 2:  92%|█████████▏| 1163/1271 [06:14<00:34,  3.09it/s, training_loss=0.348]\u001b[A\n",
            "Epoch 2:  92%|█████████▏| 1163/1271 [06:14<00:34,  3.09it/s, training_loss=1.159]\u001b[A\n",
            "Epoch 2:  92%|█████████▏| 1164/1271 [06:14<00:34,  3.11it/s, training_loss=1.159]\u001b[A\n",
            "Epoch 2:  92%|█████████▏| 1164/1271 [06:15<00:34,  3.11it/s, training_loss=0.168]\u001b[A\n",
            "Epoch 2:  92%|█████████▏| 1165/1271 [06:15<00:34,  3.11it/s, training_loss=0.168]\u001b[A\n",
            "Epoch 2:  92%|█████████▏| 1165/1271 [06:15<00:34,  3.11it/s, training_loss=0.373]\u001b[A\n",
            "Epoch 2:  92%|█████████▏| 1166/1271 [06:15<00:33,  3.09it/s, training_loss=0.373]\u001b[A\n",
            "Epoch 2:  92%|█████████▏| 1166/1271 [06:15<00:33,  3.09it/s, training_loss=0.239]\u001b[A\n",
            "Epoch 2:  92%|█████████▏| 1167/1271 [06:15<00:33,  3.09it/s, training_loss=0.239]\u001b[A\n",
            "Epoch 2:  92%|█████████▏| 1167/1271 [06:16<00:33,  3.09it/s, training_loss=0.284]\u001b[A\n",
            "Epoch 2:  92%|█████████▏| 1168/1271 [06:16<00:33,  3.09it/s, training_loss=0.284]\u001b[A\n",
            "Epoch 2:  92%|█████████▏| 1168/1271 [06:16<00:33,  3.09it/s, training_loss=0.600]\u001b[A\n",
            "Epoch 2:  92%|█████████▏| 1169/1271 [06:16<00:33,  3.09it/s, training_loss=0.600]\u001b[A\n",
            "Epoch 2:  92%|█████████▏| 1169/1271 [06:16<00:33,  3.09it/s, training_loss=0.168]\u001b[A\n",
            "Epoch 2:  92%|█████████▏| 1170/1271 [06:16<00:32,  3.11it/s, training_loss=0.168]\u001b[A\n",
            "Epoch 2:  92%|█████████▏| 1170/1271 [06:17<00:32,  3.11it/s, training_loss=0.722]\u001b[A\n",
            "Epoch 2:  92%|█████████▏| 1171/1271 [06:17<00:32,  3.11it/s, training_loss=0.722]\u001b[A\n",
            "Epoch 2:  92%|█████████▏| 1171/1271 [06:17<00:32,  3.11it/s, training_loss=0.181]\u001b[A\n",
            "Epoch 2:  92%|█████████▏| 1172/1271 [06:17<00:31,  3.12it/s, training_loss=0.181]\u001b[A\n",
            "Epoch 2:  92%|█████████▏| 1172/1271 [06:17<00:31,  3.12it/s, training_loss=0.712]\u001b[A\n",
            "Epoch 2:  92%|█████████▏| 1173/1271 [06:17<00:31,  3.12it/s, training_loss=0.712]\u001b[A\n",
            "Epoch 2:  92%|█████████▏| 1173/1271 [06:17<00:31,  3.12it/s, training_loss=0.414]\u001b[A\n",
            "Epoch 2:  92%|█████████▏| 1174/1271 [06:17<00:30,  3.13it/s, training_loss=0.414]\u001b[A\n",
            "Epoch 2:  92%|█████████▏| 1174/1271 [06:18<00:30,  3.13it/s, training_loss=0.518]\u001b[A\n",
            "Epoch 2:  92%|█████████▏| 1175/1271 [06:18<00:30,  3.13it/s, training_loss=0.518]\u001b[A\n",
            "Epoch 2:  92%|█████████▏| 1175/1271 [06:18<00:30,  3.13it/s, training_loss=0.493]\u001b[A\n",
            "Epoch 2:  93%|█████████▎| 1176/1271 [06:18<00:30,  3.12it/s, training_loss=0.493]\u001b[A\n",
            "Epoch 2:  93%|█████████▎| 1176/1271 [06:18<00:30,  3.12it/s, training_loss=0.126]\u001b[A\n",
            "Epoch 2:  93%|█████████▎| 1177/1271 [06:18<00:30,  3.11it/s, training_loss=0.126]\u001b[A\n",
            "Epoch 2:  93%|█████████▎| 1177/1271 [06:19<00:30,  3.11it/s, training_loss=0.463]\u001b[A\n",
            "Epoch 2:  93%|█████████▎| 1178/1271 [06:19<00:29,  3.12it/s, training_loss=0.463]\u001b[A\n",
            "Epoch 2:  93%|█████████▎| 1178/1271 [06:19<00:29,  3.12it/s, training_loss=0.780]\u001b[A\n",
            "Epoch 2:  93%|█████████▎| 1179/1271 [06:19<00:29,  3.12it/s, training_loss=0.780]\u001b[A\n",
            "Epoch 2:  93%|█████████▎| 1179/1271 [06:19<00:29,  3.12it/s, training_loss=0.091]\u001b[A\n",
            "Epoch 2:  93%|█████████▎| 1180/1271 [06:19<00:29,  3.12it/s, training_loss=0.091]\u001b[A\n",
            "Epoch 2:  93%|█████████▎| 1180/1271 [06:20<00:29,  3.12it/s, training_loss=0.669]\u001b[A\n",
            "Epoch 2:  93%|█████████▎| 1181/1271 [06:20<00:28,  3.13it/s, training_loss=0.669]\u001b[A\n",
            "Epoch 2:  93%|█████████▎| 1181/1271 [06:20<00:28,  3.13it/s, training_loss=0.054]\u001b[A\n",
            "Epoch 2:  93%|█████████▎| 1182/1271 [06:20<00:28,  3.13it/s, training_loss=0.054]\u001b[A\n",
            "Epoch 2:  93%|█████████▎| 1182/1271 [06:20<00:28,  3.13it/s, training_loss=0.266]\u001b[A\n",
            "Epoch 2:  93%|█████████▎| 1183/1271 [06:20<00:28,  3.14it/s, training_loss=0.266]\u001b[A\n",
            "Epoch 2:  93%|█████████▎| 1183/1271 [06:21<00:28,  3.14it/s, training_loss=0.415]\u001b[A\n",
            "Epoch 2:  93%|█████████▎| 1184/1271 [06:21<00:27,  3.13it/s, training_loss=0.415]\u001b[A\n",
            "Epoch 2:  93%|█████████▎| 1184/1271 [06:21<00:27,  3.13it/s, training_loss=0.495]\u001b[A\n",
            "Epoch 2:  93%|█████████▎| 1185/1271 [06:21<00:27,  3.13it/s, training_loss=0.495]\u001b[A\n",
            "Epoch 2:  93%|█████████▎| 1185/1271 [06:21<00:27,  3.13it/s, training_loss=0.194]\u001b[A\n",
            "Epoch 2:  93%|█████████▎| 1186/1271 [06:21<00:27,  3.13it/s, training_loss=0.194]\u001b[A\n",
            "Epoch 2:  93%|█████████▎| 1186/1271 [06:22<00:27,  3.13it/s, training_loss=0.328]\u001b[A\n",
            "Epoch 2:  93%|█████████▎| 1187/1271 [06:22<00:26,  3.13it/s, training_loss=0.328]\u001b[A\n",
            "Epoch 2:  93%|█████████▎| 1187/1271 [06:22<00:26,  3.13it/s, training_loss=0.708]\u001b[A\n",
            "Epoch 2:  93%|█████████▎| 1188/1271 [06:22<00:26,  3.12it/s, training_loss=0.708]\u001b[A\n",
            "Epoch 2:  93%|█████████▎| 1188/1271 [06:22<00:26,  3.12it/s, training_loss=0.368]\u001b[A\n",
            "Epoch 2:  94%|█████████▎| 1189/1271 [06:22<00:26,  3.11it/s, training_loss=0.368]\u001b[A\n",
            "Epoch 2:  94%|█████████▎| 1189/1271 [06:23<00:26,  3.11it/s, training_loss=0.266]\u001b[A\n",
            "Epoch 2:  94%|█████████▎| 1190/1271 [06:23<00:26,  3.11it/s, training_loss=0.266]\u001b[A\n",
            "Epoch 2:  94%|█████████▎| 1190/1271 [06:23<00:26,  3.11it/s, training_loss=0.311]\u001b[A\n",
            "Epoch 2:  94%|█████████▎| 1191/1271 [06:23<00:25,  3.10it/s, training_loss=0.311]\u001b[A\n",
            "Epoch 2:  94%|█████████▎| 1191/1271 [06:23<00:25,  3.10it/s, training_loss=0.214]\u001b[A\n",
            "Epoch 2:  94%|█████████▍| 1192/1271 [06:23<00:25,  3.12it/s, training_loss=0.214]\u001b[A\n",
            "Epoch 2:  94%|█████████▍| 1192/1271 [06:24<00:25,  3.12it/s, training_loss=0.324]\u001b[A\n",
            "Epoch 2:  94%|█████████▍| 1193/1271 [06:24<00:24,  3.14it/s, training_loss=0.324]\u001b[A\n",
            "Epoch 2:  94%|█████████▍| 1193/1271 [06:24<00:24,  3.14it/s, training_loss=0.545]\u001b[A\n",
            "Epoch 2:  94%|█████████▍| 1194/1271 [06:24<00:24,  3.14it/s, training_loss=0.545]\u001b[A\n",
            "Epoch 2:  94%|█████████▍| 1194/1271 [06:24<00:24,  3.14it/s, training_loss=0.185]\u001b[A\n",
            "Epoch 2:  94%|█████████▍| 1195/1271 [06:24<00:24,  3.13it/s, training_loss=0.185]\u001b[A\n",
            "Epoch 2:  94%|█████████▍| 1195/1271 [06:25<00:24,  3.13it/s, training_loss=0.577]\u001b[A\n",
            "Epoch 2:  94%|█████████▍| 1196/1271 [06:25<00:24,  3.12it/s, training_loss=0.577]\u001b[A\n",
            "Epoch 2:  94%|█████████▍| 1196/1271 [06:25<00:24,  3.12it/s, training_loss=0.518]\u001b[A\n",
            "Epoch 2:  94%|█████████▍| 1197/1271 [06:25<00:23,  3.13it/s, training_loss=0.518]\u001b[A\n",
            "Epoch 2:  94%|█████████▍| 1197/1271 [06:25<00:23,  3.13it/s, training_loss=0.248]\u001b[A\n",
            "Epoch 2:  94%|█████████▍| 1198/1271 [06:25<00:23,  3.10it/s, training_loss=0.248]\u001b[A\n",
            "Epoch 2:  94%|█████████▍| 1198/1271 [06:25<00:23,  3.10it/s, training_loss=0.428]\u001b[A\n",
            "Epoch 2:  94%|█████████▍| 1199/1271 [06:25<00:23,  3.09it/s, training_loss=0.428]\u001b[A\n",
            "Epoch 2:  94%|█████████▍| 1199/1271 [06:26<00:23,  3.09it/s, training_loss=0.178]\u001b[A\n",
            "Epoch 2:  94%|█████████▍| 1200/1271 [06:26<00:22,  3.10it/s, training_loss=0.178]\u001b[A\n",
            "Epoch 2:  94%|█████████▍| 1200/1271 [06:26<00:22,  3.10it/s, training_loss=0.708]\u001b[A\n",
            "Epoch 2:  94%|█████████▍| 1201/1271 [06:26<00:22,  3.08it/s, training_loss=0.708]\u001b[A\n",
            "Epoch 2:  94%|█████████▍| 1201/1271 [06:26<00:22,  3.08it/s, training_loss=0.434]\u001b[A\n",
            "Epoch 2:  95%|█████████▍| 1202/1271 [06:26<00:22,  3.08it/s, training_loss=0.434]\u001b[A\n",
            "Epoch 2:  95%|█████████▍| 1202/1271 [06:27<00:22,  3.08it/s, training_loss=0.393]\u001b[A\n",
            "Epoch 2:  95%|█████████▍| 1203/1271 [06:27<00:22,  3.08it/s, training_loss=0.393]\u001b[A\n",
            "Epoch 2:  95%|█████████▍| 1203/1271 [06:27<00:22,  3.08it/s, training_loss=0.341]\u001b[A\n",
            "Epoch 2:  95%|█████████▍| 1204/1271 [06:27<00:21,  3.07it/s, training_loss=0.341]\u001b[A\n",
            "Epoch 2:  95%|█████████▍| 1204/1271 [06:27<00:21,  3.07it/s, training_loss=0.151]\u001b[A\n",
            "Epoch 2:  95%|█████████▍| 1205/1271 [06:27<00:21,  3.08it/s, training_loss=0.151]\u001b[A\n",
            "Epoch 2:  95%|█████████▍| 1205/1271 [06:28<00:21,  3.08it/s, training_loss=0.418]\u001b[A\n",
            "Epoch 2:  95%|█████████▍| 1206/1271 [06:28<00:21,  3.08it/s, training_loss=0.418]\u001b[A\n",
            "Epoch 2:  95%|█████████▍| 1206/1271 [06:28<00:21,  3.08it/s, training_loss=0.629]\u001b[A\n",
            "Epoch 2:  95%|█████████▍| 1207/1271 [06:28<00:20,  3.09it/s, training_loss=0.629]\u001b[A\n",
            "Epoch 2:  95%|█████████▍| 1207/1271 [06:28<00:20,  3.09it/s, training_loss=0.152]\u001b[A\n",
            "Epoch 2:  95%|█████████▌| 1208/1271 [06:28<00:20,  3.10it/s, training_loss=0.152]\u001b[A\n",
            "Epoch 2:  95%|█████████▌| 1208/1271 [06:29<00:20,  3.10it/s, training_loss=0.049]\u001b[A\n",
            "Epoch 2:  95%|█████████▌| 1209/1271 [06:29<00:19,  3.11it/s, training_loss=0.049]\u001b[A\n",
            "Epoch 2:  95%|█████████▌| 1209/1271 [06:29<00:19,  3.11it/s, training_loss=0.200]\u001b[A\n",
            "Epoch 2:  95%|█████████▌| 1210/1271 [06:29<00:19,  3.10it/s, training_loss=0.200]\u001b[A\n",
            "Epoch 2:  95%|█████████▌| 1210/1271 [06:29<00:19,  3.10it/s, training_loss=0.150]\u001b[A\n",
            "Epoch 2:  95%|█████████▌| 1211/1271 [06:29<00:19,  3.11it/s, training_loss=0.150]\u001b[A\n",
            "Epoch 2:  95%|█████████▌| 1211/1271 [06:30<00:19,  3.11it/s, training_loss=0.120]\u001b[A\n",
            "Epoch 2:  95%|█████████▌| 1212/1271 [06:30<00:18,  3.12it/s, training_loss=0.120]\u001b[A\n",
            "Epoch 2:  95%|█████████▌| 1212/1271 [06:30<00:18,  3.12it/s, training_loss=0.103]\u001b[A\n",
            "Epoch 2:  95%|█████████▌| 1213/1271 [06:30<00:18,  3.11it/s, training_loss=0.103]\u001b[A\n",
            "Epoch 2:  95%|█████████▌| 1213/1271 [06:30<00:18,  3.11it/s, training_loss=0.201]\u001b[A\n",
            "Epoch 2:  96%|█████████▌| 1214/1271 [06:30<00:18,  3.12it/s, training_loss=0.201]\u001b[A\n",
            "Epoch 2:  96%|█████████▌| 1214/1271 [06:31<00:18,  3.12it/s, training_loss=0.185]\u001b[A\n",
            "Epoch 2:  96%|█████████▌| 1215/1271 [06:31<00:17,  3.12it/s, training_loss=0.185]\u001b[A\n",
            "Epoch 2:  96%|█████████▌| 1215/1271 [06:31<00:17,  3.12it/s, training_loss=0.212]\u001b[A\n",
            "Epoch 2:  96%|█████████▌| 1216/1271 [06:31<00:17,  3.12it/s, training_loss=0.212]\u001b[A\n",
            "Epoch 2:  96%|█████████▌| 1216/1271 [06:31<00:17,  3.12it/s, training_loss=0.164]\u001b[A\n",
            "Epoch 2:  96%|█████████▌| 1217/1271 [06:31<00:17,  3.14it/s, training_loss=0.164]\u001b[A\n",
            "Epoch 2:  96%|█████████▌| 1217/1271 [06:32<00:17,  3.14it/s, training_loss=0.199]\u001b[A\n",
            "Epoch 2:  96%|█████████▌| 1218/1271 [06:32<00:16,  3.15it/s, training_loss=0.199]\u001b[A\n",
            "Epoch 2:  96%|█████████▌| 1218/1271 [06:32<00:16,  3.15it/s, training_loss=0.877]\u001b[A\n",
            "Epoch 2:  96%|█████████▌| 1219/1271 [06:32<00:16,  3.13it/s, training_loss=0.877]\u001b[A\n",
            "Epoch 2:  96%|█████████▌| 1219/1271 [06:32<00:16,  3.13it/s, training_loss=0.852]\u001b[A\n",
            "Epoch 2:  96%|█████████▌| 1220/1271 [06:32<00:16,  3.13it/s, training_loss=0.852]\u001b[A\n",
            "Epoch 2:  96%|█████████▌| 1220/1271 [06:33<00:16,  3.13it/s, training_loss=0.040]\u001b[A\n",
            "Epoch 2:  96%|█████████▌| 1221/1271 [06:33<00:15,  3.14it/s, training_loss=0.040]\u001b[A\n",
            "Epoch 2:  96%|█████████▌| 1221/1271 [06:33<00:15,  3.14it/s, training_loss=0.100]\u001b[A\n",
            "Epoch 2:  96%|█████████▌| 1222/1271 [06:33<00:15,  3.14it/s, training_loss=0.100]\u001b[A\n",
            "Epoch 2:  96%|█████████▌| 1222/1271 [06:33<00:15,  3.14it/s, training_loss=0.157]\u001b[A\n",
            "Epoch 2:  96%|█████████▌| 1223/1271 [06:33<00:15,  3.13it/s, training_loss=0.157]\u001b[A\n",
            "Epoch 2:  96%|█████████▌| 1223/1271 [06:34<00:15,  3.13it/s, training_loss=0.172]\u001b[A\n",
            "Epoch 2:  96%|█████████▋| 1224/1271 [06:34<00:14,  3.14it/s, training_loss=0.172]\u001b[A\n",
            "Epoch 2:  96%|█████████▋| 1224/1271 [06:34<00:14,  3.14it/s, training_loss=0.170]\u001b[A\n",
            "Epoch 2:  96%|█████████▋| 1225/1271 [06:34<00:14,  3.14it/s, training_loss=0.170]\u001b[A\n",
            "Epoch 2:  96%|█████████▋| 1225/1271 [06:34<00:14,  3.14it/s, training_loss=0.407]\u001b[A\n",
            "Epoch 2:  96%|█████████▋| 1226/1271 [06:34<00:14,  3.12it/s, training_loss=0.407]\u001b[A\n",
            "Epoch 2:  96%|█████████▋| 1226/1271 [06:34<00:14,  3.12it/s, training_loss=0.601]\u001b[A\n",
            "Epoch 2:  97%|█████████▋| 1227/1271 [06:34<00:14,  3.12it/s, training_loss=0.601]\u001b[A\n",
            "Epoch 2:  97%|█████████▋| 1227/1271 [06:35<00:14,  3.12it/s, training_loss=0.220]\u001b[A\n",
            "Epoch 2:  97%|█████████▋| 1228/1271 [06:35<00:13,  3.13it/s, training_loss=0.220]\u001b[A\n",
            "Epoch 2:  97%|█████████▋| 1228/1271 [06:35<00:13,  3.13it/s, training_loss=0.315]\u001b[A\n",
            "Epoch 2:  97%|█████████▋| 1229/1271 [06:35<00:13,  3.14it/s, training_loss=0.315]\u001b[A\n",
            "Epoch 2:  97%|█████████▋| 1229/1271 [06:35<00:13,  3.14it/s, training_loss=0.028]\u001b[A\n",
            "Epoch 2:  97%|█████████▋| 1230/1271 [06:35<00:13,  3.13it/s, training_loss=0.028]\u001b[A\n",
            "Epoch 2:  97%|█████████▋| 1230/1271 [06:36<00:13,  3.13it/s, training_loss=0.363]\u001b[A\n",
            "Epoch 2:  97%|█████████▋| 1231/1271 [06:36<00:12,  3.11it/s, training_loss=0.363]\u001b[A\n",
            "Epoch 2:  97%|█████████▋| 1231/1271 [06:36<00:12,  3.11it/s, training_loss=0.338]\u001b[A\n",
            "Epoch 2:  97%|█████████▋| 1232/1271 [06:36<00:12,  3.10it/s, training_loss=0.338]\u001b[A\n",
            "Epoch 2:  97%|█████████▋| 1232/1271 [06:36<00:12,  3.10it/s, training_loss=0.446]\u001b[A\n",
            "Epoch 2:  97%|█████████▋| 1233/1271 [06:36<00:12,  3.11it/s, training_loss=0.446]\u001b[A\n",
            "Epoch 2:  97%|█████████▋| 1233/1271 [06:37<00:12,  3.11it/s, training_loss=0.311]\u001b[A\n",
            "Epoch 2:  97%|█████████▋| 1234/1271 [06:37<00:11,  3.12it/s, training_loss=0.311]\u001b[A\n",
            "Epoch 2:  97%|█████████▋| 1234/1271 [06:37<00:11,  3.12it/s, training_loss=0.280]\u001b[A\n",
            "Epoch 2:  97%|█████████▋| 1235/1271 [06:37<00:11,  3.11it/s, training_loss=0.280]\u001b[A\n",
            "Epoch 2:  97%|█████████▋| 1235/1271 [06:37<00:11,  3.11it/s, training_loss=0.130]\u001b[A\n",
            "Epoch 2:  97%|█████████▋| 1236/1271 [06:37<00:11,  3.12it/s, training_loss=0.130]\u001b[A\n",
            "Epoch 2:  97%|█████████▋| 1236/1271 [06:38<00:11,  3.12it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 2:  97%|█████████▋| 1237/1271 [06:38<00:10,  3.11it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 2:  97%|█████████▋| 1237/1271 [06:38<00:10,  3.11it/s, training_loss=0.120]\u001b[A\n",
            "Epoch 2:  97%|█████████▋| 1238/1271 [06:38<00:10,  3.11it/s, training_loss=0.120]\u001b[A\n",
            "Epoch 2:  97%|█████████▋| 1238/1271 [06:38<00:10,  3.11it/s, training_loss=0.161]\u001b[A\n",
            "Epoch 2:  97%|█████████▋| 1239/1271 [06:38<00:10,  3.11it/s, training_loss=0.161]\u001b[A\n",
            "Epoch 2:  97%|█████████▋| 1239/1271 [06:39<00:10,  3.11it/s, training_loss=0.471]\u001b[A\n",
            "Epoch 2:  98%|█████████▊| 1240/1271 [06:39<00:10,  3.09it/s, training_loss=0.471]\u001b[A\n",
            "Epoch 2:  98%|█████████▊| 1240/1271 [06:39<00:10,  3.09it/s, training_loss=0.126]\u001b[A\n",
            "Epoch 2:  98%|█████████▊| 1241/1271 [06:39<00:09,  3.11it/s, training_loss=0.126]\u001b[A\n",
            "Epoch 2:  98%|█████████▊| 1241/1271 [06:39<00:09,  3.11it/s, training_loss=0.178]\u001b[A\n",
            "Epoch 2:  98%|█████████▊| 1242/1271 [06:39<00:09,  3.09it/s, training_loss=0.178]\u001b[A\n",
            "Epoch 2:  98%|█████████▊| 1242/1271 [06:40<00:09,  3.09it/s, training_loss=0.110]\u001b[A\n",
            "Epoch 2:  98%|█████████▊| 1243/1271 [06:40<00:09,  3.09it/s, training_loss=0.110]\u001b[A\n",
            "Epoch 2:  98%|█████████▊| 1243/1271 [06:40<00:09,  3.09it/s, training_loss=0.575]\u001b[A\n",
            "Epoch 2:  98%|█████████▊| 1244/1271 [06:40<00:08,  3.10it/s, training_loss=0.575]\u001b[A\n",
            "Epoch 2:  98%|█████████▊| 1244/1271 [06:40<00:08,  3.10it/s, training_loss=0.234]\u001b[A\n",
            "Epoch 2:  98%|█████████▊| 1245/1271 [06:40<00:08,  3.12it/s, training_loss=0.234]\u001b[A\n",
            "Epoch 2:  98%|█████████▊| 1245/1271 [06:41<00:08,  3.12it/s, training_loss=0.981]\u001b[A\n",
            "Epoch 2:  98%|█████████▊| 1246/1271 [06:41<00:08,  3.12it/s, training_loss=0.981]\u001b[A\n",
            "Epoch 2:  98%|█████████▊| 1246/1271 [06:41<00:08,  3.12it/s, training_loss=0.148]\u001b[A\n",
            "Epoch 2:  98%|█████████▊| 1247/1271 [06:41<00:07,  3.13it/s, training_loss=0.148]\u001b[A\n",
            "Epoch 2:  98%|█████████▊| 1247/1271 [06:41<00:07,  3.13it/s, training_loss=0.446]\u001b[A\n",
            "Epoch 2:  98%|█████████▊| 1248/1271 [06:41<00:07,  3.12it/s, training_loss=0.446]\u001b[A\n",
            "Epoch 2:  98%|█████████▊| 1248/1271 [06:42<00:07,  3.12it/s, training_loss=0.330]\u001b[A\n",
            "Epoch 2:  98%|█████████▊| 1249/1271 [06:42<00:07,  3.10it/s, training_loss=0.330]\u001b[A\n",
            "Epoch 2:  98%|█████████▊| 1249/1271 [06:42<00:07,  3.10it/s, training_loss=0.279]\u001b[A\n",
            "Epoch 2:  98%|█████████▊| 1250/1271 [06:42<00:06,  3.09it/s, training_loss=0.279]\u001b[A\n",
            "Epoch 2:  98%|█████████▊| 1250/1271 [06:42<00:06,  3.09it/s, training_loss=0.455]\u001b[A\n",
            "Epoch 2:  98%|█████████▊| 1251/1271 [06:42<00:06,  3.08it/s, training_loss=0.455]\u001b[A\n",
            "Epoch 2:  98%|█████████▊| 1251/1271 [06:43<00:06,  3.08it/s, training_loss=0.480]\u001b[A\n",
            "Epoch 2:  99%|█████████▊| 1252/1271 [06:43<00:06,  3.09it/s, training_loss=0.480]\u001b[A\n",
            "Epoch 2:  99%|█████████▊| 1252/1271 [06:43<00:06,  3.09it/s, training_loss=0.447]\u001b[A\n",
            "Epoch 2:  99%|█████████▊| 1253/1271 [06:43<00:05,  3.11it/s, training_loss=0.447]\u001b[A\n",
            "Epoch 2:  99%|█████████▊| 1253/1271 [06:43<00:05,  3.11it/s, training_loss=0.149]\u001b[A\n",
            "Epoch 2:  99%|█████████▊| 1254/1271 [06:43<00:05,  3.13it/s, training_loss=0.149]\u001b[A\n",
            "Epoch 2:  99%|█████████▊| 1254/1271 [06:43<00:05,  3.13it/s, training_loss=0.376]\u001b[A\n",
            "Epoch 2:  99%|█████████▊| 1255/1271 [06:43<00:05,  3.13it/s, training_loss=0.376]\u001b[A\n",
            "Epoch 2:  99%|█████████▊| 1255/1271 [06:44<00:05,  3.13it/s, training_loss=0.041]\u001b[A\n",
            "Epoch 2:  99%|█████████▉| 1256/1271 [06:44<00:04,  3.14it/s, training_loss=0.041]\u001b[A\n",
            "Epoch 2:  99%|█████████▉| 1256/1271 [06:44<00:04,  3.14it/s, training_loss=0.238]\u001b[A\n",
            "Epoch 2:  99%|█████████▉| 1257/1271 [06:44<00:04,  3.14it/s, training_loss=0.238]\u001b[A\n",
            "Epoch 2:  99%|█████████▉| 1257/1271 [06:44<00:04,  3.14it/s, training_loss=0.337]\u001b[A\n",
            "Epoch 2:  99%|█████████▉| 1258/1271 [06:44<00:04,  3.14it/s, training_loss=0.337]\u001b[A\n",
            "Epoch 2:  99%|█████████▉| 1258/1271 [06:45<00:04,  3.14it/s, training_loss=0.133]\u001b[A\n",
            "Epoch 2:  99%|█████████▉| 1259/1271 [06:45<00:03,  3.10it/s, training_loss=0.133]\u001b[A\n",
            "Epoch 2:  99%|█████████▉| 1259/1271 [06:45<00:03,  3.10it/s, training_loss=0.826]\u001b[A\n",
            "Epoch 2:  99%|█████████▉| 1260/1271 [06:45<00:03,  3.09it/s, training_loss=0.826]\u001b[A\n",
            "Epoch 2:  99%|█████████▉| 1260/1271 [06:45<00:03,  3.09it/s, training_loss=0.376]\u001b[A\n",
            "Epoch 2:  99%|█████████▉| 1261/1271 [06:45<00:03,  3.11it/s, training_loss=0.376]\u001b[A\n",
            "Epoch 2:  99%|█████████▉| 1261/1271 [06:46<00:03,  3.11it/s, training_loss=0.277]\u001b[A\n",
            "Epoch 2:  99%|█████████▉| 1262/1271 [06:46<00:02,  3.11it/s, training_loss=0.277]\u001b[A\n",
            "Epoch 2:  99%|█████████▉| 1262/1271 [06:46<00:02,  3.11it/s, training_loss=0.401]\u001b[A\n",
            "Epoch 2:  99%|█████████▉| 1263/1271 [06:46<00:02,  3.11it/s, training_loss=0.401]\u001b[A\n",
            "Epoch 2:  99%|█████████▉| 1263/1271 [06:46<00:02,  3.11it/s, training_loss=0.494]\u001b[A\n",
            "Epoch 2:  99%|█████████▉| 1264/1271 [06:46<00:02,  3.11it/s, training_loss=0.494]\u001b[A\n",
            "Epoch 2:  99%|█████████▉| 1264/1271 [06:47<00:02,  3.11it/s, training_loss=0.108]\u001b[A\n",
            "Epoch 2: 100%|█████████▉| 1265/1271 [06:47<00:01,  3.12it/s, training_loss=0.108]\u001b[A\n",
            "Epoch 2: 100%|█████████▉| 1265/1271 [06:47<00:01,  3.12it/s, training_loss=0.428]\u001b[A\n",
            "Epoch 2: 100%|█████████▉| 1266/1271 [06:47<00:01,  3.09it/s, training_loss=0.428]\u001b[A\n",
            "Epoch 2: 100%|█████████▉| 1266/1271 [06:47<00:01,  3.09it/s, training_loss=0.446]\u001b[A\n",
            "Epoch 2: 100%|█████████▉| 1267/1271 [06:47<00:01,  3.10it/s, training_loss=0.446]\u001b[A\n",
            "Epoch 2: 100%|█████████▉| 1267/1271 [06:48<00:01,  3.10it/s, training_loss=0.514]\u001b[A\n",
            "Epoch 2: 100%|█████████▉| 1268/1271 [06:48<00:00,  3.10it/s, training_loss=0.514]\u001b[A\n",
            "Epoch 2: 100%|█████████▉| 1268/1271 [06:48<00:00,  3.10it/s, training_loss=0.210]\u001b[A\n",
            "Epoch 2: 100%|█████████▉| 1269/1271 [06:48<00:00,  3.10it/s, training_loss=0.210]\u001b[A\n",
            "Epoch 2: 100%|█████████▉| 1269/1271 [06:48<00:00,  3.10it/s, training_loss=0.058]\u001b[A\n",
            "Epoch 2: 100%|█████████▉| 1270/1271 [06:48<00:00,  3.10it/s, training_loss=0.058]\u001b[A\n",
            "Epoch 2: 100%|█████████▉| 1270/1271 [06:48<00:00,  3.10it/s, training_loss=0.506]\u001b[A\n",
            "Epoch 2: 100%|██████████| 1271/1271 [06:48<00:00,  3.71it/s, training_loss=0.506]\u001b[A\n",
            " 20%|██        | 1/5 [14:03<28:30, 427.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2\n",
            "Training loss: 1.255568378100818\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 2/5 [14:24<21:38, 432.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 1.045277183175915\n",
            "F1 Score (Weighted): 0.6863383068126755\n",
            "Recall@5: 0.962852897473997\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3:   0%|          | 0/1271 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3:   0%|          | 0/1271 [00:00<?, ?it/s, training_loss=0.460]\u001b[A\n",
            "Epoch 3:   0%|          | 1/1271 [00:00<06:41,  3.17it/s, training_loss=0.460]\u001b[A\n",
            "Epoch 3:   0%|          | 1/1271 [00:00<06:41,  3.17it/s, training_loss=0.732]\u001b[A\n",
            "Epoch 3:   0%|          | 2/1271 [00:00<06:42,  3.15it/s, training_loss=0.732]\u001b[A\n",
            "Epoch 3:   0%|          | 2/1271 [00:00<06:42,  3.15it/s, training_loss=0.355]\u001b[A\n",
            "Epoch 3:   0%|          | 3/1271 [00:00<06:47,  3.11it/s, training_loss=0.355]\u001b[A\n",
            "Epoch 3:   0%|          | 3/1271 [00:01<06:47,  3.11it/s, training_loss=0.446]\u001b[A\n",
            "Epoch 3:   0%|          | 4/1271 [00:01<06:49,  3.09it/s, training_loss=0.446]\u001b[A\n",
            "Epoch 3:   0%|          | 4/1271 [00:01<06:49,  3.09it/s, training_loss=0.254]\u001b[A\n",
            "Epoch 3:   0%|          | 5/1271 [00:01<06:48,  3.10it/s, training_loss=0.254]\u001b[A\n",
            "Epoch 3:   0%|          | 5/1271 [00:01<06:48,  3.10it/s, training_loss=0.114]\u001b[A\n",
            "Epoch 3:   0%|          | 6/1271 [00:01<06:48,  3.10it/s, training_loss=0.114]\u001b[A\n",
            "Epoch 3:   0%|          | 6/1271 [00:02<06:48,  3.10it/s, training_loss=0.023]\u001b[A\n",
            "Epoch 3:   1%|          | 7/1271 [00:02<06:49,  3.09it/s, training_loss=0.023]\u001b[A\n",
            "Epoch 3:   1%|          | 7/1271 [00:02<06:49,  3.09it/s, training_loss=0.244]\u001b[A\n",
            "Epoch 3:   1%|          | 8/1271 [00:02<06:48,  3.09it/s, training_loss=0.244]\u001b[A\n",
            "Epoch 3:   1%|          | 8/1271 [00:02<06:48,  3.09it/s, training_loss=0.126]\u001b[A\n",
            "Epoch 3:   1%|          | 9/1271 [00:02<06:46,  3.11it/s, training_loss=0.126]\u001b[A\n",
            "Epoch 3:   1%|          | 9/1271 [00:03<06:46,  3.11it/s, training_loss=0.429]\u001b[A\n",
            "Epoch 3:   1%|          | 10/1271 [00:03<06:47,  3.09it/s, training_loss=0.429]\u001b[A\n",
            "Epoch 3:   1%|          | 10/1271 [00:03<06:47,  3.09it/s, training_loss=0.217]\u001b[A\n",
            "Epoch 3:   1%|          | 11/1271 [00:03<06:48,  3.08it/s, training_loss=0.217]\u001b[A\n",
            "Epoch 3:   1%|          | 11/1271 [00:03<06:48,  3.08it/s, training_loss=0.017]\u001b[A\n",
            "Epoch 3:   1%|          | 12/1271 [00:03<06:47,  3.09it/s, training_loss=0.017]\u001b[A\n",
            "Epoch 3:   1%|          | 12/1271 [00:04<06:47,  3.09it/s, training_loss=0.451]\u001b[A\n",
            "Epoch 3:   1%|          | 13/1271 [00:04<06:49,  3.07it/s, training_loss=0.451]\u001b[A\n",
            "Epoch 3:   1%|          | 13/1271 [00:04<06:49,  3.07it/s, training_loss=0.305]\u001b[A\n",
            "Epoch 3:   1%|          | 14/1271 [00:04<06:47,  3.09it/s, training_loss=0.305]\u001b[A\n",
            "Epoch 3:   1%|          | 14/1271 [00:04<06:47,  3.09it/s, training_loss=0.365]\u001b[A\n",
            "Epoch 3:   1%|          | 15/1271 [00:04<06:47,  3.08it/s, training_loss=0.365]\u001b[A\n",
            "Epoch 3:   1%|          | 15/1271 [00:05<06:47,  3.08it/s, training_loss=0.325]\u001b[A\n",
            "Epoch 3:   1%|▏         | 16/1271 [00:05<06:49,  3.06it/s, training_loss=0.325]\u001b[A\n",
            "Epoch 3:   1%|▏         | 16/1271 [00:05<06:49,  3.06it/s, training_loss=0.497]\u001b[A\n",
            "Epoch 3:   1%|▏         | 17/1271 [00:05<06:40,  3.13it/s, training_loss=0.497]\u001b[A\n",
            "Epoch 3:   1%|▏         | 17/1271 [00:05<06:40,  3.13it/s, training_loss=0.545]\u001b[A\n",
            "Epoch 3:   1%|▏         | 18/1271 [00:05<06:42,  3.11it/s, training_loss=0.545]\u001b[A\n",
            "Epoch 3:   1%|▏         | 18/1271 [00:06<06:42,  3.11it/s, training_loss=0.636]\u001b[A\n",
            "Epoch 3:   1%|▏         | 19/1271 [00:06<06:43,  3.10it/s, training_loss=0.636]\u001b[A\n",
            "Epoch 3:   1%|▏         | 19/1271 [00:06<06:43,  3.10it/s, training_loss=0.384]\u001b[A\n",
            "Epoch 3:   2%|▏         | 20/1271 [00:06<06:43,  3.10it/s, training_loss=0.384]\u001b[A\n",
            "Epoch 3:   2%|▏         | 20/1271 [00:06<06:43,  3.10it/s, training_loss=0.317]\u001b[A\n",
            "Epoch 3:   2%|▏         | 21/1271 [00:06<06:44,  3.09it/s, training_loss=0.317]\u001b[A\n",
            "Epoch 3:   2%|▏         | 21/1271 [00:07<06:44,  3.09it/s, training_loss=0.439]\u001b[A\n",
            "Epoch 3:   2%|▏         | 22/1271 [00:07<06:44,  3.09it/s, training_loss=0.439]\u001b[A\n",
            "Epoch 3:   2%|▏         | 22/1271 [00:07<06:44,  3.09it/s, training_loss=0.277]\u001b[A\n",
            "Epoch 3:   2%|▏         | 23/1271 [00:07<06:44,  3.08it/s, training_loss=0.277]\u001b[A\n",
            "Epoch 3:   2%|▏         | 23/1271 [00:07<06:44,  3.08it/s, training_loss=0.738]\u001b[A\n",
            "Epoch 3:   2%|▏         | 24/1271 [00:07<06:44,  3.09it/s, training_loss=0.738]\u001b[A\n",
            "Epoch 3:   2%|▏         | 24/1271 [00:08<06:44,  3.09it/s, training_loss=0.295]\u001b[A\n",
            "Epoch 3:   2%|▏         | 25/1271 [00:08<06:49,  3.04it/s, training_loss=0.295]\u001b[A\n",
            "Epoch 3:   2%|▏         | 25/1271 [00:08<06:49,  3.04it/s, training_loss=0.034]\u001b[A\n",
            "Epoch 3:   2%|▏         | 26/1271 [00:08<06:45,  3.07it/s, training_loss=0.034]\u001b[A\n",
            "Epoch 3:   2%|▏         | 26/1271 [00:08<06:45,  3.07it/s, training_loss=0.648]\u001b[A\n",
            "Epoch 3:   2%|▏         | 27/1271 [00:08<06:44,  3.07it/s, training_loss=0.648]\u001b[A\n",
            "Epoch 3:   2%|▏         | 27/1271 [00:09<06:44,  3.07it/s, training_loss=0.114]\u001b[A\n",
            "Epoch 3:   2%|▏         | 28/1271 [00:09<06:42,  3.09it/s, training_loss=0.114]\u001b[A\n",
            "Epoch 3:   2%|▏         | 28/1271 [00:09<06:42,  3.09it/s, training_loss=0.191]\u001b[A\n",
            "Epoch 3:   2%|▏         | 29/1271 [00:09<06:43,  3.07it/s, training_loss=0.191]\u001b[A\n",
            "Epoch 3:   2%|▏         | 29/1271 [00:09<06:43,  3.07it/s, training_loss=0.146]\u001b[A\n",
            "Epoch 3:   2%|▏         | 30/1271 [00:09<06:42,  3.08it/s, training_loss=0.146]\u001b[A\n",
            "Epoch 3:   2%|▏         | 30/1271 [00:10<06:42,  3.08it/s, training_loss=0.462]\u001b[A\n",
            "Epoch 3:   2%|▏         | 31/1271 [00:10<06:42,  3.08it/s, training_loss=0.462]\u001b[A\n",
            "Epoch 3:   2%|▏         | 31/1271 [00:10<06:42,  3.08it/s, training_loss=0.679]\u001b[A\n",
            "Epoch 3:   3%|▎         | 32/1271 [00:10<06:41,  3.09it/s, training_loss=0.679]\u001b[A\n",
            "Epoch 3:   3%|▎         | 32/1271 [00:10<06:41,  3.09it/s, training_loss=0.338]\u001b[A\n",
            "Epoch 3:   3%|▎         | 33/1271 [00:10<06:42,  3.07it/s, training_loss=0.338]\u001b[A\n",
            "Epoch 3:   3%|▎         | 33/1271 [00:11<06:42,  3.07it/s, training_loss=0.496]\u001b[A\n",
            "Epoch 3:   3%|▎         | 34/1271 [00:11<06:42,  3.07it/s, training_loss=0.496]\u001b[A\n",
            "Epoch 3:   3%|▎         | 34/1271 [00:11<06:42,  3.07it/s, training_loss=0.101]\u001b[A\n",
            "Epoch 3:   3%|▎         | 35/1271 [00:11<06:44,  3.06it/s, training_loss=0.101]\u001b[A\n",
            "Epoch 3:   3%|▎         | 35/1271 [00:11<06:44,  3.06it/s, training_loss=0.252]\u001b[A\n",
            "Epoch 3:   3%|▎         | 36/1271 [00:11<06:45,  3.04it/s, training_loss=0.252]\u001b[A\n",
            "Epoch 3:   3%|▎         | 36/1271 [00:11<06:45,  3.04it/s, training_loss=0.222]\u001b[A\n",
            "Epoch 3:   3%|▎         | 37/1271 [00:12<06:45,  3.04it/s, training_loss=0.222]\u001b[A\n",
            "Epoch 3:   3%|▎         | 37/1271 [00:12<06:45,  3.04it/s, training_loss=0.690]\u001b[A\n",
            "Epoch 3:   3%|▎         | 38/1271 [00:12<06:46,  3.03it/s, training_loss=0.690]\u001b[A\n",
            "Epoch 3:   3%|▎         | 38/1271 [00:12<06:46,  3.03it/s, training_loss=0.125]\u001b[A\n",
            "Epoch 3:   3%|▎         | 39/1271 [00:12<06:46,  3.03it/s, training_loss=0.125]\u001b[A\n",
            "Epoch 3:   3%|▎         | 39/1271 [00:12<06:46,  3.03it/s, training_loss=0.436]\u001b[A\n",
            "Epoch 3:   3%|▎         | 40/1271 [00:12<06:44,  3.04it/s, training_loss=0.436]\u001b[A\n",
            "Epoch 3:   3%|▎         | 40/1271 [00:13<06:44,  3.04it/s, training_loss=0.361]\u001b[A\n",
            "Epoch 3:   3%|▎         | 41/1271 [00:13<06:45,  3.03it/s, training_loss=0.361]\u001b[A\n",
            "Epoch 3:   3%|▎         | 41/1271 [00:13<06:45,  3.03it/s, training_loss=0.506]\u001b[A\n",
            "Epoch 3:   3%|▎         | 42/1271 [00:13<06:43,  3.04it/s, training_loss=0.506]\u001b[A\n",
            "Epoch 3:   3%|▎         | 42/1271 [00:13<06:43,  3.04it/s, training_loss=0.125]\u001b[A\n",
            "Epoch 3:   3%|▎         | 43/1271 [00:13<06:43,  3.04it/s, training_loss=0.125]\u001b[A\n",
            "Epoch 3:   3%|▎         | 43/1271 [00:14<06:43,  3.04it/s, training_loss=0.192]\u001b[A\n",
            "Epoch 3:   3%|▎         | 44/1271 [00:14<06:41,  3.05it/s, training_loss=0.192]\u001b[A\n",
            "Epoch 3:   3%|▎         | 44/1271 [00:14<06:41,  3.05it/s, training_loss=0.154]\u001b[A\n",
            "Epoch 3:   4%|▎         | 45/1271 [00:14<06:39,  3.07it/s, training_loss=0.154]\u001b[A\n",
            "Epoch 3:   4%|▎         | 45/1271 [00:14<06:39,  3.07it/s, training_loss=0.572]\u001b[A\n",
            "Epoch 3:   4%|▎         | 46/1271 [00:14<06:38,  3.07it/s, training_loss=0.572]\u001b[A\n",
            "Epoch 3:   4%|▎         | 46/1271 [00:15<06:38,  3.07it/s, training_loss=0.160]\u001b[A\n",
            "Epoch 3:   4%|▎         | 47/1271 [00:15<06:37,  3.08it/s, training_loss=0.160]\u001b[A\n",
            "Epoch 3:   4%|▎         | 47/1271 [00:15<06:37,  3.08it/s, training_loss=0.364]\u001b[A\n",
            "Epoch 3:   4%|▍         | 48/1271 [00:15<06:38,  3.07it/s, training_loss=0.364]\u001b[A\n",
            "Epoch 3:   4%|▍         | 48/1271 [00:15<06:38,  3.07it/s, training_loss=0.320]\u001b[A\n",
            "Epoch 3:   4%|▍         | 49/1271 [00:15<06:36,  3.08it/s, training_loss=0.320]\u001b[A\n",
            "Epoch 3:   4%|▍         | 49/1271 [00:16<06:36,  3.08it/s, training_loss=0.213]\u001b[A\n",
            "Epoch 3:   4%|▍         | 50/1271 [00:16<06:35,  3.08it/s, training_loss=0.213]\u001b[A\n",
            "Epoch 3:   4%|▍         | 50/1271 [00:16<06:35,  3.08it/s, training_loss=0.375]\u001b[A\n",
            "Epoch 3:   4%|▍         | 51/1271 [00:16<06:34,  3.09it/s, training_loss=0.375]\u001b[A\n",
            "Epoch 3:   4%|▍         | 51/1271 [00:16<06:34,  3.09it/s, training_loss=0.267]\u001b[A\n",
            "Epoch 3:   4%|▍         | 52/1271 [00:16<06:33,  3.10it/s, training_loss=0.267]\u001b[A\n",
            "Epoch 3:   4%|▍         | 52/1271 [00:17<06:33,  3.10it/s, training_loss=0.147]\u001b[A\n",
            "Epoch 3:   4%|▍         | 53/1271 [00:17<06:33,  3.10it/s, training_loss=0.147]\u001b[A\n",
            "Epoch 3:   4%|▍         | 53/1271 [00:17<06:33,  3.10it/s, training_loss=0.392]\u001b[A\n",
            "Epoch 3:   4%|▍         | 54/1271 [00:17<06:32,  3.10it/s, training_loss=0.392]\u001b[A\n",
            "Epoch 3:   4%|▍         | 54/1271 [00:17<06:32,  3.10it/s, training_loss=0.229]\u001b[A\n",
            "Epoch 3:   4%|▍         | 55/1271 [00:17<06:31,  3.10it/s, training_loss=0.229]\u001b[A\n",
            "Epoch 3:   4%|▍         | 55/1271 [00:18<06:31,  3.10it/s, training_loss=0.291]\u001b[A\n",
            "Epoch 3:   4%|▍         | 56/1271 [00:18<06:30,  3.11it/s, training_loss=0.291]\u001b[A\n",
            "Epoch 3:   4%|▍         | 56/1271 [00:18<06:30,  3.11it/s, training_loss=0.332]\u001b[A\n",
            "Epoch 3:   4%|▍         | 57/1271 [00:18<06:31,  3.10it/s, training_loss=0.332]\u001b[A\n",
            "Epoch 3:   4%|▍         | 57/1271 [00:18<06:31,  3.10it/s, training_loss=0.171]\u001b[A\n",
            "Epoch 3:   5%|▍         | 58/1271 [00:18<06:32,  3.09it/s, training_loss=0.171]\u001b[A\n",
            "Epoch 3:   5%|▍         | 58/1271 [00:19<06:32,  3.09it/s, training_loss=0.340]\u001b[A\n",
            "Epoch 3:   5%|▍         | 59/1271 [00:19<06:31,  3.09it/s, training_loss=0.340]\u001b[A\n",
            "Epoch 3:   5%|▍         | 59/1271 [00:19<06:31,  3.09it/s, training_loss=0.214]\u001b[A\n",
            "Epoch 3:   5%|▍         | 60/1271 [00:19<06:32,  3.08it/s, training_loss=0.214]\u001b[A\n",
            "Epoch 3:   5%|▍         | 60/1271 [00:19<06:32,  3.08it/s, training_loss=0.614]\u001b[A\n",
            "Epoch 3:   5%|▍         | 61/1271 [00:19<06:31,  3.09it/s, training_loss=0.614]\u001b[A\n",
            "Epoch 3:   5%|▍         | 61/1271 [00:20<06:31,  3.09it/s, training_loss=0.135]\u001b[A\n",
            "Epoch 3:   5%|▍         | 62/1271 [00:20<06:30,  3.10it/s, training_loss=0.135]\u001b[A\n",
            "Epoch 3:   5%|▍         | 62/1271 [00:20<06:30,  3.10it/s, training_loss=0.329]\u001b[A\n",
            "Epoch 3:   5%|▍         | 63/1271 [00:20<06:29,  3.10it/s, training_loss=0.329]\u001b[A\n",
            "Epoch 3:   5%|▍         | 63/1271 [00:20<06:29,  3.10it/s, training_loss=0.374]\u001b[A\n",
            "Epoch 3:   5%|▌         | 64/1271 [00:20<06:28,  3.11it/s, training_loss=0.374]\u001b[A\n",
            "Epoch 3:   5%|▌         | 64/1271 [00:21<06:28,  3.11it/s, training_loss=0.255]\u001b[A\n",
            "Epoch 3:   5%|▌         | 65/1271 [00:21<06:28,  3.11it/s, training_loss=0.255]\u001b[A\n",
            "Epoch 3:   5%|▌         | 65/1271 [00:21<06:28,  3.11it/s, training_loss=0.025]\u001b[A\n",
            "Epoch 3:   5%|▌         | 66/1271 [00:21<06:26,  3.11it/s, training_loss=0.025]\u001b[A\n",
            "Epoch 3:   5%|▌         | 66/1271 [00:21<06:26,  3.11it/s, training_loss=0.203]\u001b[A\n",
            "Epoch 3:   5%|▌         | 67/1271 [00:21<06:27,  3.11it/s, training_loss=0.203]\u001b[A\n",
            "Epoch 3:   5%|▌         | 67/1271 [00:22<06:27,  3.11it/s, training_loss=0.330]\u001b[A\n",
            "Epoch 3:   5%|▌         | 68/1271 [00:22<06:25,  3.12it/s, training_loss=0.330]\u001b[A\n",
            "Epoch 3:   5%|▌         | 68/1271 [00:22<06:25,  3.12it/s, training_loss=0.450]\u001b[A\n",
            "Epoch 3:   5%|▌         | 69/1271 [00:22<06:25,  3.12it/s, training_loss=0.450]\u001b[A\n",
            "Epoch 3:   5%|▌         | 69/1271 [00:22<06:25,  3.12it/s, training_loss=0.166]\u001b[A\n",
            "Epoch 3:   6%|▌         | 70/1271 [00:22<06:25,  3.11it/s, training_loss=0.166]\u001b[A\n",
            "Epoch 3:   6%|▌         | 70/1271 [00:23<06:25,  3.11it/s, training_loss=0.681]\u001b[A\n",
            "Epoch 3:   6%|▌         | 71/1271 [00:23<06:26,  3.11it/s, training_loss=0.681]\u001b[A\n",
            "Epoch 3:   6%|▌         | 71/1271 [00:23<06:26,  3.11it/s, training_loss=0.202]\u001b[A\n",
            "Epoch 3:   6%|▌         | 72/1271 [00:23<06:27,  3.10it/s, training_loss=0.202]\u001b[A\n",
            "Epoch 3:   6%|▌         | 72/1271 [00:23<06:27,  3.10it/s, training_loss=0.659]\u001b[A\n",
            "Epoch 3:   6%|▌         | 73/1271 [00:23<06:26,  3.10it/s, training_loss=0.659]\u001b[A\n",
            "Epoch 3:   6%|▌         | 73/1271 [00:23<06:26,  3.10it/s, training_loss=0.026]\u001b[A\n",
            "Epoch 3:   6%|▌         | 74/1271 [00:23<06:26,  3.10it/s, training_loss=0.026]\u001b[A\n",
            "Epoch 3:   6%|▌         | 74/1271 [00:24<06:26,  3.10it/s, training_loss=0.301]\u001b[A\n",
            "Epoch 3:   6%|▌         | 75/1271 [00:24<06:26,  3.10it/s, training_loss=0.301]\u001b[A\n",
            "Epoch 3:   6%|▌         | 75/1271 [00:24<06:26,  3.10it/s, training_loss=0.410]\u001b[A\n",
            "Epoch 3:   6%|▌         | 76/1271 [00:24<06:24,  3.10it/s, training_loss=0.410]\u001b[A\n",
            "Epoch 3:   6%|▌         | 76/1271 [00:24<06:24,  3.10it/s, training_loss=0.442]\u001b[A\n",
            "Epoch 3:   6%|▌         | 77/1271 [00:24<06:26,  3.09it/s, training_loss=0.442]\u001b[A\n",
            "Epoch 3:   6%|▌         | 77/1271 [00:25<06:26,  3.09it/s, training_loss=0.425]\u001b[A\n",
            "Epoch 3:   6%|▌         | 78/1271 [00:25<06:27,  3.08it/s, training_loss=0.425]\u001b[A\n",
            "Epoch 3:   6%|▌         | 78/1271 [00:25<06:27,  3.08it/s, training_loss=0.348]\u001b[A\n",
            "Epoch 3:   6%|▌         | 79/1271 [00:25<06:27,  3.08it/s, training_loss=0.348]\u001b[A\n",
            "Epoch 3:   6%|▌         | 79/1271 [00:25<06:27,  3.08it/s, training_loss=0.602]\u001b[A\n",
            "Epoch 3:   6%|▋         | 80/1271 [00:25<06:26,  3.08it/s, training_loss=0.602]\u001b[A\n",
            "Epoch 3:   6%|▋         | 80/1271 [00:26<06:26,  3.08it/s, training_loss=0.112]\u001b[A\n",
            "Epoch 3:   6%|▋         | 81/1271 [00:26<06:26,  3.08it/s, training_loss=0.112]\u001b[A\n",
            "Epoch 3:   6%|▋         | 81/1271 [00:26<06:26,  3.08it/s, training_loss=0.284]\u001b[A\n",
            "Epoch 3:   6%|▋         | 82/1271 [00:26<06:24,  3.09it/s, training_loss=0.284]\u001b[A\n",
            "Epoch 3:   6%|▋         | 82/1271 [00:26<06:24,  3.09it/s, training_loss=0.350]\u001b[A\n",
            "Epoch 3:   7%|▋         | 83/1271 [00:26<06:22,  3.11it/s, training_loss=0.350]\u001b[A\n",
            "Epoch 3:   7%|▋         | 83/1271 [00:27<06:22,  3.11it/s, training_loss=0.663]\u001b[A\n",
            "Epoch 3:   7%|▋         | 84/1271 [00:27<06:24,  3.09it/s, training_loss=0.663]\u001b[A\n",
            "Epoch 3:   7%|▋         | 84/1271 [00:27<06:24,  3.09it/s, training_loss=0.286]\u001b[A\n",
            "Epoch 3:   7%|▋         | 85/1271 [00:27<06:22,  3.10it/s, training_loss=0.286]\u001b[A\n",
            "Epoch 3:   7%|▋         | 85/1271 [00:27<06:22,  3.10it/s, training_loss=0.431]\u001b[A\n",
            "Epoch 3:   7%|▋         | 86/1271 [00:27<06:21,  3.11it/s, training_loss=0.431]\u001b[A\n",
            "Epoch 3:   7%|▋         | 86/1271 [00:28<06:21,  3.11it/s, training_loss=0.566]\u001b[A\n",
            "Epoch 3:   7%|▋         | 87/1271 [00:28<06:22,  3.10it/s, training_loss=0.566]\u001b[A\n",
            "Epoch 3:   7%|▋         | 87/1271 [00:28<06:22,  3.10it/s, training_loss=0.536]\u001b[A\n",
            "Epoch 3:   7%|▋         | 88/1271 [00:28<06:23,  3.09it/s, training_loss=0.536]\u001b[A\n",
            "Epoch 3:   7%|▋         | 88/1271 [00:28<06:23,  3.09it/s, training_loss=0.291]\u001b[A\n",
            "Epoch 3:   7%|▋         | 89/1271 [00:28<06:22,  3.09it/s, training_loss=0.291]\u001b[A\n",
            "Epoch 3:   7%|▋         | 89/1271 [00:29<06:22,  3.09it/s, training_loss=0.304]\u001b[A\n",
            "Epoch 3:   7%|▋         | 90/1271 [00:29<06:20,  3.10it/s, training_loss=0.304]\u001b[A\n",
            "Epoch 3:   7%|▋         | 90/1271 [00:29<06:20,  3.10it/s, training_loss=0.011]\u001b[A\n",
            "Epoch 3:   7%|▋         | 91/1271 [00:29<06:20,  3.10it/s, training_loss=0.011]\u001b[A\n",
            "Epoch 3:   7%|▋         | 91/1271 [00:29<06:20,  3.10it/s, training_loss=0.168]\u001b[A\n",
            "Epoch 3:   7%|▋         | 92/1271 [00:29<06:18,  3.11it/s, training_loss=0.168]\u001b[A\n",
            "Epoch 3:   7%|▋         | 92/1271 [00:30<06:18,  3.11it/s, training_loss=0.812]\u001b[A\n",
            "Epoch 3:   7%|▋         | 93/1271 [00:30<06:19,  3.10it/s, training_loss=0.812]\u001b[A\n",
            "Epoch 3:   7%|▋         | 93/1271 [00:30<06:19,  3.10it/s, training_loss=0.584]\u001b[A\n",
            "Epoch 3:   7%|▋         | 94/1271 [00:30<06:20,  3.09it/s, training_loss=0.584]\u001b[A\n",
            "Epoch 3:   7%|▋         | 94/1271 [00:30<06:20,  3.09it/s, training_loss=0.019]\u001b[A\n",
            "Epoch 3:   7%|▋         | 95/1271 [00:30<06:22,  3.08it/s, training_loss=0.019]\u001b[A\n",
            "Epoch 3:   7%|▋         | 95/1271 [00:31<06:22,  3.08it/s, training_loss=0.529]\u001b[A\n",
            "Epoch 3:   8%|▊         | 96/1271 [00:31<06:20,  3.09it/s, training_loss=0.529]\u001b[A\n",
            "Epoch 3:   8%|▊         | 96/1271 [00:31<06:20,  3.09it/s, training_loss=0.445]\u001b[A\n",
            "Epoch 3:   8%|▊         | 97/1271 [00:31<06:19,  3.10it/s, training_loss=0.445]\u001b[A\n",
            "Epoch 3:   8%|▊         | 97/1271 [00:31<06:19,  3.10it/s, training_loss=0.120]\u001b[A\n",
            "Epoch 3:   8%|▊         | 98/1271 [00:31<06:17,  3.11it/s, training_loss=0.120]\u001b[A\n",
            "Epoch 3:   8%|▊         | 98/1271 [00:32<06:17,  3.11it/s, training_loss=0.157]\u001b[A\n",
            "Epoch 3:   8%|▊         | 99/1271 [00:32<06:15,  3.12it/s, training_loss=0.157]\u001b[A\n",
            "Epoch 3:   8%|▊         | 99/1271 [00:32<06:15,  3.12it/s, training_loss=0.172]\u001b[A\n",
            "Epoch 3:   8%|▊         | 100/1271 [00:32<06:14,  3.13it/s, training_loss=0.172]\u001b[A\n",
            "Epoch 3:   8%|▊         | 100/1271 [00:32<06:14,  3.13it/s, training_loss=0.159]\u001b[A\n",
            "Epoch 3:   8%|▊         | 101/1271 [00:32<06:14,  3.13it/s, training_loss=0.159]\u001b[A\n",
            "Epoch 3:   8%|▊         | 101/1271 [00:33<06:14,  3.13it/s, training_loss=0.606]\u001b[A\n",
            "Epoch 3:   8%|▊         | 102/1271 [00:33<06:12,  3.14it/s, training_loss=0.606]\u001b[A\n",
            "Epoch 3:   8%|▊         | 102/1271 [00:33<06:12,  3.14it/s, training_loss=0.043]\u001b[A\n",
            "Epoch 3:   8%|▊         | 103/1271 [00:33<06:13,  3.13it/s, training_loss=0.043]\u001b[A\n",
            "Epoch 3:   8%|▊         | 103/1271 [00:33<06:13,  3.13it/s, training_loss=0.202]\u001b[A\n",
            "Epoch 3:   8%|▊         | 104/1271 [00:33<06:16,  3.10it/s, training_loss=0.202]\u001b[A\n",
            "Epoch 3:   8%|▊         | 104/1271 [00:33<06:16,  3.10it/s, training_loss=0.047]\u001b[A\n",
            "Epoch 3:   8%|▊         | 105/1271 [00:33<06:16,  3.10it/s, training_loss=0.047]\u001b[A\n",
            "Epoch 3:   8%|▊         | 105/1271 [00:34<06:16,  3.10it/s, training_loss=0.557]\u001b[A\n",
            "Epoch 3:   8%|▊         | 106/1271 [00:34<06:18,  3.08it/s, training_loss=0.557]\u001b[A\n",
            "Epoch 3:   8%|▊         | 106/1271 [00:34<06:18,  3.08it/s, training_loss=0.475]\u001b[A\n",
            "Epoch 3:   8%|▊         | 107/1271 [00:34<06:16,  3.09it/s, training_loss=0.475]\u001b[A\n",
            "Epoch 3:   8%|▊         | 107/1271 [00:34<06:16,  3.09it/s, training_loss=0.091]\u001b[A\n",
            "Epoch 3:   8%|▊         | 108/1271 [00:34<06:15,  3.10it/s, training_loss=0.091]\u001b[A\n",
            "Epoch 3:   8%|▊         | 108/1271 [00:35<06:15,  3.10it/s, training_loss=0.213]\u001b[A\n",
            "Epoch 3:   9%|▊         | 109/1271 [00:35<06:15,  3.10it/s, training_loss=0.213]\u001b[A\n",
            "Epoch 3:   9%|▊         | 109/1271 [00:35<06:15,  3.10it/s, training_loss=0.077]\u001b[A\n",
            "Epoch 3:   9%|▊         | 110/1271 [00:35<06:13,  3.11it/s, training_loss=0.077]\u001b[A\n",
            "Epoch 3:   9%|▊         | 110/1271 [00:35<06:13,  3.11it/s, training_loss=0.048]\u001b[A\n",
            "Epoch 3:   9%|▊         | 111/1271 [00:35<06:12,  3.12it/s, training_loss=0.048]\u001b[A\n",
            "Epoch 3:   9%|▊         | 111/1271 [00:36<06:12,  3.12it/s, training_loss=0.542]\u001b[A\n",
            "Epoch 3:   9%|▉         | 112/1271 [00:36<06:13,  3.11it/s, training_loss=0.542]\u001b[A\n",
            "Epoch 3:   9%|▉         | 112/1271 [00:36<06:13,  3.11it/s, training_loss=0.224]\u001b[A\n",
            "Epoch 3:   9%|▉         | 113/1271 [00:36<06:13,  3.10it/s, training_loss=0.224]\u001b[A\n",
            "Epoch 3:   9%|▉         | 113/1271 [00:36<06:13,  3.10it/s, training_loss=0.019]\u001b[A\n",
            "Epoch 3:   9%|▉         | 114/1271 [00:36<06:10,  3.12it/s, training_loss=0.019]\u001b[A\n",
            "Epoch 3:   9%|▉         | 114/1271 [00:37<06:10,  3.12it/s, training_loss=0.291]\u001b[A\n",
            "Epoch 3:   9%|▉         | 115/1271 [00:37<06:10,  3.12it/s, training_loss=0.291]\u001b[A\n",
            "Epoch 3:   9%|▉         | 115/1271 [00:37<06:10,  3.12it/s, training_loss=0.911]\u001b[A\n",
            "Epoch 3:   9%|▉         | 116/1271 [00:37<06:08,  3.14it/s, training_loss=0.911]\u001b[A\n",
            "Epoch 3:   9%|▉         | 116/1271 [00:37<06:08,  3.14it/s, training_loss=0.800]\u001b[A\n",
            "Epoch 3:   9%|▉         | 117/1271 [00:37<06:09,  3.12it/s, training_loss=0.800]\u001b[A\n",
            "Epoch 3:   9%|▉         | 117/1271 [00:38<06:09,  3.12it/s, training_loss=0.204]\u001b[A\n",
            "Epoch 3:   9%|▉         | 118/1271 [00:38<06:11,  3.10it/s, training_loss=0.204]\u001b[A\n",
            "Epoch 3:   9%|▉         | 118/1271 [00:38<06:11,  3.10it/s, training_loss=0.764]\u001b[A\n",
            "Epoch 3:   9%|▉         | 119/1271 [00:38<06:11,  3.10it/s, training_loss=0.764]\u001b[A\n",
            "Epoch 3:   9%|▉         | 119/1271 [00:38<06:11,  3.10it/s, training_loss=0.158]\u001b[A\n",
            "Epoch 3:   9%|▉         | 120/1271 [00:38<06:10,  3.11it/s, training_loss=0.158]\u001b[A\n",
            "Epoch 3:   9%|▉         | 120/1271 [00:39<06:10,  3.11it/s, training_loss=0.534]\u001b[A\n",
            "Epoch 3:  10%|▉         | 121/1271 [00:39<06:09,  3.11it/s, training_loss=0.534]\u001b[A\n",
            "Epoch 3:  10%|▉         | 121/1271 [00:39<06:09,  3.11it/s, training_loss=0.195]\u001b[A\n",
            "Epoch 3:  10%|▉         | 122/1271 [00:39<06:08,  3.12it/s, training_loss=0.195]\u001b[A\n",
            "Epoch 3:  10%|▉         | 122/1271 [00:39<06:08,  3.12it/s, training_loss=0.435]\u001b[A\n",
            "Epoch 3:  10%|▉         | 123/1271 [00:39<06:10,  3.10it/s, training_loss=0.435]\u001b[A\n",
            "Epoch 3:  10%|▉         | 123/1271 [00:40<06:10,  3.10it/s, training_loss=0.642]\u001b[A\n",
            "Epoch 3:  10%|▉         | 124/1271 [00:40<06:10,  3.10it/s, training_loss=0.642]\u001b[A\n",
            "Epoch 3:  10%|▉         | 124/1271 [00:40<06:10,  3.10it/s, training_loss=0.166]\u001b[A\n",
            "Epoch 3:  10%|▉         | 125/1271 [00:40<06:07,  3.12it/s, training_loss=0.166]\u001b[A\n",
            "Epoch 3:  10%|▉         | 125/1271 [00:40<06:07,  3.12it/s, training_loss=0.356]\u001b[A\n",
            "Epoch 3:  10%|▉         | 126/1271 [00:40<06:07,  3.12it/s, training_loss=0.356]\u001b[A\n",
            "Epoch 3:  10%|▉         | 126/1271 [00:41<06:07,  3.12it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 3:  10%|▉         | 127/1271 [00:41<06:05,  3.13it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 3:  10%|▉         | 127/1271 [00:41<06:05,  3.13it/s, training_loss=0.488]\u001b[A\n",
            "Epoch 3:  10%|█         | 128/1271 [00:41<06:02,  3.15it/s, training_loss=0.488]\u001b[A\n",
            "Epoch 3:  10%|█         | 128/1271 [00:41<06:02,  3.15it/s, training_loss=0.523]\u001b[A\n",
            "Epoch 3:  10%|█         | 129/1271 [00:41<06:01,  3.16it/s, training_loss=0.523]\u001b[A\n",
            "Epoch 3:  10%|█         | 129/1271 [00:41<06:01,  3.16it/s, training_loss=0.213]\u001b[A\n",
            "Epoch 3:  10%|█         | 130/1271 [00:41<06:01,  3.16it/s, training_loss=0.213]\u001b[A\n",
            "Epoch 3:  10%|█         | 130/1271 [00:42<06:01,  3.16it/s, training_loss=0.244]\u001b[A\n",
            "Epoch 3:  10%|█         | 131/1271 [00:42<06:01,  3.15it/s, training_loss=0.244]\u001b[A\n",
            "Epoch 3:  10%|█         | 131/1271 [00:42<06:01,  3.15it/s, training_loss=0.570]\u001b[A\n",
            "Epoch 3:  10%|█         | 132/1271 [00:42<06:02,  3.14it/s, training_loss=0.570]\u001b[A\n",
            "Epoch 3:  10%|█         | 132/1271 [00:42<06:02,  3.14it/s, training_loss=0.032]\u001b[A\n",
            "Epoch 3:  10%|█         | 133/1271 [00:42<06:04,  3.12it/s, training_loss=0.032]\u001b[A\n",
            "Epoch 3:  10%|█         | 133/1271 [00:43<06:04,  3.12it/s, training_loss=0.186]\u001b[A\n",
            "Epoch 3:  11%|█         | 134/1271 [00:43<06:04,  3.12it/s, training_loss=0.186]\u001b[A\n",
            "Epoch 3:  11%|█         | 134/1271 [00:43<06:04,  3.12it/s, training_loss=0.234]\u001b[A\n",
            "Epoch 3:  11%|█         | 135/1271 [00:43<06:02,  3.13it/s, training_loss=0.234]\u001b[A\n",
            "Epoch 3:  11%|█         | 135/1271 [00:43<06:02,  3.13it/s, training_loss=0.159]\u001b[A\n",
            "Epoch 3:  11%|█         | 136/1271 [00:43<06:01,  3.14it/s, training_loss=0.159]\u001b[A\n",
            "Epoch 3:  11%|█         | 136/1271 [00:44<06:01,  3.14it/s, training_loss=0.453]\u001b[A\n",
            "Epoch 3:  11%|█         | 137/1271 [00:44<06:01,  3.14it/s, training_loss=0.453]\u001b[A\n",
            "Epoch 3:  11%|█         | 137/1271 [00:44<06:01,  3.14it/s, training_loss=0.440]\u001b[A\n",
            "Epoch 3:  11%|█         | 138/1271 [00:44<06:01,  3.13it/s, training_loss=0.440]\u001b[A\n",
            "Epoch 3:  11%|█         | 138/1271 [00:44<06:01,  3.13it/s, training_loss=0.186]\u001b[A\n",
            "Epoch 3:  11%|█         | 139/1271 [00:44<06:02,  3.12it/s, training_loss=0.186]\u001b[A\n",
            "Epoch 3:  11%|█         | 139/1271 [00:45<06:02,  3.12it/s, training_loss=0.321]\u001b[A\n",
            "Epoch 3:  11%|█         | 140/1271 [00:45<06:01,  3.13it/s, training_loss=0.321]\u001b[A\n",
            "Epoch 3:  11%|█         | 140/1271 [00:45<06:01,  3.13it/s, training_loss=0.203]\u001b[A\n",
            "Epoch 3:  11%|█         | 141/1271 [00:45<06:00,  3.13it/s, training_loss=0.203]\u001b[A\n",
            "Epoch 3:  11%|█         | 141/1271 [00:45<06:00,  3.13it/s, training_loss=0.414]\u001b[A\n",
            "Epoch 3:  11%|█         | 142/1271 [00:45<06:00,  3.14it/s, training_loss=0.414]\u001b[A\n",
            "Epoch 3:  11%|█         | 142/1271 [00:46<06:00,  3.14it/s, training_loss=0.590]\u001b[A\n",
            "Epoch 3:  11%|█▏        | 143/1271 [00:46<06:01,  3.12it/s, training_loss=0.590]\u001b[A\n",
            "Epoch 3:  11%|█▏        | 143/1271 [00:46<06:01,  3.12it/s, training_loss=0.215]\u001b[A\n",
            "Epoch 3:  11%|█▏        | 144/1271 [00:46<06:01,  3.12it/s, training_loss=0.215]\u001b[A\n",
            "Epoch 3:  11%|█▏        | 144/1271 [00:46<06:01,  3.12it/s, training_loss=0.437]\u001b[A\n",
            "Epoch 3:  11%|█▏        | 145/1271 [00:46<06:00,  3.12it/s, training_loss=0.437]\u001b[A\n",
            "Epoch 3:  11%|█▏        | 145/1271 [00:47<06:00,  3.12it/s, training_loss=0.114]\u001b[A\n",
            "Epoch 3:  11%|█▏        | 146/1271 [00:47<05:59,  3.13it/s, training_loss=0.114]\u001b[A\n",
            "Epoch 3:  11%|█▏        | 146/1271 [00:47<05:59,  3.13it/s, training_loss=0.311]\u001b[A\n",
            "Epoch 3:  12%|█▏        | 147/1271 [00:47<05:59,  3.13it/s, training_loss=0.311]\u001b[A\n",
            "Epoch 3:  12%|█▏        | 147/1271 [00:47<05:59,  3.13it/s, training_loss=0.206]\u001b[A\n",
            "Epoch 3:  12%|█▏        | 148/1271 [00:47<05:58,  3.13it/s, training_loss=0.206]\u001b[A\n",
            "Epoch 3:  12%|█▏        | 148/1271 [00:48<05:58,  3.13it/s, training_loss=0.322]\u001b[A\n",
            "Epoch 3:  12%|█▏        | 149/1271 [00:48<05:57,  3.14it/s, training_loss=0.322]\u001b[A\n",
            "Epoch 3:  12%|█▏        | 149/1271 [00:48<05:57,  3.14it/s, training_loss=0.321]\u001b[A\n",
            "Epoch 3:  12%|█▏        | 150/1271 [00:48<05:56,  3.15it/s, training_loss=0.321]\u001b[A\n",
            "Epoch 3:  12%|█▏        | 150/1271 [00:48<05:56,  3.15it/s, training_loss=0.458]\u001b[A\n",
            "Epoch 3:  12%|█▏        | 151/1271 [00:48<05:57,  3.13it/s, training_loss=0.458]\u001b[A\n",
            "Epoch 3:  12%|█▏        | 151/1271 [00:49<05:57,  3.13it/s, training_loss=0.164]\u001b[A\n",
            "Epoch 3:  12%|█▏        | 152/1271 [00:49<05:58,  3.12it/s, training_loss=0.164]\u001b[A\n",
            "Epoch 3:  12%|█▏        | 152/1271 [00:49<05:58,  3.12it/s, training_loss=0.574]\u001b[A\n",
            "Epoch 3:  12%|█▏        | 153/1271 [00:49<05:57,  3.13it/s, training_loss=0.574]\u001b[A\n",
            "Epoch 3:  12%|█▏        | 153/1271 [00:49<05:57,  3.13it/s, training_loss=0.296]\u001b[A\n",
            "Epoch 3:  12%|█▏        | 154/1271 [00:49<05:55,  3.14it/s, training_loss=0.296]\u001b[A\n",
            "Epoch 3:  12%|█▏        | 154/1271 [00:49<05:55,  3.14it/s, training_loss=0.063]\u001b[A\n",
            "Epoch 3:  12%|█▏        | 155/1271 [00:49<05:57,  3.12it/s, training_loss=0.063]\u001b[A\n",
            "Epoch 3:  12%|█▏        | 155/1271 [00:50<05:57,  3.12it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 3:  12%|█▏        | 156/1271 [00:50<05:55,  3.13it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 3:  12%|█▏        | 156/1271 [00:50<05:55,  3.13it/s, training_loss=0.431]\u001b[A\n",
            "Epoch 3:  12%|█▏        | 157/1271 [00:50<05:55,  3.14it/s, training_loss=0.431]\u001b[A\n",
            "Epoch 3:  12%|█▏        | 157/1271 [00:50<05:55,  3.14it/s, training_loss=0.018]\u001b[A\n",
            "Epoch 3:  12%|█▏        | 158/1271 [00:50<05:53,  3.15it/s, training_loss=0.018]\u001b[A\n",
            "Epoch 3:  12%|█▏        | 158/1271 [00:51<05:53,  3.15it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 3:  13%|█▎        | 159/1271 [00:51<05:52,  3.15it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 3:  13%|█▎        | 159/1271 [00:51<05:52,  3.15it/s, training_loss=0.182]\u001b[A\n",
            "Epoch 3:  13%|█▎        | 160/1271 [00:51<05:52,  3.15it/s, training_loss=0.182]\u001b[A\n",
            "Epoch 3:  13%|█▎        | 160/1271 [00:51<05:52,  3.15it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 3:  13%|█▎        | 161/1271 [00:51<05:54,  3.13it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 3:  13%|█▎        | 161/1271 [00:52<05:54,  3.13it/s, training_loss=0.427]\u001b[A\n",
            "Epoch 3:  13%|█▎        | 162/1271 [00:52<05:55,  3.12it/s, training_loss=0.427]\u001b[A\n",
            "Epoch 3:  13%|█▎        | 162/1271 [00:52<05:55,  3.12it/s, training_loss=0.148]\u001b[A\n",
            "Epoch 3:  13%|█▎        | 163/1271 [00:52<05:58,  3.09it/s, training_loss=0.148]\u001b[A\n",
            "Epoch 3:  13%|█▎        | 163/1271 [00:52<05:58,  3.09it/s, training_loss=0.400]\u001b[A\n",
            "Epoch 3:  13%|█▎        | 164/1271 [00:52<05:57,  3.09it/s, training_loss=0.400]\u001b[A\n",
            "Epoch 3:  13%|█▎        | 164/1271 [00:53<05:57,  3.09it/s, training_loss=0.165]\u001b[A\n",
            "Epoch 3:  13%|█▎        | 165/1271 [00:53<05:56,  3.10it/s, training_loss=0.165]\u001b[A\n",
            "Epoch 3:  13%|█▎        | 165/1271 [00:53<05:56,  3.10it/s, training_loss=0.287]\u001b[A\n",
            "Epoch 3:  13%|█▎        | 166/1271 [00:53<05:53,  3.13it/s, training_loss=0.287]\u001b[A\n",
            "Epoch 3:  13%|█▎        | 166/1271 [00:53<05:53,  3.13it/s, training_loss=0.833]\u001b[A\n",
            "Epoch 3:  13%|█▎        | 167/1271 [00:53<05:53,  3.12it/s, training_loss=0.833]\u001b[A\n",
            "Epoch 3:  13%|█▎        | 167/1271 [00:54<05:53,  3.12it/s, training_loss=0.700]\u001b[A\n",
            "Epoch 3:  13%|█▎        | 168/1271 [00:54<05:53,  3.12it/s, training_loss=0.700]\u001b[A\n",
            "Epoch 3:  13%|█▎        | 168/1271 [00:54<05:53,  3.12it/s, training_loss=0.273]\u001b[A\n",
            "Epoch 3:  13%|█▎        | 169/1271 [00:54<05:54,  3.11it/s, training_loss=0.273]\u001b[A\n",
            "Epoch 3:  13%|█▎        | 169/1271 [00:54<05:54,  3.11it/s, training_loss=0.402]\u001b[A\n",
            "Epoch 3:  13%|█▎        | 170/1271 [00:54<05:53,  3.11it/s, training_loss=0.402]\u001b[A\n",
            "Epoch 3:  13%|█▎        | 170/1271 [00:55<05:53,  3.11it/s, training_loss=0.818]\u001b[A\n",
            "Epoch 3:  13%|█▎        | 171/1271 [00:55<05:52,  3.12it/s, training_loss=0.818]\u001b[A\n",
            "Epoch 3:  13%|█▎        | 171/1271 [00:55<05:52,  3.12it/s, training_loss=0.081]\u001b[A\n",
            "Epoch 3:  14%|█▎        | 172/1271 [00:55<05:51,  3.13it/s, training_loss=0.081]\u001b[A\n",
            "Epoch 3:  14%|█▎        | 172/1271 [00:55<05:51,  3.13it/s, training_loss=0.192]\u001b[A\n",
            "Epoch 3:  14%|█▎        | 173/1271 [00:55<05:50,  3.13it/s, training_loss=0.192]\u001b[A\n",
            "Epoch 3:  14%|█▎        | 173/1271 [00:56<05:50,  3.13it/s, training_loss=0.172]\u001b[A\n",
            "Epoch 3:  14%|█▎        | 174/1271 [00:56<05:49,  3.14it/s, training_loss=0.172]\u001b[A\n",
            "Epoch 3:  14%|█▎        | 174/1271 [00:56<05:49,  3.14it/s, training_loss=0.143]\u001b[A\n",
            "Epoch 3:  14%|█▍        | 175/1271 [00:56<05:47,  3.15it/s, training_loss=0.143]\u001b[A\n",
            "Epoch 3:  14%|█▍        | 175/1271 [00:56<05:47,  3.15it/s, training_loss=0.846]\u001b[A\n",
            "Epoch 3:  14%|█▍        | 176/1271 [00:56<05:48,  3.14it/s, training_loss=0.846]\u001b[A\n",
            "Epoch 3:  14%|█▍        | 176/1271 [00:57<05:48,  3.14it/s, training_loss=0.295]\u001b[A\n",
            "Epoch 3:  14%|█▍        | 177/1271 [00:57<05:47,  3.15it/s, training_loss=0.295]\u001b[A\n",
            "Epoch 3:  14%|█▍        | 177/1271 [00:57<05:47,  3.15it/s, training_loss=0.383]\u001b[A\n",
            "Epoch 3:  14%|█▍        | 178/1271 [00:57<05:47,  3.15it/s, training_loss=0.383]\u001b[A\n",
            "Epoch 3:  14%|█▍        | 178/1271 [00:57<05:47,  3.15it/s, training_loss=0.724]\u001b[A\n",
            "Epoch 3:  14%|█▍        | 179/1271 [00:57<05:45,  3.16it/s, training_loss=0.724]\u001b[A\n",
            "Epoch 3:  14%|█▍        | 179/1271 [00:57<05:45,  3.16it/s, training_loss=0.452]\u001b[A\n",
            "Epoch 3:  14%|█▍        | 180/1271 [00:57<05:46,  3.14it/s, training_loss=0.452]\u001b[A\n",
            "Epoch 3:  14%|█▍        | 180/1271 [00:58<05:46,  3.14it/s, training_loss=0.376]\u001b[A\n",
            "Epoch 3:  14%|█▍        | 181/1271 [00:58<05:49,  3.12it/s, training_loss=0.376]\u001b[A\n",
            "Epoch 3:  14%|█▍        | 181/1271 [00:58<05:49,  3.12it/s, training_loss=0.301]\u001b[A\n",
            "Epoch 3:  14%|█▍        | 182/1271 [00:58<05:47,  3.14it/s, training_loss=0.301]\u001b[A\n",
            "Epoch 3:  14%|█▍        | 182/1271 [00:58<05:47,  3.14it/s, training_loss=0.284]\u001b[A\n",
            "Epoch 3:  14%|█▍        | 183/1271 [00:58<05:47,  3.13it/s, training_loss=0.284]\u001b[A\n",
            "Epoch 3:  14%|█▍        | 183/1271 [00:59<05:47,  3.13it/s, training_loss=0.751]\u001b[A\n",
            "Epoch 3:  14%|█▍        | 184/1271 [00:59<05:48,  3.12it/s, training_loss=0.751]\u001b[A\n",
            "Epoch 3:  14%|█▍        | 184/1271 [00:59<05:48,  3.12it/s, training_loss=0.356]\u001b[A\n",
            "Epoch 3:  15%|█▍        | 185/1271 [00:59<05:48,  3.12it/s, training_loss=0.356]\u001b[A\n",
            "Epoch 3:  15%|█▍        | 185/1271 [00:59<05:48,  3.12it/s, training_loss=0.382]\u001b[A\n",
            "Epoch 3:  15%|█▍        | 186/1271 [00:59<05:47,  3.12it/s, training_loss=0.382]\u001b[A\n",
            "Epoch 3:  15%|█▍        | 186/1271 [01:00<05:47,  3.12it/s, training_loss=0.596]\u001b[A\n",
            "Epoch 3:  15%|█▍        | 187/1271 [01:00<05:49,  3.10it/s, training_loss=0.596]\u001b[A\n",
            "Epoch 3:  15%|█▍        | 187/1271 [01:00<05:49,  3.10it/s, training_loss=0.323]\u001b[A\n",
            "Epoch 3:  15%|█▍        | 188/1271 [01:00<05:50,  3.09it/s, training_loss=0.323]\u001b[A\n",
            "Epoch 3:  15%|█▍        | 188/1271 [01:00<05:50,  3.09it/s, training_loss=0.522]\u001b[A\n",
            "Epoch 3:  15%|█▍        | 189/1271 [01:00<05:50,  3.09it/s, training_loss=0.522]\u001b[A\n",
            "Epoch 3:  15%|█▍        | 189/1271 [01:01<05:50,  3.09it/s, training_loss=0.159]\u001b[A\n",
            "Epoch 3:  15%|█▍        | 190/1271 [01:01<05:48,  3.10it/s, training_loss=0.159]\u001b[A\n",
            "Epoch 3:  15%|█▍        | 190/1271 [01:01<05:48,  3.10it/s, training_loss=0.245]\u001b[A\n",
            "Epoch 3:  15%|█▌        | 191/1271 [01:01<05:46,  3.12it/s, training_loss=0.245]\u001b[A\n",
            "Epoch 3:  15%|█▌        | 191/1271 [01:01<05:46,  3.12it/s, training_loss=0.295]\u001b[A\n",
            "Epoch 3:  15%|█▌        | 192/1271 [01:01<05:44,  3.13it/s, training_loss=0.295]\u001b[A\n",
            "Epoch 3:  15%|█▌        | 192/1271 [01:02<05:44,  3.13it/s, training_loss=0.475]\u001b[A\n",
            "Epoch 3:  15%|█▌        | 193/1271 [01:02<05:42,  3.15it/s, training_loss=0.475]\u001b[A\n",
            "Epoch 3:  15%|█▌        | 193/1271 [01:02<05:42,  3.15it/s, training_loss=0.502]\u001b[A\n",
            "Epoch 3:  15%|█▌        | 194/1271 [01:02<05:42,  3.14it/s, training_loss=0.502]\u001b[A\n",
            "Epoch 3:  15%|█▌        | 194/1271 [01:02<05:42,  3.14it/s, training_loss=0.209]\u001b[A\n",
            "Epoch 3:  15%|█▌        | 195/1271 [01:02<05:43,  3.13it/s, training_loss=0.209]\u001b[A\n",
            "Epoch 3:  15%|█▌        | 195/1271 [01:03<05:43,  3.13it/s, training_loss=0.676]\u001b[A\n",
            "Epoch 3:  15%|█▌        | 196/1271 [01:03<05:42,  3.14it/s, training_loss=0.676]\u001b[A\n",
            "Epoch 3:  15%|█▌        | 196/1271 [01:03<05:42,  3.14it/s, training_loss=0.391]\u001b[A\n",
            "Epoch 3:  15%|█▌        | 197/1271 [01:03<05:44,  3.12it/s, training_loss=0.391]\u001b[A\n",
            "Epoch 3:  15%|█▌        | 197/1271 [01:03<05:44,  3.12it/s, training_loss=0.232]\u001b[A\n",
            "Epoch 3:  16%|█▌        | 198/1271 [01:03<05:43,  3.13it/s, training_loss=0.232]\u001b[A\n",
            "Epoch 3:  16%|█▌        | 198/1271 [01:04<05:43,  3.13it/s, training_loss=0.222]\u001b[A\n",
            "Epoch 3:  16%|█▌        | 199/1271 [01:04<05:42,  3.13it/s, training_loss=0.222]\u001b[A\n",
            "Epoch 3:  16%|█▌        | 199/1271 [01:04<05:42,  3.13it/s, training_loss=0.705]\u001b[A\n",
            "Epoch 3:  16%|█▌        | 200/1271 [01:04<05:40,  3.14it/s, training_loss=0.705]\u001b[A\n",
            "Epoch 3:  16%|█▌        | 200/1271 [01:04<05:40,  3.14it/s, training_loss=0.257]\u001b[A\n",
            "Epoch 3:  16%|█▌        | 201/1271 [01:04<05:40,  3.14it/s, training_loss=0.257]\u001b[A\n",
            "Epoch 3:  16%|█▌        | 201/1271 [01:05<05:40,  3.14it/s, training_loss=0.655]\u001b[A\n",
            "Epoch 3:  16%|█▌        | 202/1271 [01:05<05:39,  3.15it/s, training_loss=0.655]\u001b[A\n",
            "Epoch 3:  16%|█▌        | 202/1271 [01:05<05:39,  3.15it/s, training_loss=0.429]\u001b[A\n",
            "Epoch 3:  16%|█▌        | 203/1271 [01:05<05:43,  3.11it/s, training_loss=0.429]\u001b[A\n",
            "Epoch 3:  16%|█▌        | 203/1271 [01:05<05:43,  3.11it/s, training_loss=0.281]\u001b[A\n",
            "Epoch 3:  16%|█▌        | 204/1271 [01:05<05:42,  3.11it/s, training_loss=0.281]\u001b[A\n",
            "Epoch 3:  16%|█▌        | 204/1271 [01:05<05:42,  3.11it/s, training_loss=0.761]\u001b[A\n",
            "Epoch 3:  16%|█▌        | 205/1271 [01:05<05:42,  3.11it/s, training_loss=0.761]\u001b[A\n",
            "Epoch 3:  16%|█▌        | 205/1271 [01:06<05:42,  3.11it/s, training_loss=0.269]\u001b[A\n",
            "Epoch 3:  16%|█▌        | 206/1271 [01:06<05:42,  3.11it/s, training_loss=0.269]\u001b[A\n",
            "Epoch 3:  16%|█▌        | 206/1271 [01:06<05:42,  3.11it/s, training_loss=0.498]\u001b[A\n",
            "Epoch 3:  16%|█▋        | 207/1271 [01:06<05:40,  3.12it/s, training_loss=0.498]\u001b[A\n",
            "Epoch 3:  16%|█▋        | 207/1271 [01:06<05:40,  3.12it/s, training_loss=0.042]\u001b[A\n",
            "Epoch 3:  16%|█▋        | 208/1271 [01:06<05:39,  3.13it/s, training_loss=0.042]\u001b[A\n",
            "Epoch 3:  16%|█▋        | 208/1271 [01:07<05:39,  3.13it/s, training_loss=0.135]\u001b[A\n",
            "Epoch 3:  16%|█▋        | 209/1271 [01:07<05:41,  3.11it/s, training_loss=0.135]\u001b[A\n",
            "Epoch 3:  16%|█▋        | 209/1271 [01:07<05:41,  3.11it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 3:  17%|█▋        | 210/1271 [01:07<05:41,  3.10it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 3:  17%|█▋        | 210/1271 [01:07<05:41,  3.10it/s, training_loss=0.460]\u001b[A\n",
            "Epoch 3:  17%|█▋        | 211/1271 [01:07<05:42,  3.10it/s, training_loss=0.460]\u001b[A\n",
            "Epoch 3:  17%|█▋        | 211/1271 [01:08<05:42,  3.10it/s, training_loss=0.160]\u001b[A\n",
            "Epoch 3:  17%|█▋        | 212/1271 [01:08<05:41,  3.11it/s, training_loss=0.160]\u001b[A\n",
            "Epoch 3:  17%|█▋        | 212/1271 [01:08<05:41,  3.11it/s, training_loss=0.353]\u001b[A\n",
            "Epoch 3:  17%|█▋        | 213/1271 [01:08<05:42,  3.09it/s, training_loss=0.353]\u001b[A\n",
            "Epoch 3:  17%|█▋        | 213/1271 [01:08<05:42,  3.09it/s, training_loss=0.310]\u001b[A\n",
            "Epoch 3:  17%|█▋        | 214/1271 [01:08<05:40,  3.10it/s, training_loss=0.310]\u001b[A\n",
            "Epoch 3:  17%|█▋        | 214/1271 [01:09<05:40,  3.10it/s, training_loss=0.644]\u001b[A\n",
            "Epoch 3:  17%|█▋        | 215/1271 [01:09<05:40,  3.10it/s, training_loss=0.644]\u001b[A\n",
            "Epoch 3:  17%|█▋        | 215/1271 [01:09<05:40,  3.10it/s, training_loss=0.383]\u001b[A\n",
            "Epoch 3:  17%|█▋        | 216/1271 [01:09<05:39,  3.11it/s, training_loss=0.383]\u001b[A\n",
            "Epoch 3:  17%|█▋        | 216/1271 [01:09<05:39,  3.11it/s, training_loss=0.399]\u001b[A\n",
            "Epoch 3:  17%|█▋        | 217/1271 [01:09<05:39,  3.10it/s, training_loss=0.399]\u001b[A\n",
            "Epoch 3:  17%|█▋        | 217/1271 [01:10<05:39,  3.10it/s, training_loss=0.251]\u001b[A\n",
            "Epoch 3:  17%|█▋        | 218/1271 [01:10<05:38,  3.11it/s, training_loss=0.251]\u001b[A\n",
            "Epoch 3:  17%|█▋        | 218/1271 [01:10<05:38,  3.11it/s, training_loss=0.267]\u001b[A\n",
            "Epoch 3:  17%|█▋        | 219/1271 [01:10<05:37,  3.12it/s, training_loss=0.267]\u001b[A\n",
            "Epoch 3:  17%|█▋        | 219/1271 [01:10<05:37,  3.12it/s, training_loss=0.371]\u001b[A\n",
            "Epoch 3:  17%|█▋        | 220/1271 [01:10<05:36,  3.13it/s, training_loss=0.371]\u001b[A\n",
            "Epoch 3:  17%|█▋        | 220/1271 [01:11<05:36,  3.13it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 3:  17%|█▋        | 221/1271 [01:11<05:34,  3.14it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 3:  17%|█▋        | 221/1271 [01:11<05:34,  3.14it/s, training_loss=0.202]\u001b[A\n",
            "Epoch 3:  17%|█▋        | 222/1271 [01:11<05:33,  3.14it/s, training_loss=0.202]\u001b[A\n",
            "Epoch 3:  17%|█▋        | 222/1271 [01:11<05:33,  3.14it/s, training_loss=0.714]\u001b[A\n",
            "Epoch 3:  18%|█▊        | 223/1271 [01:11<05:32,  3.15it/s, training_loss=0.714]\u001b[A\n",
            "Epoch 3:  18%|█▊        | 223/1271 [01:12<05:32,  3.15it/s, training_loss=0.477]\u001b[A\n",
            "Epoch 3:  18%|█▊        | 224/1271 [01:12<05:32,  3.15it/s, training_loss=0.477]\u001b[A\n",
            "Epoch 3:  18%|█▊        | 224/1271 [01:12<05:32,  3.15it/s, training_loss=0.007]\u001b[A\n",
            "Epoch 3:  18%|█▊        | 225/1271 [01:12<05:32,  3.15it/s, training_loss=0.007]\u001b[A\n",
            "Epoch 3:  18%|█▊        | 225/1271 [01:12<05:32,  3.15it/s, training_loss=0.437]\u001b[A\n",
            "Epoch 3:  18%|█▊        | 226/1271 [01:12<05:32,  3.14it/s, training_loss=0.437]\u001b[A\n",
            "Epoch 3:  18%|█▊        | 226/1271 [01:13<05:32,  3.14it/s, training_loss=0.251]\u001b[A\n",
            "Epoch 3:  18%|█▊        | 227/1271 [01:13<05:34,  3.12it/s, training_loss=0.251]\u001b[A\n",
            "Epoch 3:  18%|█▊        | 227/1271 [01:13<05:34,  3.12it/s, training_loss=0.108]\u001b[A\n",
            "Epoch 3:  18%|█▊        | 228/1271 [01:13<05:35,  3.11it/s, training_loss=0.108]\u001b[A\n",
            "Epoch 3:  18%|█▊        | 228/1271 [01:13<05:35,  3.11it/s, training_loss=0.312]\u001b[A\n",
            "Epoch 3:  18%|█▊        | 229/1271 [01:13<05:37,  3.09it/s, training_loss=0.312]\u001b[A\n",
            "Epoch 3:  18%|█▊        | 229/1271 [01:14<05:37,  3.09it/s, training_loss=0.219]\u001b[A\n",
            "Epoch 3:  18%|█▊        | 230/1271 [01:14<05:36,  3.09it/s, training_loss=0.219]\u001b[A\n",
            "Epoch 3:  18%|█▊        | 230/1271 [01:14<05:36,  3.09it/s, training_loss=0.598]\u001b[A\n",
            "Epoch 3:  18%|█▊        | 231/1271 [01:14<05:36,  3.09it/s, training_loss=0.598]\u001b[A\n",
            "Epoch 3:  18%|█▊        | 231/1271 [01:14<05:36,  3.09it/s, training_loss=0.308]\u001b[A\n",
            "Epoch 3:  18%|█▊        | 232/1271 [01:14<05:36,  3.09it/s, training_loss=0.308]\u001b[A\n",
            "Epoch 3:  18%|█▊        | 232/1271 [01:14<05:36,  3.09it/s, training_loss=0.088]\u001b[A\n",
            "Epoch 3:  18%|█▊        | 233/1271 [01:14<05:34,  3.10it/s, training_loss=0.088]\u001b[A\n",
            "Epoch 3:  18%|█▊        | 233/1271 [01:15<05:34,  3.10it/s, training_loss=0.154]\u001b[A\n",
            "Epoch 3:  18%|█▊        | 234/1271 [01:15<05:33,  3.11it/s, training_loss=0.154]\u001b[A\n",
            "Epoch 3:  18%|█▊        | 234/1271 [01:15<05:33,  3.11it/s, training_loss=0.059]\u001b[A\n",
            "Epoch 3:  18%|█▊        | 235/1271 [01:15<05:32,  3.12it/s, training_loss=0.059]\u001b[A\n",
            "Epoch 3:  18%|█▊        | 235/1271 [01:15<05:32,  3.12it/s, training_loss=0.373]\u001b[A\n",
            "Epoch 3:  19%|█▊        | 236/1271 [01:15<05:32,  3.12it/s, training_loss=0.373]\u001b[A\n",
            "Epoch 3:  19%|█▊        | 236/1271 [01:16<05:32,  3.12it/s, training_loss=0.301]\u001b[A\n",
            "Epoch 3:  19%|█▊        | 237/1271 [01:16<05:32,  3.11it/s, training_loss=0.301]\u001b[A\n",
            "Epoch 3:  19%|█▊        | 237/1271 [01:16<05:32,  3.11it/s, training_loss=0.528]\u001b[A\n",
            "Epoch 3:  19%|█▊        | 238/1271 [01:16<05:33,  3.10it/s, training_loss=0.528]\u001b[A\n",
            "Epoch 3:  19%|█▊        | 238/1271 [01:16<05:33,  3.10it/s, training_loss=0.187]\u001b[A\n",
            "Epoch 3:  19%|█▉        | 239/1271 [01:16<05:32,  3.10it/s, training_loss=0.187]\u001b[A\n",
            "Epoch 3:  19%|█▉        | 239/1271 [01:17<05:32,  3.10it/s, training_loss=0.399]\u001b[A\n",
            "Epoch 3:  19%|█▉        | 240/1271 [01:17<05:34,  3.08it/s, training_loss=0.399]\u001b[A\n",
            "Epoch 3:  19%|█▉        | 240/1271 [01:17<05:34,  3.08it/s, training_loss=0.156]\u001b[A\n",
            "Epoch 3:  19%|█▉        | 241/1271 [01:17<05:32,  3.10it/s, training_loss=0.156]\u001b[A\n",
            "Epoch 3:  19%|█▉        | 241/1271 [01:17<05:32,  3.10it/s, training_loss=0.626]\u001b[A\n",
            "Epoch 3:  19%|█▉        | 242/1271 [01:17<05:32,  3.09it/s, training_loss=0.626]\u001b[A\n",
            "Epoch 3:  19%|█▉        | 242/1271 [01:18<05:32,  3.09it/s, training_loss=0.282]\u001b[A\n",
            "Epoch 3:  19%|█▉        | 243/1271 [01:18<05:31,  3.10it/s, training_loss=0.282]\u001b[A\n",
            "Epoch 3:  19%|█▉        | 243/1271 [01:18<05:31,  3.10it/s, training_loss=0.533]\u001b[A\n",
            "Epoch 3:  19%|█▉        | 244/1271 [01:18<05:30,  3.10it/s, training_loss=0.533]\u001b[A\n",
            "Epoch 3:  19%|█▉        | 244/1271 [01:18<05:30,  3.10it/s, training_loss=0.380]\u001b[A\n",
            "Epoch 3:  19%|█▉        | 245/1271 [01:18<05:32,  3.09it/s, training_loss=0.380]\u001b[A\n",
            "Epoch 3:  19%|█▉        | 245/1271 [01:19<05:32,  3.09it/s, training_loss=0.116]\u001b[A\n",
            "Epoch 3:  19%|█▉        | 246/1271 [01:19<05:31,  3.09it/s, training_loss=0.116]\u001b[A\n",
            "Epoch 3:  19%|█▉        | 246/1271 [01:19<05:31,  3.09it/s, training_loss=0.047]\u001b[A\n",
            "Epoch 3:  19%|█▉        | 247/1271 [01:19<05:33,  3.07it/s, training_loss=0.047]\u001b[A\n",
            "Epoch 3:  19%|█▉        | 247/1271 [01:19<05:33,  3.07it/s, training_loss=0.013]\u001b[A\n",
            "Epoch 3:  20%|█▉        | 248/1271 [01:19<05:32,  3.08it/s, training_loss=0.013]\u001b[A\n",
            "Epoch 3:  20%|█▉        | 248/1271 [01:20<05:32,  3.08it/s, training_loss=0.112]\u001b[A\n",
            "Epoch 3:  20%|█▉        | 249/1271 [01:20<05:29,  3.10it/s, training_loss=0.112]\u001b[A\n",
            "Epoch 3:  20%|█▉        | 249/1271 [01:20<05:29,  3.10it/s, training_loss=0.337]\u001b[A\n",
            "Epoch 3:  20%|█▉        | 250/1271 [01:20<05:29,  3.10it/s, training_loss=0.337]\u001b[A\n",
            "Epoch 3:  20%|█▉        | 250/1271 [01:20<05:29,  3.10it/s, training_loss=0.103]\u001b[A\n",
            "Epoch 3:  20%|█▉        | 251/1271 [01:20<05:28,  3.10it/s, training_loss=0.103]\u001b[A\n",
            "Epoch 3:  20%|█▉        | 251/1271 [01:21<05:28,  3.10it/s, training_loss=0.207]\u001b[A\n",
            "Epoch 3:  20%|█▉        | 252/1271 [01:21<05:25,  3.13it/s, training_loss=0.207]\u001b[A\n",
            "Epoch 3:  20%|█▉        | 252/1271 [01:21<05:25,  3.13it/s, training_loss=0.357]\u001b[A\n",
            "Epoch 3:  20%|█▉        | 253/1271 [01:21<05:27,  3.11it/s, training_loss=0.357]\u001b[A\n",
            "Epoch 3:  20%|█▉        | 253/1271 [01:21<05:27,  3.11it/s, training_loss=0.364]\u001b[A\n",
            "Epoch 3:  20%|█▉        | 254/1271 [01:21<05:28,  3.10it/s, training_loss=0.364]\u001b[A\n",
            "Epoch 3:  20%|█▉        | 254/1271 [01:22<05:28,  3.10it/s, training_loss=0.618]\u001b[A\n",
            "Epoch 3:  20%|██        | 255/1271 [01:22<05:28,  3.09it/s, training_loss=0.618]\u001b[A\n",
            "Epoch 3:  20%|██        | 255/1271 [01:22<05:28,  3.09it/s, training_loss=0.067]\u001b[A\n",
            "Epoch 3:  20%|██        | 256/1271 [01:22<05:26,  3.11it/s, training_loss=0.067]\u001b[A\n",
            "Epoch 3:  20%|██        | 256/1271 [01:22<05:26,  3.11it/s, training_loss=0.264]\u001b[A\n",
            "Epoch 3:  20%|██        | 257/1271 [01:22<05:24,  3.13it/s, training_loss=0.264]\u001b[A\n",
            "Epoch 3:  20%|██        | 257/1271 [01:23<05:24,  3.13it/s, training_loss=0.235]\u001b[A\n",
            "Epoch 3:  20%|██        | 258/1271 [01:23<05:23,  3.13it/s, training_loss=0.235]\u001b[A\n",
            "Epoch 3:  20%|██        | 258/1271 [01:23<05:23,  3.13it/s, training_loss=0.293]\u001b[A\n",
            "Epoch 3:  20%|██        | 259/1271 [01:23<05:22,  3.14it/s, training_loss=0.293]\u001b[A\n",
            "Epoch 3:  20%|██        | 259/1271 [01:23<05:22,  3.14it/s, training_loss=0.040]\u001b[A\n",
            "Epoch 3:  20%|██        | 260/1271 [01:23<05:21,  3.14it/s, training_loss=0.040]\u001b[A\n",
            "Epoch 3:  20%|██        | 260/1271 [01:23<05:21,  3.14it/s, training_loss=0.207]\u001b[A\n",
            "Epoch 3:  21%|██        | 261/1271 [01:23<05:22,  3.13it/s, training_loss=0.207]\u001b[A\n",
            "Epoch 3:  21%|██        | 261/1271 [01:24<05:22,  3.13it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 3:  21%|██        | 262/1271 [01:24<05:20,  3.15it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 3:  21%|██        | 262/1271 [01:24<05:20,  3.15it/s, training_loss=0.251]\u001b[A\n",
            "Epoch 3:  21%|██        | 263/1271 [01:24<05:22,  3.13it/s, training_loss=0.251]\u001b[A\n",
            "Epoch 3:  21%|██        | 263/1271 [01:24<05:22,  3.13it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 3:  21%|██        | 264/1271 [01:24<05:21,  3.13it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 3:  21%|██        | 264/1271 [01:25<05:21,  3.13it/s, training_loss=0.100]\u001b[A\n",
            "Epoch 3:  21%|██        | 265/1271 [01:25<05:22,  3.12it/s, training_loss=0.100]\u001b[A\n",
            "Epoch 3:  21%|██        | 265/1271 [01:25<05:22,  3.12it/s, training_loss=0.248]\u001b[A\n",
            "Epoch 3:  21%|██        | 266/1271 [01:25<05:24,  3.10it/s, training_loss=0.248]\u001b[A\n",
            "Epoch 3:  21%|██        | 266/1271 [01:25<05:24,  3.10it/s, training_loss=0.088]\u001b[A\n",
            "Epoch 3:  21%|██        | 267/1271 [01:25<05:23,  3.10it/s, training_loss=0.088]\u001b[A\n",
            "Epoch 3:  21%|██        | 267/1271 [01:26<05:23,  3.10it/s, training_loss=0.397]\u001b[A\n",
            "Epoch 3:  21%|██        | 268/1271 [01:26<05:25,  3.08it/s, training_loss=0.397]\u001b[A\n",
            "Epoch 3:  21%|██        | 268/1271 [01:26<05:25,  3.08it/s, training_loss=0.593]\u001b[A\n",
            "Epoch 3:  21%|██        | 269/1271 [01:26<05:24,  3.09it/s, training_loss=0.593]\u001b[A\n",
            "Epoch 3:  21%|██        | 269/1271 [01:26<05:24,  3.09it/s, training_loss=0.054]\u001b[A\n",
            "Epoch 3:  21%|██        | 270/1271 [01:26<05:23,  3.09it/s, training_loss=0.054]\u001b[A\n",
            "Epoch 3:  21%|██        | 270/1271 [01:27<05:23,  3.09it/s, training_loss=0.455]\u001b[A\n",
            "Epoch 3:  21%|██▏       | 271/1271 [01:27<05:24,  3.09it/s, training_loss=0.455]\u001b[A\n",
            "Epoch 3:  21%|██▏       | 271/1271 [01:27<05:24,  3.09it/s, training_loss=0.093]\u001b[A\n",
            "Epoch 3:  21%|██▏       | 272/1271 [01:27<05:23,  3.08it/s, training_loss=0.093]\u001b[A\n",
            "Epoch 3:  21%|██▏       | 272/1271 [01:27<05:23,  3.08it/s, training_loss=0.133]\u001b[A\n",
            "Epoch 3:  21%|██▏       | 273/1271 [01:27<05:22,  3.10it/s, training_loss=0.133]\u001b[A\n",
            "Epoch 3:  21%|██▏       | 273/1271 [01:28<05:22,  3.10it/s, training_loss=0.703]\u001b[A\n",
            "Epoch 3:  22%|██▏       | 274/1271 [01:28<05:22,  3.09it/s, training_loss=0.703]\u001b[A\n",
            "Epoch 3:  22%|██▏       | 274/1271 [01:28<05:22,  3.09it/s, training_loss=0.157]\u001b[A\n",
            "Epoch 3:  22%|██▏       | 275/1271 [01:28<05:22,  3.09it/s, training_loss=0.157]\u001b[A\n",
            "Epoch 3:  22%|██▏       | 275/1271 [01:28<05:22,  3.09it/s, training_loss=0.282]\u001b[A\n",
            "Epoch 3:  22%|██▏       | 276/1271 [01:28<05:20,  3.10it/s, training_loss=0.282]\u001b[A\n",
            "Epoch 3:  22%|██▏       | 276/1271 [01:29<05:20,  3.10it/s, training_loss=0.459]\u001b[A\n",
            "Epoch 3:  22%|██▏       | 277/1271 [01:29<05:20,  3.10it/s, training_loss=0.459]\u001b[A\n",
            "Epoch 3:  22%|██▏       | 277/1271 [01:29<05:20,  3.10it/s, training_loss=0.055]\u001b[A\n",
            "Epoch 3:  22%|██▏       | 278/1271 [01:29<05:18,  3.12it/s, training_loss=0.055]\u001b[A\n",
            "Epoch 3:  22%|██▏       | 278/1271 [01:29<05:18,  3.12it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 3:  22%|██▏       | 279/1271 [01:29<05:18,  3.11it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 3:  22%|██▏       | 279/1271 [01:30<05:18,  3.11it/s, training_loss=0.174]\u001b[A\n",
            "Epoch 3:  22%|██▏       | 280/1271 [01:30<05:18,  3.11it/s, training_loss=0.174]\u001b[A\n",
            "Epoch 3:  22%|██▏       | 280/1271 [01:30<05:18,  3.11it/s, training_loss=0.390]\u001b[A\n",
            "Epoch 3:  22%|██▏       | 281/1271 [01:30<05:18,  3.11it/s, training_loss=0.390]\u001b[A\n",
            "Epoch 3:  22%|██▏       | 281/1271 [01:30<05:18,  3.11it/s, training_loss=0.202]\u001b[A\n",
            "Epoch 3:  22%|██▏       | 282/1271 [01:30<05:17,  3.12it/s, training_loss=0.202]\u001b[A\n",
            "Epoch 3:  22%|██▏       | 282/1271 [01:31<05:17,  3.12it/s, training_loss=0.176]\u001b[A\n",
            "Epoch 3:  22%|██▏       | 283/1271 [01:31<05:17,  3.11it/s, training_loss=0.176]\u001b[A\n",
            "Epoch 3:  22%|██▏       | 283/1271 [01:31<05:17,  3.11it/s, training_loss=0.095]\u001b[A\n",
            "Epoch 3:  22%|██▏       | 284/1271 [01:31<05:18,  3.10it/s, training_loss=0.095]\u001b[A\n",
            "Epoch 3:  22%|██▏       | 284/1271 [01:31<05:18,  3.10it/s, training_loss=0.120]\u001b[A\n",
            "Epoch 3:  22%|██▏       | 285/1271 [01:31<05:20,  3.07it/s, training_loss=0.120]\u001b[A\n",
            "Epoch 3:  22%|██▏       | 285/1271 [01:32<05:20,  3.07it/s, training_loss=0.421]\u001b[A\n",
            "Epoch 3:  23%|██▎       | 286/1271 [01:32<05:20,  3.08it/s, training_loss=0.421]\u001b[A\n",
            "Epoch 3:  23%|██▎       | 286/1271 [01:32<05:20,  3.08it/s, training_loss=0.111]\u001b[A\n",
            "Epoch 3:  23%|██▎       | 287/1271 [01:32<05:20,  3.07it/s, training_loss=0.111]\u001b[A\n",
            "Epoch 3:  23%|██▎       | 287/1271 [01:32<05:20,  3.07it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 3:  23%|██▎       | 288/1271 [01:32<05:19,  3.08it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 3:  23%|██▎       | 288/1271 [01:33<05:19,  3.08it/s, training_loss=0.282]\u001b[A\n",
            "Epoch 3:  23%|██▎       | 289/1271 [01:33<05:17,  3.09it/s, training_loss=0.282]\u001b[A\n",
            "Epoch 3:  23%|██▎       | 289/1271 [01:33<05:17,  3.09it/s, training_loss=0.255]\u001b[A\n",
            "Epoch 3:  23%|██▎       | 290/1271 [01:33<05:16,  3.10it/s, training_loss=0.255]\u001b[A\n",
            "Epoch 3:  23%|██▎       | 290/1271 [01:33<05:16,  3.10it/s, training_loss=0.216]\u001b[A\n",
            "Epoch 3:  23%|██▎       | 291/1271 [01:33<05:18,  3.08it/s, training_loss=0.216]\u001b[A\n",
            "Epoch 3:  23%|██▎       | 291/1271 [01:33<05:18,  3.08it/s, training_loss=0.092]\u001b[A\n",
            "Epoch 3:  23%|██▎       | 292/1271 [01:34<05:17,  3.08it/s, training_loss=0.092]\u001b[A\n",
            "Epoch 3:  23%|██▎       | 292/1271 [01:34<05:17,  3.08it/s, training_loss=0.228]\u001b[A\n",
            "Epoch 3:  23%|██▎       | 293/1271 [01:34<05:16,  3.09it/s, training_loss=0.228]\u001b[A\n",
            "Epoch 3:  23%|██▎       | 293/1271 [01:34<05:16,  3.09it/s, training_loss=0.187]\u001b[A\n",
            "Epoch 3:  23%|██▎       | 294/1271 [01:34<05:15,  3.10it/s, training_loss=0.187]\u001b[A\n",
            "Epoch 3:  23%|██▎       | 294/1271 [01:34<05:15,  3.10it/s, training_loss=0.032]\u001b[A\n",
            "Epoch 3:  23%|██▎       | 295/1271 [01:34<05:15,  3.09it/s, training_loss=0.032]\u001b[A\n",
            "Epoch 3:  23%|██▎       | 295/1271 [01:35<05:15,  3.09it/s, training_loss=0.425]\u001b[A\n",
            "Epoch 3:  23%|██▎       | 296/1271 [01:35<05:13,  3.11it/s, training_loss=0.425]\u001b[A\n",
            "Epoch 3:  23%|██▎       | 296/1271 [01:35<05:13,  3.11it/s, training_loss=0.343]\u001b[A\n",
            "Epoch 3:  23%|██▎       | 297/1271 [01:35<05:12,  3.12it/s, training_loss=0.343]\u001b[A\n",
            "Epoch 3:  23%|██▎       | 297/1271 [01:35<05:12,  3.12it/s, training_loss=0.017]\u001b[A\n",
            "Epoch 3:  23%|██▎       | 298/1271 [01:35<05:11,  3.13it/s, training_loss=0.017]\u001b[A\n",
            "Epoch 3:  23%|██▎       | 298/1271 [01:36<05:11,  3.13it/s, training_loss=0.211]\u001b[A\n",
            "Epoch 3:  24%|██▎       | 299/1271 [01:36<05:11,  3.12it/s, training_loss=0.211]\u001b[A\n",
            "Epoch 3:  24%|██▎       | 299/1271 [01:36<05:11,  3.12it/s, training_loss=0.231]\u001b[A\n",
            "Epoch 3:  24%|██▎       | 300/1271 [01:36<05:12,  3.10it/s, training_loss=0.231]\u001b[A\n",
            "Epoch 3:  24%|██▎       | 300/1271 [01:36<05:12,  3.10it/s, training_loss=0.734]\u001b[A\n",
            "Epoch 3:  24%|██▎       | 301/1271 [01:36<05:10,  3.12it/s, training_loss=0.734]\u001b[A\n",
            "Epoch 3:  24%|██▎       | 301/1271 [01:37<05:10,  3.12it/s, training_loss=0.126]\u001b[A\n",
            "Epoch 3:  24%|██▍       | 302/1271 [01:37<05:10,  3.12it/s, training_loss=0.126]\u001b[A\n",
            "Epoch 3:  24%|██▍       | 302/1271 [01:37<05:10,  3.12it/s, training_loss=0.205]\u001b[A\n",
            "Epoch 3:  24%|██▍       | 303/1271 [01:37<05:10,  3.12it/s, training_loss=0.205]\u001b[A\n",
            "Epoch 3:  24%|██▍       | 303/1271 [01:37<05:10,  3.12it/s, training_loss=0.091]\u001b[A\n",
            "Epoch 3:  24%|██▍       | 304/1271 [01:37<05:10,  3.11it/s, training_loss=0.091]\u001b[A\n",
            "Epoch 3:  24%|██▍       | 304/1271 [01:38<05:10,  3.11it/s, training_loss=0.387]\u001b[A\n",
            "Epoch 3:  24%|██▍       | 305/1271 [01:38<05:10,  3.11it/s, training_loss=0.387]\u001b[A\n",
            "Epoch 3:  24%|██▍       | 305/1271 [01:38<05:10,  3.11it/s, training_loss=0.366]\u001b[A\n",
            "Epoch 3:  24%|██▍       | 306/1271 [01:38<05:11,  3.09it/s, training_loss=0.366]\u001b[A\n",
            "Epoch 3:  24%|██▍       | 306/1271 [01:38<05:11,  3.09it/s, training_loss=0.728]\u001b[A\n",
            "Epoch 3:  24%|██▍       | 307/1271 [01:38<05:14,  3.07it/s, training_loss=0.728]\u001b[A\n",
            "Epoch 3:  24%|██▍       | 307/1271 [01:39<05:14,  3.07it/s, training_loss=0.109]\u001b[A\n",
            "Epoch 3:  24%|██▍       | 308/1271 [01:39<05:13,  3.07it/s, training_loss=0.109]\u001b[A\n",
            "Epoch 3:  24%|██▍       | 308/1271 [01:39<05:13,  3.07it/s, training_loss=0.348]\u001b[A\n",
            "Epoch 3:  24%|██▍       | 309/1271 [01:39<05:14,  3.06it/s, training_loss=0.348]\u001b[A\n",
            "Epoch 3:  24%|██▍       | 309/1271 [01:39<05:14,  3.06it/s, training_loss=0.371]\u001b[A\n",
            "Epoch 3:  24%|██▍       | 310/1271 [01:39<05:15,  3.05it/s, training_loss=0.371]\u001b[A\n",
            "Epoch 3:  24%|██▍       | 310/1271 [01:40<05:15,  3.05it/s, training_loss=0.463]\u001b[A\n",
            "Epoch 3:  24%|██▍       | 311/1271 [01:40<05:13,  3.06it/s, training_loss=0.463]\u001b[A\n",
            "Epoch 3:  24%|██▍       | 311/1271 [01:40<05:13,  3.06it/s, training_loss=0.246]\u001b[A\n",
            "Epoch 3:  25%|██▍       | 312/1271 [01:40<05:13,  3.06it/s, training_loss=0.246]\u001b[A\n",
            "Epoch 3:  25%|██▍       | 312/1271 [01:40<05:13,  3.06it/s, training_loss=0.020]\u001b[A\n",
            "Epoch 3:  25%|██▍       | 313/1271 [01:40<05:10,  3.08it/s, training_loss=0.020]\u001b[A\n",
            "Epoch 3:  25%|██▍       | 313/1271 [01:41<05:10,  3.08it/s, training_loss=0.214]\u001b[A\n",
            "Epoch 3:  25%|██▍       | 314/1271 [01:41<05:09,  3.09it/s, training_loss=0.214]\u001b[A\n",
            "Epoch 3:  25%|██▍       | 314/1271 [01:41<05:09,  3.09it/s, training_loss=0.413]\u001b[A\n",
            "Epoch 3:  25%|██▍       | 315/1271 [01:41<05:08,  3.10it/s, training_loss=0.413]\u001b[A\n",
            "Epoch 3:  25%|██▍       | 315/1271 [01:41<05:08,  3.10it/s, training_loss=0.128]\u001b[A\n",
            "Epoch 3:  25%|██▍       | 316/1271 [01:41<05:07,  3.11it/s, training_loss=0.128]\u001b[A\n",
            "Epoch 3:  25%|██▍       | 316/1271 [01:42<05:07,  3.11it/s, training_loss=0.224]\u001b[A\n",
            "Epoch 3:  25%|██▍       | 317/1271 [01:42<05:06,  3.11it/s, training_loss=0.224]\u001b[A\n",
            "Epoch 3:  25%|██▍       | 317/1271 [01:42<05:06,  3.11it/s, training_loss=0.340]\u001b[A\n",
            "Epoch 3:  25%|██▌       | 318/1271 [01:42<05:05,  3.12it/s, training_loss=0.340]\u001b[A\n",
            "Epoch 3:  25%|██▌       | 318/1271 [01:42<05:05,  3.12it/s, training_loss=0.551]\u001b[A\n",
            "Epoch 3:  25%|██▌       | 319/1271 [01:42<05:06,  3.11it/s, training_loss=0.551]\u001b[A\n",
            "Epoch 3:  25%|██▌       | 319/1271 [01:43<05:06,  3.11it/s, training_loss=0.277]\u001b[A\n",
            "Epoch 3:  25%|██▌       | 320/1271 [01:43<05:06,  3.10it/s, training_loss=0.277]\u001b[A\n",
            "Epoch 3:  25%|██▌       | 320/1271 [01:43<05:06,  3.10it/s, training_loss=0.221]\u001b[A\n",
            "Epoch 3:  25%|██▌       | 321/1271 [01:43<05:06,  3.10it/s, training_loss=0.221]\u001b[A\n",
            "Epoch 3:  25%|██▌       | 321/1271 [01:43<05:06,  3.10it/s, training_loss=0.571]\u001b[A\n",
            "Epoch 3:  25%|██▌       | 322/1271 [01:43<05:05,  3.11it/s, training_loss=0.571]\u001b[A\n",
            "Epoch 3:  25%|██▌       | 322/1271 [01:43<05:05,  3.11it/s, training_loss=0.728]\u001b[A\n",
            "Epoch 3:  25%|██▌       | 323/1271 [01:44<05:04,  3.11it/s, training_loss=0.728]\u001b[A\n",
            "Epoch 3:  25%|██▌       | 323/1271 [01:44<05:04,  3.11it/s, training_loss=0.095]\u001b[A\n",
            "Epoch 3:  25%|██▌       | 324/1271 [01:44<05:04,  3.11it/s, training_loss=0.095]\u001b[A\n",
            "Epoch 3:  25%|██▌       | 324/1271 [01:44<05:04,  3.11it/s, training_loss=0.359]\u001b[A\n",
            "Epoch 3:  26%|██▌       | 325/1271 [01:44<05:04,  3.10it/s, training_loss=0.359]\u001b[A\n",
            "Epoch 3:  26%|██▌       | 325/1271 [01:44<05:04,  3.10it/s, training_loss=0.146]\u001b[A\n",
            "Epoch 3:  26%|██▌       | 326/1271 [01:44<05:04,  3.11it/s, training_loss=0.146]\u001b[A\n",
            "Epoch 3:  26%|██▌       | 326/1271 [01:45<05:04,  3.11it/s, training_loss=0.151]\u001b[A\n",
            "Epoch 3:  26%|██▌       | 327/1271 [01:45<05:03,  3.11it/s, training_loss=0.151]\u001b[A\n",
            "Epoch 3:  26%|██▌       | 327/1271 [01:45<05:03,  3.11it/s, training_loss=0.212]\u001b[A\n",
            "Epoch 3:  26%|██▌       | 328/1271 [01:45<05:03,  3.10it/s, training_loss=0.212]\u001b[A\n",
            "Epoch 3:  26%|██▌       | 328/1271 [01:45<05:03,  3.10it/s, training_loss=0.129]\u001b[A\n",
            "Epoch 3:  26%|██▌       | 329/1271 [01:45<05:04,  3.09it/s, training_loss=0.129]\u001b[A\n",
            "Epoch 3:  26%|██▌       | 329/1271 [01:46<05:04,  3.09it/s, training_loss=0.329]\u001b[A\n",
            "Epoch 3:  26%|██▌       | 330/1271 [01:46<05:04,  3.09it/s, training_loss=0.329]\u001b[A\n",
            "Epoch 3:  26%|██▌       | 330/1271 [01:46<05:04,  3.09it/s, training_loss=0.130]\u001b[A\n",
            "Epoch 3:  26%|██▌       | 331/1271 [01:46<05:04,  3.09it/s, training_loss=0.130]\u001b[A\n",
            "Epoch 3:  26%|██▌       | 331/1271 [01:46<05:04,  3.09it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 3:  26%|██▌       | 332/1271 [01:46<05:03,  3.09it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 3:  26%|██▌       | 332/1271 [01:47<05:03,  3.09it/s, training_loss=0.500]\u001b[A\n",
            "Epoch 3:  26%|██▌       | 333/1271 [01:47<05:03,  3.09it/s, training_loss=0.500]\u001b[A\n",
            "Epoch 3:  26%|██▌       | 333/1271 [01:47<05:03,  3.09it/s, training_loss=0.135]\u001b[A\n",
            "Epoch 3:  26%|██▋       | 334/1271 [01:47<05:02,  3.10it/s, training_loss=0.135]\u001b[A\n",
            "Epoch 3:  26%|██▋       | 334/1271 [01:47<05:02,  3.10it/s, training_loss=0.219]\u001b[A\n",
            "Epoch 3:  26%|██▋       | 335/1271 [01:47<05:02,  3.10it/s, training_loss=0.219]\u001b[A\n",
            "Epoch 3:  26%|██▋       | 335/1271 [01:48<05:02,  3.10it/s, training_loss=0.422]\u001b[A\n",
            "Epoch 3:  26%|██▋       | 336/1271 [01:48<05:00,  3.11it/s, training_loss=0.422]\u001b[A\n",
            "Epoch 3:  26%|██▋       | 336/1271 [01:48<05:00,  3.11it/s, training_loss=0.123]\u001b[A\n",
            "Epoch 3:  27%|██▋       | 337/1271 [01:48<05:00,  3.10it/s, training_loss=0.123]\u001b[A\n",
            "Epoch 3:  27%|██▋       | 337/1271 [01:48<05:00,  3.10it/s, training_loss=0.357]\u001b[A\n",
            "Epoch 3:  27%|██▋       | 338/1271 [01:48<05:01,  3.10it/s, training_loss=0.357]\u001b[A\n",
            "Epoch 3:  27%|██▋       | 338/1271 [01:49<05:01,  3.10it/s, training_loss=0.574]\u001b[A\n",
            "Epoch 3:  27%|██▋       | 339/1271 [01:49<04:59,  3.11it/s, training_loss=0.574]\u001b[A\n",
            "Epoch 3:  27%|██▋       | 339/1271 [01:49<04:59,  3.11it/s, training_loss=0.550]\u001b[A\n",
            "Epoch 3:  27%|██▋       | 340/1271 [01:49<04:59,  3.11it/s, training_loss=0.550]\u001b[A\n",
            "Epoch 3:  27%|██▋       | 340/1271 [01:49<04:59,  3.11it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 3:  27%|██▋       | 341/1271 [01:49<04:58,  3.11it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 3:  27%|██▋       | 341/1271 [01:50<04:58,  3.11it/s, training_loss=0.160]\u001b[A\n",
            "Epoch 3:  27%|██▋       | 342/1271 [01:50<04:58,  3.11it/s, training_loss=0.160]\u001b[A\n",
            "Epoch 3:  27%|██▋       | 342/1271 [01:50<04:58,  3.11it/s, training_loss=0.080]\u001b[A\n",
            "Epoch 3:  27%|██▋       | 343/1271 [01:50<04:59,  3.10it/s, training_loss=0.080]\u001b[A\n",
            "Epoch 3:  27%|██▋       | 343/1271 [01:50<04:59,  3.10it/s, training_loss=0.176]\u001b[A\n",
            "Epoch 3:  27%|██▋       | 344/1271 [01:50<04:57,  3.11it/s, training_loss=0.176]\u001b[A\n",
            "Epoch 3:  27%|██▋       | 344/1271 [01:51<04:57,  3.11it/s, training_loss=0.184]\u001b[A\n",
            "Epoch 3:  27%|██▋       | 345/1271 [01:51<04:56,  3.12it/s, training_loss=0.184]\u001b[A\n",
            "Epoch 3:  27%|██▋       | 345/1271 [01:51<04:56,  3.12it/s, training_loss=0.407]\u001b[A\n",
            "Epoch 3:  27%|██▋       | 346/1271 [01:51<04:56,  3.12it/s, training_loss=0.407]\u001b[A\n",
            "Epoch 3:  27%|██▋       | 346/1271 [01:51<04:56,  3.12it/s, training_loss=0.697]\u001b[A\n",
            "Epoch 3:  27%|██▋       | 347/1271 [01:51<04:56,  3.12it/s, training_loss=0.697]\u001b[A\n",
            "Epoch 3:  27%|██▋       | 347/1271 [01:52<04:56,  3.12it/s, training_loss=0.506]\u001b[A\n",
            "Epoch 3:  27%|██▋       | 348/1271 [01:52<04:58,  3.09it/s, training_loss=0.506]\u001b[A\n",
            "Epoch 3:  27%|██▋       | 348/1271 [01:52<04:58,  3.09it/s, training_loss=0.524]\u001b[A\n",
            "Epoch 3:  27%|██▋       | 349/1271 [01:52<04:58,  3.09it/s, training_loss=0.524]\u001b[A\n",
            "Epoch 3:  27%|██▋       | 349/1271 [01:52<04:58,  3.09it/s, training_loss=0.212]\u001b[A\n",
            "Epoch 3:  28%|██▊       | 350/1271 [01:52<04:56,  3.10it/s, training_loss=0.212]\u001b[A\n",
            "Epoch 3:  28%|██▊       | 350/1271 [01:53<04:56,  3.10it/s, training_loss=0.211]\u001b[A\n",
            "Epoch 3:  28%|██▊       | 351/1271 [01:53<04:56,  3.10it/s, training_loss=0.211]\u001b[A\n",
            "Epoch 3:  28%|██▊       | 351/1271 [01:53<04:56,  3.10it/s, training_loss=0.281]\u001b[A\n",
            "Epoch 3:  28%|██▊       | 352/1271 [01:53<04:58,  3.08it/s, training_loss=0.281]\u001b[A\n",
            "Epoch 3:  28%|██▊       | 352/1271 [01:53<04:58,  3.08it/s, training_loss=0.761]\u001b[A\n",
            "Epoch 3:  28%|██▊       | 353/1271 [01:53<04:57,  3.09it/s, training_loss=0.761]\u001b[A\n",
            "Epoch 3:  28%|██▊       | 353/1271 [01:53<04:57,  3.09it/s, training_loss=0.154]\u001b[A\n",
            "Epoch 3:  28%|██▊       | 354/1271 [01:54<04:58,  3.07it/s, training_loss=0.154]\u001b[A\n",
            "Epoch 3:  28%|██▊       | 354/1271 [01:54<04:58,  3.07it/s, training_loss=0.351]\u001b[A\n",
            "Epoch 3:  28%|██▊       | 355/1271 [01:54<04:56,  3.09it/s, training_loss=0.351]\u001b[A\n",
            "Epoch 3:  28%|██▊       | 355/1271 [01:54<04:56,  3.09it/s, training_loss=0.207]\u001b[A\n",
            "Epoch 3:  28%|██▊       | 356/1271 [01:54<04:55,  3.09it/s, training_loss=0.207]\u001b[A\n",
            "Epoch 3:  28%|██▊       | 356/1271 [01:54<04:55,  3.09it/s, training_loss=0.355]\u001b[A\n",
            "Epoch 3:  28%|██▊       | 357/1271 [01:54<04:54,  3.10it/s, training_loss=0.355]\u001b[A\n",
            "Epoch 3:  28%|██▊       | 357/1271 [01:55<04:54,  3.10it/s, training_loss=0.334]\u001b[A\n",
            "Epoch 3:  28%|██▊       | 358/1271 [01:55<04:54,  3.10it/s, training_loss=0.334]\u001b[A\n",
            "Epoch 3:  28%|██▊       | 358/1271 [01:55<04:54,  3.10it/s, training_loss=0.119]\u001b[A\n",
            "Epoch 3:  28%|██▊       | 359/1271 [01:55<04:52,  3.11it/s, training_loss=0.119]\u001b[A\n",
            "Epoch 3:  28%|██▊       | 359/1271 [01:55<04:52,  3.11it/s, training_loss=0.665]\u001b[A\n",
            "Epoch 3:  28%|██▊       | 360/1271 [01:55<04:52,  3.11it/s, training_loss=0.665]\u001b[A\n",
            "Epoch 3:  28%|██▊       | 360/1271 [01:56<04:52,  3.11it/s, training_loss=0.412]\u001b[A\n",
            "Epoch 3:  28%|██▊       | 361/1271 [01:56<04:52,  3.11it/s, training_loss=0.412]\u001b[A\n",
            "Epoch 3:  28%|██▊       | 361/1271 [01:56<04:52,  3.11it/s, training_loss=0.290]\u001b[A\n",
            "Epoch 3:  28%|██▊       | 362/1271 [01:56<04:55,  3.08it/s, training_loss=0.290]\u001b[A\n",
            "Epoch 3:  28%|██▊       | 362/1271 [01:56<04:55,  3.08it/s, training_loss=0.447]\u001b[A\n",
            "Epoch 3:  29%|██▊       | 363/1271 [01:56<04:54,  3.09it/s, training_loss=0.447]\u001b[A\n",
            "Epoch 3:  29%|██▊       | 363/1271 [01:57<04:54,  3.09it/s, training_loss=0.304]\u001b[A\n",
            "Epoch 3:  29%|██▊       | 364/1271 [01:57<04:53,  3.09it/s, training_loss=0.304]\u001b[A\n",
            "Epoch 3:  29%|██▊       | 364/1271 [01:57<04:53,  3.09it/s, training_loss=0.687]\u001b[A\n",
            "Epoch 3:  29%|██▊       | 365/1271 [01:57<04:53,  3.08it/s, training_loss=0.687]\u001b[A\n",
            "Epoch 3:  29%|██▊       | 365/1271 [01:57<04:53,  3.08it/s, training_loss=0.501]\u001b[A\n",
            "Epoch 3:  29%|██▉       | 366/1271 [01:57<04:54,  3.08it/s, training_loss=0.501]\u001b[A\n",
            "Epoch 3:  29%|██▉       | 366/1271 [01:58<04:54,  3.08it/s, training_loss=0.046]\u001b[A\n",
            "Epoch 3:  29%|██▉       | 367/1271 [01:58<04:53,  3.08it/s, training_loss=0.046]\u001b[A\n",
            "Epoch 3:  29%|██▉       | 367/1271 [01:58<04:53,  3.08it/s, training_loss=0.569]\u001b[A\n",
            "Epoch 3:  29%|██▉       | 368/1271 [01:58<04:55,  3.06it/s, training_loss=0.569]\u001b[A\n",
            "Epoch 3:  29%|██▉       | 368/1271 [01:58<04:55,  3.06it/s, training_loss=0.177]\u001b[A\n",
            "Epoch 3:  29%|██▉       | 369/1271 [01:58<04:52,  3.08it/s, training_loss=0.177]\u001b[A\n",
            "Epoch 3:  29%|██▉       | 369/1271 [01:59<04:52,  3.08it/s, training_loss=0.430]\u001b[A\n",
            "Epoch 3:  29%|██▉       | 370/1271 [01:59<04:53,  3.07it/s, training_loss=0.430]\u001b[A\n",
            "Epoch 3:  29%|██▉       | 370/1271 [01:59<04:53,  3.07it/s, training_loss=0.290]\u001b[A\n",
            "Epoch 3:  29%|██▉       | 371/1271 [01:59<04:52,  3.07it/s, training_loss=0.290]\u001b[A\n",
            "Epoch 3:  29%|██▉       | 371/1271 [01:59<04:52,  3.07it/s, training_loss=0.007]\u001b[A\n",
            "Epoch 3:  29%|██▉       | 372/1271 [01:59<04:50,  3.10it/s, training_loss=0.007]\u001b[A\n",
            "Epoch 3:  29%|██▉       | 372/1271 [02:00<04:50,  3.10it/s, training_loss=0.107]\u001b[A\n",
            "Epoch 3:  29%|██▉       | 373/1271 [02:00<04:49,  3.10it/s, training_loss=0.107]\u001b[A\n",
            "Epoch 3:  29%|██▉       | 373/1271 [02:00<04:49,  3.10it/s, training_loss=0.217]\u001b[A\n",
            "Epoch 3:  29%|██▉       | 374/1271 [02:00<04:49,  3.10it/s, training_loss=0.217]\u001b[A\n",
            "Epoch 3:  29%|██▉       | 374/1271 [02:00<04:49,  3.10it/s, training_loss=0.244]\u001b[A\n",
            "Epoch 3:  30%|██▉       | 375/1271 [02:00<04:49,  3.09it/s, training_loss=0.244]\u001b[A\n",
            "Epoch 3:  30%|██▉       | 375/1271 [02:01<04:49,  3.09it/s, training_loss=0.019]\u001b[A\n",
            "Epoch 3:  30%|██▉       | 376/1271 [02:01<04:48,  3.10it/s, training_loss=0.019]\u001b[A\n",
            "Epoch 3:  30%|██▉       | 376/1271 [02:01<04:48,  3.10it/s, training_loss=0.700]\u001b[A\n",
            "Epoch 3:  30%|██▉       | 377/1271 [02:01<04:47,  3.11it/s, training_loss=0.700]\u001b[A\n",
            "Epoch 3:  30%|██▉       | 377/1271 [02:01<04:47,  3.11it/s, training_loss=0.635]\u001b[A\n",
            "Epoch 3:  30%|██▉       | 378/1271 [02:01<04:47,  3.11it/s, training_loss=0.635]\u001b[A\n",
            "Epoch 3:  30%|██▉       | 378/1271 [02:02<04:47,  3.11it/s, training_loss=0.541]\u001b[A\n",
            "Epoch 3:  30%|██▉       | 379/1271 [02:02<04:47,  3.11it/s, training_loss=0.541]\u001b[A\n",
            "Epoch 3:  30%|██▉       | 379/1271 [02:02<04:47,  3.11it/s, training_loss=0.159]\u001b[A\n",
            "Epoch 3:  30%|██▉       | 380/1271 [02:02<04:47,  3.10it/s, training_loss=0.159]\u001b[A\n",
            "Epoch 3:  30%|██▉       | 380/1271 [02:02<04:47,  3.10it/s, training_loss=0.310]\u001b[A\n",
            "Epoch 3:  30%|██▉       | 381/1271 [02:02<04:46,  3.11it/s, training_loss=0.310]\u001b[A\n",
            "Epoch 3:  30%|██▉       | 381/1271 [02:03<04:46,  3.11it/s, training_loss=0.418]\u001b[A\n",
            "Epoch 3:  30%|███       | 382/1271 [02:03<04:45,  3.11it/s, training_loss=0.418]\u001b[A\n",
            "Epoch 3:  30%|███       | 382/1271 [02:03<04:45,  3.11it/s, training_loss=0.666]\u001b[A\n",
            "Epoch 3:  30%|███       | 383/1271 [02:03<04:46,  3.10it/s, training_loss=0.666]\u001b[A\n",
            "Epoch 3:  30%|███       | 383/1271 [02:03<04:46,  3.10it/s, training_loss=0.282]\u001b[A\n",
            "Epoch 3:  30%|███       | 384/1271 [02:03<04:45,  3.10it/s, training_loss=0.282]\u001b[A\n",
            "Epoch 3:  30%|███       | 384/1271 [02:04<04:45,  3.10it/s, training_loss=0.487]\u001b[A\n",
            "Epoch 3:  30%|███       | 385/1271 [02:04<04:45,  3.10it/s, training_loss=0.487]\u001b[A\n",
            "Epoch 3:  30%|███       | 385/1271 [02:04<04:45,  3.10it/s, training_loss=0.175]\u001b[A\n",
            "Epoch 3:  30%|███       | 386/1271 [02:04<04:45,  3.10it/s, training_loss=0.175]\u001b[A\n",
            "Epoch 3:  30%|███       | 386/1271 [02:04<04:45,  3.10it/s, training_loss=0.231]\u001b[A\n",
            "Epoch 3:  30%|███       | 387/1271 [02:04<04:46,  3.09it/s, training_loss=0.231]\u001b[A\n",
            "Epoch 3:  30%|███       | 387/1271 [02:04<04:46,  3.09it/s, training_loss=0.481]\u001b[A\n",
            "Epoch 3:  31%|███       | 388/1271 [02:04<04:46,  3.08it/s, training_loss=0.481]\u001b[A\n",
            "Epoch 3:  31%|███       | 388/1271 [02:05<04:46,  3.08it/s, training_loss=0.292]\u001b[A\n",
            "Epoch 3:  31%|███       | 389/1271 [02:05<04:44,  3.09it/s, training_loss=0.292]\u001b[A\n",
            "Epoch 3:  31%|███       | 389/1271 [02:05<04:44,  3.09it/s, training_loss=0.323]\u001b[A\n",
            "Epoch 3:  31%|███       | 390/1271 [02:05<04:43,  3.11it/s, training_loss=0.323]\u001b[A\n",
            "Epoch 3:  31%|███       | 390/1271 [02:05<04:43,  3.11it/s, training_loss=0.532]\u001b[A\n",
            "Epoch 3:  31%|███       | 391/1271 [02:05<04:43,  3.10it/s, training_loss=0.532]\u001b[A\n",
            "Epoch 3:  31%|███       | 391/1271 [02:06<04:43,  3.10it/s, training_loss=0.068]\u001b[A\n",
            "Epoch 3:  31%|███       | 392/1271 [02:06<04:41,  3.12it/s, training_loss=0.068]\u001b[A\n",
            "Epoch 3:  31%|███       | 392/1271 [02:06<04:41,  3.12it/s, training_loss=0.031]\u001b[A\n",
            "Epoch 3:  31%|███       | 393/1271 [02:06<04:40,  3.14it/s, training_loss=0.031]\u001b[A\n",
            "Epoch 3:  31%|███       | 393/1271 [02:06<04:40,  3.14it/s, training_loss=0.345]\u001b[A\n",
            "Epoch 3:  31%|███       | 394/1271 [02:06<04:41,  3.11it/s, training_loss=0.345]\u001b[A\n",
            "Epoch 3:  31%|███       | 394/1271 [02:07<04:41,  3.11it/s, training_loss=0.553]\u001b[A\n",
            "Epoch 3:  31%|███       | 395/1271 [02:07<04:42,  3.10it/s, training_loss=0.553]\u001b[A\n",
            "Epoch 3:  31%|███       | 395/1271 [02:07<04:42,  3.10it/s, training_loss=0.069]\u001b[A\n",
            "Epoch 3:  31%|███       | 396/1271 [02:07<04:42,  3.10it/s, training_loss=0.069]\u001b[A\n",
            "Epoch 3:  31%|███       | 396/1271 [02:07<04:42,  3.10it/s, training_loss=0.514]\u001b[A\n",
            "Epoch 3:  31%|███       | 397/1271 [02:07<04:41,  3.11it/s, training_loss=0.514]\u001b[A\n",
            "Epoch 3:  31%|███       | 397/1271 [02:08<04:41,  3.11it/s, training_loss=0.252]\u001b[A\n",
            "Epoch 3:  31%|███▏      | 398/1271 [02:08<04:40,  3.11it/s, training_loss=0.252]\u001b[A\n",
            "Epoch 3:  31%|███▏      | 398/1271 [02:08<04:40,  3.11it/s, training_loss=0.732]\u001b[A\n",
            "Epoch 3:  31%|███▏      | 399/1271 [02:08<04:40,  3.10it/s, training_loss=0.732]\u001b[A\n",
            "Epoch 3:  31%|███▏      | 399/1271 [02:08<04:40,  3.10it/s, training_loss=0.589]\u001b[A\n",
            "Epoch 3:  31%|███▏      | 400/1271 [02:08<04:40,  3.10it/s, training_loss=0.589]\u001b[A\n",
            "Epoch 3:  31%|███▏      | 400/1271 [02:09<04:40,  3.10it/s, training_loss=0.252]\u001b[A\n",
            "Epoch 3:  32%|███▏      | 401/1271 [02:09<04:41,  3.09it/s, training_loss=0.252]\u001b[A\n",
            "Epoch 3:  32%|███▏      | 401/1271 [02:09<04:41,  3.09it/s, training_loss=0.376]\u001b[A\n",
            "Epoch 3:  32%|███▏      | 402/1271 [02:09<04:41,  3.08it/s, training_loss=0.376]\u001b[A\n",
            "Epoch 3:  32%|███▏      | 402/1271 [02:09<04:41,  3.08it/s, training_loss=0.024]\u001b[A\n",
            "Epoch 3:  32%|███▏      | 403/1271 [02:09<04:43,  3.06it/s, training_loss=0.024]\u001b[A\n",
            "Epoch 3:  32%|███▏      | 403/1271 [02:10<04:43,  3.06it/s, training_loss=0.379]\u001b[A\n",
            "Epoch 3:  32%|███▏      | 404/1271 [02:10<04:42,  3.07it/s, training_loss=0.379]\u001b[A\n",
            "Epoch 3:  32%|███▏      | 404/1271 [02:10<04:42,  3.07it/s, training_loss=0.334]\u001b[A\n",
            "Epoch 3:  32%|███▏      | 405/1271 [02:10<04:40,  3.08it/s, training_loss=0.334]\u001b[A\n",
            "Epoch 3:  32%|███▏      | 405/1271 [02:10<04:40,  3.08it/s, training_loss=0.298]\u001b[A\n",
            "Epoch 3:  32%|███▏      | 406/1271 [02:10<04:39,  3.10it/s, training_loss=0.298]\u001b[A\n",
            "Epoch 3:  32%|███▏      | 406/1271 [02:11<04:39,  3.10it/s, training_loss=0.797]\u001b[A\n",
            "Epoch 3:  32%|███▏      | 407/1271 [02:11<04:38,  3.10it/s, training_loss=0.797]\u001b[A\n",
            "Epoch 3:  32%|███▏      | 407/1271 [02:11<04:38,  3.10it/s, training_loss=0.525]\u001b[A\n",
            "Epoch 3:  32%|███▏      | 408/1271 [02:11<04:38,  3.10it/s, training_loss=0.525]\u001b[A\n",
            "Epoch 3:  32%|███▏      | 408/1271 [02:11<04:38,  3.10it/s, training_loss=0.460]\u001b[A\n",
            "Epoch 3:  32%|███▏      | 409/1271 [02:11<04:39,  3.09it/s, training_loss=0.460]\u001b[A\n",
            "Epoch 3:  32%|███▏      | 409/1271 [02:12<04:39,  3.09it/s, training_loss=0.242]\u001b[A\n",
            "Epoch 3:  32%|███▏      | 410/1271 [02:12<04:41,  3.06it/s, training_loss=0.242]\u001b[A\n",
            "Epoch 3:  32%|███▏      | 410/1271 [02:12<04:41,  3.06it/s, training_loss=0.014]\u001b[A\n",
            "Epoch 3:  32%|███▏      | 411/1271 [02:12<04:38,  3.09it/s, training_loss=0.014]\u001b[A\n",
            "Epoch 3:  32%|███▏      | 411/1271 [02:12<04:38,  3.09it/s, training_loss=0.022]\u001b[A\n",
            "Epoch 3:  32%|███▏      | 412/1271 [02:12<04:38,  3.09it/s, training_loss=0.022]\u001b[A\n",
            "Epoch 3:  32%|███▏      | 412/1271 [02:13<04:38,  3.09it/s, training_loss=0.395]\u001b[A\n",
            "Epoch 3:  32%|███▏      | 413/1271 [02:13<04:36,  3.11it/s, training_loss=0.395]\u001b[A\n",
            "Epoch 3:  32%|███▏      | 413/1271 [02:13<04:36,  3.11it/s, training_loss=0.038]\u001b[A\n",
            "Epoch 3:  33%|███▎      | 414/1271 [02:13<04:34,  3.12it/s, training_loss=0.038]\u001b[A\n",
            "Epoch 3:  33%|███▎      | 414/1271 [02:13<04:34,  3.12it/s, training_loss=0.314]\u001b[A\n",
            "Epoch 3:  33%|███▎      | 415/1271 [02:13<04:33,  3.13it/s, training_loss=0.314]\u001b[A\n",
            "Epoch 3:  33%|███▎      | 415/1271 [02:14<04:33,  3.13it/s, training_loss=0.206]\u001b[A\n",
            "Epoch 3:  33%|███▎      | 416/1271 [02:14<04:33,  3.12it/s, training_loss=0.206]\u001b[A\n",
            "Epoch 3:  33%|███▎      | 416/1271 [02:14<04:33,  3.12it/s, training_loss=0.195]\u001b[A\n",
            "Epoch 3:  33%|███▎      | 417/1271 [02:14<04:34,  3.11it/s, training_loss=0.195]\u001b[A\n",
            "Epoch 3:  33%|███▎      | 417/1271 [02:14<04:34,  3.11it/s, training_loss=0.190]\u001b[A\n",
            "Epoch 3:  33%|███▎      | 418/1271 [02:14<04:33,  3.11it/s, training_loss=0.190]\u001b[A\n",
            "Epoch 3:  33%|███▎      | 418/1271 [02:14<04:33,  3.11it/s, training_loss=0.502]\u001b[A\n",
            "Epoch 3:  33%|███▎      | 419/1271 [02:14<04:33,  3.11it/s, training_loss=0.502]\u001b[A\n",
            "Epoch 3:  33%|███▎      | 419/1271 [02:15<04:33,  3.11it/s, training_loss=0.121]\u001b[A\n",
            "Epoch 3:  33%|███▎      | 420/1271 [02:15<04:34,  3.10it/s, training_loss=0.121]\u001b[A\n",
            "Epoch 3:  33%|███▎      | 420/1271 [02:15<04:34,  3.10it/s, training_loss=0.477]\u001b[A\n",
            "Epoch 3:  33%|███▎      | 421/1271 [02:15<04:32,  3.12it/s, training_loss=0.477]\u001b[A\n",
            "Epoch 3:  33%|███▎      | 421/1271 [02:15<04:32,  3.12it/s, training_loss=0.563]\u001b[A\n",
            "Epoch 3:  33%|███▎      | 422/1271 [02:15<04:32,  3.12it/s, training_loss=0.563]\u001b[A\n",
            "Epoch 3:  33%|███▎      | 422/1271 [02:16<04:32,  3.12it/s, training_loss=0.186]\u001b[A\n",
            "Epoch 3:  33%|███▎      | 423/1271 [02:16<04:31,  3.12it/s, training_loss=0.186]\u001b[A\n",
            "Epoch 3:  33%|███▎      | 423/1271 [02:16<04:31,  3.12it/s, training_loss=1.013]\u001b[A\n",
            "Epoch 3:  33%|███▎      | 424/1271 [02:16<04:32,  3.10it/s, training_loss=1.013]\u001b[A\n",
            "Epoch 3:  33%|███▎      | 424/1271 [02:16<04:32,  3.10it/s, training_loss=0.292]\u001b[A\n",
            "Epoch 3:  33%|███▎      | 425/1271 [02:16<04:32,  3.11it/s, training_loss=0.292]\u001b[A\n",
            "Epoch 3:  33%|███▎      | 425/1271 [02:17<04:32,  3.11it/s, training_loss=0.349]\u001b[A\n",
            "Epoch 3:  34%|███▎      | 426/1271 [02:17<04:31,  3.11it/s, training_loss=0.349]\u001b[A\n",
            "Epoch 3:  34%|███▎      | 426/1271 [02:17<04:31,  3.11it/s, training_loss=0.089]\u001b[A\n",
            "Epoch 3:  34%|███▎      | 427/1271 [02:17<04:31,  3.11it/s, training_loss=0.089]\u001b[A\n",
            "Epoch 3:  34%|███▎      | 427/1271 [02:17<04:31,  3.11it/s, training_loss=0.314]\u001b[A\n",
            "Epoch 3:  34%|███▎      | 428/1271 [02:17<04:30,  3.12it/s, training_loss=0.314]\u001b[A\n",
            "Epoch 3:  34%|███▎      | 428/1271 [02:18<04:30,  3.12it/s, training_loss=0.306]\u001b[A\n",
            "Epoch 3:  34%|███▍      | 429/1271 [02:18<04:29,  3.12it/s, training_loss=0.306]\u001b[A\n",
            "Epoch 3:  34%|███▍      | 429/1271 [02:18<04:29,  3.12it/s, training_loss=0.498]\u001b[A\n",
            "Epoch 3:  34%|███▍      | 430/1271 [02:18<04:29,  3.12it/s, training_loss=0.498]\u001b[A\n",
            "Epoch 3:  34%|███▍      | 430/1271 [02:18<04:29,  3.12it/s, training_loss=0.764]\u001b[A\n",
            "Epoch 3:  34%|███▍      | 431/1271 [02:18<04:31,  3.10it/s, training_loss=0.764]\u001b[A\n",
            "Epoch 3:  34%|███▍      | 431/1271 [02:19<04:31,  3.10it/s, training_loss=0.147]\u001b[A\n",
            "Epoch 3:  34%|███▍      | 432/1271 [02:19<04:30,  3.11it/s, training_loss=0.147]\u001b[A\n",
            "Epoch 3:  34%|███▍      | 432/1271 [02:19<04:30,  3.11it/s, training_loss=0.522]\u001b[A\n",
            "Epoch 3:  34%|███▍      | 433/1271 [02:19<04:30,  3.09it/s, training_loss=0.522]\u001b[A\n",
            "Epoch 3:  34%|███▍      | 433/1271 [02:19<04:30,  3.09it/s, training_loss=0.770]\u001b[A\n",
            "Epoch 3:  34%|███▍      | 434/1271 [02:19<04:30,  3.09it/s, training_loss=0.770]\u001b[A\n",
            "Epoch 3:  34%|███▍      | 434/1271 [02:20<04:30,  3.09it/s, training_loss=0.168]\u001b[A\n",
            "Epoch 3:  34%|███▍      | 435/1271 [02:20<04:28,  3.11it/s, training_loss=0.168]\u001b[A\n",
            "Epoch 3:  34%|███▍      | 435/1271 [02:20<04:28,  3.11it/s, training_loss=0.366]\u001b[A\n",
            "Epoch 3:  34%|███▍      | 436/1271 [02:20<04:28,  3.11it/s, training_loss=0.366]\u001b[A\n",
            "Epoch 3:  34%|███▍      | 436/1271 [02:20<04:28,  3.11it/s, training_loss=0.250]\u001b[A\n",
            "Epoch 3:  34%|███▍      | 437/1271 [02:20<04:28,  3.11it/s, training_loss=0.250]\u001b[A\n",
            "Epoch 3:  34%|███▍      | 437/1271 [02:21<04:28,  3.11it/s, training_loss=0.048]\u001b[A\n",
            "Epoch 3:  34%|███▍      | 438/1271 [02:21<04:27,  3.12it/s, training_loss=0.048]\u001b[A\n",
            "Epoch 3:  34%|███▍      | 438/1271 [02:21<04:27,  3.12it/s, training_loss=0.107]\u001b[A\n",
            "Epoch 3:  35%|███▍      | 439/1271 [02:21<04:26,  3.12it/s, training_loss=0.107]\u001b[A\n",
            "Epoch 3:  35%|███▍      | 439/1271 [02:21<04:26,  3.12it/s, training_loss=0.072]\u001b[A\n",
            "Epoch 3:  35%|███▍      | 440/1271 [02:21<04:26,  3.12it/s, training_loss=0.072]\u001b[A\n",
            "Epoch 3:  35%|███▍      | 440/1271 [02:22<04:26,  3.12it/s, training_loss=0.305]\u001b[A\n",
            "Epoch 3:  35%|███▍      | 441/1271 [02:22<04:26,  3.12it/s, training_loss=0.305]\u001b[A\n",
            "Epoch 3:  35%|███▍      | 441/1271 [02:22<04:26,  3.12it/s, training_loss=0.271]\u001b[A\n",
            "Epoch 3:  35%|███▍      | 442/1271 [02:22<04:24,  3.13it/s, training_loss=0.271]\u001b[A\n",
            "Epoch 3:  35%|███▍      | 442/1271 [02:22<04:24,  3.13it/s, training_loss=0.481]\u001b[A\n",
            "Epoch 3:  35%|███▍      | 443/1271 [02:22<04:24,  3.13it/s, training_loss=0.481]\u001b[A\n",
            "Epoch 3:  35%|███▍      | 443/1271 [02:23<04:24,  3.13it/s, training_loss=0.074]\u001b[A\n",
            "Epoch 3:  35%|███▍      | 444/1271 [02:23<04:24,  3.13it/s, training_loss=0.074]\u001b[A\n",
            "Epoch 3:  35%|███▍      | 444/1271 [02:23<04:24,  3.13it/s, training_loss=0.014]\u001b[A\n",
            "Epoch 3:  35%|███▌      | 445/1271 [02:23<04:24,  3.12it/s, training_loss=0.014]\u001b[A\n",
            "Epoch 3:  35%|███▌      | 445/1271 [02:23<04:24,  3.12it/s, training_loss=0.315]\u001b[A\n",
            "Epoch 3:  35%|███▌      | 446/1271 [02:23<04:25,  3.11it/s, training_loss=0.315]\u001b[A\n",
            "Epoch 3:  35%|███▌      | 446/1271 [02:23<04:25,  3.11it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 3:  35%|███▌      | 447/1271 [02:23<04:24,  3.11it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 3:  35%|███▌      | 447/1271 [02:24<04:24,  3.11it/s, training_loss=0.327]\u001b[A\n",
            "Epoch 3:  35%|███▌      | 448/1271 [02:24<04:24,  3.12it/s, training_loss=0.327]\u001b[A\n",
            "Epoch 3:  35%|███▌      | 448/1271 [02:24<04:24,  3.12it/s, training_loss=0.193]\u001b[A\n",
            "Epoch 3:  35%|███▌      | 449/1271 [02:24<04:22,  3.13it/s, training_loss=0.193]\u001b[A\n",
            "Epoch 3:  35%|███▌      | 449/1271 [02:24<04:22,  3.13it/s, training_loss=0.122]\u001b[A\n",
            "Epoch 3:  35%|███▌      | 450/1271 [02:24<04:21,  3.14it/s, training_loss=0.122]\u001b[A\n",
            "Epoch 3:  35%|███▌      | 450/1271 [02:25<04:21,  3.14it/s, training_loss=0.597]\u001b[A\n",
            "Epoch 3:  35%|███▌      | 451/1271 [02:25<04:21,  3.14it/s, training_loss=0.597]\u001b[A\n",
            "Epoch 3:  35%|███▌      | 451/1271 [02:25<04:21,  3.14it/s, training_loss=0.219]\u001b[A\n",
            "Epoch 3:  36%|███▌      | 452/1271 [02:25<04:22,  3.12it/s, training_loss=0.219]\u001b[A\n",
            "Epoch 3:  36%|███▌      | 452/1271 [02:25<04:22,  3.12it/s, training_loss=0.531]\u001b[A\n",
            "Epoch 3:  36%|███▌      | 453/1271 [02:25<04:22,  3.12it/s, training_loss=0.531]\u001b[A\n",
            "Epoch 3:  36%|███▌      | 453/1271 [02:26<04:22,  3.12it/s, training_loss=0.527]\u001b[A\n",
            "Epoch 3:  36%|███▌      | 454/1271 [02:26<04:22,  3.11it/s, training_loss=0.527]\u001b[A\n",
            "Epoch 3:  36%|███▌      | 454/1271 [02:26<04:22,  3.11it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 3:  36%|███▌      | 455/1271 [02:26<04:22,  3.11it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 3:  36%|███▌      | 455/1271 [02:26<04:22,  3.11it/s, training_loss=0.439]\u001b[A\n",
            "Epoch 3:  36%|███▌      | 456/1271 [02:26<04:21,  3.12it/s, training_loss=0.439]\u001b[A\n",
            "Epoch 3:  36%|███▌      | 456/1271 [02:27<04:21,  3.12it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 3:  36%|███▌      | 457/1271 [02:27<04:21,  3.11it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 3:  36%|███▌      | 457/1271 [02:27<04:21,  3.11it/s, training_loss=0.641]\u001b[A\n",
            "Epoch 3:  36%|███▌      | 458/1271 [02:27<04:21,  3.11it/s, training_loss=0.641]\u001b[A\n",
            "Epoch 3:  36%|███▌      | 458/1271 [02:27<04:21,  3.11it/s, training_loss=0.021]\u001b[A\n",
            "Epoch 3:  36%|███▌      | 459/1271 [02:27<04:21,  3.10it/s, training_loss=0.021]\u001b[A\n",
            "Epoch 3:  36%|███▌      | 459/1271 [02:28<04:21,  3.10it/s, training_loss=0.137]\u001b[A\n",
            "Epoch 3:  36%|███▌      | 460/1271 [02:28<04:20,  3.11it/s, training_loss=0.137]\u001b[A\n",
            "Epoch 3:  36%|███▌      | 460/1271 [02:28<04:20,  3.11it/s, training_loss=0.421]\u001b[A\n",
            "Epoch 3:  36%|███▋      | 461/1271 [02:28<04:19,  3.12it/s, training_loss=0.421]\u001b[A\n",
            "Epoch 3:  36%|███▋      | 461/1271 [02:28<04:19,  3.12it/s, training_loss=0.350]\u001b[A\n",
            "Epoch 3:  36%|███▋      | 462/1271 [02:28<04:19,  3.12it/s, training_loss=0.350]\u001b[A\n",
            "Epoch 3:  36%|███▋      | 462/1271 [02:29<04:19,  3.12it/s, training_loss=0.217]\u001b[A\n",
            "Epoch 3:  36%|███▋      | 463/1271 [02:29<04:18,  3.13it/s, training_loss=0.217]\u001b[A\n",
            "Epoch 3:  36%|███▋      | 463/1271 [02:29<04:18,  3.13it/s, training_loss=0.298]\u001b[A\n",
            "Epoch 3:  37%|███▋      | 464/1271 [02:29<04:17,  3.13it/s, training_loss=0.298]\u001b[A\n",
            "Epoch 3:  37%|███▋      | 464/1271 [02:29<04:17,  3.13it/s, training_loss=0.922]\u001b[A\n",
            "Epoch 3:  37%|███▋      | 465/1271 [02:29<04:17,  3.13it/s, training_loss=0.922]\u001b[A\n",
            "Epoch 3:  37%|███▋      | 465/1271 [02:30<04:17,  3.13it/s, training_loss=0.624]\u001b[A\n",
            "Epoch 3:  37%|███▋      | 466/1271 [02:30<04:16,  3.14it/s, training_loss=0.624]\u001b[A\n",
            "Epoch 3:  37%|███▋      | 466/1271 [02:30<04:16,  3.14it/s, training_loss=0.629]\u001b[A\n",
            "Epoch 3:  37%|███▋      | 467/1271 [02:30<04:16,  3.13it/s, training_loss=0.629]\u001b[A\n",
            "Epoch 3:  37%|███▋      | 467/1271 [02:30<04:16,  3.13it/s, training_loss=0.267]\u001b[A\n",
            "Epoch 3:  37%|███▋      | 468/1271 [02:30<04:16,  3.14it/s, training_loss=0.267]\u001b[A\n",
            "Epoch 3:  37%|███▋      | 468/1271 [02:31<04:16,  3.14it/s, training_loss=0.802]\u001b[A\n",
            "Epoch 3:  37%|███▋      | 469/1271 [02:31<04:16,  3.13it/s, training_loss=0.802]\u001b[A\n",
            "Epoch 3:  37%|███▋      | 469/1271 [02:31<04:16,  3.13it/s, training_loss=0.163]\u001b[A\n",
            "Epoch 3:  37%|███▋      | 470/1271 [02:31<04:16,  3.12it/s, training_loss=0.163]\u001b[A\n",
            "Epoch 3:  37%|███▋      | 470/1271 [02:31<04:16,  3.12it/s, training_loss=0.325]\u001b[A\n",
            "Epoch 3:  37%|███▋      | 471/1271 [02:31<04:17,  3.11it/s, training_loss=0.325]\u001b[A\n",
            "Epoch 3:  37%|███▋      | 471/1271 [02:31<04:17,  3.11it/s, training_loss=0.084]\u001b[A\n",
            "Epoch 3:  37%|███▋      | 472/1271 [02:31<04:16,  3.11it/s, training_loss=0.084]\u001b[A\n",
            "Epoch 3:  37%|███▋      | 472/1271 [02:32<04:16,  3.11it/s, training_loss=0.575]\u001b[A\n",
            "Epoch 3:  37%|███▋      | 473/1271 [02:32<04:15,  3.12it/s, training_loss=0.575]\u001b[A\n",
            "Epoch 3:  37%|███▋      | 473/1271 [02:32<04:15,  3.12it/s, training_loss=0.330]\u001b[A\n",
            "Epoch 3:  37%|███▋      | 474/1271 [02:32<04:14,  3.13it/s, training_loss=0.330]\u001b[A\n",
            "Epoch 3:  37%|███▋      | 474/1271 [02:32<04:14,  3.13it/s, training_loss=0.256]\u001b[A\n",
            "Epoch 3:  37%|███▋      | 475/1271 [02:32<04:13,  3.13it/s, training_loss=0.256]\u001b[A\n",
            "Epoch 3:  37%|███▋      | 475/1271 [02:33<04:13,  3.13it/s, training_loss=0.398]\u001b[A\n",
            "Epoch 3:  37%|███▋      | 476/1271 [02:33<04:14,  3.12it/s, training_loss=0.398]\u001b[A\n",
            "Epoch 3:  37%|███▋      | 476/1271 [02:33<04:14,  3.12it/s, training_loss=0.458]\u001b[A\n",
            "Epoch 3:  38%|███▊      | 477/1271 [02:33<04:13,  3.13it/s, training_loss=0.458]\u001b[A\n",
            "Epoch 3:  38%|███▊      | 477/1271 [02:33<04:13,  3.13it/s, training_loss=0.854]\u001b[A\n",
            "Epoch 3:  38%|███▊      | 478/1271 [02:33<04:14,  3.11it/s, training_loss=0.854]\u001b[A\n",
            "Epoch 3:  38%|███▊      | 478/1271 [02:34<04:14,  3.11it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 3:  38%|███▊      | 479/1271 [02:34<04:13,  3.12it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 3:  38%|███▊      | 479/1271 [02:34<04:13,  3.12it/s, training_loss=0.368]\u001b[A\n",
            "Epoch 3:  38%|███▊      | 480/1271 [02:34<04:13,  3.12it/s, training_loss=0.368]\u001b[A\n",
            "Epoch 3:  38%|███▊      | 480/1271 [02:34<04:13,  3.12it/s, training_loss=0.454]\u001b[A\n",
            "Epoch 3:  38%|███▊      | 481/1271 [02:34<04:13,  3.12it/s, training_loss=0.454]\u001b[A\n",
            "Epoch 3:  38%|███▊      | 481/1271 [02:35<04:13,  3.12it/s, training_loss=0.387]\u001b[A\n",
            "Epoch 3:  38%|███▊      | 482/1271 [02:35<04:13,  3.11it/s, training_loss=0.387]\u001b[A\n",
            "Epoch 3:  38%|███▊      | 482/1271 [02:35<04:13,  3.11it/s, training_loss=0.413]\u001b[A\n",
            "Epoch 3:  38%|███▊      | 483/1271 [02:35<04:12,  3.12it/s, training_loss=0.413]\u001b[A\n",
            "Epoch 3:  38%|███▊      | 483/1271 [02:35<04:12,  3.12it/s, training_loss=0.201]\u001b[A\n",
            "Epoch 3:  38%|███▊      | 484/1271 [02:35<04:12,  3.11it/s, training_loss=0.201]\u001b[A\n",
            "Epoch 3:  38%|███▊      | 484/1271 [02:36<04:12,  3.11it/s, training_loss=0.296]\u001b[A\n",
            "Epoch 3:  38%|███▊      | 485/1271 [02:36<04:14,  3.09it/s, training_loss=0.296]\u001b[A\n",
            "Epoch 3:  38%|███▊      | 485/1271 [02:36<04:14,  3.09it/s, training_loss=0.581]\u001b[A\n",
            "Epoch 3:  38%|███▊      | 486/1271 [02:36<04:13,  3.10it/s, training_loss=0.581]\u001b[A\n",
            "Epoch 3:  38%|███▊      | 486/1271 [02:36<04:13,  3.10it/s, training_loss=0.509]\u001b[A\n",
            "Epoch 3:  38%|███▊      | 487/1271 [02:36<04:13,  3.09it/s, training_loss=0.509]\u001b[A\n",
            "Epoch 3:  38%|███▊      | 487/1271 [02:37<04:13,  3.09it/s, training_loss=0.383]\u001b[A\n",
            "Epoch 3:  38%|███▊      | 488/1271 [02:37<04:13,  3.08it/s, training_loss=0.383]\u001b[A\n",
            "Epoch 3:  38%|███▊      | 488/1271 [02:37<04:13,  3.08it/s, training_loss=0.235]\u001b[A\n",
            "Epoch 3:  38%|███▊      | 489/1271 [02:37<04:13,  3.09it/s, training_loss=0.235]\u001b[A\n",
            "Epoch 3:  38%|███▊      | 489/1271 [02:37<04:13,  3.09it/s, training_loss=0.482]\u001b[A\n",
            "Epoch 3:  39%|███▊      | 490/1271 [02:37<04:11,  3.10it/s, training_loss=0.482]\u001b[A\n",
            "Epoch 3:  39%|███▊      | 490/1271 [02:38<04:11,  3.10it/s, training_loss=0.479]\u001b[A\n",
            "Epoch 3:  39%|███▊      | 491/1271 [02:38<04:11,  3.10it/s, training_loss=0.479]\u001b[A\n",
            "Epoch 3:  39%|███▊      | 491/1271 [02:38<04:11,  3.10it/s, training_loss=0.032]\u001b[A\n",
            "Epoch 3:  39%|███▊      | 492/1271 [02:38<04:09,  3.12it/s, training_loss=0.032]\u001b[A\n",
            "Epoch 3:  39%|███▊      | 492/1271 [02:38<04:09,  3.12it/s, training_loss=0.086]\u001b[A\n",
            "Epoch 3:  39%|███▉      | 493/1271 [02:38<04:09,  3.12it/s, training_loss=0.086]\u001b[A\n",
            "Epoch 3:  39%|███▉      | 493/1271 [02:39<04:09,  3.12it/s, training_loss=0.031]\u001b[A\n",
            "Epoch 3:  39%|███▉      | 494/1271 [02:39<04:12,  3.08it/s, training_loss=0.031]\u001b[A\n",
            "Epoch 3:  39%|███▉      | 494/1271 [02:39<04:12,  3.08it/s, training_loss=0.087]\u001b[A\n",
            "Epoch 3:  39%|███▉      | 495/1271 [02:39<04:12,  3.08it/s, training_loss=0.087]\u001b[A\n",
            "Epoch 3:  39%|███▉      | 495/1271 [02:39<04:12,  3.08it/s, training_loss=0.394]\u001b[A\n",
            "Epoch 3:  39%|███▉      | 496/1271 [02:39<04:11,  3.09it/s, training_loss=0.394]\u001b[A\n",
            "Epoch 3:  39%|███▉      | 496/1271 [02:40<04:11,  3.09it/s, training_loss=0.037]\u001b[A\n",
            "Epoch 3:  39%|███▉      | 497/1271 [02:40<04:09,  3.10it/s, training_loss=0.037]\u001b[A\n",
            "Epoch 3:  39%|███▉      | 497/1271 [02:40<04:09,  3.10it/s, training_loss=0.491]\u001b[A\n",
            "Epoch 3:  39%|███▉      | 498/1271 [02:40<04:09,  3.10it/s, training_loss=0.491]\u001b[A\n",
            "Epoch 3:  39%|███▉      | 498/1271 [02:40<04:09,  3.10it/s, training_loss=0.401]\u001b[A\n",
            "Epoch 3:  39%|███▉      | 499/1271 [02:40<04:09,  3.09it/s, training_loss=0.401]\u001b[A\n",
            "Epoch 3:  39%|███▉      | 499/1271 [02:40<04:09,  3.09it/s, training_loss=0.096]\u001b[A\n",
            "Epoch 3:  39%|███▉      | 500/1271 [02:41<04:08,  3.10it/s, training_loss=0.096]\u001b[A\n",
            "Epoch 3:  39%|███▉      | 500/1271 [02:41<04:08,  3.10it/s, training_loss=0.346]\u001b[A\n",
            "Epoch 3:  39%|███▉      | 501/1271 [02:41<04:09,  3.08it/s, training_loss=0.346]\u001b[A\n",
            "Epoch 3:  39%|███▉      | 501/1271 [02:41<04:09,  3.08it/s, training_loss=0.180]\u001b[A\n",
            "Epoch 3:  39%|███▉      | 502/1271 [02:41<04:07,  3.10it/s, training_loss=0.180]\u001b[A\n",
            "Epoch 3:  39%|███▉      | 502/1271 [02:41<04:07,  3.10it/s, training_loss=0.378]\u001b[A\n",
            "Epoch 3:  40%|███▉      | 503/1271 [02:41<04:07,  3.10it/s, training_loss=0.378]\u001b[A\n",
            "Epoch 3:  40%|███▉      | 503/1271 [02:42<04:07,  3.10it/s, training_loss=0.017]\u001b[A\n",
            "Epoch 3:  40%|███▉      | 504/1271 [02:42<04:08,  3.09it/s, training_loss=0.017]\u001b[A\n",
            "Epoch 3:  40%|███▉      | 504/1271 [02:42<04:08,  3.09it/s, training_loss=0.127]\u001b[A\n",
            "Epoch 3:  40%|███▉      | 505/1271 [02:42<04:08,  3.08it/s, training_loss=0.127]\u001b[A\n",
            "Epoch 3:  40%|███▉      | 505/1271 [02:42<04:08,  3.08it/s, training_loss=0.019]\u001b[A\n",
            "Epoch 3:  40%|███▉      | 506/1271 [02:42<04:09,  3.07it/s, training_loss=0.019]\u001b[A\n",
            "Epoch 3:  40%|███▉      | 506/1271 [02:43<04:09,  3.07it/s, training_loss=0.241]\u001b[A\n",
            "Epoch 3:  40%|███▉      | 507/1271 [02:43<04:07,  3.08it/s, training_loss=0.241]\u001b[A\n",
            "Epoch 3:  40%|███▉      | 507/1271 [02:43<04:07,  3.08it/s, training_loss=0.268]\u001b[A\n",
            "Epoch 3:  40%|███▉      | 508/1271 [02:43<04:06,  3.09it/s, training_loss=0.268]\u001b[A\n",
            "Epoch 3:  40%|███▉      | 508/1271 [02:43<04:06,  3.09it/s, training_loss=0.504]\u001b[A\n",
            "Epoch 3:  40%|████      | 509/1271 [02:43<04:05,  3.11it/s, training_loss=0.504]\u001b[A\n",
            "Epoch 3:  40%|████      | 509/1271 [02:44<04:05,  3.11it/s, training_loss=0.725]\u001b[A\n",
            "Epoch 3:  40%|████      | 510/1271 [02:44<04:05,  3.11it/s, training_loss=0.725]\u001b[A\n",
            "Epoch 3:  40%|████      | 510/1271 [02:44<04:05,  3.11it/s, training_loss=0.287]\u001b[A\n",
            "Epoch 3:  40%|████      | 511/1271 [02:44<04:04,  3.11it/s, training_loss=0.287]\u001b[A\n",
            "Epoch 3:  40%|████      | 511/1271 [02:44<04:04,  3.11it/s, training_loss=0.511]\u001b[A\n",
            "Epoch 3:  40%|████      | 512/1271 [02:44<04:04,  3.10it/s, training_loss=0.511]\u001b[A\n",
            "Epoch 3:  40%|████      | 512/1271 [02:45<04:04,  3.10it/s, training_loss=0.015]\u001b[A\n",
            "Epoch 3:  40%|████      | 513/1271 [02:45<04:02,  3.12it/s, training_loss=0.015]\u001b[A\n",
            "Epoch 3:  40%|████      | 513/1271 [02:45<04:02,  3.12it/s, training_loss=0.292]\u001b[A\n",
            "Epoch 3:  40%|████      | 514/1271 [02:45<04:03,  3.11it/s, training_loss=0.292]\u001b[A\n",
            "Epoch 3:  40%|████      | 514/1271 [02:45<04:03,  3.11it/s, training_loss=0.608]\u001b[A\n",
            "Epoch 3:  41%|████      | 515/1271 [02:45<04:02,  3.12it/s, training_loss=0.608]\u001b[A\n",
            "Epoch 3:  41%|████      | 515/1271 [02:46<04:02,  3.12it/s, training_loss=0.214]\u001b[A\n",
            "Epoch 3:  41%|████      | 516/1271 [02:46<04:01,  3.13it/s, training_loss=0.214]\u001b[A\n",
            "Epoch 3:  41%|████      | 516/1271 [02:46<04:01,  3.13it/s, training_loss=0.263]\u001b[A\n",
            "Epoch 3:  41%|████      | 517/1271 [02:46<04:00,  3.13it/s, training_loss=0.263]\u001b[A\n",
            "Epoch 3:  41%|████      | 517/1271 [02:46<04:00,  3.13it/s, training_loss=0.821]\u001b[A\n",
            "Epoch 3:  41%|████      | 518/1271 [02:46<04:00,  3.12it/s, training_loss=0.821]\u001b[A\n",
            "Epoch 3:  41%|████      | 518/1271 [02:47<04:00,  3.12it/s, training_loss=0.152]\u001b[A\n",
            "Epoch 3:  41%|████      | 519/1271 [02:47<04:00,  3.13it/s, training_loss=0.152]\u001b[A\n",
            "Epoch 3:  41%|████      | 519/1271 [02:47<04:00,  3.13it/s, training_loss=0.305]\u001b[A\n",
            "Epoch 3:  41%|████      | 520/1271 [02:47<03:59,  3.14it/s, training_loss=0.305]\u001b[A\n",
            "Epoch 3:  41%|████      | 520/1271 [02:47<03:59,  3.14it/s, training_loss=0.410]\u001b[A\n",
            "Epoch 3:  41%|████      | 521/1271 [02:47<04:01,  3.10it/s, training_loss=0.410]\u001b[A\n",
            "Epoch 3:  41%|████      | 521/1271 [02:48<04:01,  3.10it/s, training_loss=0.217]\u001b[A\n",
            "Epoch 3:  41%|████      | 522/1271 [02:48<04:00,  3.11it/s, training_loss=0.217]\u001b[A\n",
            "Epoch 3:  41%|████      | 522/1271 [02:48<04:00,  3.11it/s, training_loss=0.569]\u001b[A\n",
            "Epoch 3:  41%|████      | 523/1271 [02:48<04:00,  3.12it/s, training_loss=0.569]\u001b[A\n",
            "Epoch 3:  41%|████      | 523/1271 [02:48<04:00,  3.12it/s, training_loss=0.176]\u001b[A\n",
            "Epoch 3:  41%|████      | 524/1271 [02:48<03:59,  3.12it/s, training_loss=0.176]\u001b[A\n",
            "Epoch 3:  41%|████      | 524/1271 [02:49<03:59,  3.12it/s, training_loss=0.103]\u001b[A\n",
            "Epoch 3:  41%|████▏     | 525/1271 [02:49<04:00,  3.11it/s, training_loss=0.103]\u001b[A\n",
            "Epoch 3:  41%|████▏     | 525/1271 [02:49<04:00,  3.11it/s, training_loss=0.101]\u001b[A\n",
            "Epoch 3:  41%|████▏     | 526/1271 [02:49<03:59,  3.11it/s, training_loss=0.101]\u001b[A\n",
            "Epoch 3:  41%|████▏     | 526/1271 [02:49<03:59,  3.11it/s, training_loss=0.445]\u001b[A\n",
            "Epoch 3:  41%|████▏     | 527/1271 [02:49<03:58,  3.12it/s, training_loss=0.445]\u001b[A\n",
            "Epoch 3:  41%|████▏     | 527/1271 [02:50<03:58,  3.12it/s, training_loss=0.404]\u001b[A\n",
            "Epoch 3:  42%|████▏     | 528/1271 [02:50<03:59,  3.10it/s, training_loss=0.404]\u001b[A\n",
            "Epoch 3:  42%|████▏     | 528/1271 [02:50<03:59,  3.10it/s, training_loss=0.295]\u001b[A\n",
            "Epoch 3:  42%|████▏     | 529/1271 [02:50<03:58,  3.11it/s, training_loss=0.295]\u001b[A\n",
            "Epoch 3:  42%|████▏     | 529/1271 [02:50<03:58,  3.11it/s, training_loss=0.134]\u001b[A\n",
            "Epoch 3:  42%|████▏     | 530/1271 [02:50<03:57,  3.12it/s, training_loss=0.134]\u001b[A\n",
            "Epoch 3:  42%|████▏     | 530/1271 [02:50<03:57,  3.12it/s, training_loss=0.692]\u001b[A\n",
            "Epoch 3:  42%|████▏     | 531/1271 [02:50<03:58,  3.10it/s, training_loss=0.692]\u001b[A\n",
            "Epoch 3:  42%|████▏     | 531/1271 [02:51<03:58,  3.10it/s, training_loss=0.377]\u001b[A\n",
            "Epoch 3:  42%|████▏     | 532/1271 [02:51<03:58,  3.10it/s, training_loss=0.377]\u001b[A\n",
            "Epoch 3:  42%|████▏     | 532/1271 [02:51<03:58,  3.10it/s, training_loss=0.317]\u001b[A\n",
            "Epoch 3:  42%|████▏     | 533/1271 [02:51<03:56,  3.12it/s, training_loss=0.317]\u001b[A\n",
            "Epoch 3:  42%|████▏     | 533/1271 [02:51<03:56,  3.12it/s, training_loss=0.242]\u001b[A\n",
            "Epoch 3:  42%|████▏     | 534/1271 [02:51<03:56,  3.12it/s, training_loss=0.242]\u001b[A\n",
            "Epoch 3:  42%|████▏     | 534/1271 [02:52<03:56,  3.12it/s, training_loss=0.345]\u001b[A\n",
            "Epoch 3:  42%|████▏     | 535/1271 [02:52<03:58,  3.09it/s, training_loss=0.345]\u001b[A\n",
            "Epoch 3:  42%|████▏     | 535/1271 [02:52<03:58,  3.09it/s, training_loss=0.140]\u001b[A\n",
            "Epoch 3:  42%|████▏     | 536/1271 [02:52<03:59,  3.07it/s, training_loss=0.140]\u001b[A\n",
            "Epoch 3:  42%|████▏     | 536/1271 [02:52<03:59,  3.07it/s, training_loss=1.089]\u001b[A\n",
            "Epoch 3:  42%|████▏     | 537/1271 [02:52<03:58,  3.08it/s, training_loss=1.089]\u001b[A\n",
            "Epoch 3:  42%|████▏     | 537/1271 [02:53<03:58,  3.08it/s, training_loss=0.104]\u001b[A\n",
            "Epoch 3:  42%|████▏     | 538/1271 [02:53<03:58,  3.08it/s, training_loss=0.104]\u001b[A\n",
            "Epoch 3:  42%|████▏     | 538/1271 [02:53<03:58,  3.08it/s, training_loss=0.242]\u001b[A\n",
            "Epoch 3:  42%|████▏     | 539/1271 [02:53<03:57,  3.08it/s, training_loss=0.242]\u001b[A\n",
            "Epoch 3:  42%|████▏     | 539/1271 [02:53<03:57,  3.08it/s, training_loss=0.398]\u001b[A\n",
            "Epoch 3:  42%|████▏     | 540/1271 [02:53<03:55,  3.11it/s, training_loss=0.398]\u001b[A\n",
            "Epoch 3:  42%|████▏     | 540/1271 [02:54<03:55,  3.11it/s, training_loss=0.239]\u001b[A\n",
            "Epoch 3:  43%|████▎     | 541/1271 [02:54<03:54,  3.11it/s, training_loss=0.239]\u001b[A\n",
            "Epoch 3:  43%|████▎     | 541/1271 [02:54<03:54,  3.11it/s, training_loss=0.729]\u001b[A\n",
            "Epoch 3:  43%|████▎     | 542/1271 [02:54<03:54,  3.11it/s, training_loss=0.729]\u001b[A\n",
            "Epoch 3:  43%|████▎     | 542/1271 [02:54<03:54,  3.11it/s, training_loss=0.610]\u001b[A\n",
            "Epoch 3:  43%|████▎     | 543/1271 [02:54<03:52,  3.13it/s, training_loss=0.610]\u001b[A\n",
            "Epoch 3:  43%|████▎     | 543/1271 [02:55<03:52,  3.13it/s, training_loss=0.013]\u001b[A\n",
            "Epoch 3:  43%|████▎     | 544/1271 [02:55<03:51,  3.14it/s, training_loss=0.013]\u001b[A\n",
            "Epoch 3:  43%|████▎     | 544/1271 [02:55<03:51,  3.14it/s, training_loss=0.145]\u001b[A\n",
            "Epoch 3:  43%|████▎     | 545/1271 [02:55<03:52,  3.12it/s, training_loss=0.145]\u001b[A\n",
            "Epoch 3:  43%|████▎     | 545/1271 [02:55<03:52,  3.12it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 3:  43%|████▎     | 546/1271 [02:55<03:51,  3.13it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 3:  43%|████▎     | 546/1271 [02:56<03:51,  3.13it/s, training_loss=0.281]\u001b[A\n",
            "Epoch 3:  43%|████▎     | 547/1271 [02:56<03:53,  3.11it/s, training_loss=0.281]\u001b[A\n",
            "Epoch 3:  43%|████▎     | 547/1271 [02:56<03:53,  3.11it/s, training_loss=0.291]\u001b[A\n",
            "Epoch 3:  43%|████▎     | 548/1271 [02:56<03:51,  3.12it/s, training_loss=0.291]\u001b[A\n",
            "Epoch 3:  43%|████▎     | 548/1271 [02:56<03:51,  3.12it/s, training_loss=0.205]\u001b[A\n",
            "Epoch 3:  43%|████▎     | 549/1271 [02:56<03:50,  3.13it/s, training_loss=0.205]\u001b[A\n",
            "Epoch 3:  43%|████▎     | 549/1271 [02:57<03:50,  3.13it/s, training_loss=0.424]\u001b[A\n",
            "Epoch 3:  43%|████▎     | 550/1271 [02:57<03:50,  3.13it/s, training_loss=0.424]\u001b[A\n",
            "Epoch 3:  43%|████▎     | 550/1271 [02:57<03:50,  3.13it/s, training_loss=0.042]\u001b[A\n",
            "Epoch 3:  43%|████▎     | 551/1271 [02:57<03:50,  3.12it/s, training_loss=0.042]\u001b[A\n",
            "Epoch 3:  43%|████▎     | 551/1271 [02:57<03:50,  3.12it/s, training_loss=0.579]\u001b[A\n",
            "Epoch 3:  43%|████▎     | 552/1271 [02:57<03:50,  3.12it/s, training_loss=0.579]\u001b[A\n",
            "Epoch 3:  43%|████▎     | 552/1271 [02:58<03:50,  3.12it/s, training_loss=0.470]\u001b[A\n",
            "Epoch 3:  44%|████▎     | 553/1271 [02:58<03:48,  3.14it/s, training_loss=0.470]\u001b[A\n",
            "Epoch 3:  44%|████▎     | 553/1271 [02:58<03:48,  3.14it/s, training_loss=0.858]\u001b[A\n",
            "Epoch 3:  44%|████▎     | 554/1271 [02:58<03:48,  3.14it/s, training_loss=0.858]\u001b[A\n",
            "Epoch 3:  44%|████▎     | 554/1271 [02:58<03:48,  3.14it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 3:  44%|████▎     | 555/1271 [02:58<03:47,  3.14it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 3:  44%|████▎     | 555/1271 [02:58<03:47,  3.14it/s, training_loss=0.089]\u001b[A\n",
            "Epoch 3:  44%|████▎     | 556/1271 [02:59<03:48,  3.13it/s, training_loss=0.089]\u001b[A\n",
            "Epoch 3:  44%|████▎     | 556/1271 [02:59<03:48,  3.13it/s, training_loss=0.122]\u001b[A\n",
            "Epoch 3:  44%|████▍     | 557/1271 [02:59<03:49,  3.12it/s, training_loss=0.122]\u001b[A\n",
            "Epoch 3:  44%|████▍     | 557/1271 [02:59<03:49,  3.12it/s, training_loss=0.187]\u001b[A\n",
            "Epoch 3:  44%|████▍     | 558/1271 [02:59<03:48,  3.13it/s, training_loss=0.187]\u001b[A\n",
            "Epoch 3:  44%|████▍     | 558/1271 [02:59<03:48,  3.13it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 3:  44%|████▍     | 559/1271 [02:59<03:47,  3.13it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 3:  44%|████▍     | 559/1271 [03:00<03:47,  3.13it/s, training_loss=0.220]\u001b[A\n",
            "Epoch 3:  44%|████▍     | 560/1271 [03:00<03:47,  3.12it/s, training_loss=0.220]\u001b[A\n",
            "Epoch 3:  44%|████▍     | 560/1271 [03:00<03:47,  3.12it/s, training_loss=0.511]\u001b[A\n",
            "Epoch 3:  44%|████▍     | 561/1271 [03:00<03:47,  3.13it/s, training_loss=0.511]\u001b[A\n",
            "Epoch 3:  44%|████▍     | 561/1271 [03:00<03:47,  3.13it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 3:  44%|████▍     | 562/1271 [03:00<03:46,  3.13it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 3:  44%|████▍     | 562/1271 [03:01<03:46,  3.13it/s, training_loss=0.196]\u001b[A\n",
            "Epoch 3:  44%|████▍     | 563/1271 [03:01<03:46,  3.13it/s, training_loss=0.196]\u001b[A\n",
            "Epoch 3:  44%|████▍     | 563/1271 [03:01<03:46,  3.13it/s, training_loss=0.255]\u001b[A\n",
            "Epoch 3:  44%|████▍     | 564/1271 [03:01<03:45,  3.13it/s, training_loss=0.255]\u001b[A\n",
            "Epoch 3:  44%|████▍     | 564/1271 [03:01<03:45,  3.13it/s, training_loss=0.124]\u001b[A\n",
            "Epoch 3:  44%|████▍     | 565/1271 [03:01<03:45,  3.13it/s, training_loss=0.124]\u001b[A\n",
            "Epoch 3:  44%|████▍     | 565/1271 [03:02<03:45,  3.13it/s, training_loss=0.214]\u001b[A\n",
            "Epoch 3:  45%|████▍     | 566/1271 [03:02<03:45,  3.12it/s, training_loss=0.214]\u001b[A\n",
            "Epoch 3:  45%|████▍     | 566/1271 [03:02<03:45,  3.12it/s, training_loss=0.615]\u001b[A\n",
            "Epoch 3:  45%|████▍     | 567/1271 [03:02<03:46,  3.10it/s, training_loss=0.615]\u001b[A\n",
            "Epoch 3:  45%|████▍     | 567/1271 [03:02<03:46,  3.10it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 3:  45%|████▍     | 568/1271 [03:02<03:46,  3.10it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 3:  45%|████▍     | 568/1271 [03:03<03:46,  3.10it/s, training_loss=0.034]\u001b[A\n",
            "Epoch 3:  45%|████▍     | 569/1271 [03:03<03:46,  3.10it/s, training_loss=0.034]\u001b[A\n",
            "Epoch 3:  45%|████▍     | 569/1271 [03:03<03:46,  3.10it/s, training_loss=0.365]\u001b[A\n",
            "Epoch 3:  45%|████▍     | 570/1271 [03:03<03:45,  3.11it/s, training_loss=0.365]\u001b[A\n",
            "Epoch 3:  45%|████▍     | 570/1271 [03:03<03:45,  3.11it/s, training_loss=0.111]\u001b[A\n",
            "Epoch 3:  45%|████▍     | 571/1271 [03:03<03:45,  3.11it/s, training_loss=0.111]\u001b[A\n",
            "Epoch 3:  45%|████▍     | 571/1271 [03:04<03:45,  3.11it/s, training_loss=0.294]\u001b[A\n",
            "Epoch 3:  45%|████▌     | 572/1271 [03:04<03:44,  3.11it/s, training_loss=0.294]\u001b[A\n",
            "Epoch 3:  45%|████▌     | 572/1271 [03:04<03:44,  3.11it/s, training_loss=0.183]\u001b[A\n",
            "Epoch 3:  45%|████▌     | 573/1271 [03:04<03:43,  3.12it/s, training_loss=0.183]\u001b[A\n",
            "Epoch 3:  45%|████▌     | 573/1271 [03:04<03:43,  3.12it/s, training_loss=1.129]\u001b[A\n",
            "Epoch 3:  45%|████▌     | 574/1271 [03:04<03:45,  3.10it/s, training_loss=1.129]\u001b[A\n",
            "Epoch 3:  45%|████▌     | 574/1271 [03:05<03:45,  3.10it/s, training_loss=0.188]\u001b[A\n",
            "Epoch 3:  45%|████▌     | 575/1271 [03:05<03:44,  3.10it/s, training_loss=0.188]\u001b[A\n",
            "Epoch 3:  45%|████▌     | 575/1271 [03:05<03:44,  3.10it/s, training_loss=0.122]\u001b[A\n",
            "Epoch 3:  45%|████▌     | 576/1271 [03:05<03:43,  3.11it/s, training_loss=0.122]\u001b[A\n",
            "Epoch 3:  45%|████▌     | 576/1271 [03:05<03:43,  3.11it/s, training_loss=0.023]\u001b[A\n",
            "Epoch 3:  45%|████▌     | 577/1271 [03:05<03:43,  3.11it/s, training_loss=0.023]\u001b[A\n",
            "Epoch 3:  45%|████▌     | 577/1271 [03:06<03:43,  3.11it/s, training_loss=0.201]\u001b[A\n",
            "Epoch 3:  45%|████▌     | 578/1271 [03:06<03:42,  3.11it/s, training_loss=0.201]\u001b[A\n",
            "Epoch 3:  45%|████▌     | 578/1271 [03:06<03:42,  3.11it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 3:  46%|████▌     | 579/1271 [03:06<03:41,  3.13it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 3:  46%|████▌     | 579/1271 [03:06<03:41,  3.13it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 3:  46%|████▌     | 580/1271 [03:06<03:40,  3.14it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 3:  46%|████▌     | 580/1271 [03:07<03:40,  3.14it/s, training_loss=0.944]\u001b[A\n",
            "Epoch 3:  46%|████▌     | 581/1271 [03:07<03:40,  3.13it/s, training_loss=0.944]\u001b[A\n",
            "Epoch 3:  46%|████▌     | 581/1271 [03:07<03:40,  3.13it/s, training_loss=0.288]\u001b[A\n",
            "Epoch 3:  46%|████▌     | 582/1271 [03:07<03:40,  3.13it/s, training_loss=0.288]\u001b[A\n",
            "Epoch 3:  46%|████▌     | 582/1271 [03:07<03:40,  3.13it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 3:  46%|████▌     | 583/1271 [03:07<03:39,  3.13it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 3:  46%|████▌     | 583/1271 [03:07<03:39,  3.13it/s, training_loss=0.124]\u001b[A\n",
            "Epoch 3:  46%|████▌     | 584/1271 [03:07<03:40,  3.11it/s, training_loss=0.124]\u001b[A\n",
            "Epoch 3:  46%|████▌     | 584/1271 [03:08<03:40,  3.11it/s, training_loss=0.092]\u001b[A\n",
            "Epoch 3:  46%|████▌     | 585/1271 [03:08<03:40,  3.10it/s, training_loss=0.092]\u001b[A\n",
            "Epoch 3:  46%|████▌     | 585/1271 [03:08<03:40,  3.10it/s, training_loss=0.533]\u001b[A\n",
            "Epoch 3:  46%|████▌     | 586/1271 [03:08<03:40,  3.11it/s, training_loss=0.533]\u001b[A\n",
            "Epoch 3:  46%|████▌     | 586/1271 [03:08<03:40,  3.11it/s, training_loss=0.477]\u001b[A\n",
            "Epoch 3:  46%|████▌     | 587/1271 [03:08<03:38,  3.12it/s, training_loss=0.477]\u001b[A\n",
            "Epoch 3:  46%|████▌     | 587/1271 [03:09<03:38,  3.12it/s, training_loss=0.591]\u001b[A\n",
            "Epoch 3:  46%|████▋     | 588/1271 [03:09<03:38,  3.13it/s, training_loss=0.591]\u001b[A\n",
            "Epoch 3:  46%|████▋     | 588/1271 [03:09<03:38,  3.13it/s, training_loss=0.088]\u001b[A\n",
            "Epoch 3:  46%|████▋     | 589/1271 [03:09<03:38,  3.13it/s, training_loss=0.088]\u001b[A\n",
            "Epoch 3:  46%|████▋     | 589/1271 [03:09<03:38,  3.13it/s, training_loss=0.457]\u001b[A\n",
            "Epoch 3:  46%|████▋     | 590/1271 [03:09<03:38,  3.12it/s, training_loss=0.457]\u001b[A\n",
            "Epoch 3:  46%|████▋     | 590/1271 [03:10<03:38,  3.12it/s, training_loss=0.245]\u001b[A\n",
            "Epoch 3:  46%|████▋     | 591/1271 [03:10<03:38,  3.11it/s, training_loss=0.245]\u001b[A\n",
            "Epoch 3:  46%|████▋     | 591/1271 [03:10<03:38,  3.11it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 3:  47%|████▋     | 592/1271 [03:10<03:37,  3.12it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 3:  47%|████▋     | 592/1271 [03:10<03:37,  3.12it/s, training_loss=0.256]\u001b[A\n",
            "Epoch 3:  47%|████▋     | 593/1271 [03:10<03:36,  3.12it/s, training_loss=0.256]\u001b[A\n",
            "Epoch 3:  47%|████▋     | 593/1271 [03:11<03:36,  3.12it/s, training_loss=0.243]\u001b[A\n",
            "Epoch 3:  47%|████▋     | 594/1271 [03:11<03:36,  3.13it/s, training_loss=0.243]\u001b[A\n",
            "Epoch 3:  47%|████▋     | 594/1271 [03:11<03:36,  3.13it/s, training_loss=0.126]\u001b[A\n",
            "Epoch 3:  47%|████▋     | 595/1271 [03:11<03:35,  3.13it/s, training_loss=0.126]\u001b[A\n",
            "Epoch 3:  47%|████▋     | 595/1271 [03:11<03:35,  3.13it/s, training_loss=0.332]\u001b[A\n",
            "Epoch 3:  47%|████▋     | 596/1271 [03:11<03:36,  3.12it/s, training_loss=0.332]\u001b[A\n",
            "Epoch 3:  47%|████▋     | 596/1271 [03:12<03:36,  3.12it/s, training_loss=0.440]\u001b[A\n",
            "Epoch 3:  47%|████▋     | 597/1271 [03:12<03:35,  3.13it/s, training_loss=0.440]\u001b[A\n",
            "Epoch 3:  47%|████▋     | 597/1271 [03:12<03:35,  3.13it/s, training_loss=0.552]\u001b[A\n",
            "Epoch 3:  47%|████▋     | 598/1271 [03:12<03:35,  3.13it/s, training_loss=0.552]\u001b[A\n",
            "Epoch 3:  47%|████▋     | 598/1271 [03:12<03:35,  3.13it/s, training_loss=0.495]\u001b[A\n",
            "Epoch 3:  47%|████▋     | 599/1271 [03:12<03:35,  3.13it/s, training_loss=0.495]\u001b[A\n",
            "Epoch 3:  47%|████▋     | 599/1271 [03:13<03:35,  3.13it/s, training_loss=0.099]\u001b[A\n",
            "Epoch 3:  47%|████▋     | 600/1271 [03:13<03:34,  3.13it/s, training_loss=0.099]\u001b[A\n",
            "Epoch 3:  47%|████▋     | 600/1271 [03:13<03:34,  3.13it/s, training_loss=0.227]\u001b[A\n",
            "Epoch 3:  47%|████▋     | 601/1271 [03:13<03:34,  3.12it/s, training_loss=0.227]\u001b[A\n",
            "Epoch 3:  47%|████▋     | 601/1271 [03:13<03:34,  3.12it/s, training_loss=0.239]\u001b[A\n",
            "Epoch 3:  47%|████▋     | 602/1271 [03:13<03:34,  3.13it/s, training_loss=0.239]\u001b[A\n",
            "Epoch 3:  47%|████▋     | 602/1271 [03:14<03:34,  3.13it/s, training_loss=0.345]\u001b[A\n",
            "Epoch 3:  47%|████▋     | 603/1271 [03:14<03:35,  3.10it/s, training_loss=0.345]\u001b[A\n",
            "Epoch 3:  47%|████▋     | 603/1271 [03:14<03:35,  3.10it/s, training_loss=0.063]\u001b[A\n",
            "Epoch 3:  48%|████▊     | 604/1271 [03:14<03:34,  3.11it/s, training_loss=0.063]\u001b[A\n",
            "Epoch 3:  48%|████▊     | 604/1271 [03:14<03:34,  3.11it/s, training_loss=0.099]\u001b[A\n",
            "Epoch 3:  48%|████▊     | 605/1271 [03:14<03:34,  3.11it/s, training_loss=0.099]\u001b[A\n",
            "Epoch 3:  48%|████▊     | 605/1271 [03:15<03:34,  3.11it/s, training_loss=0.095]\u001b[A\n",
            "Epoch 3:  48%|████▊     | 606/1271 [03:15<03:34,  3.11it/s, training_loss=0.095]\u001b[A\n",
            "Epoch 3:  48%|████▊     | 606/1271 [03:15<03:34,  3.11it/s, training_loss=0.256]\u001b[A\n",
            "Epoch 3:  48%|████▊     | 607/1271 [03:15<03:33,  3.11it/s, training_loss=0.256]\u001b[A\n",
            "Epoch 3:  48%|████▊     | 607/1271 [03:15<03:33,  3.11it/s, training_loss=0.772]\u001b[A\n",
            "Epoch 3:  48%|████▊     | 608/1271 [03:15<03:32,  3.12it/s, training_loss=0.772]\u001b[A\n",
            "Epoch 3:  48%|████▊     | 608/1271 [03:15<03:32,  3.12it/s, training_loss=0.391]\u001b[A\n",
            "Epoch 3:  48%|████▊     | 609/1271 [03:15<03:31,  3.13it/s, training_loss=0.391]\u001b[A\n",
            "Epoch 3:  48%|████▊     | 609/1271 [03:16<03:31,  3.13it/s, training_loss=0.512]\u001b[A\n",
            "Epoch 3:  48%|████▊     | 610/1271 [03:16<03:33,  3.10it/s, training_loss=0.512]\u001b[A\n",
            "Epoch 3:  48%|████▊     | 610/1271 [03:16<03:33,  3.10it/s, training_loss=0.237]\u001b[A\n",
            "Epoch 3:  48%|████▊     | 611/1271 [03:16<03:32,  3.10it/s, training_loss=0.237]\u001b[A\n",
            "Epoch 3:  48%|████▊     | 611/1271 [03:16<03:32,  3.10it/s, training_loss=0.758]\u001b[A\n",
            "Epoch 3:  48%|████▊     | 612/1271 [03:16<03:31,  3.11it/s, training_loss=0.758]\u001b[A\n",
            "Epoch 3:  48%|████▊     | 612/1271 [03:17<03:31,  3.11it/s, training_loss=0.446]\u001b[A\n",
            "Epoch 3:  48%|████▊     | 613/1271 [03:17<03:30,  3.13it/s, training_loss=0.446]\u001b[A\n",
            "Epoch 3:  48%|████▊     | 613/1271 [03:17<03:30,  3.13it/s, training_loss=0.158]\u001b[A\n",
            "Epoch 3:  48%|████▊     | 614/1271 [03:17<03:31,  3.11it/s, training_loss=0.158]\u001b[A\n",
            "Epoch 3:  48%|████▊     | 614/1271 [03:17<03:31,  3.11it/s, training_loss=0.034]\u001b[A\n",
            "Epoch 3:  48%|████▊     | 615/1271 [03:17<03:30,  3.12it/s, training_loss=0.034]\u001b[A\n",
            "Epoch 3:  48%|████▊     | 615/1271 [03:18<03:30,  3.12it/s, training_loss=0.309]\u001b[A\n",
            "Epoch 3:  48%|████▊     | 616/1271 [03:18<03:30,  3.11it/s, training_loss=0.309]\u001b[A\n",
            "Epoch 3:  48%|████▊     | 616/1271 [03:18<03:30,  3.11it/s, training_loss=0.212]\u001b[A\n",
            "Epoch 3:  49%|████▊     | 617/1271 [03:18<03:29,  3.12it/s, training_loss=0.212]\u001b[A\n",
            "Epoch 3:  49%|████▊     | 617/1271 [03:18<03:29,  3.12it/s, training_loss=0.515]\u001b[A\n",
            "Epoch 3:  49%|████▊     | 618/1271 [03:18<03:30,  3.11it/s, training_loss=0.515]\u001b[A\n",
            "Epoch 3:  49%|████▊     | 618/1271 [03:19<03:30,  3.11it/s, training_loss=0.033]\u001b[A\n",
            "Epoch 3:  49%|████▊     | 619/1271 [03:19<03:30,  3.10it/s, training_loss=0.033]\u001b[A\n",
            "Epoch 3:  49%|████▊     | 619/1271 [03:19<03:30,  3.10it/s, training_loss=0.263]\u001b[A\n",
            "Epoch 3:  49%|████▉     | 620/1271 [03:19<03:29,  3.11it/s, training_loss=0.263]\u001b[A\n",
            "Epoch 3:  49%|████▉     | 620/1271 [03:19<03:29,  3.11it/s, training_loss=0.150]\u001b[A\n",
            "Epoch 3:  49%|████▉     | 621/1271 [03:19<03:30,  3.09it/s, training_loss=0.150]\u001b[A\n",
            "Epoch 3:  49%|████▉     | 621/1271 [03:20<03:30,  3.09it/s, training_loss=0.397]\u001b[A\n",
            "Epoch 3:  49%|████▉     | 622/1271 [03:20<03:29,  3.10it/s, training_loss=0.397]\u001b[A\n",
            "Epoch 3:  49%|████▉     | 622/1271 [03:20<03:29,  3.10it/s, training_loss=0.233]\u001b[A\n",
            "Epoch 3:  49%|████▉     | 623/1271 [03:20<03:28,  3.11it/s, training_loss=0.233]\u001b[A\n",
            "Epoch 3:  49%|████▉     | 623/1271 [03:20<03:28,  3.11it/s, training_loss=0.327]\u001b[A\n",
            "Epoch 3:  49%|████▉     | 624/1271 [03:20<03:27,  3.12it/s, training_loss=0.327]\u001b[A\n",
            "Epoch 3:  49%|████▉     | 624/1271 [03:21<03:27,  3.12it/s, training_loss=0.025]\u001b[A\n",
            "Epoch 3:  49%|████▉     | 625/1271 [03:21<03:26,  3.13it/s, training_loss=0.025]\u001b[A\n",
            "Epoch 3:  49%|████▉     | 625/1271 [03:21<03:26,  3.13it/s, training_loss=0.420]\u001b[A\n",
            "Epoch 3:  49%|████▉     | 626/1271 [03:21<03:26,  3.13it/s, training_loss=0.420]\u001b[A\n",
            "Epoch 3:  49%|████▉     | 626/1271 [03:21<03:26,  3.13it/s, training_loss=0.120]\u001b[A\n",
            "Epoch 3:  49%|████▉     | 627/1271 [03:21<03:25,  3.13it/s, training_loss=0.120]\u001b[A\n",
            "Epoch 3:  49%|████▉     | 627/1271 [03:22<03:25,  3.13it/s, training_loss=0.238]\u001b[A\n",
            "Epoch 3:  49%|████▉     | 628/1271 [03:22<03:26,  3.12it/s, training_loss=0.238]\u001b[A\n",
            "Epoch 3:  49%|████▉     | 628/1271 [03:22<03:26,  3.12it/s, training_loss=0.166]\u001b[A\n",
            "Epoch 3:  49%|████▉     | 629/1271 [03:22<03:25,  3.12it/s, training_loss=0.166]\u001b[A\n",
            "Epoch 3:  49%|████▉     | 629/1271 [03:22<03:25,  3.12it/s, training_loss=0.377]\u001b[A\n",
            "Epoch 3:  50%|████▉     | 630/1271 [03:22<03:25,  3.12it/s, training_loss=0.377]\u001b[A\n",
            "Epoch 3:  50%|████▉     | 630/1271 [03:23<03:25,  3.12it/s, training_loss=0.337]\u001b[A\n",
            "Epoch 3:  50%|████▉     | 631/1271 [03:23<03:25,  3.11it/s, training_loss=0.337]\u001b[A\n",
            "Epoch 3:  50%|████▉     | 631/1271 [03:23<03:25,  3.11it/s, training_loss=0.070]\u001b[A\n",
            "Epoch 3:  50%|████▉     | 632/1271 [03:23<03:26,  3.10it/s, training_loss=0.070]\u001b[A\n",
            "Epoch 3:  50%|████▉     | 632/1271 [03:23<03:26,  3.10it/s, training_loss=0.147]\u001b[A\n",
            "Epoch 3:  50%|████▉     | 633/1271 [03:23<03:26,  3.09it/s, training_loss=0.147]\u001b[A\n",
            "Epoch 3:  50%|████▉     | 633/1271 [03:24<03:26,  3.09it/s, training_loss=0.093]\u001b[A\n",
            "Epoch 3:  50%|████▉     | 634/1271 [03:24<03:26,  3.08it/s, training_loss=0.093]\u001b[A\n",
            "Epoch 3:  50%|████▉     | 634/1271 [03:24<03:26,  3.08it/s, training_loss=0.442]\u001b[A\n",
            "Epoch 3:  50%|████▉     | 635/1271 [03:24<03:25,  3.10it/s, training_loss=0.442]\u001b[A\n",
            "Epoch 3:  50%|████▉     | 635/1271 [03:24<03:25,  3.10it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 3:  50%|█████     | 636/1271 [03:24<03:24,  3.11it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 3:  50%|█████     | 636/1271 [03:24<03:24,  3.11it/s, training_loss=0.312]\u001b[A\n",
            "Epoch 3:  50%|█████     | 637/1271 [03:25<03:23,  3.11it/s, training_loss=0.312]\u001b[A\n",
            "Epoch 3:  50%|█████     | 637/1271 [03:25<03:23,  3.11it/s, training_loss=0.393]\u001b[A\n",
            "Epoch 3:  50%|█████     | 638/1271 [03:25<03:23,  3.11it/s, training_loss=0.393]\u001b[A\n",
            "Epoch 3:  50%|█████     | 638/1271 [03:25<03:23,  3.11it/s, training_loss=0.167]\u001b[A\n",
            "Epoch 3:  50%|█████     | 639/1271 [03:25<03:23,  3.10it/s, training_loss=0.167]\u001b[A\n",
            "Epoch 3:  50%|█████     | 639/1271 [03:25<03:23,  3.10it/s, training_loss=0.605]\u001b[A\n",
            "Epoch 3:  50%|█████     | 640/1271 [03:25<03:23,  3.11it/s, training_loss=0.605]\u001b[A\n",
            "Epoch 3:  50%|█████     | 640/1271 [03:26<03:23,  3.11it/s, training_loss=0.341]\u001b[A\n",
            "Epoch 3:  50%|█████     | 641/1271 [03:26<03:22,  3.12it/s, training_loss=0.341]\u001b[A\n",
            "Epoch 3:  50%|█████     | 641/1271 [03:26<03:22,  3.12it/s, training_loss=0.288]\u001b[A\n",
            "Epoch 3:  51%|█████     | 642/1271 [03:26<03:21,  3.12it/s, training_loss=0.288]\u001b[A\n",
            "Epoch 3:  51%|█████     | 642/1271 [03:26<03:21,  3.12it/s, training_loss=0.271]\u001b[A\n",
            "Epoch 3:  51%|█████     | 643/1271 [03:26<03:22,  3.10it/s, training_loss=0.271]\u001b[A\n",
            "Epoch 3:  51%|█████     | 643/1271 [03:27<03:22,  3.10it/s, training_loss=0.582]\u001b[A\n",
            "Epoch 3:  51%|█████     | 644/1271 [03:27<03:21,  3.11it/s, training_loss=0.582]\u001b[A\n",
            "Epoch 3:  51%|█████     | 644/1271 [03:27<03:21,  3.11it/s, training_loss=0.296]\u001b[A\n",
            "Epoch 3:  51%|█████     | 645/1271 [03:27<03:20,  3.12it/s, training_loss=0.296]\u001b[A\n",
            "Epoch 3:  51%|█████     | 645/1271 [03:27<03:20,  3.12it/s, training_loss=0.423]\u001b[A\n",
            "Epoch 3:  51%|█████     | 646/1271 [03:27<03:19,  3.13it/s, training_loss=0.423]\u001b[A\n",
            "Epoch 3:  51%|█████     | 646/1271 [03:28<03:19,  3.13it/s, training_loss=0.374]\u001b[A\n",
            "Epoch 3:  51%|█████     | 647/1271 [03:28<03:19,  3.13it/s, training_loss=0.374]\u001b[A\n",
            "Epoch 3:  51%|█████     | 647/1271 [03:28<03:19,  3.13it/s, training_loss=0.243]\u001b[A\n",
            "Epoch 3:  51%|█████     | 648/1271 [03:28<03:21,  3.09it/s, training_loss=0.243]\u001b[A\n",
            "Epoch 3:  51%|█████     | 648/1271 [03:28<03:21,  3.09it/s, training_loss=0.196]\u001b[A\n",
            "Epoch 3:  51%|█████     | 649/1271 [03:28<03:20,  3.10it/s, training_loss=0.196]\u001b[A\n",
            "Epoch 3:  51%|█████     | 649/1271 [03:29<03:20,  3.10it/s, training_loss=0.131]\u001b[A\n",
            "Epoch 3:  51%|█████     | 650/1271 [03:29<03:20,  3.09it/s, training_loss=0.131]\u001b[A\n",
            "Epoch 3:  51%|█████     | 650/1271 [03:29<03:20,  3.09it/s, training_loss=0.085]\u001b[A\n",
            "Epoch 3:  51%|█████     | 651/1271 [03:29<03:20,  3.10it/s, training_loss=0.085]\u001b[A\n",
            "Epoch 3:  51%|█████     | 651/1271 [03:29<03:20,  3.10it/s, training_loss=0.129]\u001b[A\n",
            "Epoch 3:  51%|█████▏    | 652/1271 [03:29<03:19,  3.11it/s, training_loss=0.129]\u001b[A\n",
            "Epoch 3:  51%|█████▏    | 652/1271 [03:30<03:19,  3.11it/s, training_loss=0.488]\u001b[A\n",
            "Epoch 3:  51%|█████▏    | 653/1271 [03:30<03:20,  3.09it/s, training_loss=0.488]\u001b[A\n",
            "Epoch 3:  51%|█████▏    | 653/1271 [03:30<03:20,  3.09it/s, training_loss=0.226]\u001b[A\n",
            "Epoch 3:  51%|█████▏    | 654/1271 [03:30<03:19,  3.09it/s, training_loss=0.226]\u001b[A\n",
            "Epoch 3:  51%|█████▏    | 654/1271 [03:30<03:19,  3.09it/s, training_loss=0.197]\u001b[A\n",
            "Epoch 3:  52%|█████▏    | 655/1271 [03:30<03:20,  3.07it/s, training_loss=0.197]\u001b[A\n",
            "Epoch 3:  52%|█████▏    | 655/1271 [03:31<03:20,  3.07it/s, training_loss=0.326]\u001b[A\n",
            "Epoch 3:  52%|█████▏    | 656/1271 [03:31<03:19,  3.08it/s, training_loss=0.326]\u001b[A\n",
            "Epoch 3:  52%|█████▏    | 656/1271 [03:31<03:19,  3.08it/s, training_loss=0.068]\u001b[A\n",
            "Epoch 3:  52%|█████▏    | 657/1271 [03:31<03:18,  3.09it/s, training_loss=0.068]\u001b[A\n",
            "Epoch 3:  52%|█████▏    | 657/1271 [03:31<03:18,  3.09it/s, training_loss=0.516]\u001b[A\n",
            "Epoch 3:  52%|█████▏    | 658/1271 [03:31<03:18,  3.09it/s, training_loss=0.516]\u001b[A\n",
            "Epoch 3:  52%|█████▏    | 658/1271 [03:32<03:18,  3.09it/s, training_loss=0.254]\u001b[A\n",
            "Epoch 3:  52%|█████▏    | 659/1271 [03:32<03:18,  3.09it/s, training_loss=0.254]\u001b[A\n",
            "Epoch 3:  52%|█████▏    | 659/1271 [03:32<03:18,  3.09it/s, training_loss=0.739]\u001b[A\n",
            "Epoch 3:  52%|█████▏    | 660/1271 [03:32<03:18,  3.07it/s, training_loss=0.739]\u001b[A\n",
            "Epoch 3:  52%|█████▏    | 660/1271 [03:32<03:18,  3.07it/s, training_loss=0.727]\u001b[A\n",
            "Epoch 3:  52%|█████▏    | 661/1271 [03:32<03:17,  3.09it/s, training_loss=0.727]\u001b[A\n",
            "Epoch 3:  52%|█████▏    | 661/1271 [03:33<03:17,  3.09it/s, training_loss=0.080]\u001b[A\n",
            "Epoch 3:  52%|█████▏    | 662/1271 [03:33<03:17,  3.09it/s, training_loss=0.080]\u001b[A\n",
            "Epoch 3:  52%|█████▏    | 662/1271 [03:33<03:17,  3.09it/s, training_loss=0.755]\u001b[A\n",
            "Epoch 3:  52%|█████▏    | 663/1271 [03:33<03:17,  3.09it/s, training_loss=0.755]\u001b[A\n",
            "Epoch 3:  52%|█████▏    | 663/1271 [03:33<03:17,  3.09it/s, training_loss=0.492]\u001b[A\n",
            "Epoch 3:  52%|█████▏    | 664/1271 [03:33<03:16,  3.10it/s, training_loss=0.492]\u001b[A\n",
            "Epoch 3:  52%|█████▏    | 664/1271 [03:34<03:16,  3.10it/s, training_loss=0.266]\u001b[A\n",
            "Epoch 3:  52%|█████▏    | 665/1271 [03:34<03:16,  3.09it/s, training_loss=0.266]\u001b[A\n",
            "Epoch 3:  52%|█████▏    | 665/1271 [03:34<03:16,  3.09it/s, training_loss=0.675]\u001b[A\n",
            "Epoch 3:  52%|█████▏    | 666/1271 [03:34<03:16,  3.08it/s, training_loss=0.675]\u001b[A\n",
            "Epoch 3:  52%|█████▏    | 666/1271 [03:34<03:16,  3.08it/s, training_loss=0.373]\u001b[A\n",
            "Epoch 3:  52%|█████▏    | 667/1271 [03:34<03:15,  3.08it/s, training_loss=0.373]\u001b[A\n",
            "Epoch 3:  52%|█████▏    | 667/1271 [03:35<03:15,  3.08it/s, training_loss=0.329]\u001b[A\n",
            "Epoch 3:  53%|█████▎    | 668/1271 [03:35<03:15,  3.09it/s, training_loss=0.329]\u001b[A\n",
            "Epoch 3:  53%|█████▎    | 668/1271 [03:35<03:15,  3.09it/s, training_loss=0.244]\u001b[A\n",
            "Epoch 3:  53%|█████▎    | 669/1271 [03:35<03:14,  3.10it/s, training_loss=0.244]\u001b[A\n",
            "Epoch 3:  53%|█████▎    | 669/1271 [03:35<03:14,  3.10it/s, training_loss=0.092]\u001b[A\n",
            "Epoch 3:  53%|█████▎    | 670/1271 [03:35<03:12,  3.12it/s, training_loss=0.092]\u001b[A\n",
            "Epoch 3:  53%|█████▎    | 670/1271 [03:35<03:12,  3.12it/s, training_loss=0.311]\u001b[A\n",
            "Epoch 3:  53%|█████▎    | 671/1271 [03:35<03:12,  3.12it/s, training_loss=0.311]\u001b[A\n",
            "Epoch 3:  53%|█████▎    | 671/1271 [03:36<03:12,  3.12it/s, training_loss=0.509]\u001b[A\n",
            "Epoch 3:  53%|█████▎    | 672/1271 [03:36<03:12,  3.11it/s, training_loss=0.509]\u001b[A\n",
            "Epoch 3:  53%|█████▎    | 672/1271 [03:36<03:12,  3.11it/s, training_loss=0.170]\u001b[A\n",
            "Epoch 3:  53%|█████▎    | 673/1271 [03:36<03:12,  3.11it/s, training_loss=0.170]\u001b[A\n",
            "Epoch 3:  53%|█████▎    | 673/1271 [03:36<03:12,  3.11it/s, training_loss=0.563]\u001b[A\n",
            "Epoch 3:  53%|█████▎    | 674/1271 [03:36<03:11,  3.11it/s, training_loss=0.563]\u001b[A\n",
            "Epoch 3:  53%|█████▎    | 674/1271 [03:37<03:11,  3.11it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 3:  53%|█████▎    | 675/1271 [03:37<03:10,  3.13it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 3:  53%|█████▎    | 675/1271 [03:37<03:10,  3.13it/s, training_loss=0.052]\u001b[A\n",
            "Epoch 3:  53%|█████▎    | 676/1271 [03:37<03:10,  3.12it/s, training_loss=0.052]\u001b[A\n",
            "Epoch 3:  53%|█████▎    | 676/1271 [03:37<03:10,  3.12it/s, training_loss=0.784]\u001b[A\n",
            "Epoch 3:  53%|█████▎    | 677/1271 [03:37<03:10,  3.12it/s, training_loss=0.784]\u001b[A\n",
            "Epoch 3:  53%|█████▎    | 677/1271 [03:38<03:10,  3.12it/s, training_loss=0.069]\u001b[A\n",
            "Epoch 3:  53%|█████▎    | 678/1271 [03:38<03:10,  3.12it/s, training_loss=0.069]\u001b[A\n",
            "Epoch 3:  53%|█████▎    | 678/1271 [03:38<03:10,  3.12it/s, training_loss=0.804]\u001b[A\n",
            "Epoch 3:  53%|█████▎    | 679/1271 [03:38<03:09,  3.12it/s, training_loss=0.804]\u001b[A\n",
            "Epoch 3:  53%|█████▎    | 679/1271 [03:38<03:09,  3.12it/s, training_loss=0.474]\u001b[A\n",
            "Epoch 3:  54%|█████▎    | 680/1271 [03:38<03:08,  3.13it/s, training_loss=0.474]\u001b[A\n",
            "Epoch 3:  54%|█████▎    | 680/1271 [03:39<03:08,  3.13it/s, training_loss=0.043]\u001b[A\n",
            "Epoch 3:  54%|█████▎    | 681/1271 [03:39<03:08,  3.13it/s, training_loss=0.043]\u001b[A\n",
            "Epoch 3:  54%|█████▎    | 681/1271 [03:39<03:08,  3.13it/s, training_loss=0.194]\u001b[A\n",
            "Epoch 3:  54%|█████▎    | 682/1271 [03:39<03:09,  3.11it/s, training_loss=0.194]\u001b[A\n",
            "Epoch 3:  54%|█████▎    | 682/1271 [03:39<03:09,  3.11it/s, training_loss=0.420]\u001b[A\n",
            "Epoch 3:  54%|█████▎    | 683/1271 [03:39<03:10,  3.08it/s, training_loss=0.420]\u001b[A\n",
            "Epoch 3:  54%|█████▎    | 683/1271 [03:40<03:10,  3.08it/s, training_loss=0.105]\u001b[A\n",
            "Epoch 3:  54%|█████▍    | 684/1271 [03:40<03:09,  3.09it/s, training_loss=0.105]\u001b[A\n",
            "Epoch 3:  54%|█████▍    | 684/1271 [03:40<03:09,  3.09it/s, training_loss=0.228]\u001b[A\n",
            "Epoch 3:  54%|█████▍    | 685/1271 [03:40<03:10,  3.08it/s, training_loss=0.228]\u001b[A\n",
            "Epoch 3:  54%|█████▍    | 685/1271 [03:40<03:10,  3.08it/s, training_loss=0.716]\u001b[A\n",
            "Epoch 3:  54%|█████▍    | 686/1271 [03:40<03:11,  3.05it/s, training_loss=0.716]\u001b[A\n",
            "Epoch 3:  54%|█████▍    | 686/1271 [03:41<03:11,  3.05it/s, training_loss=0.519]\u001b[A\n",
            "Epoch 3:  54%|█████▍    | 687/1271 [03:41<03:10,  3.07it/s, training_loss=0.519]\u001b[A\n",
            "Epoch 3:  54%|█████▍    | 687/1271 [03:41<03:10,  3.07it/s, training_loss=0.533]\u001b[A\n",
            "Epoch 3:  54%|█████▍    | 688/1271 [03:41<03:08,  3.09it/s, training_loss=0.533]\u001b[A\n",
            "Epoch 3:  54%|█████▍    | 688/1271 [03:41<03:08,  3.09it/s, training_loss=0.011]\u001b[A\n",
            "Epoch 3:  54%|█████▍    | 689/1271 [03:41<03:08,  3.09it/s, training_loss=0.011]\u001b[A\n",
            "Epoch 3:  54%|█████▍    | 689/1271 [03:42<03:08,  3.09it/s, training_loss=0.691]\u001b[A\n",
            "Epoch 3:  54%|█████▍    | 690/1271 [03:42<03:07,  3.10it/s, training_loss=0.691]\u001b[A\n",
            "Epoch 3:  54%|█████▍    | 690/1271 [03:42<03:07,  3.10it/s, training_loss=0.344]\u001b[A\n",
            "Epoch 3:  54%|█████▍    | 691/1271 [03:42<03:06,  3.11it/s, training_loss=0.344]\u001b[A\n",
            "Epoch 3:  54%|█████▍    | 691/1271 [03:42<03:06,  3.11it/s, training_loss=0.170]\u001b[A\n",
            "Epoch 3:  54%|█████▍    | 692/1271 [03:42<03:06,  3.11it/s, training_loss=0.170]\u001b[A\n",
            "Epoch 3:  54%|█████▍    | 692/1271 [03:43<03:06,  3.11it/s, training_loss=1.075]\u001b[A\n",
            "Epoch 3:  55%|█████▍    | 693/1271 [03:43<03:05,  3.12it/s, training_loss=1.075]\u001b[A\n",
            "Epoch 3:  55%|█████▍    | 693/1271 [03:43<03:05,  3.12it/s, training_loss=0.233]\u001b[A\n",
            "Epoch 3:  55%|█████▍    | 694/1271 [03:43<03:04,  3.12it/s, training_loss=0.233]\u001b[A\n",
            "Epoch 3:  55%|█████▍    | 694/1271 [03:43<03:04,  3.12it/s, training_loss=0.155]\u001b[A\n",
            "Epoch 3:  55%|█████▍    | 695/1271 [03:43<03:07,  3.07it/s, training_loss=0.155]\u001b[A\n",
            "Epoch 3:  55%|█████▍    | 695/1271 [03:44<03:07,  3.07it/s, training_loss=0.119]\u001b[A\n",
            "Epoch 3:  55%|█████▍    | 696/1271 [03:44<03:06,  3.08it/s, training_loss=0.119]\u001b[A\n",
            "Epoch 3:  55%|█████▍    | 696/1271 [03:44<03:06,  3.08it/s, training_loss=0.030]\u001b[A\n",
            "Epoch 3:  55%|█████▍    | 697/1271 [03:44<03:06,  3.08it/s, training_loss=0.030]\u001b[A\n",
            "Epoch 3:  55%|█████▍    | 697/1271 [03:44<03:06,  3.08it/s, training_loss=0.646]\u001b[A\n",
            "Epoch 3:  55%|█████▍    | 698/1271 [03:44<03:06,  3.08it/s, training_loss=0.646]\u001b[A\n",
            "Epoch 3:  55%|█████▍    | 698/1271 [03:45<03:06,  3.08it/s, training_loss=0.265]\u001b[A\n",
            "Epoch 3:  55%|█████▍    | 699/1271 [03:45<03:06,  3.07it/s, training_loss=0.265]\u001b[A\n",
            "Epoch 3:  55%|█████▍    | 699/1271 [03:45<03:06,  3.07it/s, training_loss=0.170]\u001b[A\n",
            "Epoch 3:  55%|█████▌    | 700/1271 [03:45<03:05,  3.07it/s, training_loss=0.170]\u001b[A\n",
            "Epoch 3:  55%|█████▌    | 700/1271 [03:45<03:05,  3.07it/s, training_loss=0.573]\u001b[A\n",
            "Epoch 3:  55%|█████▌    | 701/1271 [03:45<03:04,  3.10it/s, training_loss=0.573]\u001b[A\n",
            "Epoch 3:  55%|█████▌    | 701/1271 [03:45<03:04,  3.10it/s, training_loss=0.096]\u001b[A\n",
            "Epoch 3:  55%|█████▌    | 702/1271 [03:45<03:03,  3.11it/s, training_loss=0.096]\u001b[A\n",
            "Epoch 3:  55%|█████▌    | 702/1271 [03:46<03:03,  3.11it/s, training_loss=0.087]\u001b[A\n",
            "Epoch 3:  55%|█████▌    | 703/1271 [03:46<03:03,  3.10it/s, training_loss=0.087]\u001b[A\n",
            "Epoch 3:  55%|█████▌    | 703/1271 [03:46<03:03,  3.10it/s, training_loss=0.118]\u001b[A\n",
            "Epoch 3:  55%|█████▌    | 704/1271 [03:46<03:03,  3.09it/s, training_loss=0.118]\u001b[A\n",
            "Epoch 3:  55%|█████▌    | 704/1271 [03:46<03:03,  3.09it/s, training_loss=0.222]\u001b[A\n",
            "Epoch 3:  55%|█████▌    | 705/1271 [03:46<03:02,  3.10it/s, training_loss=0.222]\u001b[A\n",
            "Epoch 3:  55%|█████▌    | 705/1271 [03:47<03:02,  3.10it/s, training_loss=0.483]\u001b[A\n",
            "Epoch 3:  56%|█████▌    | 706/1271 [03:47<03:01,  3.11it/s, training_loss=0.483]\u001b[A\n",
            "Epoch 3:  56%|█████▌    | 706/1271 [03:47<03:01,  3.11it/s, training_loss=0.159]\u001b[A\n",
            "Epoch 3:  56%|█████▌    | 707/1271 [03:47<03:01,  3.11it/s, training_loss=0.159]\u001b[A\n",
            "Epoch 3:  56%|█████▌    | 707/1271 [03:47<03:01,  3.11it/s, training_loss=0.311]\u001b[A\n",
            "Epoch 3:  56%|█████▌    | 708/1271 [03:47<03:01,  3.10it/s, training_loss=0.311]\u001b[A\n",
            "Epoch 3:  56%|█████▌    | 708/1271 [03:48<03:01,  3.10it/s, training_loss=0.120]\u001b[A\n",
            "Epoch 3:  56%|█████▌    | 709/1271 [03:48<03:02,  3.08it/s, training_loss=0.120]\u001b[A\n",
            "Epoch 3:  56%|█████▌    | 709/1271 [03:48<03:02,  3.08it/s, training_loss=0.173]\u001b[A\n",
            "Epoch 3:  56%|█████▌    | 710/1271 [03:48<03:01,  3.10it/s, training_loss=0.173]\u001b[A\n",
            "Epoch 3:  56%|█████▌    | 710/1271 [03:48<03:01,  3.10it/s, training_loss=0.581]\u001b[A\n",
            "Epoch 3:  56%|█████▌    | 711/1271 [03:48<03:01,  3.09it/s, training_loss=0.581]\u001b[A\n",
            "Epoch 3:  56%|█████▌    | 711/1271 [03:49<03:01,  3.09it/s, training_loss=0.115]\u001b[A\n",
            "Epoch 3:  56%|█████▌    | 712/1271 [03:49<02:59,  3.11it/s, training_loss=0.115]\u001b[A\n",
            "Epoch 3:  56%|█████▌    | 712/1271 [03:49<02:59,  3.11it/s, training_loss=0.515]\u001b[A\n",
            "Epoch 3:  56%|█████▌    | 713/1271 [03:49<02:59,  3.11it/s, training_loss=0.515]\u001b[A\n",
            "Epoch 3:  56%|█████▌    | 713/1271 [03:49<02:59,  3.11it/s, training_loss=0.417]\u001b[A\n",
            "Epoch 3:  56%|█████▌    | 714/1271 [03:49<02:59,  3.11it/s, training_loss=0.417]\u001b[A\n",
            "Epoch 3:  56%|█████▌    | 714/1271 [03:50<02:59,  3.11it/s, training_loss=0.368]\u001b[A\n",
            "Epoch 3:  56%|█████▋    | 715/1271 [03:50<02:58,  3.11it/s, training_loss=0.368]\u001b[A\n",
            "Epoch 3:  56%|█████▋    | 715/1271 [03:50<02:58,  3.11it/s, training_loss=0.182]\u001b[A\n",
            "Epoch 3:  56%|█████▋    | 716/1271 [03:50<02:57,  3.12it/s, training_loss=0.182]\u001b[A\n",
            "Epoch 3:  56%|█████▋    | 716/1271 [03:50<02:57,  3.12it/s, training_loss=0.086]\u001b[A\n",
            "Epoch 3:  56%|█████▋    | 717/1271 [03:50<02:57,  3.13it/s, training_loss=0.086]\u001b[A\n",
            "Epoch 3:  56%|█████▋    | 717/1271 [03:51<02:57,  3.13it/s, training_loss=0.224]\u001b[A\n",
            "Epoch 3:  56%|█████▋    | 718/1271 [03:51<02:56,  3.13it/s, training_loss=0.224]\u001b[A\n",
            "Epoch 3:  56%|█████▋    | 718/1271 [03:51<02:56,  3.13it/s, training_loss=0.585]\u001b[A\n",
            "Epoch 3:  57%|█████▋    | 719/1271 [03:51<02:56,  3.12it/s, training_loss=0.585]\u001b[A\n",
            "Epoch 3:  57%|█████▋    | 719/1271 [03:51<02:56,  3.12it/s, training_loss=0.634]\u001b[A\n",
            "Epoch 3:  57%|█████▋    | 720/1271 [03:51<02:56,  3.12it/s, training_loss=0.634]\u001b[A\n",
            "Epoch 3:  57%|█████▋    | 720/1271 [03:52<02:56,  3.12it/s, training_loss=0.396]\u001b[A\n",
            "Epoch 3:  57%|█████▋    | 721/1271 [03:52<02:56,  3.11it/s, training_loss=0.396]\u001b[A\n",
            "Epoch 3:  57%|█████▋    | 721/1271 [03:52<02:56,  3.11it/s, training_loss=0.695]\u001b[A\n",
            "Epoch 3:  57%|█████▋    | 722/1271 [03:52<02:56,  3.11it/s, training_loss=0.695]\u001b[A\n",
            "Epoch 3:  57%|█████▋    | 722/1271 [03:52<02:56,  3.11it/s, training_loss=0.601]\u001b[A\n",
            "Epoch 3:  57%|█████▋    | 723/1271 [03:52<02:56,  3.11it/s, training_loss=0.601]\u001b[A\n",
            "Epoch 3:  57%|█████▋    | 723/1271 [03:53<02:56,  3.11it/s, training_loss=0.084]\u001b[A\n",
            "Epoch 3:  57%|█████▋    | 724/1271 [03:53<02:56,  3.10it/s, training_loss=0.084]\u001b[A\n",
            "Epoch 3:  57%|█████▋    | 724/1271 [03:53<02:56,  3.10it/s, training_loss=0.186]\u001b[A\n",
            "Epoch 3:  57%|█████▋    | 725/1271 [03:53<02:55,  3.11it/s, training_loss=0.186]\u001b[A\n",
            "Epoch 3:  57%|█████▋    | 725/1271 [03:53<02:55,  3.11it/s, training_loss=0.220]\u001b[A\n",
            "Epoch 3:  57%|█████▋    | 726/1271 [03:53<02:55,  3.10it/s, training_loss=0.220]\u001b[A\n",
            "Epoch 3:  57%|█████▋    | 726/1271 [03:54<02:55,  3.10it/s, training_loss=0.474]\u001b[A\n",
            "Epoch 3:  57%|█████▋    | 727/1271 [03:54<02:55,  3.11it/s, training_loss=0.474]\u001b[A\n",
            "Epoch 3:  57%|█████▋    | 727/1271 [03:54<02:55,  3.11it/s, training_loss=0.016]\u001b[A\n",
            "Epoch 3:  57%|█████▋    | 728/1271 [03:54<02:55,  3.10it/s, training_loss=0.016]\u001b[A\n",
            "Epoch 3:  57%|█████▋    | 728/1271 [03:54<02:55,  3.10it/s, training_loss=0.326]\u001b[A\n",
            "Epoch 3:  57%|█████▋    | 729/1271 [03:54<02:55,  3.09it/s, training_loss=0.326]\u001b[A\n",
            "Epoch 3:  57%|█████▋    | 729/1271 [03:54<02:55,  3.09it/s, training_loss=0.135]\u001b[A\n",
            "Epoch 3:  57%|█████▋    | 730/1271 [03:54<02:54,  3.10it/s, training_loss=0.135]\u001b[A\n",
            "Epoch 3:  57%|█████▋    | 730/1271 [03:55<02:54,  3.10it/s, training_loss=0.171]\u001b[A\n",
            "Epoch 3:  58%|█████▊    | 731/1271 [03:55<02:54,  3.10it/s, training_loss=0.171]\u001b[A\n",
            "Epoch 3:  58%|█████▊    | 731/1271 [03:55<02:54,  3.10it/s, training_loss=0.054]\u001b[A\n",
            "Epoch 3:  58%|█████▊    | 732/1271 [03:55<02:54,  3.10it/s, training_loss=0.054]\u001b[A\n",
            "Epoch 3:  58%|█████▊    | 732/1271 [03:55<02:54,  3.10it/s, training_loss=0.268]\u001b[A\n",
            "Epoch 3:  58%|█████▊    | 733/1271 [03:55<02:53,  3.10it/s, training_loss=0.268]\u001b[A\n",
            "Epoch 3:  58%|█████▊    | 733/1271 [03:56<02:53,  3.10it/s, training_loss=0.345]\u001b[A\n",
            "Epoch 3:  58%|█████▊    | 734/1271 [03:56<02:52,  3.11it/s, training_loss=0.345]\u001b[A\n",
            "Epoch 3:  58%|█████▊    | 734/1271 [03:56<02:52,  3.11it/s, training_loss=0.266]\u001b[A\n",
            "Epoch 3:  58%|█████▊    | 735/1271 [03:56<02:52,  3.10it/s, training_loss=0.266]\u001b[A\n",
            "Epoch 3:  58%|█████▊    | 735/1271 [03:56<02:52,  3.10it/s, training_loss=0.200]\u001b[A\n",
            "Epoch 3:  58%|█████▊    | 736/1271 [03:56<02:51,  3.11it/s, training_loss=0.200]\u001b[A\n",
            "Epoch 3:  58%|█████▊    | 736/1271 [03:57<02:51,  3.11it/s, training_loss=0.394]\u001b[A\n",
            "Epoch 3:  58%|█████▊    | 737/1271 [03:57<02:51,  3.11it/s, training_loss=0.394]\u001b[A\n",
            "Epoch 3:  58%|█████▊    | 737/1271 [03:57<02:51,  3.11it/s, training_loss=0.613]\u001b[A\n",
            "Epoch 3:  58%|█████▊    | 738/1271 [03:57<02:51,  3.11it/s, training_loss=0.613]\u001b[A\n",
            "Epoch 3:  58%|█████▊    | 738/1271 [03:57<02:51,  3.11it/s, training_loss=0.086]\u001b[A\n",
            "Epoch 3:  58%|█████▊    | 739/1271 [03:57<02:51,  3.11it/s, training_loss=0.086]\u001b[A\n",
            "Epoch 3:  58%|█████▊    | 739/1271 [03:58<02:51,  3.11it/s, training_loss=0.128]\u001b[A\n",
            "Epoch 3:  58%|█████▊    | 740/1271 [03:58<02:50,  3.11it/s, training_loss=0.128]\u001b[A\n",
            "Epoch 3:  58%|█████▊    | 740/1271 [03:58<02:50,  3.11it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 3:  58%|█████▊    | 741/1271 [03:58<02:50,  3.10it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 3:  58%|█████▊    | 741/1271 [03:58<02:50,  3.10it/s, training_loss=0.331]\u001b[A\n",
            "Epoch 3:  58%|█████▊    | 742/1271 [03:58<02:49,  3.12it/s, training_loss=0.331]\u001b[A\n",
            "Epoch 3:  58%|█████▊    | 742/1271 [03:59<02:49,  3.12it/s, training_loss=0.095]\u001b[A\n",
            "Epoch 3:  58%|█████▊    | 743/1271 [03:59<02:49,  3.12it/s, training_loss=0.095]\u001b[A\n",
            "Epoch 3:  58%|█████▊    | 743/1271 [03:59<02:49,  3.12it/s, training_loss=0.482]\u001b[A\n",
            "Epoch 3:  59%|█████▊    | 744/1271 [03:59<02:49,  3.11it/s, training_loss=0.482]\u001b[A\n",
            "Epoch 3:  59%|█████▊    | 744/1271 [03:59<02:49,  3.11it/s, training_loss=0.146]\u001b[A\n",
            "Epoch 3:  59%|█████▊    | 745/1271 [03:59<02:49,  3.10it/s, training_loss=0.146]\u001b[A\n",
            "Epoch 3:  59%|█████▊    | 745/1271 [04:00<02:49,  3.10it/s, training_loss=0.369]\u001b[A\n",
            "Epoch 3:  59%|█████▊    | 746/1271 [04:00<02:49,  3.10it/s, training_loss=0.369]\u001b[A\n",
            "Epoch 3:  59%|█████▊    | 746/1271 [04:00<02:49,  3.10it/s, training_loss=0.169]\u001b[A\n",
            "Epoch 3:  59%|█████▉    | 747/1271 [04:00<02:49,  3.09it/s, training_loss=0.169]\u001b[A\n",
            "Epoch 3:  59%|█████▉    | 747/1271 [04:00<02:49,  3.09it/s, training_loss=0.271]\u001b[A\n",
            "Epoch 3:  59%|█████▉    | 748/1271 [04:00<02:49,  3.09it/s, training_loss=0.271]\u001b[A\n",
            "Epoch 3:  59%|█████▉    | 748/1271 [04:01<02:49,  3.09it/s, training_loss=0.146]\u001b[A\n",
            "Epoch 3:  59%|█████▉    | 749/1271 [04:01<02:48,  3.10it/s, training_loss=0.146]\u001b[A\n",
            "Epoch 3:  59%|█████▉    | 749/1271 [04:01<02:48,  3.10it/s, training_loss=0.146]\u001b[A\n",
            "Epoch 3:  59%|█████▉    | 750/1271 [04:01<02:48,  3.08it/s, training_loss=0.146]\u001b[A\n",
            "Epoch 3:  59%|█████▉    | 750/1271 [04:01<02:48,  3.08it/s, training_loss=0.122]\u001b[A\n",
            "Epoch 3:  59%|█████▉    | 751/1271 [04:01<02:47,  3.10it/s, training_loss=0.122]\u001b[A\n",
            "Epoch 3:  59%|█████▉    | 751/1271 [04:02<02:47,  3.10it/s, training_loss=0.074]\u001b[A\n",
            "Epoch 3:  59%|█████▉    | 752/1271 [04:02<02:46,  3.12it/s, training_loss=0.074]\u001b[A\n",
            "Epoch 3:  59%|█████▉    | 752/1271 [04:02<02:46,  3.12it/s, training_loss=0.645]\u001b[A\n",
            "Epoch 3:  59%|█████▉    | 753/1271 [04:02<02:45,  3.12it/s, training_loss=0.645]\u001b[A\n",
            "Epoch 3:  59%|█████▉    | 753/1271 [04:02<02:45,  3.12it/s, training_loss=0.150]\u001b[A\n",
            "Epoch 3:  59%|█████▉    | 754/1271 [04:02<02:45,  3.12it/s, training_loss=0.150]\u001b[A\n",
            "Epoch 3:  59%|█████▉    | 754/1271 [04:03<02:45,  3.12it/s, training_loss=0.591]\u001b[A\n",
            "Epoch 3:  59%|█████▉    | 755/1271 [04:03<02:44,  3.13it/s, training_loss=0.591]\u001b[A\n",
            "Epoch 3:  59%|█████▉    | 755/1271 [04:03<02:44,  3.13it/s, training_loss=0.134]\u001b[A\n",
            "Epoch 3:  59%|█████▉    | 756/1271 [04:03<02:46,  3.09it/s, training_loss=0.134]\u001b[A\n",
            "Epoch 3:  59%|█████▉    | 756/1271 [04:03<02:46,  3.09it/s, training_loss=0.460]\u001b[A\n",
            "Epoch 3:  60%|█████▉    | 757/1271 [04:03<02:46,  3.09it/s, training_loss=0.460]\u001b[A\n",
            "Epoch 3:  60%|█████▉    | 757/1271 [04:04<02:46,  3.09it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 3:  60%|█████▉    | 758/1271 [04:04<02:45,  3.10it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 3:  60%|█████▉    | 758/1271 [04:04<02:45,  3.10it/s, training_loss=0.508]\u001b[A\n",
            "Epoch 3:  60%|█████▉    | 759/1271 [04:04<02:44,  3.11it/s, training_loss=0.508]\u001b[A\n",
            "Epoch 3:  60%|█████▉    | 759/1271 [04:04<02:44,  3.11it/s, training_loss=0.346]\u001b[A\n",
            "Epoch 3:  60%|█████▉    | 760/1271 [04:04<02:43,  3.12it/s, training_loss=0.346]\u001b[A\n",
            "Epoch 3:  60%|█████▉    | 760/1271 [04:04<02:43,  3.12it/s, training_loss=0.126]\u001b[A\n",
            "Epoch 3:  60%|█████▉    | 761/1271 [04:04<02:44,  3.10it/s, training_loss=0.126]\u001b[A\n",
            "Epoch 3:  60%|█████▉    | 761/1271 [04:05<02:44,  3.10it/s, training_loss=0.285]\u001b[A\n",
            "Epoch 3:  60%|█████▉    | 762/1271 [04:05<02:43,  3.11it/s, training_loss=0.285]\u001b[A\n",
            "Epoch 3:  60%|█████▉    | 762/1271 [04:05<02:43,  3.11it/s, training_loss=0.075]\u001b[A\n",
            "Epoch 3:  60%|██████    | 763/1271 [04:05<02:42,  3.12it/s, training_loss=0.075]\u001b[A\n",
            "Epoch 3:  60%|██████    | 763/1271 [04:05<02:42,  3.12it/s, training_loss=0.295]\u001b[A\n",
            "Epoch 3:  60%|██████    | 764/1271 [04:05<02:43,  3.11it/s, training_loss=0.295]\u001b[A\n",
            "Epoch 3:  60%|██████    | 764/1271 [04:06<02:43,  3.11it/s, training_loss=0.571]\u001b[A\n",
            "Epoch 3:  60%|██████    | 765/1271 [04:06<02:42,  3.11it/s, training_loss=0.571]\u001b[A\n",
            "Epoch 3:  60%|██████    | 765/1271 [04:06<02:42,  3.11it/s, training_loss=0.062]\u001b[A\n",
            "Epoch 3:  60%|██████    | 766/1271 [04:06<02:42,  3.11it/s, training_loss=0.062]\u001b[A\n",
            "Epoch 3:  60%|██████    | 766/1271 [04:06<02:42,  3.11it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 3:  60%|██████    | 767/1271 [04:06<02:42,  3.10it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 3:  60%|██████    | 767/1271 [04:07<02:42,  3.10it/s, training_loss=0.745]\u001b[A\n",
            "Epoch 3:  60%|██████    | 768/1271 [04:07<02:43,  3.08it/s, training_loss=0.745]\u001b[A\n",
            "Epoch 3:  60%|██████    | 768/1271 [04:07<02:43,  3.08it/s, training_loss=0.218]\u001b[A\n",
            "Epoch 3:  61%|██████    | 769/1271 [04:07<02:42,  3.09it/s, training_loss=0.218]\u001b[A\n",
            "Epoch 3:  61%|██████    | 769/1271 [04:07<02:42,  3.09it/s, training_loss=0.108]\u001b[A\n",
            "Epoch 3:  61%|██████    | 770/1271 [04:07<02:42,  3.09it/s, training_loss=0.108]\u001b[A\n",
            "Epoch 3:  61%|██████    | 770/1271 [04:08<02:42,  3.09it/s, training_loss=0.452]\u001b[A\n",
            "Epoch 3:  61%|██████    | 771/1271 [04:08<02:40,  3.11it/s, training_loss=0.452]\u001b[A\n",
            "Epoch 3:  61%|██████    | 771/1271 [04:08<02:40,  3.11it/s, training_loss=0.189]\u001b[A\n",
            "Epoch 3:  61%|██████    | 772/1271 [04:08<02:40,  3.11it/s, training_loss=0.189]\u001b[A\n",
            "Epoch 3:  61%|██████    | 772/1271 [04:08<02:40,  3.11it/s, training_loss=0.580]\u001b[A\n",
            "Epoch 3:  61%|██████    | 773/1271 [04:08<02:40,  3.11it/s, training_loss=0.580]\u001b[A\n",
            "Epoch 3:  61%|██████    | 773/1271 [04:09<02:40,  3.11it/s, training_loss=0.245]\u001b[A\n",
            "Epoch 3:  61%|██████    | 774/1271 [04:09<02:39,  3.12it/s, training_loss=0.245]\u001b[A\n",
            "Epoch 3:  61%|██████    | 774/1271 [04:09<02:39,  3.12it/s, training_loss=0.303]\u001b[A\n",
            "Epoch 3:  61%|██████    | 775/1271 [04:09<02:38,  3.12it/s, training_loss=0.303]\u001b[A\n",
            "Epoch 3:  61%|██████    | 775/1271 [04:09<02:38,  3.12it/s, training_loss=0.130]\u001b[A\n",
            "Epoch 3:  61%|██████    | 776/1271 [04:09<02:39,  3.10it/s, training_loss=0.130]\u001b[A\n",
            "Epoch 3:  61%|██████    | 776/1271 [04:10<02:39,  3.10it/s, training_loss=0.242]\u001b[A\n",
            "Epoch 3:  61%|██████    | 777/1271 [04:10<02:39,  3.10it/s, training_loss=0.242]\u001b[A\n",
            "Epoch 3:  61%|██████    | 777/1271 [04:10<02:39,  3.10it/s, training_loss=0.069]\u001b[A\n",
            "Epoch 3:  61%|██████    | 778/1271 [04:10<02:38,  3.10it/s, training_loss=0.069]\u001b[A\n",
            "Epoch 3:  61%|██████    | 778/1271 [04:10<02:38,  3.10it/s, training_loss=0.534]\u001b[A\n",
            "Epoch 3:  61%|██████▏   | 779/1271 [04:10<02:38,  3.11it/s, training_loss=0.534]\u001b[A\n",
            "Epoch 3:  61%|██████▏   | 779/1271 [04:11<02:38,  3.11it/s, training_loss=0.547]\u001b[A\n",
            "Epoch 3:  61%|██████▏   | 780/1271 [04:11<02:37,  3.12it/s, training_loss=0.547]\u001b[A\n",
            "Epoch 3:  61%|██████▏   | 780/1271 [04:11<02:37,  3.12it/s, training_loss=0.724]\u001b[A\n",
            "Epoch 3:  61%|██████▏   | 781/1271 [04:11<02:37,  3.11it/s, training_loss=0.724]\u001b[A\n",
            "Epoch 3:  61%|██████▏   | 781/1271 [04:11<02:37,  3.11it/s, training_loss=0.881]\u001b[A\n",
            "Epoch 3:  62%|██████▏   | 782/1271 [04:11<02:37,  3.11it/s, training_loss=0.881]\u001b[A\n",
            "Epoch 3:  62%|██████▏   | 782/1271 [04:12<02:37,  3.11it/s, training_loss=0.358]\u001b[A\n",
            "Epoch 3:  62%|██████▏   | 783/1271 [04:12<02:37,  3.09it/s, training_loss=0.358]\u001b[A\n",
            "Epoch 3:  62%|██████▏   | 783/1271 [04:12<02:37,  3.09it/s, training_loss=0.227]\u001b[A\n",
            "Epoch 3:  62%|██████▏   | 784/1271 [04:12<02:36,  3.11it/s, training_loss=0.227]\u001b[A\n",
            "Epoch 3:  62%|██████▏   | 784/1271 [04:12<02:36,  3.11it/s, training_loss=0.877]\u001b[A\n",
            "Epoch 3:  62%|██████▏   | 785/1271 [04:12<02:35,  3.12it/s, training_loss=0.877]\u001b[A\n",
            "Epoch 3:  62%|██████▏   | 785/1271 [04:13<02:35,  3.12it/s, training_loss=0.013]\u001b[A\n",
            "Epoch 3:  62%|██████▏   | 786/1271 [04:13<02:36,  3.10it/s, training_loss=0.013]\u001b[A\n",
            "Epoch 3:  62%|██████▏   | 786/1271 [04:13<02:36,  3.10it/s, training_loss=0.487]\u001b[A\n",
            "Epoch 3:  62%|██████▏   | 787/1271 [04:13<02:35,  3.11it/s, training_loss=0.487]\u001b[A\n",
            "Epoch 3:  62%|██████▏   | 787/1271 [04:13<02:35,  3.11it/s, training_loss=0.517]\u001b[A\n",
            "Epoch 3:  62%|██████▏   | 788/1271 [04:13<02:34,  3.12it/s, training_loss=0.517]\u001b[A\n",
            "Epoch 3:  62%|██████▏   | 788/1271 [04:13<02:34,  3.12it/s, training_loss=0.205]\u001b[A\n",
            "Epoch 3:  62%|██████▏   | 789/1271 [04:13<02:34,  3.12it/s, training_loss=0.205]\u001b[A\n",
            "Epoch 3:  62%|██████▏   | 789/1271 [04:14<02:34,  3.12it/s, training_loss=0.233]\u001b[A\n",
            "Epoch 3:  62%|██████▏   | 790/1271 [04:14<02:35,  3.09it/s, training_loss=0.233]\u001b[A\n",
            "Epoch 3:  62%|██████▏   | 790/1271 [04:14<02:35,  3.09it/s, training_loss=0.370]\u001b[A\n",
            "Epoch 3:  62%|██████▏   | 791/1271 [04:14<02:35,  3.09it/s, training_loss=0.370]\u001b[A\n",
            "Epoch 3:  62%|██████▏   | 791/1271 [04:14<02:35,  3.09it/s, training_loss=0.242]\u001b[A\n",
            "Epoch 3:  62%|██████▏   | 792/1271 [04:14<02:34,  3.09it/s, training_loss=0.242]\u001b[A\n",
            "Epoch 3:  62%|██████▏   | 792/1271 [04:15<02:34,  3.09it/s, training_loss=0.753]\u001b[A\n",
            "Epoch 3:  62%|██████▏   | 793/1271 [04:15<02:34,  3.10it/s, training_loss=0.753]\u001b[A\n",
            "Epoch 3:  62%|██████▏   | 793/1271 [04:15<02:34,  3.10it/s, training_loss=0.154]\u001b[A\n",
            "Epoch 3:  62%|██████▏   | 794/1271 [04:15<02:34,  3.09it/s, training_loss=0.154]\u001b[A\n",
            "Epoch 3:  62%|██████▏   | 794/1271 [04:15<02:34,  3.09it/s, training_loss=0.120]\u001b[A\n",
            "Epoch 3:  63%|██████▎   | 795/1271 [04:15<02:33,  3.10it/s, training_loss=0.120]\u001b[A\n",
            "Epoch 3:  63%|██████▎   | 795/1271 [04:16<02:33,  3.10it/s, training_loss=0.353]\u001b[A\n",
            "Epoch 3:  63%|██████▎   | 796/1271 [04:16<02:33,  3.08it/s, training_loss=0.353]\u001b[A\n",
            "Epoch 3:  63%|██████▎   | 796/1271 [04:16<02:33,  3.08it/s, training_loss=0.150]\u001b[A\n",
            "Epoch 3:  63%|██████▎   | 797/1271 [04:16<02:32,  3.10it/s, training_loss=0.150]\u001b[A\n",
            "Epoch 3:  63%|██████▎   | 797/1271 [04:16<02:32,  3.10it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 3:  63%|██████▎   | 798/1271 [04:16<02:31,  3.11it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 3:  63%|██████▎   | 798/1271 [04:17<02:31,  3.11it/s, training_loss=0.285]\u001b[A\n",
            "Epoch 3:  63%|██████▎   | 799/1271 [04:17<02:31,  3.11it/s, training_loss=0.285]\u001b[A\n",
            "Epoch 3:  63%|██████▎   | 799/1271 [04:17<02:31,  3.11it/s, training_loss=0.267]\u001b[A\n",
            "Epoch 3:  63%|██████▎   | 800/1271 [04:17<02:30,  3.12it/s, training_loss=0.267]\u001b[A\n",
            "Epoch 3:  63%|██████▎   | 800/1271 [04:17<02:30,  3.12it/s, training_loss=0.027]\u001b[A\n",
            "Epoch 3:  63%|██████▎   | 801/1271 [04:17<02:31,  3.11it/s, training_loss=0.027]\u001b[A\n",
            "Epoch 3:  63%|██████▎   | 801/1271 [04:18<02:31,  3.11it/s, training_loss=0.157]\u001b[A\n",
            "Epoch 3:  63%|██████▎   | 802/1271 [04:18<02:30,  3.12it/s, training_loss=0.157]\u001b[A\n",
            "Epoch 3:  63%|██████▎   | 802/1271 [04:18<02:30,  3.12it/s, training_loss=0.024]\u001b[A\n",
            "Epoch 3:  63%|██████▎   | 803/1271 [04:18<02:30,  3.10it/s, training_loss=0.024]\u001b[A\n",
            "Epoch 3:  63%|██████▎   | 803/1271 [04:18<02:30,  3.10it/s, training_loss=0.185]\u001b[A\n",
            "Epoch 3:  63%|██████▎   | 804/1271 [04:18<02:30,  3.10it/s, training_loss=0.185]\u001b[A\n",
            "Epoch 3:  63%|██████▎   | 804/1271 [04:19<02:30,  3.10it/s, training_loss=0.027]\u001b[A\n",
            "Epoch 3:  63%|██████▎   | 805/1271 [04:19<02:30,  3.10it/s, training_loss=0.027]\u001b[A\n",
            "Epoch 3:  63%|██████▎   | 805/1271 [04:19<02:30,  3.10it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 3:  63%|██████▎   | 806/1271 [04:19<02:30,  3.10it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 3:  63%|██████▎   | 806/1271 [04:19<02:30,  3.10it/s, training_loss=0.128]\u001b[A\n",
            "Epoch 3:  63%|██████▎   | 807/1271 [04:19<02:29,  3.10it/s, training_loss=0.128]\u001b[A\n",
            "Epoch 3:  63%|██████▎   | 807/1271 [04:20<02:29,  3.10it/s, training_loss=0.067]\u001b[A\n",
            "Epoch 3:  64%|██████▎   | 808/1271 [04:20<02:28,  3.11it/s, training_loss=0.067]\u001b[A\n",
            "Epoch 3:  64%|██████▎   | 808/1271 [04:20<02:28,  3.11it/s, training_loss=0.149]\u001b[A\n",
            "Epoch 3:  64%|██████▎   | 809/1271 [04:20<02:27,  3.13it/s, training_loss=0.149]\u001b[A\n",
            "Epoch 3:  64%|██████▎   | 809/1271 [04:20<02:27,  3.13it/s, training_loss=0.592]\u001b[A\n",
            "Epoch 3:  64%|██████▎   | 810/1271 [04:20<02:27,  3.13it/s, training_loss=0.592]\u001b[A\n",
            "Epoch 3:  64%|██████▎   | 810/1271 [04:21<02:27,  3.13it/s, training_loss=0.587]\u001b[A\n",
            "Epoch 3:  64%|██████▍   | 811/1271 [04:21<02:27,  3.11it/s, training_loss=0.587]\u001b[A\n",
            "Epoch 3:  64%|██████▍   | 811/1271 [04:21<02:27,  3.11it/s, training_loss=0.074]\u001b[A\n",
            "Epoch 3:  64%|██████▍   | 812/1271 [04:21<02:27,  3.10it/s, training_loss=0.074]\u001b[A\n",
            "Epoch 3:  64%|██████▍   | 812/1271 [04:21<02:27,  3.10it/s, training_loss=0.317]\u001b[A\n",
            "Epoch 3:  64%|██████▍   | 813/1271 [04:21<02:27,  3.11it/s, training_loss=0.317]\u001b[A\n",
            "Epoch 3:  64%|██████▍   | 813/1271 [04:22<02:27,  3.11it/s, training_loss=0.327]\u001b[A\n",
            "Epoch 3:  64%|██████▍   | 814/1271 [04:22<02:26,  3.11it/s, training_loss=0.327]\u001b[A\n",
            "Epoch 3:  64%|██████▍   | 814/1271 [04:22<02:26,  3.11it/s, training_loss=0.089]\u001b[A\n",
            "Epoch 3:  64%|██████▍   | 815/1271 [04:22<02:26,  3.11it/s, training_loss=0.089]\u001b[A\n",
            "Epoch 3:  64%|██████▍   | 815/1271 [04:22<02:26,  3.11it/s, training_loss=0.574]\u001b[A\n",
            "Epoch 3:  64%|██████▍   | 816/1271 [04:22<02:26,  3.10it/s, training_loss=0.574]\u001b[A\n",
            "Epoch 3:  64%|██████▍   | 816/1271 [04:22<02:26,  3.10it/s, training_loss=0.457]\u001b[A\n",
            "Epoch 3:  64%|██████▍   | 817/1271 [04:23<02:26,  3.10it/s, training_loss=0.457]\u001b[A\n",
            "Epoch 3:  64%|██████▍   | 817/1271 [04:23<02:26,  3.10it/s, training_loss=0.246]\u001b[A\n",
            "Epoch 3:  64%|██████▍   | 818/1271 [04:23<02:26,  3.10it/s, training_loss=0.246]\u001b[A\n",
            "Epoch 3:  64%|██████▍   | 818/1271 [04:23<02:26,  3.10it/s, training_loss=0.149]\u001b[A\n",
            "Epoch 3:  64%|██████▍   | 819/1271 [04:23<02:26,  3.09it/s, training_loss=0.149]\u001b[A\n",
            "Epoch 3:  64%|██████▍   | 819/1271 [04:23<02:26,  3.09it/s, training_loss=0.247]\u001b[A\n",
            "Epoch 3:  65%|██████▍   | 820/1271 [04:23<02:26,  3.08it/s, training_loss=0.247]\u001b[A\n",
            "Epoch 3:  65%|██████▍   | 820/1271 [04:24<02:26,  3.08it/s, training_loss=0.259]\u001b[A\n",
            "Epoch 3:  65%|██████▍   | 821/1271 [04:24<02:25,  3.09it/s, training_loss=0.259]\u001b[A\n",
            "Epoch 3:  65%|██████▍   | 821/1271 [04:24<02:25,  3.09it/s, training_loss=0.297]\u001b[A\n",
            "Epoch 3:  65%|██████▍   | 822/1271 [04:24<02:25,  3.09it/s, training_loss=0.297]\u001b[A\n",
            "Epoch 3:  65%|██████▍   | 822/1271 [04:24<02:25,  3.09it/s, training_loss=1.077]\u001b[A\n",
            "Epoch 3:  65%|██████▍   | 823/1271 [04:24<02:24,  3.11it/s, training_loss=1.077]\u001b[A\n",
            "Epoch 3:  65%|██████▍   | 823/1271 [04:25<02:24,  3.11it/s, training_loss=0.227]\u001b[A\n",
            "Epoch 3:  65%|██████▍   | 824/1271 [04:25<02:23,  3.12it/s, training_loss=0.227]\u001b[A\n",
            "Epoch 3:  65%|██████▍   | 824/1271 [04:25<02:23,  3.12it/s, training_loss=0.021]\u001b[A\n",
            "Epoch 3:  65%|██████▍   | 825/1271 [04:25<02:22,  3.13it/s, training_loss=0.021]\u001b[A\n",
            "Epoch 3:  65%|██████▍   | 825/1271 [04:25<02:22,  3.13it/s, training_loss=0.309]\u001b[A\n",
            "Epoch 3:  65%|██████▍   | 826/1271 [04:25<02:24,  3.08it/s, training_loss=0.309]\u001b[A\n",
            "Epoch 3:  65%|██████▍   | 826/1271 [04:26<02:24,  3.08it/s, training_loss=0.314]\u001b[A\n",
            "Epoch 3:  65%|██████▌   | 827/1271 [04:26<02:24,  3.08it/s, training_loss=0.314]\u001b[A\n",
            "Epoch 3:  65%|██████▌   | 827/1271 [04:26<02:24,  3.08it/s, training_loss=0.068]\u001b[A\n",
            "Epoch 3:  65%|██████▌   | 828/1271 [04:26<02:23,  3.09it/s, training_loss=0.068]\u001b[A\n",
            "Epoch 3:  65%|██████▌   | 828/1271 [04:26<02:23,  3.09it/s, training_loss=0.497]\u001b[A\n",
            "Epoch 3:  65%|██████▌   | 829/1271 [04:26<02:22,  3.11it/s, training_loss=0.497]\u001b[A\n",
            "Epoch 3:  65%|██████▌   | 829/1271 [04:27<02:22,  3.11it/s, training_loss=0.482]\u001b[A\n",
            "Epoch 3:  65%|██████▌   | 830/1271 [04:27<02:21,  3.11it/s, training_loss=0.482]\u001b[A\n",
            "Epoch 3:  65%|██████▌   | 830/1271 [04:27<02:21,  3.11it/s, training_loss=0.410]\u001b[A\n",
            "Epoch 3:  65%|██████▌   | 831/1271 [04:27<02:21,  3.11it/s, training_loss=0.410]\u001b[A\n",
            "Epoch 3:  65%|██████▌   | 831/1271 [04:27<02:21,  3.11it/s, training_loss=0.721]\u001b[A\n",
            "Epoch 3:  65%|██████▌   | 832/1271 [04:27<02:20,  3.13it/s, training_loss=0.721]\u001b[A\n",
            "Epoch 3:  65%|██████▌   | 832/1271 [04:28<02:20,  3.13it/s, training_loss=0.565]\u001b[A\n",
            "Epoch 3:  66%|██████▌   | 833/1271 [04:28<02:20,  3.12it/s, training_loss=0.565]\u001b[A\n",
            "Epoch 3:  66%|██████▌   | 833/1271 [04:28<02:20,  3.12it/s, training_loss=0.485]\u001b[A\n",
            "Epoch 3:  66%|██████▌   | 834/1271 [04:28<02:19,  3.13it/s, training_loss=0.485]\u001b[A\n",
            "Epoch 3:  66%|██████▌   | 834/1271 [04:28<02:19,  3.13it/s, training_loss=0.299]\u001b[A\n",
            "Epoch 3:  66%|██████▌   | 835/1271 [04:28<02:20,  3.10it/s, training_loss=0.299]\u001b[A\n",
            "Epoch 3:  66%|██████▌   | 835/1271 [04:29<02:20,  3.10it/s, training_loss=0.196]\u001b[A\n",
            "Epoch 3:  66%|██████▌   | 836/1271 [04:29<02:19,  3.11it/s, training_loss=0.196]\u001b[A\n",
            "Epoch 3:  66%|██████▌   | 836/1271 [04:29<02:19,  3.11it/s, training_loss=0.563]\u001b[A\n",
            "Epoch 3:  66%|██████▌   | 837/1271 [04:29<02:19,  3.12it/s, training_loss=0.563]\u001b[A\n",
            "Epoch 3:  66%|██████▌   | 837/1271 [04:29<02:19,  3.12it/s, training_loss=0.202]\u001b[A\n",
            "Epoch 3:  66%|██████▌   | 838/1271 [04:29<02:19,  3.10it/s, training_loss=0.202]\u001b[A\n",
            "Epoch 3:  66%|██████▌   | 838/1271 [04:30<02:19,  3.10it/s, training_loss=0.590]\u001b[A\n",
            "Epoch 3:  66%|██████▌   | 839/1271 [04:30<02:19,  3.10it/s, training_loss=0.590]\u001b[A\n",
            "Epoch 3:  66%|██████▌   | 839/1271 [04:30<02:19,  3.10it/s, training_loss=0.314]\u001b[A\n",
            "Epoch 3:  66%|██████▌   | 840/1271 [04:30<02:18,  3.11it/s, training_loss=0.314]\u001b[A\n",
            "Epoch 3:  66%|██████▌   | 840/1271 [04:30<02:18,  3.11it/s, training_loss=0.548]\u001b[A\n",
            "Epoch 3:  66%|██████▌   | 841/1271 [04:30<02:17,  3.12it/s, training_loss=0.548]\u001b[A\n",
            "Epoch 3:  66%|██████▌   | 841/1271 [04:31<02:17,  3.12it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 3:  66%|██████▌   | 842/1271 [04:31<02:17,  3.12it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 3:  66%|██████▌   | 842/1271 [04:31<02:17,  3.12it/s, training_loss=0.437]\u001b[A\n",
            "Epoch 3:  66%|██████▋   | 843/1271 [04:31<02:16,  3.13it/s, training_loss=0.437]\u001b[A\n",
            "Epoch 3:  66%|██████▋   | 843/1271 [04:31<02:16,  3.13it/s, training_loss=0.017]\u001b[A\n",
            "Epoch 3:  66%|██████▋   | 844/1271 [04:31<02:16,  3.12it/s, training_loss=0.017]\u001b[A\n",
            "Epoch 3:  66%|██████▋   | 844/1271 [04:31<02:16,  3.12it/s, training_loss=0.379]\u001b[A\n",
            "Epoch 3:  66%|██████▋   | 845/1271 [04:32<02:15,  3.14it/s, training_loss=0.379]\u001b[A\n",
            "Epoch 3:  66%|██████▋   | 845/1271 [04:32<02:15,  3.14it/s, training_loss=0.397]\u001b[A\n",
            "Epoch 3:  67%|██████▋   | 846/1271 [04:32<02:15,  3.14it/s, training_loss=0.397]\u001b[A\n",
            "Epoch 3:  67%|██████▋   | 846/1271 [04:32<02:15,  3.14it/s, training_loss=0.153]\u001b[A\n",
            "Epoch 3:  67%|██████▋   | 847/1271 [04:32<02:14,  3.14it/s, training_loss=0.153]\u001b[A\n",
            "Epoch 3:  67%|██████▋   | 847/1271 [04:32<02:14,  3.14it/s, training_loss=0.356]\u001b[A\n",
            "Epoch 3:  67%|██████▋   | 848/1271 [04:32<02:15,  3.12it/s, training_loss=0.356]\u001b[A\n",
            "Epoch 3:  67%|██████▋   | 848/1271 [04:33<02:15,  3.12it/s, training_loss=0.197]\u001b[A\n",
            "Epoch 3:  67%|██████▋   | 849/1271 [04:33<02:14,  3.13it/s, training_loss=0.197]\u001b[A\n",
            "Epoch 3:  67%|██████▋   | 849/1271 [04:33<02:14,  3.13it/s, training_loss=0.136]\u001b[A\n",
            "Epoch 3:  67%|██████▋   | 850/1271 [04:33<02:14,  3.14it/s, training_loss=0.136]\u001b[A\n",
            "Epoch 3:  67%|██████▋   | 850/1271 [04:33<02:14,  3.14it/s, training_loss=0.206]\u001b[A\n",
            "Epoch 3:  67%|██████▋   | 851/1271 [04:33<02:15,  3.11it/s, training_loss=0.206]\u001b[A\n",
            "Epoch 3:  67%|██████▋   | 851/1271 [04:34<02:15,  3.11it/s, training_loss=0.506]\u001b[A\n",
            "Epoch 3:  67%|██████▋   | 852/1271 [04:34<02:15,  3.09it/s, training_loss=0.506]\u001b[A\n",
            "Epoch 3:  67%|██████▋   | 852/1271 [04:34<02:15,  3.09it/s, training_loss=0.416]\u001b[A\n",
            "Epoch 3:  67%|██████▋   | 853/1271 [04:34<02:14,  3.11it/s, training_loss=0.416]\u001b[A\n",
            "Epoch 3:  67%|██████▋   | 853/1271 [04:34<02:14,  3.11it/s, training_loss=0.705]\u001b[A\n",
            "Epoch 3:  67%|██████▋   | 854/1271 [04:34<02:13,  3.12it/s, training_loss=0.705]\u001b[A\n",
            "Epoch 3:  67%|██████▋   | 854/1271 [04:35<02:13,  3.12it/s, training_loss=0.674]\u001b[A\n",
            "Epoch 3:  67%|██████▋   | 855/1271 [04:35<02:13,  3.12it/s, training_loss=0.674]\u001b[A\n",
            "Epoch 3:  67%|██████▋   | 855/1271 [04:35<02:13,  3.12it/s, training_loss=0.020]\u001b[A\n",
            "Epoch 3:  67%|██████▋   | 856/1271 [04:35<02:13,  3.11it/s, training_loss=0.020]\u001b[A\n",
            "Epoch 3:  67%|██████▋   | 856/1271 [04:35<02:13,  3.11it/s, training_loss=0.133]\u001b[A\n",
            "Epoch 3:  67%|██████▋   | 857/1271 [04:35<02:12,  3.13it/s, training_loss=0.133]\u001b[A\n",
            "Epoch 3:  67%|██████▋   | 857/1271 [04:36<02:12,  3.13it/s, training_loss=0.927]\u001b[A\n",
            "Epoch 3:  68%|██████▊   | 858/1271 [04:36<02:12,  3.13it/s, training_loss=0.927]\u001b[A\n",
            "Epoch 3:  68%|██████▊   | 858/1271 [04:36<02:12,  3.13it/s, training_loss=0.314]\u001b[A\n",
            "Epoch 3:  68%|██████▊   | 859/1271 [04:36<02:12,  3.10it/s, training_loss=0.314]\u001b[A\n",
            "Epoch 3:  68%|██████▊   | 859/1271 [04:36<02:12,  3.10it/s, training_loss=0.156]\u001b[A\n",
            "Epoch 3:  68%|██████▊   | 860/1271 [04:36<02:12,  3.09it/s, training_loss=0.156]\u001b[A\n",
            "Epoch 3:  68%|██████▊   | 860/1271 [04:37<02:12,  3.09it/s, training_loss=0.410]\u001b[A\n",
            "Epoch 3:  68%|██████▊   | 861/1271 [04:37<02:12,  3.10it/s, training_loss=0.410]\u001b[A\n",
            "Epoch 3:  68%|██████▊   | 861/1271 [04:37<02:12,  3.10it/s, training_loss=0.559]\u001b[A\n",
            "Epoch 3:  68%|██████▊   | 862/1271 [04:37<02:11,  3.11it/s, training_loss=0.559]\u001b[A\n",
            "Epoch 3:  68%|██████▊   | 862/1271 [04:37<02:11,  3.11it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 3:  68%|██████▊   | 863/1271 [04:37<02:12,  3.09it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 3:  68%|██████▊   | 863/1271 [04:38<02:12,  3.09it/s, training_loss=0.252]\u001b[A\n",
            "Epoch 3:  68%|██████▊   | 864/1271 [04:38<02:12,  3.07it/s, training_loss=0.252]\u001b[A\n",
            "Epoch 3:  68%|██████▊   | 864/1271 [04:38<02:12,  3.07it/s, training_loss=0.131]\u001b[A\n",
            "Epoch 3:  68%|██████▊   | 865/1271 [04:38<02:11,  3.08it/s, training_loss=0.131]\u001b[A\n",
            "Epoch 3:  68%|██████▊   | 865/1271 [04:38<02:11,  3.08it/s, training_loss=0.661]\u001b[A\n",
            "Epoch 3:  68%|██████▊   | 866/1271 [04:38<02:11,  3.09it/s, training_loss=0.661]\u001b[A\n",
            "Epoch 3:  68%|██████▊   | 866/1271 [04:39<02:11,  3.09it/s, training_loss=0.256]\u001b[A\n",
            "Epoch 3:  68%|██████▊   | 867/1271 [04:39<02:10,  3.09it/s, training_loss=0.256]\u001b[A\n",
            "Epoch 3:  68%|██████▊   | 867/1271 [04:39<02:10,  3.09it/s, training_loss=0.482]\u001b[A\n",
            "Epoch 3:  68%|██████▊   | 868/1271 [04:39<02:09,  3.10it/s, training_loss=0.482]\u001b[A\n",
            "Epoch 3:  68%|██████▊   | 868/1271 [04:39<02:09,  3.10it/s, training_loss=0.390]\u001b[A\n",
            "Epoch 3:  68%|██████▊   | 869/1271 [04:39<02:10,  3.09it/s, training_loss=0.390]\u001b[A\n",
            "Epoch 3:  68%|██████▊   | 869/1271 [04:40<02:10,  3.09it/s, training_loss=0.315]\u001b[A\n",
            "Epoch 3:  68%|██████▊   | 870/1271 [04:40<02:09,  3.08it/s, training_loss=0.315]\u001b[A\n",
            "Epoch 3:  68%|██████▊   | 870/1271 [04:40<02:09,  3.08it/s, training_loss=0.096]\u001b[A\n",
            "Epoch 3:  69%|██████▊   | 871/1271 [04:40<02:09,  3.09it/s, training_loss=0.096]\u001b[A\n",
            "Epoch 3:  69%|██████▊   | 871/1271 [04:40<02:09,  3.09it/s, training_loss=0.215]\u001b[A\n",
            "Epoch 3:  69%|██████▊   | 872/1271 [04:40<02:08,  3.12it/s, training_loss=0.215]\u001b[A\n",
            "Epoch 3:  69%|██████▊   | 872/1271 [04:41<02:08,  3.12it/s, training_loss=0.293]\u001b[A\n",
            "Epoch 3:  69%|██████▊   | 873/1271 [04:41<02:08,  3.10it/s, training_loss=0.293]\u001b[A\n",
            "Epoch 3:  69%|██████▊   | 873/1271 [04:41<02:08,  3.10it/s, training_loss=0.878]\u001b[A\n",
            "Epoch 3:  69%|██████▉   | 874/1271 [04:41<02:07,  3.11it/s, training_loss=0.878]\u001b[A\n",
            "Epoch 3:  69%|██████▉   | 874/1271 [04:41<02:07,  3.11it/s, training_loss=0.170]\u001b[A\n",
            "Epoch 3:  69%|██████▉   | 875/1271 [04:41<02:07,  3.11it/s, training_loss=0.170]\u001b[A\n",
            "Epoch 3:  69%|██████▉   | 875/1271 [04:41<02:07,  3.11it/s, training_loss=0.157]\u001b[A\n",
            "Epoch 3:  69%|██████▉   | 876/1271 [04:41<02:07,  3.10it/s, training_loss=0.157]\u001b[A\n",
            "Epoch 3:  69%|██████▉   | 876/1271 [04:42<02:07,  3.10it/s, training_loss=0.251]\u001b[A\n",
            "Epoch 3:  69%|██████▉   | 877/1271 [04:42<02:07,  3.10it/s, training_loss=0.251]\u001b[A\n",
            "Epoch 3:  69%|██████▉   | 877/1271 [04:42<02:07,  3.10it/s, training_loss=0.433]\u001b[A\n",
            "Epoch 3:  69%|██████▉   | 878/1271 [04:42<02:06,  3.12it/s, training_loss=0.433]\u001b[A\n",
            "Epoch 3:  69%|██████▉   | 878/1271 [04:42<02:06,  3.12it/s, training_loss=0.130]\u001b[A\n",
            "Epoch 3:  69%|██████▉   | 879/1271 [04:42<02:05,  3.12it/s, training_loss=0.130]\u001b[A\n",
            "Epoch 3:  69%|██████▉   | 879/1271 [04:43<02:05,  3.12it/s, training_loss=0.401]\u001b[A\n",
            "Epoch 3:  69%|██████▉   | 880/1271 [04:43<02:05,  3.11it/s, training_loss=0.401]\u001b[A\n",
            "Epoch 3:  69%|██████▉   | 880/1271 [04:43<02:05,  3.11it/s, training_loss=0.673]\u001b[A\n",
            "Epoch 3:  69%|██████▉   | 881/1271 [04:43<02:05,  3.10it/s, training_loss=0.673]\u001b[A\n",
            "Epoch 3:  69%|██████▉   | 881/1271 [04:43<02:05,  3.10it/s, training_loss=0.250]\u001b[A\n",
            "Epoch 3:  69%|██████▉   | 882/1271 [04:43<02:05,  3.11it/s, training_loss=0.250]\u001b[A\n",
            "Epoch 3:  69%|██████▉   | 882/1271 [04:44<02:05,  3.11it/s, training_loss=0.162]\u001b[A\n",
            "Epoch 3:  69%|██████▉   | 883/1271 [04:44<02:04,  3.11it/s, training_loss=0.162]\u001b[A\n",
            "Epoch 3:  69%|██████▉   | 883/1271 [04:44<02:04,  3.11it/s, training_loss=0.141]\u001b[A\n",
            "Epoch 3:  70%|██████▉   | 884/1271 [04:44<02:04,  3.12it/s, training_loss=0.141]\u001b[A\n",
            "Epoch 3:  70%|██████▉   | 884/1271 [04:44<02:04,  3.12it/s, training_loss=0.393]\u001b[A\n",
            "Epoch 3:  70%|██████▉   | 885/1271 [04:44<02:03,  3.13it/s, training_loss=0.393]\u001b[A\n",
            "Epoch 3:  70%|██████▉   | 885/1271 [04:45<02:03,  3.13it/s, training_loss=0.007]\u001b[A\n",
            "Epoch 3:  70%|██████▉   | 886/1271 [04:45<02:02,  3.13it/s, training_loss=0.007]\u001b[A\n",
            "Epoch 3:  70%|██████▉   | 886/1271 [04:45<02:02,  3.13it/s, training_loss=0.155]\u001b[A\n",
            "Epoch 3:  70%|██████▉   | 887/1271 [04:45<02:03,  3.12it/s, training_loss=0.155]\u001b[A\n",
            "Epoch 3:  70%|██████▉   | 887/1271 [04:45<02:03,  3.12it/s, training_loss=0.264]\u001b[A\n",
            "Epoch 3:  70%|██████▉   | 888/1271 [04:45<02:02,  3.12it/s, training_loss=0.264]\u001b[A\n",
            "Epoch 3:  70%|██████▉   | 888/1271 [04:46<02:02,  3.12it/s, training_loss=0.397]\u001b[A\n",
            "Epoch 3:  70%|██████▉   | 889/1271 [04:46<02:02,  3.12it/s, training_loss=0.397]\u001b[A\n",
            "Epoch 3:  70%|██████▉   | 889/1271 [04:46<02:02,  3.12it/s, training_loss=0.584]\u001b[A\n",
            "Epoch 3:  70%|███████   | 890/1271 [04:46<02:02,  3.11it/s, training_loss=0.584]\u001b[A\n",
            "Epoch 3:  70%|███████   | 890/1271 [04:46<02:02,  3.11it/s, training_loss=0.156]\u001b[A\n",
            "Epoch 3:  70%|███████   | 891/1271 [04:46<02:01,  3.12it/s, training_loss=0.156]\u001b[A\n",
            "Epoch 3:  70%|███████   | 891/1271 [04:47<02:01,  3.12it/s, training_loss=0.106]\u001b[A\n",
            "Epoch 3:  70%|███████   | 892/1271 [04:47<02:02,  3.09it/s, training_loss=0.106]\u001b[A\n",
            "Epoch 3:  70%|███████   | 892/1271 [04:47<02:02,  3.09it/s, training_loss=0.051]\u001b[A\n",
            "Epoch 3:  70%|███████   | 893/1271 [04:47<02:02,  3.09it/s, training_loss=0.051]\u001b[A\n",
            "Epoch 3:  70%|███████   | 893/1271 [04:47<02:02,  3.09it/s, training_loss=0.296]\u001b[A\n",
            "Epoch 3:  70%|███████   | 894/1271 [04:47<02:02,  3.09it/s, training_loss=0.296]\u001b[A\n",
            "Epoch 3:  70%|███████   | 894/1271 [04:48<02:02,  3.09it/s, training_loss=0.217]\u001b[A\n",
            "Epoch 3:  70%|███████   | 895/1271 [04:48<02:01,  3.10it/s, training_loss=0.217]\u001b[A\n",
            "Epoch 3:  70%|███████   | 895/1271 [04:48<02:01,  3.10it/s, training_loss=0.093]\u001b[A\n",
            "Epoch 3:  70%|███████   | 896/1271 [04:48<02:02,  3.06it/s, training_loss=0.093]\u001b[A\n",
            "Epoch 3:  70%|███████   | 896/1271 [04:48<02:02,  3.06it/s, training_loss=0.297]\u001b[A\n",
            "Epoch 3:  71%|███████   | 897/1271 [04:48<02:01,  3.08it/s, training_loss=0.297]\u001b[A\n",
            "Epoch 3:  71%|███████   | 897/1271 [04:49<02:01,  3.08it/s, training_loss=0.069]\u001b[A\n",
            "Epoch 3:  71%|███████   | 898/1271 [04:49<02:01,  3.07it/s, training_loss=0.069]\u001b[A\n",
            "Epoch 3:  71%|███████   | 898/1271 [04:49<02:01,  3.07it/s, training_loss=0.055]\u001b[A\n",
            "Epoch 3:  71%|███████   | 899/1271 [04:49<02:00,  3.08it/s, training_loss=0.055]\u001b[A\n",
            "Epoch 3:  71%|███████   | 899/1271 [04:49<02:00,  3.08it/s, training_loss=0.095]\u001b[A\n",
            "Epoch 3:  71%|███████   | 900/1271 [04:49<01:59,  3.10it/s, training_loss=0.095]\u001b[A\n",
            "Epoch 3:  71%|███████   | 900/1271 [04:50<01:59,  3.10it/s, training_loss=0.221]\u001b[A\n",
            "Epoch 3:  71%|███████   | 901/1271 [04:50<01:59,  3.09it/s, training_loss=0.221]\u001b[A\n",
            "Epoch 3:  71%|███████   | 901/1271 [04:50<01:59,  3.09it/s, training_loss=0.385]\u001b[A\n",
            "Epoch 3:  71%|███████   | 902/1271 [04:50<01:58,  3.11it/s, training_loss=0.385]\u001b[A\n",
            "Epoch 3:  71%|███████   | 902/1271 [04:50<01:58,  3.11it/s, training_loss=0.446]\u001b[A\n",
            "Epoch 3:  71%|███████   | 903/1271 [04:50<01:58,  3.12it/s, training_loss=0.446]\u001b[A\n",
            "Epoch 3:  71%|███████   | 903/1271 [04:51<01:58,  3.12it/s, training_loss=0.475]\u001b[A\n",
            "Epoch 3:  71%|███████   | 904/1271 [04:51<01:57,  3.12it/s, training_loss=0.475]\u001b[A\n",
            "Epoch 3:  71%|███████   | 904/1271 [04:51<01:57,  3.12it/s, training_loss=0.492]\u001b[A\n",
            "Epoch 3:  71%|███████   | 905/1271 [04:51<01:57,  3.12it/s, training_loss=0.492]\u001b[A\n",
            "Epoch 3:  71%|███████   | 905/1271 [04:51<01:57,  3.12it/s, training_loss=0.554]\u001b[A\n",
            "Epoch 3:  71%|███████▏  | 906/1271 [04:51<01:57,  3.12it/s, training_loss=0.554]\u001b[A\n",
            "Epoch 3:  71%|███████▏  | 906/1271 [04:51<01:57,  3.12it/s, training_loss=0.080]\u001b[A\n",
            "Epoch 3:  71%|███████▏  | 907/1271 [04:51<01:57,  3.11it/s, training_loss=0.080]\u001b[A\n",
            "Epoch 3:  71%|███████▏  | 907/1271 [04:52<01:57,  3.11it/s, training_loss=0.659]\u001b[A\n",
            "Epoch 3:  71%|███████▏  | 908/1271 [04:52<01:56,  3.12it/s, training_loss=0.659]\u001b[A\n",
            "Epoch 3:  71%|███████▏  | 908/1271 [04:52<01:56,  3.12it/s, training_loss=0.800]\u001b[A\n",
            "Epoch 3:  72%|███████▏  | 909/1271 [04:52<01:56,  3.11it/s, training_loss=0.800]\u001b[A\n",
            "Epoch 3:  72%|███████▏  | 909/1271 [04:52<01:56,  3.11it/s, training_loss=0.395]\u001b[A\n",
            "Epoch 3:  72%|███████▏  | 910/1271 [04:52<01:55,  3.11it/s, training_loss=0.395]\u001b[A\n",
            "Epoch 3:  72%|███████▏  | 910/1271 [04:53<01:55,  3.11it/s, training_loss=0.272]\u001b[A\n",
            "Epoch 3:  72%|███████▏  | 911/1271 [04:53<01:56,  3.09it/s, training_loss=0.272]\u001b[A\n",
            "Epoch 3:  72%|███████▏  | 911/1271 [04:53<01:56,  3.09it/s, training_loss=0.068]\u001b[A\n",
            "Epoch 3:  72%|███████▏  | 912/1271 [04:53<01:55,  3.10it/s, training_loss=0.068]\u001b[A\n",
            "Epoch 3:  72%|███████▏  | 912/1271 [04:53<01:55,  3.10it/s, training_loss=0.230]\u001b[A\n",
            "Epoch 3:  72%|███████▏  | 913/1271 [04:53<01:55,  3.10it/s, training_loss=0.230]\u001b[A\n",
            "Epoch 3:  72%|███████▏  | 913/1271 [04:54<01:55,  3.10it/s, training_loss=0.409]\u001b[A\n",
            "Epoch 3:  72%|███████▏  | 914/1271 [04:54<01:54,  3.11it/s, training_loss=0.409]\u001b[A\n",
            "Epoch 3:  72%|███████▏  | 914/1271 [04:54<01:54,  3.11it/s, training_loss=0.139]\u001b[A\n",
            "Epoch 3:  72%|███████▏  | 915/1271 [04:54<01:54,  3.11it/s, training_loss=0.139]\u001b[A\n",
            "Epoch 3:  72%|███████▏  | 915/1271 [04:54<01:54,  3.11it/s, training_loss=0.163]\u001b[A\n",
            "Epoch 3:  72%|███████▏  | 916/1271 [04:54<01:53,  3.13it/s, training_loss=0.163]\u001b[A\n",
            "Epoch 3:  72%|███████▏  | 916/1271 [04:55<01:53,  3.13it/s, training_loss=0.171]\u001b[A\n",
            "Epoch 3:  72%|███████▏  | 917/1271 [04:55<01:52,  3.14it/s, training_loss=0.171]\u001b[A\n",
            "Epoch 3:  72%|███████▏  | 917/1271 [04:55<01:52,  3.14it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 3:  72%|███████▏  | 918/1271 [04:55<01:52,  3.15it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 3:  72%|███████▏  | 918/1271 [04:55<01:52,  3.15it/s, training_loss=0.024]\u001b[A\n",
            "Epoch 3:  72%|███████▏  | 919/1271 [04:55<01:52,  3.14it/s, training_loss=0.024]\u001b[A\n",
            "Epoch 3:  72%|███████▏  | 919/1271 [04:56<01:52,  3.14it/s, training_loss=0.383]\u001b[A\n",
            "Epoch 3:  72%|███████▏  | 920/1271 [04:56<01:51,  3.14it/s, training_loss=0.383]\u001b[A\n",
            "Epoch 3:  72%|███████▏  | 920/1271 [04:56<01:51,  3.14it/s, training_loss=0.142]\u001b[A\n",
            "Epoch 3:  72%|███████▏  | 921/1271 [04:56<01:52,  3.12it/s, training_loss=0.142]\u001b[A\n",
            "Epoch 3:  72%|███████▏  | 921/1271 [04:56<01:52,  3.12it/s, training_loss=0.221]\u001b[A\n",
            "Epoch 3:  73%|███████▎  | 922/1271 [04:56<01:51,  3.13it/s, training_loss=0.221]\u001b[A\n",
            "Epoch 3:  73%|███████▎  | 922/1271 [04:57<01:51,  3.13it/s, training_loss=0.199]\u001b[A\n",
            "Epoch 3:  73%|███████▎  | 923/1271 [04:57<01:51,  3.13it/s, training_loss=0.199]\u001b[A\n",
            "Epoch 3:  73%|███████▎  | 923/1271 [04:57<01:51,  3.13it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 3:  73%|███████▎  | 924/1271 [04:57<01:51,  3.12it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 3:  73%|███████▎  | 924/1271 [04:57<01:51,  3.12it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 3:  73%|███████▎  | 925/1271 [04:57<01:50,  3.12it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 3:  73%|███████▎  | 925/1271 [04:58<01:50,  3.12it/s, training_loss=0.535]\u001b[A\n",
            "Epoch 3:  73%|███████▎  | 926/1271 [04:58<01:51,  3.10it/s, training_loss=0.535]\u001b[A\n",
            "Epoch 3:  73%|███████▎  | 926/1271 [04:58<01:51,  3.10it/s, training_loss=0.524]\u001b[A\n",
            "Epoch 3:  73%|███████▎  | 927/1271 [04:58<01:51,  3.10it/s, training_loss=0.524]\u001b[A\n",
            "Epoch 3:  73%|███████▎  | 927/1271 [04:58<01:51,  3.10it/s, training_loss=0.407]\u001b[A\n",
            "Epoch 3:  73%|███████▎  | 928/1271 [04:58<01:51,  3.09it/s, training_loss=0.407]\u001b[A\n",
            "Epoch 3:  73%|███████▎  | 928/1271 [04:59<01:51,  3.09it/s, training_loss=0.267]\u001b[A\n",
            "Epoch 3:  73%|███████▎  | 929/1271 [04:59<01:50,  3.10it/s, training_loss=0.267]\u001b[A\n",
            "Epoch 3:  73%|███████▎  | 929/1271 [04:59<01:50,  3.10it/s, training_loss=0.898]\u001b[A\n",
            "Epoch 3:  73%|███████▎  | 930/1271 [04:59<01:49,  3.10it/s, training_loss=0.898]\u001b[A\n",
            "Epoch 3:  73%|███████▎  | 930/1271 [04:59<01:49,  3.10it/s, training_loss=0.448]\u001b[A\n",
            "Epoch 3:  73%|███████▎  | 931/1271 [04:59<01:49,  3.11it/s, training_loss=0.448]\u001b[A\n",
            "Epoch 3:  73%|███████▎  | 931/1271 [04:59<01:49,  3.11it/s, training_loss=0.497]\u001b[A\n",
            "Epoch 3:  73%|███████▎  | 932/1271 [05:00<01:49,  3.09it/s, training_loss=0.497]\u001b[A\n",
            "Epoch 3:  73%|███████▎  | 932/1271 [05:00<01:49,  3.09it/s, training_loss=0.519]\u001b[A\n",
            "Epoch 3:  73%|███████▎  | 933/1271 [05:00<01:49,  3.09it/s, training_loss=0.519]\u001b[A\n",
            "Epoch 3:  73%|███████▎  | 933/1271 [05:00<01:49,  3.09it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 3:  73%|███████▎  | 934/1271 [05:00<01:49,  3.08it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 3:  73%|███████▎  | 934/1271 [05:00<01:49,  3.08it/s, training_loss=0.286]\u001b[A\n",
            "Epoch 3:  74%|███████▎  | 935/1271 [05:00<01:48,  3.09it/s, training_loss=0.286]\u001b[A\n",
            "Epoch 3:  74%|███████▎  | 935/1271 [05:01<01:48,  3.09it/s, training_loss=0.145]\u001b[A\n",
            "Epoch 3:  74%|███████▎  | 936/1271 [05:01<01:48,  3.09it/s, training_loss=0.145]\u001b[A\n",
            "Epoch 3:  74%|███████▎  | 936/1271 [05:01<01:48,  3.09it/s, training_loss=0.324]\u001b[A\n",
            "Epoch 3:  74%|███████▎  | 937/1271 [05:01<01:47,  3.11it/s, training_loss=0.324]\u001b[A\n",
            "Epoch 3:  74%|███████▎  | 937/1271 [05:01<01:47,  3.11it/s, training_loss=0.189]\u001b[A\n",
            "Epoch 3:  74%|███████▍  | 938/1271 [05:01<01:47,  3.10it/s, training_loss=0.189]\u001b[A\n",
            "Epoch 3:  74%|███████▍  | 938/1271 [05:02<01:47,  3.10it/s, training_loss=0.264]\u001b[A\n",
            "Epoch 3:  74%|███████▍  | 939/1271 [05:02<01:46,  3.12it/s, training_loss=0.264]\u001b[A\n",
            "Epoch 3:  74%|███████▍  | 939/1271 [05:02<01:46,  3.12it/s, training_loss=0.330]\u001b[A\n",
            "Epoch 3:  74%|███████▍  | 940/1271 [05:02<01:46,  3.11it/s, training_loss=0.330]\u001b[A\n",
            "Epoch 3:  74%|███████▍  | 940/1271 [05:02<01:46,  3.11it/s, training_loss=0.091]\u001b[A\n",
            "Epoch 3:  74%|███████▍  | 941/1271 [05:02<01:46,  3.10it/s, training_loss=0.091]\u001b[A\n",
            "Epoch 3:  74%|███████▍  | 941/1271 [05:03<01:46,  3.10it/s, training_loss=0.058]\u001b[A\n",
            "Epoch 3:  74%|███████▍  | 942/1271 [05:03<01:45,  3.12it/s, training_loss=0.058]\u001b[A\n",
            "Epoch 3:  74%|███████▍  | 942/1271 [05:03<01:45,  3.12it/s, training_loss=0.703]\u001b[A\n",
            "Epoch 3:  74%|███████▍  | 943/1271 [05:03<01:45,  3.12it/s, training_loss=0.703]\u001b[A\n",
            "Epoch 3:  74%|███████▍  | 943/1271 [05:03<01:45,  3.12it/s, training_loss=0.229]\u001b[A\n",
            "Epoch 3:  74%|███████▍  | 944/1271 [05:03<01:44,  3.12it/s, training_loss=0.229]\u001b[A\n",
            "Epoch 3:  74%|███████▍  | 944/1271 [05:04<01:44,  3.12it/s, training_loss=0.265]\u001b[A\n",
            "Epoch 3:  74%|███████▍  | 945/1271 [05:04<01:45,  3.10it/s, training_loss=0.265]\u001b[A\n",
            "Epoch 3:  74%|███████▍  | 945/1271 [05:04<01:45,  3.10it/s, training_loss=0.191]\u001b[A\n",
            "Epoch 3:  74%|███████▍  | 946/1271 [05:04<01:44,  3.11it/s, training_loss=0.191]\u001b[A\n",
            "Epoch 3:  74%|███████▍  | 946/1271 [05:04<01:44,  3.11it/s, training_loss=0.390]\u001b[A\n",
            "Epoch 3:  75%|███████▍  | 947/1271 [05:04<01:44,  3.09it/s, training_loss=0.390]\u001b[A\n",
            "Epoch 3:  75%|███████▍  | 947/1271 [05:05<01:44,  3.09it/s, training_loss=0.291]\u001b[A\n",
            "Epoch 3:  75%|███████▍  | 948/1271 [05:05<01:44,  3.10it/s, training_loss=0.291]\u001b[A\n",
            "Epoch 3:  75%|███████▍  | 948/1271 [05:05<01:44,  3.10it/s, training_loss=0.053]\u001b[A\n",
            "Epoch 3:  75%|███████▍  | 949/1271 [05:05<01:42,  3.13it/s, training_loss=0.053]\u001b[A\n",
            "Epoch 3:  75%|███████▍  | 949/1271 [05:05<01:42,  3.13it/s, training_loss=0.359]\u001b[A\n",
            "Epoch 3:  75%|███████▍  | 950/1271 [05:05<01:42,  3.14it/s, training_loss=0.359]\u001b[A\n",
            "Epoch 3:  75%|███████▍  | 950/1271 [05:06<01:42,  3.14it/s, training_loss=0.018]\u001b[A\n",
            "Epoch 3:  75%|███████▍  | 951/1271 [05:06<01:42,  3.14it/s, training_loss=0.018]\u001b[A\n",
            "Epoch 3:  75%|███████▍  | 951/1271 [05:06<01:42,  3.14it/s, training_loss=0.130]\u001b[A\n",
            "Epoch 3:  75%|███████▍  | 952/1271 [05:06<01:41,  3.13it/s, training_loss=0.130]\u001b[A\n",
            "Epoch 3:  75%|███████▍  | 952/1271 [05:06<01:41,  3.13it/s, training_loss=0.357]\u001b[A\n",
            "Epoch 3:  75%|███████▍  | 953/1271 [05:06<01:41,  3.12it/s, training_loss=0.357]\u001b[A\n",
            "Epoch 3:  75%|███████▍  | 953/1271 [05:07<01:41,  3.12it/s, training_loss=0.137]\u001b[A\n",
            "Epoch 3:  75%|███████▌  | 954/1271 [05:07<01:41,  3.12it/s, training_loss=0.137]\u001b[A\n",
            "Epoch 3:  75%|███████▌  | 954/1271 [05:07<01:41,  3.12it/s, training_loss=0.079]\u001b[A\n",
            "Epoch 3:  75%|███████▌  | 955/1271 [05:07<01:41,  3.12it/s, training_loss=0.079]\u001b[A\n",
            "Epoch 3:  75%|███████▌  | 955/1271 [05:07<01:41,  3.12it/s, training_loss=0.630]\u001b[A\n",
            "Epoch 3:  75%|███████▌  | 956/1271 [05:07<01:41,  3.12it/s, training_loss=0.630]\u001b[A\n",
            "Epoch 3:  75%|███████▌  | 956/1271 [05:08<01:41,  3.12it/s, training_loss=0.461]\u001b[A\n",
            "Epoch 3:  75%|███████▌  | 957/1271 [05:08<01:40,  3.11it/s, training_loss=0.461]\u001b[A\n",
            "Epoch 3:  75%|███████▌  | 957/1271 [05:08<01:40,  3.11it/s, training_loss=0.248]\u001b[A\n",
            "Epoch 3:  75%|███████▌  | 958/1271 [05:08<01:39,  3.13it/s, training_loss=0.248]\u001b[A\n",
            "Epoch 3:  75%|███████▌  | 958/1271 [05:08<01:39,  3.13it/s, training_loss=0.650]\u001b[A\n",
            "Epoch 3:  75%|███████▌  | 959/1271 [05:08<01:39,  3.14it/s, training_loss=0.650]\u001b[A\n",
            "Epoch 3:  75%|███████▌  | 959/1271 [05:08<01:39,  3.14it/s, training_loss=0.373]\u001b[A\n",
            "Epoch 3:  76%|███████▌  | 960/1271 [05:08<01:39,  3.13it/s, training_loss=0.373]\u001b[A\n",
            "Epoch 3:  76%|███████▌  | 960/1271 [05:09<01:39,  3.13it/s, training_loss=0.471]\u001b[A\n",
            "Epoch 3:  76%|███████▌  | 961/1271 [05:09<01:38,  3.13it/s, training_loss=0.471]\u001b[A\n",
            "Epoch 3:  76%|███████▌  | 961/1271 [05:09<01:38,  3.13it/s, training_loss=0.311]\u001b[A\n",
            "Epoch 3:  76%|███████▌  | 962/1271 [05:09<01:38,  3.14it/s, training_loss=0.311]\u001b[A\n",
            "Epoch 3:  76%|███████▌  | 962/1271 [05:09<01:38,  3.14it/s, training_loss=0.160]\u001b[A\n",
            "Epoch 3:  76%|███████▌  | 963/1271 [05:09<01:38,  3.13it/s, training_loss=0.160]\u001b[A\n",
            "Epoch 3:  76%|███████▌  | 963/1271 [05:10<01:38,  3.13it/s, training_loss=0.304]\u001b[A\n",
            "Epoch 3:  76%|███████▌  | 964/1271 [05:10<01:38,  3.12it/s, training_loss=0.304]\u001b[A\n",
            "Epoch 3:  76%|███████▌  | 964/1271 [05:10<01:38,  3.12it/s, training_loss=0.405]\u001b[A\n",
            "Epoch 3:  76%|███████▌  | 965/1271 [05:10<01:38,  3.11it/s, training_loss=0.405]\u001b[A\n",
            "Epoch 3:  76%|███████▌  | 965/1271 [05:10<01:38,  3.11it/s, training_loss=0.104]\u001b[A\n",
            "Epoch 3:  76%|███████▌  | 966/1271 [05:10<01:37,  3.12it/s, training_loss=0.104]\u001b[A\n",
            "Epoch 3:  76%|███████▌  | 966/1271 [05:11<01:37,  3.12it/s, training_loss=0.500]\u001b[A\n",
            "Epoch 3:  76%|███████▌  | 967/1271 [05:11<01:37,  3.12it/s, training_loss=0.500]\u001b[A\n",
            "Epoch 3:  76%|███████▌  | 967/1271 [05:11<01:37,  3.12it/s, training_loss=0.090]\u001b[A\n",
            "Epoch 3:  76%|███████▌  | 968/1271 [05:11<01:37,  3.10it/s, training_loss=0.090]\u001b[A\n",
            "Epoch 3:  76%|███████▌  | 968/1271 [05:11<01:37,  3.10it/s, training_loss=0.072]\u001b[A\n",
            "Epoch 3:  76%|███████▌  | 969/1271 [05:11<01:37,  3.10it/s, training_loss=0.072]\u001b[A\n",
            "Epoch 3:  76%|███████▌  | 969/1271 [05:12<01:37,  3.10it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 3:  76%|███████▋  | 970/1271 [05:12<01:36,  3.12it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 3:  76%|███████▋  | 970/1271 [05:12<01:36,  3.12it/s, training_loss=0.349]\u001b[A\n",
            "Epoch 3:  76%|███████▋  | 971/1271 [05:12<01:36,  3.12it/s, training_loss=0.349]\u001b[A\n",
            "Epoch 3:  76%|███████▋  | 971/1271 [05:12<01:36,  3.12it/s, training_loss=0.178]\u001b[A\n",
            "Epoch 3:  76%|███████▋  | 972/1271 [05:12<01:35,  3.12it/s, training_loss=0.178]\u001b[A\n",
            "Epoch 3:  76%|███████▋  | 972/1271 [05:13<01:35,  3.12it/s, training_loss=0.372]\u001b[A\n",
            "Epoch 3:  77%|███████▋  | 973/1271 [05:13<01:35,  3.11it/s, training_loss=0.372]\u001b[A\n",
            "Epoch 3:  77%|███████▋  | 973/1271 [05:13<01:35,  3.11it/s, training_loss=0.626]\u001b[A\n",
            "Epoch 3:  77%|███████▋  | 974/1271 [05:13<01:35,  3.12it/s, training_loss=0.626]\u001b[A\n",
            "Epoch 3:  77%|███████▋  | 974/1271 [05:13<01:35,  3.12it/s, training_loss=0.419]\u001b[A\n",
            "Epoch 3:  77%|███████▋  | 975/1271 [05:13<01:34,  3.13it/s, training_loss=0.419]\u001b[A\n",
            "Epoch 3:  77%|███████▋  | 975/1271 [05:14<01:34,  3.13it/s, training_loss=0.656]\u001b[A\n",
            "Epoch 3:  77%|███████▋  | 976/1271 [05:14<01:34,  3.13it/s, training_loss=0.656]\u001b[A\n",
            "Epoch 3:  77%|███████▋  | 976/1271 [05:14<01:34,  3.13it/s, training_loss=0.194]\u001b[A\n",
            "Epoch 3:  77%|███████▋  | 977/1271 [05:14<01:34,  3.12it/s, training_loss=0.194]\u001b[A\n",
            "Epoch 3:  77%|███████▋  | 977/1271 [05:14<01:34,  3.12it/s, training_loss=0.237]\u001b[A\n",
            "Epoch 3:  77%|███████▋  | 978/1271 [05:14<01:33,  3.12it/s, training_loss=0.237]\u001b[A\n",
            "Epoch 3:  77%|███████▋  | 978/1271 [05:15<01:33,  3.12it/s, training_loss=0.485]\u001b[A\n",
            "Epoch 3:  77%|███████▋  | 979/1271 [05:15<01:33,  3.12it/s, training_loss=0.485]\u001b[A\n",
            "Epoch 3:  77%|███████▋  | 979/1271 [05:15<01:33,  3.12it/s, training_loss=0.347]\u001b[A\n",
            "Epoch 3:  77%|███████▋  | 980/1271 [05:15<01:33,  3.11it/s, training_loss=0.347]\u001b[A\n",
            "Epoch 3:  77%|███████▋  | 980/1271 [05:15<01:33,  3.11it/s, training_loss=0.200]\u001b[A\n",
            "Epoch 3:  77%|███████▋  | 981/1271 [05:15<01:33,  3.10it/s, training_loss=0.200]\u001b[A\n",
            "Epoch 3:  77%|███████▋  | 981/1271 [05:16<01:33,  3.10it/s, training_loss=0.531]\u001b[A\n",
            "Epoch 3:  77%|███████▋  | 982/1271 [05:16<01:33,  3.09it/s, training_loss=0.531]\u001b[A\n",
            "Epoch 3:  77%|███████▋  | 982/1271 [05:16<01:33,  3.09it/s, training_loss=0.670]\u001b[A\n",
            "Epoch 3:  77%|███████▋  | 983/1271 [05:16<01:32,  3.11it/s, training_loss=0.670]\u001b[A\n",
            "Epoch 3:  77%|███████▋  | 983/1271 [05:16<01:32,  3.11it/s, training_loss=0.511]\u001b[A\n",
            "Epoch 3:  77%|███████▋  | 984/1271 [05:16<01:31,  3.12it/s, training_loss=0.511]\u001b[A\n",
            "Epoch 3:  77%|███████▋  | 984/1271 [05:17<01:31,  3.12it/s, training_loss=0.096]\u001b[A\n",
            "Epoch 3:  77%|███████▋  | 985/1271 [05:17<01:31,  3.12it/s, training_loss=0.096]\u001b[A\n",
            "Epoch 3:  77%|███████▋  | 985/1271 [05:17<01:31,  3.12it/s, training_loss=0.168]\u001b[A\n",
            "Epoch 3:  78%|███████▊  | 986/1271 [05:17<01:31,  3.11it/s, training_loss=0.168]\u001b[A\n",
            "Epoch 3:  78%|███████▊  | 986/1271 [05:17<01:31,  3.11it/s, training_loss=0.379]\u001b[A\n",
            "Epoch 3:  78%|███████▊  | 987/1271 [05:17<01:31,  3.11it/s, training_loss=0.379]\u001b[A\n",
            "Epoch 3:  78%|███████▊  | 987/1271 [05:17<01:31,  3.11it/s, training_loss=0.854]\u001b[A\n",
            "Epoch 3:  78%|███████▊  | 988/1271 [05:17<01:30,  3.11it/s, training_loss=0.854]\u001b[A\n",
            "Epoch 3:  78%|███████▊  | 988/1271 [05:18<01:30,  3.11it/s, training_loss=0.386]\u001b[A\n",
            "Epoch 3:  78%|███████▊  | 989/1271 [05:18<01:30,  3.11it/s, training_loss=0.386]\u001b[A\n",
            "Epoch 3:  78%|███████▊  | 989/1271 [05:18<01:30,  3.11it/s, training_loss=0.066]\u001b[A\n",
            "Epoch 3:  78%|███████▊  | 990/1271 [05:18<01:30,  3.11it/s, training_loss=0.066]\u001b[A\n",
            "Epoch 3:  78%|███████▊  | 990/1271 [05:18<01:30,  3.11it/s, training_loss=0.098]\u001b[A\n",
            "Epoch 3:  78%|███████▊  | 991/1271 [05:18<01:29,  3.12it/s, training_loss=0.098]\u001b[A\n",
            "Epoch 3:  78%|███████▊  | 991/1271 [05:19<01:29,  3.12it/s, training_loss=0.540]\u001b[A\n",
            "Epoch 3:  78%|███████▊  | 992/1271 [05:19<01:29,  3.12it/s, training_loss=0.540]\u001b[A\n",
            "Epoch 3:  78%|███████▊  | 992/1271 [05:19<01:29,  3.12it/s, training_loss=0.546]\u001b[A\n",
            "Epoch 3:  78%|███████▊  | 993/1271 [05:19<01:28,  3.12it/s, training_loss=0.546]\u001b[A\n",
            "Epoch 3:  78%|███████▊  | 993/1271 [05:19<01:28,  3.12it/s, training_loss=0.419]\u001b[A\n",
            "Epoch 3:  78%|███████▊  | 994/1271 [05:19<01:28,  3.12it/s, training_loss=0.419]\u001b[A\n",
            "Epoch 3:  78%|███████▊  | 994/1271 [05:20<01:28,  3.12it/s, training_loss=0.188]\u001b[A\n",
            "Epoch 3:  78%|███████▊  | 995/1271 [05:20<01:28,  3.11it/s, training_loss=0.188]\u001b[A\n",
            "Epoch 3:  78%|███████▊  | 995/1271 [05:20<01:28,  3.11it/s, training_loss=0.087]\u001b[A\n",
            "Epoch 3:  78%|███████▊  | 996/1271 [05:20<01:28,  3.12it/s, training_loss=0.087]\u001b[A\n",
            "Epoch 3:  78%|███████▊  | 996/1271 [05:20<01:28,  3.12it/s, training_loss=0.196]\u001b[A\n",
            "Epoch 3:  78%|███████▊  | 997/1271 [05:20<01:27,  3.12it/s, training_loss=0.196]\u001b[A\n",
            "Epoch 3:  78%|███████▊  | 997/1271 [05:21<01:27,  3.12it/s, training_loss=0.589]\u001b[A\n",
            "Epoch 3:  79%|███████▊  | 998/1271 [05:21<01:27,  3.14it/s, training_loss=0.589]\u001b[A\n",
            "Epoch 3:  79%|███████▊  | 998/1271 [05:21<01:27,  3.14it/s, training_loss=0.624]\u001b[A\n",
            "Epoch 3:  79%|███████▊  | 999/1271 [05:21<01:26,  3.14it/s, training_loss=0.624]\u001b[A\n",
            "Epoch 3:  79%|███████▊  | 999/1271 [05:21<01:26,  3.14it/s, training_loss=0.080]\u001b[A\n",
            "Epoch 3:  79%|███████▊  | 1000/1271 [05:21<01:26,  3.14it/s, training_loss=0.080]\u001b[A\n",
            "Epoch 3:  79%|███████▊  | 1000/1271 [05:22<01:26,  3.14it/s, training_loss=0.148]\u001b[A\n",
            "Epoch 3:  79%|███████▉  | 1001/1271 [05:22<01:26,  3.13it/s, training_loss=0.148]\u001b[A\n",
            "Epoch 3:  79%|███████▉  | 1001/1271 [05:22<01:26,  3.13it/s, training_loss=0.369]\u001b[A\n",
            "Epoch 3:  79%|███████▉  | 1002/1271 [05:22<01:26,  3.12it/s, training_loss=0.369]\u001b[A\n",
            "Epoch 3:  79%|███████▉  | 1002/1271 [05:22<01:26,  3.12it/s, training_loss=0.430]\u001b[A\n",
            "Epoch 3:  79%|███████▉  | 1003/1271 [05:22<01:26,  3.11it/s, training_loss=0.430]\u001b[A\n",
            "Epoch 3:  79%|███████▉  | 1003/1271 [05:23<01:26,  3.11it/s, training_loss=0.457]\u001b[A\n",
            "Epoch 3:  79%|███████▉  | 1004/1271 [05:23<01:25,  3.11it/s, training_loss=0.457]\u001b[A\n",
            "Epoch 3:  79%|███████▉  | 1004/1271 [05:23<01:25,  3.11it/s, training_loss=0.150]\u001b[A\n",
            "Epoch 3:  79%|███████▉  | 1005/1271 [05:23<01:25,  3.11it/s, training_loss=0.150]\u001b[A\n",
            "Epoch 3:  79%|███████▉  | 1005/1271 [05:23<01:25,  3.11it/s, training_loss=0.220]\u001b[A\n",
            "Epoch 3:  79%|███████▉  | 1006/1271 [05:23<01:25,  3.10it/s, training_loss=0.220]\u001b[A\n",
            "Epoch 3:  79%|███████▉  | 1006/1271 [05:24<01:25,  3.10it/s, training_loss=0.336]\u001b[A\n",
            "Epoch 3:  79%|███████▉  | 1007/1271 [05:24<01:24,  3.11it/s, training_loss=0.336]\u001b[A\n",
            "Epoch 3:  79%|███████▉  | 1007/1271 [05:24<01:24,  3.11it/s, training_loss=0.118]\u001b[A\n",
            "Epoch 3:  79%|███████▉  | 1008/1271 [05:24<01:24,  3.11it/s, training_loss=0.118]\u001b[A\n",
            "Epoch 3:  79%|███████▉  | 1008/1271 [05:24<01:24,  3.11it/s, training_loss=0.079]\u001b[A\n",
            "Epoch 3:  79%|███████▉  | 1009/1271 [05:24<01:23,  3.12it/s, training_loss=0.079]\u001b[A\n",
            "Epoch 3:  79%|███████▉  | 1009/1271 [05:25<01:23,  3.12it/s, training_loss=0.017]\u001b[A\n",
            "Epoch 3:  79%|███████▉  | 1010/1271 [05:25<01:24,  3.11it/s, training_loss=0.017]\u001b[A\n",
            "Epoch 3:  79%|███████▉  | 1010/1271 [05:25<01:24,  3.11it/s, training_loss=0.190]\u001b[A\n",
            "Epoch 3:  80%|███████▉  | 1011/1271 [05:25<01:23,  3.12it/s, training_loss=0.190]\u001b[A\n",
            "Epoch 3:  80%|███████▉  | 1011/1271 [05:25<01:23,  3.12it/s, training_loss=0.419]\u001b[A\n",
            "Epoch 3:  80%|███████▉  | 1012/1271 [05:25<01:23,  3.11it/s, training_loss=0.419]\u001b[A\n",
            "Epoch 3:  80%|███████▉  | 1012/1271 [05:25<01:23,  3.11it/s, training_loss=0.288]\u001b[A\n",
            "Epoch 3:  80%|███████▉  | 1013/1271 [05:25<01:22,  3.11it/s, training_loss=0.288]\u001b[A\n",
            "Epoch 3:  80%|███████▉  | 1013/1271 [05:26<01:22,  3.11it/s, training_loss=0.572]\u001b[A\n",
            "Epoch 3:  80%|███████▉  | 1014/1271 [05:26<01:22,  3.12it/s, training_loss=0.572]\u001b[A\n",
            "Epoch 3:  80%|███████▉  | 1014/1271 [05:26<01:22,  3.12it/s, training_loss=1.085]\u001b[A\n",
            "Epoch 3:  80%|███████▉  | 1015/1271 [05:26<01:22,  3.11it/s, training_loss=1.085]\u001b[A\n",
            "Epoch 3:  80%|███████▉  | 1015/1271 [05:26<01:22,  3.11it/s, training_loss=0.764]\u001b[A\n",
            "Epoch 3:  80%|███████▉  | 1016/1271 [05:26<01:21,  3.13it/s, training_loss=0.764]\u001b[A\n",
            "Epoch 3:  80%|███████▉  | 1016/1271 [05:27<01:21,  3.13it/s, training_loss=0.247]\u001b[A\n",
            "Epoch 3:  80%|████████  | 1017/1271 [05:27<01:21,  3.13it/s, training_loss=0.247]\u001b[A\n",
            "Epoch 3:  80%|████████  | 1017/1271 [05:27<01:21,  3.13it/s, training_loss=0.133]\u001b[A\n",
            "Epoch 3:  80%|████████  | 1018/1271 [05:27<01:20,  3.13it/s, training_loss=0.133]\u001b[A\n",
            "Epoch 3:  80%|████████  | 1018/1271 [05:27<01:20,  3.13it/s, training_loss=0.026]\u001b[A\n",
            "Epoch 3:  80%|████████  | 1019/1271 [05:27<01:20,  3.13it/s, training_loss=0.026]\u001b[A\n",
            "Epoch 3:  80%|████████  | 1019/1271 [05:28<01:20,  3.13it/s, training_loss=0.418]\u001b[A\n",
            "Epoch 3:  80%|████████  | 1020/1271 [05:28<01:19,  3.15it/s, training_loss=0.418]\u001b[A\n",
            "Epoch 3:  80%|████████  | 1020/1271 [05:28<01:19,  3.15it/s, training_loss=0.431]\u001b[A\n",
            "Epoch 3:  80%|████████  | 1021/1271 [05:28<01:19,  3.15it/s, training_loss=0.431]\u001b[A\n",
            "Epoch 3:  80%|████████  | 1021/1271 [05:28<01:19,  3.15it/s, training_loss=0.763]\u001b[A\n",
            "Epoch 3:  80%|████████  | 1022/1271 [05:28<01:19,  3.13it/s, training_loss=0.763]\u001b[A\n",
            "Epoch 3:  80%|████████  | 1022/1271 [05:29<01:19,  3.13it/s, training_loss=0.366]\u001b[A\n",
            "Epoch 3:  80%|████████  | 1023/1271 [05:29<01:19,  3.14it/s, training_loss=0.366]\u001b[A\n",
            "Epoch 3:  80%|████████  | 1023/1271 [05:29<01:19,  3.14it/s, training_loss=0.249]\u001b[A\n",
            "Epoch 3:  81%|████████  | 1024/1271 [05:29<01:18,  3.13it/s, training_loss=0.249]\u001b[A\n",
            "Epoch 3:  81%|████████  | 1024/1271 [05:29<01:18,  3.13it/s, training_loss=0.158]\u001b[A\n",
            "Epoch 3:  81%|████████  | 1025/1271 [05:29<01:18,  3.14it/s, training_loss=0.158]\u001b[A\n",
            "Epoch 3:  81%|████████  | 1025/1271 [05:30<01:18,  3.14it/s, training_loss=0.455]\u001b[A\n",
            "Epoch 3:  81%|████████  | 1026/1271 [05:30<01:18,  3.12it/s, training_loss=0.455]\u001b[A\n",
            "Epoch 3:  81%|████████  | 1026/1271 [05:30<01:18,  3.12it/s, training_loss=0.178]\u001b[A\n",
            "Epoch 3:  81%|████████  | 1027/1271 [05:30<01:18,  3.12it/s, training_loss=0.178]\u001b[A\n",
            "Epoch 3:  81%|████████  | 1027/1271 [05:30<01:18,  3.12it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 3:  81%|████████  | 1028/1271 [05:30<01:17,  3.13it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 3:  81%|████████  | 1028/1271 [05:31<01:17,  3.13it/s, training_loss=0.209]\u001b[A\n",
            "Epoch 3:  81%|████████  | 1029/1271 [05:31<01:17,  3.12it/s, training_loss=0.209]\u001b[A\n",
            "Epoch 3:  81%|████████  | 1029/1271 [05:31<01:17,  3.12it/s, training_loss=0.480]\u001b[A\n",
            "Epoch 3:  81%|████████  | 1030/1271 [05:31<01:17,  3.11it/s, training_loss=0.480]\u001b[A\n",
            "Epoch 3:  81%|████████  | 1030/1271 [05:31<01:17,  3.11it/s, training_loss=0.226]\u001b[A\n",
            "Epoch 3:  81%|████████  | 1031/1271 [05:31<01:17,  3.11it/s, training_loss=0.226]\u001b[A\n",
            "Epoch 3:  81%|████████  | 1031/1271 [05:32<01:17,  3.11it/s, training_loss=0.385]\u001b[A\n",
            "Epoch 3:  81%|████████  | 1032/1271 [05:32<01:16,  3.11it/s, training_loss=0.385]\u001b[A\n",
            "Epoch 3:  81%|████████  | 1032/1271 [05:32<01:16,  3.11it/s, training_loss=0.153]\u001b[A\n",
            "Epoch 3:  81%|████████▏ | 1033/1271 [05:32<01:16,  3.11it/s, training_loss=0.153]\u001b[A\n",
            "Epoch 3:  81%|████████▏ | 1033/1271 [05:32<01:16,  3.11it/s, training_loss=0.807]\u001b[A\n",
            "Epoch 3:  81%|████████▏ | 1034/1271 [05:32<01:16,  3.11it/s, training_loss=0.807]\u001b[A\n",
            "Epoch 3:  81%|████████▏ | 1034/1271 [05:33<01:16,  3.11it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 3:  81%|████████▏ | 1035/1271 [05:33<01:16,  3.10it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 3:  81%|████████▏ | 1035/1271 [05:33<01:16,  3.10it/s, training_loss=0.645]\u001b[A\n",
            "Epoch 3:  82%|████████▏ | 1036/1271 [05:33<01:15,  3.10it/s, training_loss=0.645]\u001b[A\n",
            "Epoch 3:  82%|████████▏ | 1036/1271 [05:33<01:15,  3.10it/s, training_loss=0.605]\u001b[A\n",
            "Epoch 3:  82%|████████▏ | 1037/1271 [05:33<01:15,  3.09it/s, training_loss=0.605]\u001b[A\n",
            "Epoch 3:  82%|████████▏ | 1037/1271 [05:34<01:15,  3.09it/s, training_loss=0.024]\u001b[A\n",
            "Epoch 3:  82%|████████▏ | 1038/1271 [05:34<01:15,  3.10it/s, training_loss=0.024]\u001b[A\n",
            "Epoch 3:  82%|████████▏ | 1038/1271 [05:34<01:15,  3.10it/s, training_loss=0.410]\u001b[A\n",
            "Epoch 3:  82%|████████▏ | 1039/1271 [05:34<01:14,  3.11it/s, training_loss=0.410]\u001b[A\n",
            "Epoch 3:  82%|████████▏ | 1039/1271 [05:34<01:14,  3.11it/s, training_loss=0.079]\u001b[A\n",
            "Epoch 3:  82%|████████▏ | 1040/1271 [05:34<01:14,  3.11it/s, training_loss=0.079]\u001b[A\n",
            "Epoch 3:  82%|████████▏ | 1040/1271 [05:34<01:14,  3.11it/s, training_loss=0.175]\u001b[A\n",
            "Epoch 3:  82%|████████▏ | 1041/1271 [05:34<01:14,  3.11it/s, training_loss=0.175]\u001b[A\n",
            "Epoch 3:  82%|████████▏ | 1041/1271 [05:35<01:14,  3.11it/s, training_loss=0.343]\u001b[A\n",
            "Epoch 3:  82%|████████▏ | 1042/1271 [05:35<01:13,  3.12it/s, training_loss=0.343]\u001b[A\n",
            "Epoch 3:  82%|████████▏ | 1042/1271 [05:35<01:13,  3.12it/s, training_loss=0.363]\u001b[A\n",
            "Epoch 3:  82%|████████▏ | 1043/1271 [05:35<01:13,  3.12it/s, training_loss=0.363]\u001b[A\n",
            "Epoch 3:  82%|████████▏ | 1043/1271 [05:35<01:13,  3.12it/s, training_loss=0.135]\u001b[A\n",
            "Epoch 3:  82%|████████▏ | 1044/1271 [05:35<01:12,  3.12it/s, training_loss=0.135]\u001b[A\n",
            "Epoch 3:  82%|████████▏ | 1044/1271 [05:36<01:12,  3.12it/s, training_loss=0.078]\u001b[A\n",
            "Epoch 3:  82%|████████▏ | 1045/1271 [05:36<01:12,  3.13it/s, training_loss=0.078]\u001b[A\n",
            "Epoch 3:  82%|████████▏ | 1045/1271 [05:36<01:12,  3.13it/s, training_loss=0.459]\u001b[A\n",
            "Epoch 3:  82%|████████▏ | 1046/1271 [05:36<01:12,  3.12it/s, training_loss=0.459]\u001b[A\n",
            "Epoch 3:  82%|████████▏ | 1046/1271 [05:36<01:12,  3.12it/s, training_loss=0.288]\u001b[A\n",
            "Epoch 3:  82%|████████▏ | 1047/1271 [05:36<01:11,  3.13it/s, training_loss=0.288]\u001b[A\n",
            "Epoch 3:  82%|████████▏ | 1047/1271 [05:37<01:11,  3.13it/s, training_loss=0.198]\u001b[A\n",
            "Epoch 3:  82%|████████▏ | 1048/1271 [05:37<01:11,  3.13it/s, training_loss=0.198]\u001b[A\n",
            "Epoch 3:  82%|████████▏ | 1048/1271 [05:37<01:11,  3.13it/s, training_loss=0.191]\u001b[A\n",
            "Epoch 3:  83%|████████▎ | 1049/1271 [05:37<01:10,  3.13it/s, training_loss=0.191]\u001b[A\n",
            "Epoch 3:  83%|████████▎ | 1049/1271 [05:37<01:10,  3.13it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 3:  83%|████████▎ | 1050/1271 [05:37<01:11,  3.11it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 3:  83%|████████▎ | 1050/1271 [05:38<01:11,  3.11it/s, training_loss=0.586]\u001b[A\n",
            "Epoch 3:  83%|████████▎ | 1051/1271 [05:38<01:10,  3.11it/s, training_loss=0.586]\u001b[A\n",
            "Epoch 3:  83%|████████▎ | 1051/1271 [05:38<01:10,  3.11it/s, training_loss=0.359]\u001b[A\n",
            "Epoch 3:  83%|████████▎ | 1052/1271 [05:38<01:10,  3.11it/s, training_loss=0.359]\u001b[A\n",
            "Epoch 3:  83%|████████▎ | 1052/1271 [05:38<01:10,  3.11it/s, training_loss=0.473]\u001b[A\n",
            "Epoch 3:  83%|████████▎ | 1053/1271 [05:38<01:10,  3.08it/s, training_loss=0.473]\u001b[A\n",
            "Epoch 3:  83%|████████▎ | 1053/1271 [05:39<01:10,  3.08it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 3:  83%|████████▎ | 1054/1271 [05:39<01:10,  3.09it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 3:  83%|████████▎ | 1054/1271 [05:39<01:10,  3.09it/s, training_loss=0.575]\u001b[A\n",
            "Epoch 3:  83%|████████▎ | 1055/1271 [05:39<01:09,  3.09it/s, training_loss=0.575]\u001b[A\n",
            "Epoch 3:  83%|████████▎ | 1055/1271 [05:39<01:09,  3.09it/s, training_loss=0.578]\u001b[A\n",
            "Epoch 3:  83%|████████▎ | 1056/1271 [05:39<01:09,  3.09it/s, training_loss=0.578]\u001b[A\n",
            "Epoch 3:  83%|████████▎ | 1056/1271 [05:40<01:09,  3.09it/s, training_loss=0.351]\u001b[A\n",
            "Epoch 3:  83%|████████▎ | 1057/1271 [05:40<01:09,  3.09it/s, training_loss=0.351]\u001b[A\n",
            "Epoch 3:  83%|████████▎ | 1057/1271 [05:40<01:09,  3.09it/s, training_loss=0.164]\u001b[A\n",
            "Epoch 3:  83%|████████▎ | 1058/1271 [05:40<01:08,  3.10it/s, training_loss=0.164]\u001b[A\n",
            "Epoch 3:  83%|████████▎ | 1058/1271 [05:40<01:08,  3.10it/s, training_loss=0.081]\u001b[A\n",
            "Epoch 3:  83%|████████▎ | 1059/1271 [05:40<01:07,  3.12it/s, training_loss=0.081]\u001b[A\n",
            "Epoch 3:  83%|████████▎ | 1059/1271 [05:41<01:07,  3.12it/s, training_loss=0.173]\u001b[A\n",
            "Epoch 3:  83%|████████▎ | 1060/1271 [05:41<01:07,  3.11it/s, training_loss=0.173]\u001b[A\n",
            "Epoch 3:  83%|████████▎ | 1060/1271 [05:41<01:07,  3.11it/s, training_loss=0.373]\u001b[A\n",
            "Epoch 3:  83%|████████▎ | 1061/1271 [05:41<01:07,  3.11it/s, training_loss=0.373]\u001b[A\n",
            "Epoch 3:  83%|████████▎ | 1061/1271 [05:41<01:07,  3.11it/s, training_loss=0.354]\u001b[A\n",
            "Epoch 3:  84%|████████▎ | 1062/1271 [05:41<01:06,  3.12it/s, training_loss=0.354]\u001b[A\n",
            "Epoch 3:  84%|████████▎ | 1062/1271 [05:42<01:06,  3.12it/s, training_loss=0.405]\u001b[A\n",
            "Epoch 3:  84%|████████▎ | 1063/1271 [05:42<01:07,  3.10it/s, training_loss=0.405]\u001b[A\n",
            "Epoch 3:  84%|████████▎ | 1063/1271 [05:42<01:07,  3.10it/s, training_loss=0.361]\u001b[A\n",
            "Epoch 3:  84%|████████▎ | 1064/1271 [05:42<01:06,  3.10it/s, training_loss=0.361]\u001b[A\n",
            "Epoch 3:  84%|████████▎ | 1064/1271 [05:42<01:06,  3.10it/s, training_loss=0.140]\u001b[A\n",
            "Epoch 3:  84%|████████▍ | 1065/1271 [05:42<01:06,  3.11it/s, training_loss=0.140]\u001b[A\n",
            "Epoch 3:  84%|████████▍ | 1065/1271 [05:43<01:06,  3.11it/s, training_loss=0.079]\u001b[A\n",
            "Epoch 3:  84%|████████▍ | 1066/1271 [05:43<01:05,  3.11it/s, training_loss=0.079]\u001b[A\n",
            "Epoch 3:  84%|████████▍ | 1066/1271 [05:43<01:05,  3.11it/s, training_loss=0.366]\u001b[A\n",
            "Epoch 3:  84%|████████▍ | 1067/1271 [05:43<01:05,  3.10it/s, training_loss=0.366]\u001b[A\n",
            "Epoch 3:  84%|████████▍ | 1067/1271 [05:43<01:05,  3.10it/s, training_loss=0.408]\u001b[A\n",
            "Epoch 3:  84%|████████▍ | 1068/1271 [05:43<01:05,  3.10it/s, training_loss=0.408]\u001b[A\n",
            "Epoch 3:  84%|████████▍ | 1068/1271 [05:43<01:05,  3.10it/s, training_loss=0.057]\u001b[A\n",
            "Epoch 3:  84%|████████▍ | 1069/1271 [05:43<01:04,  3.12it/s, training_loss=0.057]\u001b[A\n",
            "Epoch 3:  84%|████████▍ | 1069/1271 [05:44<01:04,  3.12it/s, training_loss=0.412]\u001b[A\n",
            "Epoch 3:  84%|████████▍ | 1070/1271 [05:44<01:04,  3.13it/s, training_loss=0.412]\u001b[A\n",
            "Epoch 3:  84%|████████▍ | 1070/1271 [05:44<01:04,  3.13it/s, training_loss=0.336]\u001b[A\n",
            "Epoch 3:  84%|████████▍ | 1071/1271 [05:44<01:04,  3.11it/s, training_loss=0.336]\u001b[A\n",
            "Epoch 3:  84%|████████▍ | 1071/1271 [05:44<01:04,  3.11it/s, training_loss=0.052]\u001b[A\n",
            "Epoch 3:  84%|████████▍ | 1072/1271 [05:44<01:04,  3.10it/s, training_loss=0.052]\u001b[A\n",
            "Epoch 3:  84%|████████▍ | 1072/1271 [05:45<01:04,  3.10it/s, training_loss=0.863]\u001b[A\n",
            "Epoch 3:  84%|████████▍ | 1073/1271 [05:45<01:03,  3.09it/s, training_loss=0.863]\u001b[A\n",
            "Epoch 3:  84%|████████▍ | 1073/1271 [05:45<01:03,  3.09it/s, training_loss=0.233]\u001b[A\n",
            "Epoch 3:  85%|████████▍ | 1074/1271 [05:45<01:03,  3.10it/s, training_loss=0.233]\u001b[A\n",
            "Epoch 3:  85%|████████▍ | 1074/1271 [05:45<01:03,  3.10it/s, training_loss=0.192]\u001b[A\n",
            "Epoch 3:  85%|████████▍ | 1075/1271 [05:45<01:03,  3.10it/s, training_loss=0.192]\u001b[A\n",
            "Epoch 3:  85%|████████▍ | 1075/1271 [05:46<01:03,  3.10it/s, training_loss=0.271]\u001b[A\n",
            "Epoch 3:  85%|████████▍ | 1076/1271 [05:46<01:03,  3.08it/s, training_loss=0.271]\u001b[A\n",
            "Epoch 3:  85%|████████▍ | 1076/1271 [05:46<01:03,  3.08it/s, training_loss=0.397]\u001b[A\n",
            "Epoch 3:  85%|████████▍ | 1077/1271 [05:46<01:02,  3.09it/s, training_loss=0.397]\u001b[A\n",
            "Epoch 3:  85%|████████▍ | 1077/1271 [05:46<01:02,  3.09it/s, training_loss=0.203]\u001b[A\n",
            "Epoch 3:  85%|████████▍ | 1078/1271 [05:46<01:02,  3.10it/s, training_loss=0.203]\u001b[A\n",
            "Epoch 3:  85%|████████▍ | 1078/1271 [05:47<01:02,  3.10it/s, training_loss=0.168]\u001b[A\n",
            "Epoch 3:  85%|████████▍ | 1079/1271 [05:47<01:02,  3.09it/s, training_loss=0.168]\u001b[A\n",
            "Epoch 3:  85%|████████▍ | 1079/1271 [05:47<01:02,  3.09it/s, training_loss=0.334]\u001b[A\n",
            "Epoch 3:  85%|████████▍ | 1080/1271 [05:47<01:01,  3.09it/s, training_loss=0.334]\u001b[A\n",
            "Epoch 3:  85%|████████▍ | 1080/1271 [05:47<01:01,  3.09it/s, training_loss=0.015]\u001b[A\n",
            "Epoch 3:  85%|████████▌ | 1081/1271 [05:47<01:01,  3.11it/s, training_loss=0.015]\u001b[A\n",
            "Epoch 3:  85%|████████▌ | 1081/1271 [05:48<01:01,  3.11it/s, training_loss=0.385]\u001b[A\n",
            "Epoch 3:  85%|████████▌ | 1082/1271 [05:48<01:00,  3.10it/s, training_loss=0.385]\u001b[A\n",
            "Epoch 3:  85%|████████▌ | 1082/1271 [05:48<01:00,  3.10it/s, training_loss=0.054]\u001b[A\n",
            "Epoch 3:  85%|████████▌ | 1083/1271 [05:48<01:00,  3.08it/s, training_loss=0.054]\u001b[A\n",
            "Epoch 3:  85%|████████▌ | 1083/1271 [05:48<01:00,  3.08it/s, training_loss=0.189]\u001b[A\n",
            "Epoch 3:  85%|████████▌ | 1084/1271 [05:48<01:00,  3.09it/s, training_loss=0.189]\u001b[A\n",
            "Epoch 3:  85%|████████▌ | 1084/1271 [05:49<01:00,  3.09it/s, training_loss=0.508]\u001b[A\n",
            "Epoch 3:  85%|████████▌ | 1085/1271 [05:49<01:00,  3.06it/s, training_loss=0.508]\u001b[A\n",
            "Epoch 3:  85%|████████▌ | 1085/1271 [05:49<01:00,  3.06it/s, training_loss=0.171]\u001b[A\n",
            "Epoch 3:  85%|████████▌ | 1086/1271 [05:49<01:00,  3.08it/s, training_loss=0.171]\u001b[A\n",
            "Epoch 3:  85%|████████▌ | 1086/1271 [05:49<01:00,  3.08it/s, training_loss=0.646]\u001b[A\n",
            "Epoch 3:  86%|████████▌ | 1087/1271 [05:49<00:59,  3.08it/s, training_loss=0.646]\u001b[A\n",
            "Epoch 3:  86%|████████▌ | 1087/1271 [05:50<00:59,  3.08it/s, training_loss=0.216]\u001b[A\n",
            "Epoch 3:  86%|████████▌ | 1088/1271 [05:50<00:59,  3.10it/s, training_loss=0.216]\u001b[A\n",
            "Epoch 3:  86%|████████▌ | 1088/1271 [05:50<00:59,  3.10it/s, training_loss=0.453]\u001b[A\n",
            "Epoch 3:  86%|████████▌ | 1089/1271 [05:50<00:58,  3.09it/s, training_loss=0.453]\u001b[A\n",
            "Epoch 3:  86%|████████▌ | 1089/1271 [05:50<00:58,  3.09it/s, training_loss=0.367]\u001b[A\n",
            "Epoch 3:  86%|████████▌ | 1090/1271 [05:50<00:58,  3.10it/s, training_loss=0.367]\u001b[A\n",
            "Epoch 3:  86%|████████▌ | 1090/1271 [05:51<00:58,  3.10it/s, training_loss=0.463]\u001b[A\n",
            "Epoch 3:  86%|████████▌ | 1091/1271 [05:51<00:57,  3.11it/s, training_loss=0.463]\u001b[A\n",
            "Epoch 3:  86%|████████▌ | 1091/1271 [05:51<00:57,  3.11it/s, training_loss=0.198]\u001b[A\n",
            "Epoch 3:  86%|████████▌ | 1092/1271 [05:51<00:57,  3.12it/s, training_loss=0.198]\u001b[A\n",
            "Epoch 3:  86%|████████▌ | 1092/1271 [05:51<00:57,  3.12it/s, training_loss=0.361]\u001b[A\n",
            "Epoch 3:  86%|████████▌ | 1093/1271 [05:51<00:57,  3.09it/s, training_loss=0.361]\u001b[A\n",
            "Epoch 3:  86%|████████▌ | 1093/1271 [05:52<00:57,  3.09it/s, training_loss=0.349]\u001b[A\n",
            "Epoch 3:  86%|████████▌ | 1094/1271 [05:52<00:57,  3.10it/s, training_loss=0.349]\u001b[A\n",
            "Epoch 3:  86%|████████▌ | 1094/1271 [05:52<00:57,  3.10it/s, training_loss=0.172]\u001b[A\n",
            "Epoch 3:  86%|████████▌ | 1095/1271 [05:52<00:56,  3.10it/s, training_loss=0.172]\u001b[A\n",
            "Epoch 3:  86%|████████▌ | 1095/1271 [05:52<00:56,  3.10it/s, training_loss=0.340]\u001b[A\n",
            "Epoch 3:  86%|████████▌ | 1096/1271 [05:52<00:56,  3.12it/s, training_loss=0.340]\u001b[A\n",
            "Epoch 3:  86%|████████▌ | 1096/1271 [05:53<00:56,  3.12it/s, training_loss=0.563]\u001b[A\n",
            "Epoch 3:  86%|████████▋ | 1097/1271 [05:53<00:55,  3.13it/s, training_loss=0.563]\u001b[A\n",
            "Epoch 3:  86%|████████▋ | 1097/1271 [05:53<00:55,  3.13it/s, training_loss=0.522]\u001b[A\n",
            "Epoch 3:  86%|████████▋ | 1098/1271 [05:53<00:55,  3.13it/s, training_loss=0.522]\u001b[A\n",
            "Epoch 3:  86%|████████▋ | 1098/1271 [05:53<00:55,  3.13it/s, training_loss=0.545]\u001b[A\n",
            "Epoch 3:  86%|████████▋ | 1099/1271 [05:53<00:54,  3.13it/s, training_loss=0.545]\u001b[A\n",
            "Epoch 3:  86%|████████▋ | 1099/1271 [05:53<00:54,  3.13it/s, training_loss=0.087]\u001b[A\n",
            "Epoch 3:  87%|████████▋ | 1100/1271 [05:53<00:54,  3.12it/s, training_loss=0.087]\u001b[A\n",
            "Epoch 3:  87%|████████▋ | 1100/1271 [05:54<00:54,  3.12it/s, training_loss=0.407]\u001b[A\n",
            "Epoch 3:  87%|████████▋ | 1101/1271 [05:54<00:54,  3.11it/s, training_loss=0.407]\u001b[A\n",
            "Epoch 3:  87%|████████▋ | 1101/1271 [05:54<00:54,  3.11it/s, training_loss=0.403]\u001b[A\n",
            "Epoch 3:  87%|████████▋ | 1102/1271 [05:54<00:54,  3.11it/s, training_loss=0.403]\u001b[A\n",
            "Epoch 3:  87%|████████▋ | 1102/1271 [05:54<00:54,  3.11it/s, training_loss=0.905]\u001b[A\n",
            "Epoch 3:  87%|████████▋ | 1103/1271 [05:54<00:54,  3.10it/s, training_loss=0.905]\u001b[A\n",
            "Epoch 3:  87%|████████▋ | 1103/1271 [05:55<00:54,  3.10it/s, training_loss=0.324]\u001b[A\n",
            "Epoch 3:  87%|████████▋ | 1104/1271 [05:55<00:54,  3.09it/s, training_loss=0.324]\u001b[A\n",
            "Epoch 3:  87%|████████▋ | 1104/1271 [05:55<00:54,  3.09it/s, training_loss=0.064]\u001b[A\n",
            "Epoch 3:  87%|████████▋ | 1105/1271 [05:55<00:53,  3.10it/s, training_loss=0.064]\u001b[A\n",
            "Epoch 3:  87%|████████▋ | 1105/1271 [05:55<00:53,  3.10it/s, training_loss=0.387]\u001b[A\n",
            "Epoch 3:  87%|████████▋ | 1106/1271 [05:55<00:53,  3.10it/s, training_loss=0.387]\u001b[A\n",
            "Epoch 3:  87%|████████▋ | 1106/1271 [05:56<00:53,  3.10it/s, training_loss=0.139]\u001b[A\n",
            "Epoch 3:  87%|████████▋ | 1107/1271 [05:56<00:52,  3.11it/s, training_loss=0.139]\u001b[A\n",
            "Epoch 3:  87%|████████▋ | 1107/1271 [05:56<00:52,  3.11it/s, training_loss=0.315]\u001b[A\n",
            "Epoch 3:  87%|████████▋ | 1108/1271 [05:56<00:52,  3.12it/s, training_loss=0.315]\u001b[A\n",
            "Epoch 3:  87%|████████▋ | 1108/1271 [05:56<00:52,  3.12it/s, training_loss=0.209]\u001b[A\n",
            "Epoch 3:  87%|████████▋ | 1109/1271 [05:56<00:51,  3.12it/s, training_loss=0.209]\u001b[A\n",
            "Epoch 3:  87%|████████▋ | 1109/1271 [05:57<00:51,  3.12it/s, training_loss=0.509]\u001b[A\n",
            "Epoch 3:  87%|████████▋ | 1110/1271 [05:57<00:51,  3.11it/s, training_loss=0.509]\u001b[A\n",
            "Epoch 3:  87%|████████▋ | 1110/1271 [05:57<00:51,  3.11it/s, training_loss=0.173]\u001b[A\n",
            "Epoch 3:  87%|████████▋ | 1111/1271 [05:57<00:51,  3.10it/s, training_loss=0.173]\u001b[A\n",
            "Epoch 3:  87%|████████▋ | 1111/1271 [05:57<00:51,  3.10it/s, training_loss=0.172]\u001b[A\n",
            "Epoch 3:  87%|████████▋ | 1112/1271 [05:57<00:51,  3.10it/s, training_loss=0.172]\u001b[A\n",
            "Epoch 3:  87%|████████▋ | 1112/1271 [05:58<00:51,  3.10it/s, training_loss=0.331]\u001b[A\n",
            "Epoch 3:  88%|████████▊ | 1113/1271 [05:58<00:50,  3.11it/s, training_loss=0.331]\u001b[A\n",
            "Epoch 3:  88%|████████▊ | 1113/1271 [05:58<00:50,  3.11it/s, training_loss=0.086]\u001b[A\n",
            "Epoch 3:  88%|████████▊ | 1114/1271 [05:58<00:50,  3.12it/s, training_loss=0.086]\u001b[A\n",
            "Epoch 3:  88%|████████▊ | 1114/1271 [05:58<00:50,  3.12it/s, training_loss=0.417]\u001b[A\n",
            "Epoch 3:  88%|████████▊ | 1115/1271 [05:58<00:50,  3.10it/s, training_loss=0.417]\u001b[A\n",
            "Epoch 3:  88%|████████▊ | 1115/1271 [05:59<00:50,  3.10it/s, training_loss=0.184]\u001b[A\n",
            "Epoch 3:  88%|████████▊ | 1116/1271 [05:59<00:49,  3.12it/s, training_loss=0.184]\u001b[A\n",
            "Epoch 3:  88%|████████▊ | 1116/1271 [05:59<00:49,  3.12it/s, training_loss=0.030]\u001b[A\n",
            "Epoch 3:  88%|████████▊ | 1117/1271 [05:59<00:49,  3.12it/s, training_loss=0.030]\u001b[A\n",
            "Epoch 3:  88%|████████▊ | 1117/1271 [05:59<00:49,  3.12it/s, training_loss=0.336]\u001b[A\n",
            "Epoch 3:  88%|████████▊ | 1118/1271 [05:59<00:49,  3.12it/s, training_loss=0.336]\u001b[A\n",
            "Epoch 3:  88%|████████▊ | 1118/1271 [06:00<00:49,  3.12it/s, training_loss=0.399]\u001b[A\n",
            "Epoch 3:  88%|████████▊ | 1119/1271 [06:00<00:48,  3.10it/s, training_loss=0.399]\u001b[A\n",
            "Epoch 3:  88%|████████▊ | 1119/1271 [06:00<00:48,  3.10it/s, training_loss=0.189]\u001b[A\n",
            "Epoch 3:  88%|████████▊ | 1120/1271 [06:00<00:48,  3.11it/s, training_loss=0.189]\u001b[A\n",
            "Epoch 3:  88%|████████▊ | 1120/1271 [06:00<00:48,  3.11it/s, training_loss=0.094]\u001b[A\n",
            "Epoch 3:  88%|████████▊ | 1121/1271 [06:00<00:48,  3.11it/s, training_loss=0.094]\u001b[A\n",
            "Epoch 3:  88%|████████▊ | 1121/1271 [06:01<00:48,  3.11it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 3:  88%|████████▊ | 1122/1271 [06:01<00:47,  3.12it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 3:  88%|████████▊ | 1122/1271 [06:01<00:47,  3.12it/s, training_loss=0.277]\u001b[A\n",
            "Epoch 3:  88%|████████▊ | 1123/1271 [06:01<00:47,  3.12it/s, training_loss=0.277]\u001b[A\n",
            "Epoch 3:  88%|████████▊ | 1123/1271 [06:01<00:47,  3.12it/s, training_loss=0.020]\u001b[A\n",
            "Epoch 3:  88%|████████▊ | 1124/1271 [06:01<00:47,  3.10it/s, training_loss=0.020]\u001b[A\n",
            "Epoch 3:  88%|████████▊ | 1124/1271 [06:02<00:47,  3.10it/s, training_loss=0.233]\u001b[A\n",
            "Epoch 3:  89%|████████▊ | 1125/1271 [06:02<00:47,  3.09it/s, training_loss=0.233]\u001b[A\n",
            "Epoch 3:  89%|████████▊ | 1125/1271 [06:02<00:47,  3.09it/s, training_loss=0.191]\u001b[A\n",
            "Epoch 3:  89%|████████▊ | 1126/1271 [06:02<00:47,  3.08it/s, training_loss=0.191]\u001b[A\n",
            "Epoch 3:  89%|████████▊ | 1126/1271 [06:02<00:47,  3.08it/s, training_loss=0.748]\u001b[A\n",
            "Epoch 3:  89%|████████▊ | 1127/1271 [06:02<00:46,  3.08it/s, training_loss=0.748]\u001b[A\n",
            "Epoch 3:  89%|████████▊ | 1127/1271 [06:02<00:46,  3.08it/s, training_loss=0.615]\u001b[A\n",
            "Epoch 3:  89%|████████▊ | 1128/1271 [06:03<00:46,  3.07it/s, training_loss=0.615]\u001b[A\n",
            "Epoch 3:  89%|████████▊ | 1128/1271 [06:03<00:46,  3.07it/s, training_loss=0.242]\u001b[A\n",
            "Epoch 3:  89%|████████▉ | 1129/1271 [06:03<00:45,  3.09it/s, training_loss=0.242]\u001b[A\n",
            "Epoch 3:  89%|████████▉ | 1129/1271 [06:03<00:45,  3.09it/s, training_loss=0.173]\u001b[A\n",
            "Epoch 3:  89%|████████▉ | 1130/1271 [06:03<00:45,  3.11it/s, training_loss=0.173]\u001b[A\n",
            "Epoch 3:  89%|████████▉ | 1130/1271 [06:03<00:45,  3.11it/s, training_loss=0.397]\u001b[A\n",
            "Epoch 3:  89%|████████▉ | 1131/1271 [06:03<00:45,  3.08it/s, training_loss=0.397]\u001b[A\n",
            "Epoch 3:  89%|████████▉ | 1131/1271 [06:04<00:45,  3.08it/s, training_loss=0.126]\u001b[A\n",
            "Epoch 3:  89%|████████▉ | 1132/1271 [06:04<00:45,  3.08it/s, training_loss=0.126]\u001b[A\n",
            "Epoch 3:  89%|████████▉ | 1132/1271 [06:04<00:45,  3.08it/s, training_loss=0.098]\u001b[A\n",
            "Epoch 3:  89%|████████▉ | 1133/1271 [06:04<00:44,  3.08it/s, training_loss=0.098]\u001b[A\n",
            "Epoch 3:  89%|████████▉ | 1133/1271 [06:04<00:44,  3.08it/s, training_loss=0.017]\u001b[A\n",
            "Epoch 3:  89%|████████▉ | 1134/1271 [06:04<00:44,  3.09it/s, training_loss=0.017]\u001b[A\n",
            "Epoch 3:  89%|████████▉ | 1134/1271 [06:05<00:44,  3.09it/s, training_loss=0.079]\u001b[A\n",
            "Epoch 3:  89%|████████▉ | 1135/1271 [06:05<00:43,  3.11it/s, training_loss=0.079]\u001b[A\n",
            "Epoch 3:  89%|████████▉ | 1135/1271 [06:05<00:43,  3.11it/s, training_loss=0.504]\u001b[A\n",
            "Epoch 3:  89%|████████▉ | 1136/1271 [06:05<00:43,  3.10it/s, training_loss=0.504]\u001b[A\n",
            "Epoch 3:  89%|████████▉ | 1136/1271 [06:05<00:43,  3.10it/s, training_loss=0.251]\u001b[A\n",
            "Epoch 3:  89%|████████▉ | 1137/1271 [06:05<00:43,  3.10it/s, training_loss=0.251]\u001b[A\n",
            "Epoch 3:  89%|████████▉ | 1137/1271 [06:06<00:43,  3.10it/s, training_loss=0.108]\u001b[A\n",
            "Epoch 3:  90%|████████▉ | 1138/1271 [06:06<00:42,  3.10it/s, training_loss=0.108]\u001b[A\n",
            "Epoch 3:  90%|████████▉ | 1138/1271 [06:06<00:42,  3.10it/s, training_loss=0.256]\u001b[A\n",
            "Epoch 3:  90%|████████▉ | 1139/1271 [06:06<00:42,  3.11it/s, training_loss=0.256]\u001b[A\n",
            "Epoch 3:  90%|████████▉ | 1139/1271 [06:06<00:42,  3.11it/s, training_loss=0.391]\u001b[A\n",
            "Epoch 3:  90%|████████▉ | 1140/1271 [06:06<00:42,  3.11it/s, training_loss=0.391]\u001b[A\n",
            "Epoch 3:  90%|████████▉ | 1140/1271 [06:07<00:42,  3.11it/s, training_loss=0.127]\u001b[A\n",
            "Epoch 3:  90%|████████▉ | 1141/1271 [06:07<00:41,  3.12it/s, training_loss=0.127]\u001b[A\n",
            "Epoch 3:  90%|████████▉ | 1141/1271 [06:07<00:41,  3.12it/s, training_loss=0.130]\u001b[A\n",
            "Epoch 3:  90%|████████▉ | 1142/1271 [06:07<00:41,  3.13it/s, training_loss=0.130]\u001b[A\n",
            "Epoch 3:  90%|████████▉ | 1142/1271 [06:07<00:41,  3.13it/s, training_loss=0.534]\u001b[A\n",
            "Epoch 3:  90%|████████▉ | 1143/1271 [06:07<00:40,  3.13it/s, training_loss=0.534]\u001b[A\n",
            "Epoch 3:  90%|████████▉ | 1143/1271 [06:08<00:40,  3.13it/s, training_loss=0.279]\u001b[A\n",
            "Epoch 3:  90%|█████████ | 1144/1271 [06:08<00:40,  3.12it/s, training_loss=0.279]\u001b[A\n",
            "Epoch 3:  90%|█████████ | 1144/1271 [06:08<00:40,  3.12it/s, training_loss=0.703]\u001b[A\n",
            "Epoch 3:  90%|█████████ | 1145/1271 [06:08<00:40,  3.13it/s, training_loss=0.703]\u001b[A\n",
            "Epoch 3:  90%|█████████ | 1145/1271 [06:08<00:40,  3.13it/s, training_loss=0.133]\u001b[A\n",
            "Epoch 3:  90%|█████████ | 1146/1271 [06:08<00:39,  3.13it/s, training_loss=0.133]\u001b[A\n",
            "Epoch 3:  90%|█████████ | 1146/1271 [06:09<00:39,  3.13it/s, training_loss=0.333]\u001b[A\n",
            "Epoch 3:  90%|█████████ | 1147/1271 [06:09<00:39,  3.12it/s, training_loss=0.333]\u001b[A\n",
            "Epoch 3:  90%|█████████ | 1147/1271 [06:09<00:39,  3.12it/s, training_loss=0.481]\u001b[A\n",
            "Epoch 3:  90%|█████████ | 1148/1271 [06:09<00:39,  3.12it/s, training_loss=0.481]\u001b[A\n",
            "Epoch 3:  90%|█████████ | 1148/1271 [06:09<00:39,  3.12it/s, training_loss=0.124]\u001b[A\n",
            "Epoch 3:  90%|█████████ | 1149/1271 [06:09<00:39,  3.12it/s, training_loss=0.124]\u001b[A\n",
            "Epoch 3:  90%|█████████ | 1149/1271 [06:10<00:39,  3.12it/s, training_loss=0.049]\u001b[A\n",
            "Epoch 3:  90%|█████████ | 1150/1271 [06:10<00:38,  3.13it/s, training_loss=0.049]\u001b[A\n",
            "Epoch 3:  90%|█████████ | 1150/1271 [06:10<00:38,  3.13it/s, training_loss=0.139]\u001b[A\n",
            "Epoch 3:  91%|█████████ | 1151/1271 [06:10<00:38,  3.13it/s, training_loss=0.139]\u001b[A\n",
            "Epoch 3:  91%|█████████ | 1151/1271 [06:10<00:38,  3.13it/s, training_loss=0.233]\u001b[A\n",
            "Epoch 3:  91%|█████████ | 1152/1271 [06:10<00:37,  3.14it/s, training_loss=0.233]\u001b[A\n",
            "Epoch 3:  91%|█████████ | 1152/1271 [06:11<00:37,  3.14it/s, training_loss=0.576]\u001b[A\n",
            "Epoch 3:  91%|█████████ | 1153/1271 [06:11<00:37,  3.11it/s, training_loss=0.576]\u001b[A\n",
            "Epoch 3:  91%|█████████ | 1153/1271 [06:11<00:37,  3.11it/s, training_loss=0.093]\u001b[A\n",
            "Epoch 3:  91%|█████████ | 1154/1271 [06:11<00:37,  3.11it/s, training_loss=0.093]\u001b[A\n",
            "Epoch 3:  91%|█████████ | 1154/1271 [06:11<00:37,  3.11it/s, training_loss=0.052]\u001b[A\n",
            "Epoch 3:  91%|█████████ | 1155/1271 [06:11<00:37,  3.13it/s, training_loss=0.052]\u001b[A\n",
            "Epoch 3:  91%|█████████ | 1155/1271 [06:11<00:37,  3.13it/s, training_loss=0.778]\u001b[A\n",
            "Epoch 3:  91%|█████████ | 1156/1271 [06:11<00:36,  3.13it/s, training_loss=0.778]\u001b[A\n",
            "Epoch 3:  91%|█████████ | 1156/1271 [06:12<00:36,  3.13it/s, training_loss=0.088]\u001b[A\n",
            "Epoch 3:  91%|█████████ | 1157/1271 [06:12<00:36,  3.13it/s, training_loss=0.088]\u001b[A\n",
            "Epoch 3:  91%|█████████ | 1157/1271 [06:12<00:36,  3.13it/s, training_loss=0.625]\u001b[A\n",
            "Epoch 3:  91%|█████████ | 1158/1271 [06:12<00:36,  3.12it/s, training_loss=0.625]\u001b[A\n",
            "Epoch 3:  91%|█████████ | 1158/1271 [06:12<00:36,  3.12it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 3:  91%|█████████ | 1159/1271 [06:12<00:35,  3.13it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 3:  91%|█████████ | 1159/1271 [06:13<00:35,  3.13it/s, training_loss=0.145]\u001b[A\n",
            "Epoch 3:  91%|█████████▏| 1160/1271 [06:13<00:35,  3.12it/s, training_loss=0.145]\u001b[A\n",
            "Epoch 3:  91%|█████████▏| 1160/1271 [06:13<00:35,  3.12it/s, training_loss=0.388]\u001b[A\n",
            "Epoch 3:  91%|█████████▏| 1161/1271 [06:13<00:35,  3.13it/s, training_loss=0.388]\u001b[A\n",
            "Epoch 3:  91%|█████████▏| 1161/1271 [06:13<00:35,  3.13it/s, training_loss=0.240]\u001b[A\n",
            "Epoch 3:  91%|█████████▏| 1162/1271 [06:13<00:34,  3.14it/s, training_loss=0.240]\u001b[A\n",
            "Epoch 3:  91%|█████████▏| 1162/1271 [06:14<00:34,  3.14it/s, training_loss=0.778]\u001b[A\n",
            "Epoch 3:  92%|█████████▏| 1163/1271 [06:14<00:34,  3.13it/s, training_loss=0.778]\u001b[A\n",
            "Epoch 3:  92%|█████████▏| 1163/1271 [06:14<00:34,  3.13it/s, training_loss=0.049]\u001b[A\n",
            "Epoch 3:  92%|█████████▏| 1164/1271 [06:14<00:34,  3.13it/s, training_loss=0.049]\u001b[A\n",
            "Epoch 3:  92%|█████████▏| 1164/1271 [06:14<00:34,  3.13it/s, training_loss=0.095]\u001b[A\n",
            "Epoch 3:  92%|█████████▏| 1165/1271 [06:14<00:33,  3.13it/s, training_loss=0.095]\u001b[A\n",
            "Epoch 3:  92%|█████████▏| 1165/1271 [06:15<00:33,  3.13it/s, training_loss=0.111]\u001b[A\n",
            "Epoch 3:  92%|█████████▏| 1166/1271 [06:15<00:33,  3.12it/s, training_loss=0.111]\u001b[A\n",
            "Epoch 3:  92%|█████████▏| 1166/1271 [06:15<00:33,  3.12it/s, training_loss=0.213]\u001b[A\n",
            "Epoch 3:  92%|█████████▏| 1167/1271 [06:15<00:33,  3.11it/s, training_loss=0.213]\u001b[A\n",
            "Epoch 3:  92%|█████████▏| 1167/1271 [06:15<00:33,  3.11it/s, training_loss=0.188]\u001b[A\n",
            "Epoch 3:  92%|█████████▏| 1168/1271 [06:15<00:32,  3.13it/s, training_loss=0.188]\u001b[A\n",
            "Epoch 3:  92%|█████████▏| 1168/1271 [06:16<00:32,  3.13it/s, training_loss=0.328]\u001b[A\n",
            "Epoch 3:  92%|█████████▏| 1169/1271 [06:16<00:32,  3.13it/s, training_loss=0.328]\u001b[A\n",
            "Epoch 3:  92%|█████████▏| 1169/1271 [06:16<00:32,  3.13it/s, training_loss=0.151]\u001b[A\n",
            "Epoch 3:  92%|█████████▏| 1170/1271 [06:16<00:32,  3.10it/s, training_loss=0.151]\u001b[A\n",
            "Epoch 3:  92%|█████████▏| 1170/1271 [06:16<00:32,  3.10it/s, training_loss=0.214]\u001b[A\n",
            "Epoch 3:  92%|█████████▏| 1171/1271 [06:16<00:32,  3.12it/s, training_loss=0.214]\u001b[A\n",
            "Epoch 3:  92%|█████████▏| 1171/1271 [06:17<00:32,  3.12it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 3:  92%|█████████▏| 1172/1271 [06:17<00:31,  3.12it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 3:  92%|█████████▏| 1172/1271 [06:17<00:31,  3.12it/s, training_loss=0.276]\u001b[A\n",
            "Epoch 3:  92%|█████████▏| 1173/1271 [06:17<00:31,  3.11it/s, training_loss=0.276]\u001b[A\n",
            "Epoch 3:  92%|█████████▏| 1173/1271 [06:17<00:31,  3.11it/s, training_loss=0.508]\u001b[A\n",
            "Epoch 3:  92%|█████████▏| 1174/1271 [06:17<00:31,  3.11it/s, training_loss=0.508]\u001b[A\n",
            "Epoch 3:  92%|█████████▏| 1174/1271 [06:18<00:31,  3.11it/s, training_loss=0.072]\u001b[A\n",
            "Epoch 3:  92%|█████████▏| 1175/1271 [06:18<00:30,  3.12it/s, training_loss=0.072]\u001b[A\n",
            "Epoch 3:  92%|█████████▏| 1175/1271 [06:18<00:30,  3.12it/s, training_loss=0.543]\u001b[A\n",
            "Epoch 3:  93%|█████████▎| 1176/1271 [06:18<00:30,  3.12it/s, training_loss=0.543]\u001b[A\n",
            "Epoch 3:  93%|█████████▎| 1176/1271 [06:18<00:30,  3.12it/s, training_loss=0.107]\u001b[A\n",
            "Epoch 3:  93%|█████████▎| 1177/1271 [06:18<00:30,  3.12it/s, training_loss=0.107]\u001b[A\n",
            "Epoch 3:  93%|█████████▎| 1177/1271 [06:19<00:30,  3.12it/s, training_loss=0.556]\u001b[A\n",
            "Epoch 3:  93%|█████████▎| 1178/1271 [06:19<00:29,  3.12it/s, training_loss=0.556]\u001b[A\n",
            "Epoch 3:  93%|█████████▎| 1178/1271 [06:19<00:29,  3.12it/s, training_loss=0.207]\u001b[A\n",
            "Epoch 3:  93%|█████████▎| 1179/1271 [06:19<00:29,  3.13it/s, training_loss=0.207]\u001b[A\n",
            "Epoch 3:  93%|█████████▎| 1179/1271 [06:19<00:29,  3.13it/s, training_loss=0.024]\u001b[A\n",
            "Epoch 3:  93%|█████████▎| 1180/1271 [06:19<00:28,  3.14it/s, training_loss=0.024]\u001b[A\n",
            "Epoch 3:  93%|█████████▎| 1180/1271 [06:19<00:28,  3.14it/s, training_loss=0.017]\u001b[A\n",
            "Epoch 3:  93%|█████████▎| 1181/1271 [06:19<00:28,  3.13it/s, training_loss=0.017]\u001b[A\n",
            "Epoch 3:  93%|█████████▎| 1181/1271 [06:20<00:28,  3.13it/s, training_loss=0.555]\u001b[A\n",
            "Epoch 3:  93%|█████████▎| 1182/1271 [06:20<00:28,  3.13it/s, training_loss=0.555]\u001b[A\n",
            "Epoch 3:  93%|█████████▎| 1182/1271 [06:20<00:28,  3.13it/s, training_loss=0.088]\u001b[A\n",
            "Epoch 3:  93%|█████████▎| 1183/1271 [06:20<00:28,  3.13it/s, training_loss=0.088]\u001b[A\n",
            "Epoch 3:  93%|█████████▎| 1183/1271 [06:20<00:28,  3.13it/s, training_loss=0.520]\u001b[A\n",
            "Epoch 3:  93%|█████████▎| 1184/1271 [06:20<00:27,  3.13it/s, training_loss=0.520]\u001b[A\n",
            "Epoch 3:  93%|█████████▎| 1184/1271 [06:21<00:27,  3.13it/s, training_loss=0.626]\u001b[A\n",
            "Epoch 3:  93%|█████████▎| 1185/1271 [06:21<00:27,  3.12it/s, training_loss=0.626]\u001b[A\n",
            "Epoch 3:  93%|█████████▎| 1185/1271 [06:21<00:27,  3.12it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 3:  93%|█████████▎| 1186/1271 [06:21<00:27,  3.12it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 3:  93%|█████████▎| 1186/1271 [06:21<00:27,  3.12it/s, training_loss=0.036]\u001b[A\n",
            "Epoch 3:  93%|█████████▎| 1187/1271 [06:21<00:26,  3.12it/s, training_loss=0.036]\u001b[A\n",
            "Epoch 3:  93%|█████████▎| 1187/1271 [06:22<00:26,  3.12it/s, training_loss=0.166]\u001b[A\n",
            "Epoch 3:  93%|█████████▎| 1188/1271 [06:22<00:26,  3.11it/s, training_loss=0.166]\u001b[A\n",
            "Epoch 3:  93%|█████████▎| 1188/1271 [06:22<00:26,  3.11it/s, training_loss=0.120]\u001b[A\n",
            "Epoch 3:  94%|█████████▎| 1189/1271 [06:22<00:26,  3.10it/s, training_loss=0.120]\u001b[A\n",
            "Epoch 3:  94%|█████████▎| 1189/1271 [06:22<00:26,  3.10it/s, training_loss=0.260]\u001b[A\n",
            "Epoch 3:  94%|█████████▎| 1190/1271 [06:22<00:26,  3.10it/s, training_loss=0.260]\u001b[A\n",
            "Epoch 3:  94%|█████████▎| 1190/1271 [06:23<00:26,  3.10it/s, training_loss=0.430]\u001b[A\n",
            "Epoch 3:  94%|█████████▎| 1191/1271 [06:23<00:25,  3.11it/s, training_loss=0.430]\u001b[A\n",
            "Epoch 3:  94%|█████████▎| 1191/1271 [06:23<00:25,  3.11it/s, training_loss=0.359]\u001b[A\n",
            "Epoch 3:  94%|█████████▍| 1192/1271 [06:23<00:25,  3.11it/s, training_loss=0.359]\u001b[A\n",
            "Epoch 3:  94%|█████████▍| 1192/1271 [06:23<00:25,  3.11it/s, training_loss=0.373]\u001b[A\n",
            "Epoch 3:  94%|█████████▍| 1193/1271 [06:23<00:25,  3.12it/s, training_loss=0.373]\u001b[A\n",
            "Epoch 3:  94%|█████████▍| 1193/1271 [06:24<00:25,  3.12it/s, training_loss=0.136]\u001b[A\n",
            "Epoch 3:  94%|█████████▍| 1194/1271 [06:24<00:24,  3.11it/s, training_loss=0.136]\u001b[A\n",
            "Epoch 3:  94%|█████████▍| 1194/1271 [06:24<00:24,  3.11it/s, training_loss=0.173]\u001b[A\n",
            "Epoch 3:  94%|█████████▍| 1195/1271 [06:24<00:24,  3.11it/s, training_loss=0.173]\u001b[A\n",
            "Epoch 3:  94%|█████████▍| 1195/1271 [06:24<00:24,  3.11it/s, training_loss=0.826]\u001b[A\n",
            "Epoch 3:  94%|█████████▍| 1196/1271 [06:24<00:24,  3.12it/s, training_loss=0.826]\u001b[A\n",
            "Epoch 3:  94%|█████████▍| 1196/1271 [06:25<00:24,  3.12it/s, training_loss=0.502]\u001b[A\n",
            "Epoch 3:  94%|█████████▍| 1197/1271 [06:25<00:23,  3.11it/s, training_loss=0.502]\u001b[A\n",
            "Epoch 3:  94%|█████████▍| 1197/1271 [06:25<00:23,  3.11it/s, training_loss=0.100]\u001b[A\n",
            "Epoch 3:  94%|█████████▍| 1198/1271 [06:25<00:23,  3.12it/s, training_loss=0.100]\u001b[A\n",
            "Epoch 3:  94%|█████████▍| 1198/1271 [06:25<00:23,  3.12it/s, training_loss=0.215]\u001b[A\n",
            "Epoch 3:  94%|█████████▍| 1199/1271 [06:25<00:22,  3.13it/s, training_loss=0.215]\u001b[A\n",
            "Epoch 3:  94%|█████████▍| 1199/1271 [06:26<00:22,  3.13it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 3:  94%|█████████▍| 1200/1271 [06:26<00:22,  3.14it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 3:  94%|█████████▍| 1200/1271 [06:26<00:22,  3.14it/s, training_loss=0.028]\u001b[A\n",
            "Epoch 3:  94%|█████████▍| 1201/1271 [06:26<00:22,  3.11it/s, training_loss=0.028]\u001b[A\n",
            "Epoch 3:  94%|█████████▍| 1201/1271 [06:26<00:22,  3.11it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 3:  95%|█████████▍| 1202/1271 [06:26<00:22,  3.13it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 3:  95%|█████████▍| 1202/1271 [06:27<00:22,  3.13it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 3:  95%|█████████▍| 1203/1271 [06:27<00:21,  3.13it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 3:  95%|█████████▍| 1203/1271 [06:27<00:21,  3.13it/s, training_loss=0.530]\u001b[A\n",
            "Epoch 3:  95%|█████████▍| 1204/1271 [06:27<00:21,  3.11it/s, training_loss=0.530]\u001b[A\n",
            "Epoch 3:  95%|█████████▍| 1204/1271 [06:27<00:21,  3.11it/s, training_loss=0.137]\u001b[A\n",
            "Epoch 3:  95%|█████████▍| 1205/1271 [06:27<00:21,  3.08it/s, training_loss=0.137]\u001b[A\n",
            "Epoch 3:  95%|█████████▍| 1205/1271 [06:28<00:21,  3.08it/s, training_loss=0.075]\u001b[A\n",
            "Epoch 3:  95%|█████████▍| 1206/1271 [06:28<00:21,  3.07it/s, training_loss=0.075]\u001b[A\n",
            "Epoch 3:  95%|█████████▍| 1206/1271 [06:28<00:21,  3.07it/s, training_loss=0.094]\u001b[A\n",
            "Epoch 3:  95%|█████████▍| 1207/1271 [06:28<00:20,  3.09it/s, training_loss=0.094]\u001b[A\n",
            "Epoch 3:  95%|█████████▍| 1207/1271 [06:28<00:20,  3.09it/s, training_loss=0.264]\u001b[A\n",
            "Epoch 3:  95%|█████████▌| 1208/1271 [06:28<00:20,  3.11it/s, training_loss=0.264]\u001b[A\n",
            "Epoch 3:  95%|█████████▌| 1208/1271 [06:28<00:20,  3.11it/s, training_loss=0.562]\u001b[A\n",
            "Epoch 3:  95%|█████████▌| 1209/1271 [06:28<00:19,  3.10it/s, training_loss=0.562]\u001b[A\n",
            "Epoch 3:  95%|█████████▌| 1209/1271 [06:29<00:19,  3.10it/s, training_loss=0.276]\u001b[A\n",
            "Epoch 3:  95%|█████████▌| 1210/1271 [06:29<00:19,  3.10it/s, training_loss=0.276]\u001b[A\n",
            "Epoch 3:  95%|█████████▌| 1210/1271 [06:29<00:19,  3.10it/s, training_loss=0.333]\u001b[A\n",
            "Epoch 3:  95%|█████████▌| 1211/1271 [06:29<00:19,  3.10it/s, training_loss=0.333]\u001b[A\n",
            "Epoch 3:  95%|█████████▌| 1211/1271 [06:29<00:19,  3.10it/s, training_loss=0.472]\u001b[A\n",
            "Epoch 3:  95%|█████████▌| 1212/1271 [06:29<00:19,  3.10it/s, training_loss=0.472]\u001b[A\n",
            "Epoch 3:  95%|█████████▌| 1212/1271 [06:30<00:19,  3.10it/s, training_loss=0.113]\u001b[A\n",
            "Epoch 3:  95%|█████████▌| 1213/1271 [06:30<00:18,  3.10it/s, training_loss=0.113]\u001b[A\n",
            "Epoch 3:  95%|█████████▌| 1213/1271 [06:30<00:18,  3.10it/s, training_loss=0.291]\u001b[A\n",
            "Epoch 3:  96%|█████████▌| 1214/1271 [06:30<00:18,  3.10it/s, training_loss=0.291]\u001b[A\n",
            "Epoch 3:  96%|█████████▌| 1214/1271 [06:30<00:18,  3.10it/s, training_loss=0.267]\u001b[A\n",
            "Epoch 3:  96%|█████████▌| 1215/1271 [06:30<00:18,  3.11it/s, training_loss=0.267]\u001b[A\n",
            "Epoch 3:  96%|█████████▌| 1215/1271 [06:31<00:18,  3.11it/s, training_loss=0.075]\u001b[A\n",
            "Epoch 3:  96%|█████████▌| 1216/1271 [06:31<00:17,  3.12it/s, training_loss=0.075]\u001b[A\n",
            "Epoch 3:  96%|█████████▌| 1216/1271 [06:31<00:17,  3.12it/s, training_loss=0.264]\u001b[A\n",
            "Epoch 3:  96%|█████████▌| 1217/1271 [06:31<00:17,  3.11it/s, training_loss=0.264]\u001b[A\n",
            "Epoch 3:  96%|█████████▌| 1217/1271 [06:31<00:17,  3.11it/s, training_loss=0.543]\u001b[A\n",
            "Epoch 3:  96%|█████████▌| 1218/1271 [06:31<00:17,  3.10it/s, training_loss=0.543]\u001b[A\n",
            "Epoch 3:  96%|█████████▌| 1218/1271 [06:32<00:17,  3.10it/s, training_loss=0.148]\u001b[A\n",
            "Epoch 3:  96%|█████████▌| 1219/1271 [06:32<00:16,  3.08it/s, training_loss=0.148]\u001b[A\n",
            "Epoch 3:  96%|█████████▌| 1219/1271 [06:32<00:16,  3.08it/s, training_loss=0.243]\u001b[A\n",
            "Epoch 3:  96%|█████████▌| 1220/1271 [06:32<00:16,  3.09it/s, training_loss=0.243]\u001b[A\n",
            "Epoch 3:  96%|█████████▌| 1220/1271 [06:32<00:16,  3.09it/s, training_loss=0.213]\u001b[A\n",
            "Epoch 3:  96%|█████████▌| 1221/1271 [06:32<00:16,  3.09it/s, training_loss=0.213]\u001b[A\n",
            "Epoch 3:  96%|█████████▌| 1221/1271 [06:33<00:16,  3.09it/s, training_loss=0.777]\u001b[A\n",
            "Epoch 3:  96%|█████████▌| 1222/1271 [06:33<00:15,  3.11it/s, training_loss=0.777]\u001b[A\n",
            "Epoch 3:  96%|█████████▌| 1222/1271 [06:33<00:15,  3.11it/s, training_loss=0.143]\u001b[A\n",
            "Epoch 3:  96%|█████████▌| 1223/1271 [06:33<00:15,  3.12it/s, training_loss=0.143]\u001b[A\n",
            "Epoch 3:  96%|█████████▌| 1223/1271 [06:33<00:15,  3.12it/s, training_loss=0.390]\u001b[A\n",
            "Epoch 3:  96%|█████████▋| 1224/1271 [06:33<00:15,  3.13it/s, training_loss=0.390]\u001b[A\n",
            "Epoch 3:  96%|█████████▋| 1224/1271 [06:34<00:15,  3.13it/s, training_loss=0.259]\u001b[A\n",
            "Epoch 3:  96%|█████████▋| 1225/1271 [06:34<00:14,  3.13it/s, training_loss=0.259]\u001b[A\n",
            "Epoch 3:  96%|█████████▋| 1225/1271 [06:34<00:14,  3.13it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 3:  96%|█████████▋| 1226/1271 [06:34<00:14,  3.12it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 3:  96%|█████████▋| 1226/1271 [06:34<00:14,  3.12it/s, training_loss=0.073]\u001b[A\n",
            "Epoch 3:  97%|█████████▋| 1227/1271 [06:34<00:14,  3.11it/s, training_loss=0.073]\u001b[A\n",
            "Epoch 3:  97%|█████████▋| 1227/1271 [06:35<00:14,  3.11it/s, training_loss=0.536]\u001b[A\n",
            "Epoch 3:  97%|█████████▋| 1228/1271 [06:35<00:13,  3.09it/s, training_loss=0.536]\u001b[A\n",
            "Epoch 3:  97%|█████████▋| 1228/1271 [06:35<00:13,  3.09it/s, training_loss=0.025]\u001b[A\n",
            "Epoch 3:  97%|█████████▋| 1229/1271 [06:35<00:13,  3.06it/s, training_loss=0.025]\u001b[A\n",
            "Epoch 3:  97%|█████████▋| 1229/1271 [06:35<00:13,  3.06it/s, training_loss=0.167]\u001b[A\n",
            "Epoch 3:  97%|█████████▋| 1230/1271 [06:35<00:13,  3.08it/s, training_loss=0.167]\u001b[A\n",
            "Epoch 3:  97%|█████████▋| 1230/1271 [06:36<00:13,  3.08it/s, training_loss=0.341]\u001b[A\n",
            "Epoch 3:  97%|█████████▋| 1231/1271 [06:36<00:12,  3.09it/s, training_loss=0.341]\u001b[A\n",
            "Epoch 3:  97%|█████████▋| 1231/1271 [06:36<00:12,  3.09it/s, training_loss=0.019]\u001b[A\n",
            "Epoch 3:  97%|█████████▋| 1232/1271 [06:36<00:12,  3.08it/s, training_loss=0.019]\u001b[A\n",
            "Epoch 3:  97%|█████████▋| 1232/1271 [06:36<00:12,  3.08it/s, training_loss=0.709]\u001b[A\n",
            "Epoch 3:  97%|█████████▋| 1233/1271 [06:36<00:12,  3.08it/s, training_loss=0.709]\u001b[A\n",
            "Epoch 3:  97%|█████████▋| 1233/1271 [06:37<00:12,  3.08it/s, training_loss=0.385]\u001b[A\n",
            "Epoch 3:  97%|█████████▋| 1234/1271 [06:37<00:12,  3.07it/s, training_loss=0.385]\u001b[A\n",
            "Epoch 3:  97%|█████████▋| 1234/1271 [06:37<00:12,  3.07it/s, training_loss=0.174]\u001b[A\n",
            "Epoch 3:  97%|█████████▋| 1235/1271 [06:37<00:11,  3.08it/s, training_loss=0.174]\u001b[A\n",
            "Epoch 3:  97%|█████████▋| 1235/1271 [06:37<00:11,  3.08it/s, training_loss=0.408]\u001b[A\n",
            "Epoch 3:  97%|█████████▋| 1236/1271 [06:37<00:11,  3.09it/s, training_loss=0.408]\u001b[A\n",
            "Epoch 3:  97%|█████████▋| 1236/1271 [06:38<00:11,  3.09it/s, training_loss=0.111]\u001b[A\n",
            "Epoch 3:  97%|█████████▋| 1237/1271 [06:38<00:10,  3.11it/s, training_loss=0.111]\u001b[A\n",
            "Epoch 3:  97%|█████████▋| 1237/1271 [06:38<00:10,  3.11it/s, training_loss=0.473]\u001b[A\n",
            "Epoch 3:  97%|█████████▋| 1238/1271 [06:38<00:10,  3.11it/s, training_loss=0.473]\u001b[A\n",
            "Epoch 3:  97%|█████████▋| 1238/1271 [06:38<00:10,  3.11it/s, training_loss=0.435]\u001b[A\n",
            "Epoch 3:  97%|█████████▋| 1239/1271 [06:38<00:10,  3.11it/s, training_loss=0.435]\u001b[A\n",
            "Epoch 3:  97%|█████████▋| 1239/1271 [06:38<00:10,  3.11it/s, training_loss=0.150]\u001b[A\n",
            "Epoch 3:  98%|█████████▊| 1240/1271 [06:38<00:09,  3.10it/s, training_loss=0.150]\u001b[A\n",
            "Epoch 3:  98%|█████████▊| 1240/1271 [06:39<00:09,  3.10it/s, training_loss=0.636]\u001b[A\n",
            "Epoch 3:  98%|█████████▊| 1241/1271 [06:39<00:09,  3.11it/s, training_loss=0.636]\u001b[A\n",
            "Epoch 3:  98%|█████████▊| 1241/1271 [06:39<00:09,  3.11it/s, training_loss=0.134]\u001b[A\n",
            "Epoch 3:  98%|█████████▊| 1242/1271 [06:39<00:09,  3.10it/s, training_loss=0.134]\u001b[A\n",
            "Epoch 3:  98%|█████████▊| 1242/1271 [06:39<00:09,  3.10it/s, training_loss=0.602]\u001b[A\n",
            "Epoch 3:  98%|█████████▊| 1243/1271 [06:39<00:09,  3.10it/s, training_loss=0.602]\u001b[A\n",
            "Epoch 3:  98%|█████████▊| 1243/1271 [06:40<00:09,  3.10it/s, training_loss=0.287]\u001b[A\n",
            "Epoch 3:  98%|█████████▊| 1244/1271 [06:40<00:08,  3.11it/s, training_loss=0.287]\u001b[A\n",
            "Epoch 3:  98%|█████████▊| 1244/1271 [06:40<00:08,  3.11it/s, training_loss=0.094]\u001b[A\n",
            "Epoch 3:  98%|█████████▊| 1245/1271 [06:40<00:08,  3.12it/s, training_loss=0.094]\u001b[A\n",
            "Epoch 3:  98%|█████████▊| 1245/1271 [06:40<00:08,  3.12it/s, training_loss=0.238]\u001b[A\n",
            "Epoch 3:  98%|█████████▊| 1246/1271 [06:40<00:07,  3.14it/s, training_loss=0.238]\u001b[A\n",
            "Epoch 3:  98%|█████████▊| 1246/1271 [06:41<00:07,  3.14it/s, training_loss=0.056]\u001b[A\n",
            "Epoch 3:  98%|█████████▊| 1247/1271 [06:41<00:07,  3.14it/s, training_loss=0.056]\u001b[A\n",
            "Epoch 3:  98%|█████████▊| 1247/1271 [06:41<00:07,  3.14it/s, training_loss=0.425]\u001b[A\n",
            "Epoch 3:  98%|█████████▊| 1248/1271 [06:41<00:07,  3.14it/s, training_loss=0.425]\u001b[A\n",
            "Epoch 3:  98%|█████████▊| 1248/1271 [06:41<00:07,  3.14it/s, training_loss=0.231]\u001b[A\n",
            "Epoch 3:  98%|█████████▊| 1249/1271 [06:41<00:07,  3.12it/s, training_loss=0.231]\u001b[A\n",
            "Epoch 3:  98%|█████████▊| 1249/1271 [06:42<00:07,  3.12it/s, training_loss=0.204]\u001b[A\n",
            "Epoch 3:  98%|█████████▊| 1250/1271 [06:42<00:06,  3.11it/s, training_loss=0.204]\u001b[A\n",
            "Epoch 3:  98%|█████████▊| 1250/1271 [06:42<00:06,  3.11it/s, training_loss=0.341]\u001b[A\n",
            "Epoch 3:  98%|█████████▊| 1251/1271 [06:42<00:06,  3.10it/s, training_loss=0.341]\u001b[A\n",
            "Epoch 3:  98%|█████████▊| 1251/1271 [06:42<00:06,  3.10it/s, training_loss=0.620]\u001b[A\n",
            "Epoch 3:  99%|█████████▊| 1252/1271 [06:42<00:06,  3.11it/s, training_loss=0.620]\u001b[A\n",
            "Epoch 3:  99%|█████████▊| 1252/1271 [06:43<00:06,  3.11it/s, training_loss=0.617]\u001b[A\n",
            "Epoch 3:  99%|█████████▊| 1253/1271 [06:43<00:05,  3.12it/s, training_loss=0.617]\u001b[A\n",
            "Epoch 3:  99%|█████████▊| 1253/1271 [06:43<00:05,  3.12it/s, training_loss=0.055]\u001b[A\n",
            "Epoch 3:  99%|█████████▊| 1254/1271 [06:43<00:05,  3.12it/s, training_loss=0.055]\u001b[A\n",
            "Epoch 3:  99%|█████████▊| 1254/1271 [06:43<00:05,  3.12it/s, training_loss=0.071]\u001b[A\n",
            "Epoch 3:  99%|█████████▊| 1255/1271 [06:43<00:05,  3.12it/s, training_loss=0.071]\u001b[A\n",
            "Epoch 3:  99%|█████████▊| 1255/1271 [06:44<00:05,  3.12it/s, training_loss=0.037]\u001b[A\n",
            "Epoch 3:  99%|█████████▉| 1256/1271 [06:44<00:04,  3.11it/s, training_loss=0.037]\u001b[A\n",
            "Epoch 3:  99%|█████████▉| 1256/1271 [06:44<00:04,  3.11it/s, training_loss=0.480]\u001b[A\n",
            "Epoch 3:  99%|█████████▉| 1257/1271 [06:44<00:04,  3.10it/s, training_loss=0.480]\u001b[A\n",
            "Epoch 3:  99%|█████████▉| 1257/1271 [06:44<00:04,  3.10it/s, training_loss=0.451]\u001b[A\n",
            "Epoch 3:  99%|█████████▉| 1258/1271 [06:44<00:04,  3.10it/s, training_loss=0.451]\u001b[A\n",
            "Epoch 3:  99%|█████████▉| 1258/1271 [06:45<00:04,  3.10it/s, training_loss=0.230]\u001b[A\n",
            "Epoch 3:  99%|█████████▉| 1259/1271 [06:45<00:03,  3.12it/s, training_loss=0.230]\u001b[A\n",
            "Epoch 3:  99%|█████████▉| 1259/1271 [06:45<00:03,  3.12it/s, training_loss=0.222]\u001b[A\n",
            "Epoch 3:  99%|█████████▉| 1260/1271 [06:45<00:03,  3.13it/s, training_loss=0.222]\u001b[A\n",
            "Epoch 3:  99%|█████████▉| 1260/1271 [06:45<00:03,  3.13it/s, training_loss=0.474]\u001b[A\n",
            "Epoch 3:  99%|█████████▉| 1261/1271 [06:45<00:03,  3.11it/s, training_loss=0.474]\u001b[A\n",
            "Epoch 3:  99%|█████████▉| 1261/1271 [06:46<00:03,  3.11it/s, training_loss=0.359]\u001b[A\n",
            "Epoch 3:  99%|█████████▉| 1262/1271 [06:46<00:02,  3.10it/s, training_loss=0.359]\u001b[A\n",
            "Epoch 3:  99%|█████████▉| 1262/1271 [06:46<00:02,  3.10it/s, training_loss=0.146]\u001b[A\n",
            "Epoch 3:  99%|█████████▉| 1263/1271 [06:46<00:02,  3.09it/s, training_loss=0.146]\u001b[A\n",
            "Epoch 3:  99%|█████████▉| 1263/1271 [06:46<00:02,  3.09it/s, training_loss=0.371]\u001b[A\n",
            "Epoch 3:  99%|█████████▉| 1264/1271 [06:46<00:02,  3.10it/s, training_loss=0.371]\u001b[A\n",
            "Epoch 3:  99%|█████████▉| 1264/1271 [06:47<00:02,  3.10it/s, training_loss=0.506]\u001b[A\n",
            "Epoch 3: 100%|█████████▉| 1265/1271 [06:47<00:01,  3.08it/s, training_loss=0.506]\u001b[A\n",
            "Epoch 3: 100%|█████████▉| 1265/1271 [06:47<00:01,  3.08it/s, training_loss=0.186]\u001b[A\n",
            "Epoch 3: 100%|█████████▉| 1266/1271 [06:47<00:01,  3.10it/s, training_loss=0.186]\u001b[A\n",
            "Epoch 3: 100%|█████████▉| 1266/1271 [06:47<00:01,  3.10it/s, training_loss=0.528]\u001b[A\n",
            "Epoch 3: 100%|█████████▉| 1267/1271 [06:47<00:01,  3.11it/s, training_loss=0.528]\u001b[A\n",
            "Epoch 3: 100%|█████████▉| 1267/1271 [06:47<00:01,  3.11it/s, training_loss=0.510]\u001b[A\n",
            "Epoch 3: 100%|█████████▉| 1268/1271 [06:47<00:00,  3.11it/s, training_loss=0.510]\u001b[A\n",
            "Epoch 3: 100%|█████████▉| 1268/1271 [06:48<00:00,  3.11it/s, training_loss=0.161]\u001b[A\n",
            "Epoch 3: 100%|█████████▉| 1269/1271 [06:48<00:00,  3.12it/s, training_loss=0.161]\u001b[A\n",
            "Epoch 3: 100%|█████████▉| 1269/1271 [06:48<00:00,  3.12it/s, training_loss=0.531]\u001b[A\n",
            "Epoch 3: 100%|█████████▉| 1270/1271 [06:48<00:00,  3.13it/s, training_loss=0.531]\u001b[A\n",
            "Epoch 3: 100%|█████████▉| 1270/1271 [06:48<00:00,  3.13it/s, training_loss=0.819]\u001b[A\n",
            "Epoch 3: 100%|██████████| 1271/1271 [06:48<00:00,  3.74it/s, training_loss=0.819]\u001b[A\n",
            " 40%|████      | 2/5 [21:17<21:38, 432.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3\n",
            "Training loss: 0.9249951249639922\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 3/5 [21:38<14:26, 433.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 0.8600434106629756\n",
            "F1 Score (Weighted): 0.7261731861827846\n",
            "Recall@5: 0.9717682020802377\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4:   0%|          | 0/1271 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:   0%|          | 0/1271 [00:00<?, ?it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 4:   0%|          | 1/1271 [00:00<06:34,  3.22it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 4:   0%|          | 1/1271 [00:00<06:34,  3.22it/s, training_loss=0.432]\u001b[A\n",
            "Epoch 4:   0%|          | 2/1271 [00:00<06:38,  3.18it/s, training_loss=0.432]\u001b[A\n",
            "Epoch 4:   0%|          | 2/1271 [00:00<06:38,  3.18it/s, training_loss=0.189]\u001b[A\n",
            "Epoch 4:   0%|          | 3/1271 [00:00<06:44,  3.13it/s, training_loss=0.189]\u001b[A\n",
            "Epoch 4:   0%|          | 3/1271 [00:01<06:44,  3.13it/s, training_loss=0.394]\u001b[A\n",
            "Epoch 4:   0%|          | 4/1271 [00:01<06:45,  3.12it/s, training_loss=0.394]\u001b[A\n",
            "Epoch 4:   0%|          | 4/1271 [00:01<06:45,  3.12it/s, training_loss=0.249]\u001b[A\n",
            "Epoch 4:   0%|          | 5/1271 [00:01<06:48,  3.10it/s, training_loss=0.249]\u001b[A\n",
            "Epoch 4:   0%|          | 5/1271 [00:01<06:48,  3.10it/s, training_loss=0.757]\u001b[A\n",
            "Epoch 4:   0%|          | 6/1271 [00:01<06:49,  3.09it/s, training_loss=0.757]\u001b[A\n",
            "Epoch 4:   0%|          | 6/1271 [00:02<06:49,  3.09it/s, training_loss=0.024]\u001b[A\n",
            "Epoch 4:   1%|          | 7/1271 [00:02<06:47,  3.10it/s, training_loss=0.024]\u001b[A\n",
            "Epoch 4:   1%|          | 7/1271 [00:02<06:47,  3.10it/s, training_loss=0.111]\u001b[A\n",
            "Epoch 4:   1%|          | 8/1271 [00:02<06:46,  3.11it/s, training_loss=0.111]\u001b[A\n",
            "Epoch 4:   1%|          | 8/1271 [00:02<06:46,  3.11it/s, training_loss=0.483]\u001b[A\n",
            "Epoch 4:   1%|          | 9/1271 [00:02<06:48,  3.09it/s, training_loss=0.483]\u001b[A\n",
            "Epoch 4:   1%|          | 9/1271 [00:03<06:48,  3.09it/s, training_loss=0.118]\u001b[A\n",
            "Epoch 4:   1%|          | 10/1271 [00:03<06:48,  3.09it/s, training_loss=0.118]\u001b[A\n",
            "Epoch 4:   1%|          | 10/1271 [00:03<06:48,  3.09it/s, training_loss=0.127]\u001b[A\n",
            "Epoch 4:   1%|          | 11/1271 [00:03<06:48,  3.09it/s, training_loss=0.127]\u001b[A\n",
            "Epoch 4:   1%|          | 11/1271 [00:03<06:48,  3.09it/s, training_loss=0.085]\u001b[A\n",
            "Epoch 4:   1%|          | 12/1271 [00:03<06:48,  3.08it/s, training_loss=0.085]\u001b[A\n",
            "Epoch 4:   1%|          | 12/1271 [00:04<06:48,  3.08it/s, training_loss=0.357]\u001b[A\n",
            "Epoch 4:   1%|          | 13/1271 [00:04<06:49,  3.07it/s, training_loss=0.357]\u001b[A\n",
            "Epoch 4:   1%|          | 13/1271 [00:04<06:49,  3.07it/s, training_loss=0.356]\u001b[A\n",
            "Epoch 4:   1%|          | 14/1271 [00:04<06:49,  3.07it/s, training_loss=0.356]\u001b[A\n",
            "Epoch 4:   1%|          | 14/1271 [00:04<06:49,  3.07it/s, training_loss=0.463]\u001b[A\n",
            "Epoch 4:   1%|          | 15/1271 [00:04<06:49,  3.07it/s, training_loss=0.463]\u001b[A\n",
            "Epoch 4:   1%|          | 15/1271 [00:05<06:49,  3.07it/s, training_loss=0.025]\u001b[A\n",
            "Epoch 4:   1%|▏         | 16/1271 [00:05<06:47,  3.08it/s, training_loss=0.025]\u001b[A\n",
            "Epoch 4:   1%|▏         | 16/1271 [00:05<06:47,  3.08it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 4:   1%|▏         | 17/1271 [00:05<06:47,  3.07it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 4:   1%|▏         | 17/1271 [00:05<06:47,  3.07it/s, training_loss=0.144]\u001b[A\n",
            "Epoch 4:   1%|▏         | 18/1271 [00:05<06:49,  3.06it/s, training_loss=0.144]\u001b[A\n",
            "Epoch 4:   1%|▏         | 18/1271 [00:06<06:49,  3.06it/s, training_loss=0.072]\u001b[A\n",
            "Epoch 4:   1%|▏         | 19/1271 [00:06<06:50,  3.05it/s, training_loss=0.072]\u001b[A\n",
            "Epoch 4:   1%|▏         | 19/1271 [00:06<06:50,  3.05it/s, training_loss=0.216]\u001b[A\n",
            "Epoch 4:   2%|▏         | 20/1271 [00:06<06:46,  3.07it/s, training_loss=0.216]\u001b[A\n",
            "Epoch 4:   2%|▏         | 20/1271 [00:06<06:46,  3.07it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 4:   2%|▏         | 21/1271 [00:06<06:47,  3.07it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 4:   2%|▏         | 21/1271 [00:07<06:47,  3.07it/s, training_loss=0.071]\u001b[A\n",
            "Epoch 4:   2%|▏         | 22/1271 [00:07<06:46,  3.07it/s, training_loss=0.071]\u001b[A\n",
            "Epoch 4:   2%|▏         | 22/1271 [00:07<06:46,  3.07it/s, training_loss=0.074]\u001b[A\n",
            "Epoch 4:   2%|▏         | 23/1271 [00:07<06:49,  3.05it/s, training_loss=0.074]\u001b[A\n",
            "Epoch 4:   2%|▏         | 23/1271 [00:07<06:49,  3.05it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 4:   2%|▏         | 24/1271 [00:07<06:45,  3.07it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 4:   2%|▏         | 24/1271 [00:08<06:45,  3.07it/s, training_loss=0.828]\u001b[A\n",
            "Epoch 4:   2%|▏         | 25/1271 [00:08<06:46,  3.07it/s, training_loss=0.828]\u001b[A\n",
            "Epoch 4:   2%|▏         | 25/1271 [00:08<06:46,  3.07it/s, training_loss=0.514]\u001b[A\n",
            "Epoch 4:   2%|▏         | 26/1271 [00:08<06:46,  3.06it/s, training_loss=0.514]\u001b[A\n",
            "Epoch 4:   2%|▏         | 26/1271 [00:08<06:46,  3.06it/s, training_loss=0.031]\u001b[A\n",
            "Epoch 4:   2%|▏         | 27/1271 [00:08<06:43,  3.08it/s, training_loss=0.031]\u001b[A\n",
            "Epoch 4:   2%|▏         | 27/1271 [00:09<06:43,  3.08it/s, training_loss=0.198]\u001b[A\n",
            "Epoch 4:   2%|▏         | 28/1271 [00:09<06:41,  3.09it/s, training_loss=0.198]\u001b[A\n",
            "Epoch 4:   2%|▏         | 28/1271 [00:09<06:41,  3.09it/s, training_loss=0.010]\u001b[A\n",
            "Epoch 4:   2%|▏         | 29/1271 [00:09<06:43,  3.08it/s, training_loss=0.010]\u001b[A\n",
            "Epoch 4:   2%|▏         | 29/1271 [00:09<06:43,  3.08it/s, training_loss=0.166]\u001b[A\n",
            "Epoch 4:   2%|▏         | 30/1271 [00:09<06:42,  3.08it/s, training_loss=0.166]\u001b[A\n",
            "Epoch 4:   2%|▏         | 30/1271 [00:10<06:42,  3.08it/s, training_loss=0.278]\u001b[A\n",
            "Epoch 4:   2%|▏         | 31/1271 [00:10<06:40,  3.10it/s, training_loss=0.278]\u001b[A\n",
            "Epoch 4:   2%|▏         | 31/1271 [00:10<06:40,  3.10it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 4:   3%|▎         | 32/1271 [00:10<06:41,  3.08it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 4:   3%|▎         | 32/1271 [00:10<06:41,  3.08it/s, training_loss=0.110]\u001b[A\n",
            "Epoch 4:   3%|▎         | 33/1271 [00:10<06:41,  3.09it/s, training_loss=0.110]\u001b[A\n",
            "Epoch 4:   3%|▎         | 33/1271 [00:11<06:41,  3.09it/s, training_loss=0.144]\u001b[A\n",
            "Epoch 4:   3%|▎         | 34/1271 [00:11<06:40,  3.09it/s, training_loss=0.144]\u001b[A\n",
            "Epoch 4:   3%|▎         | 34/1271 [00:11<06:40,  3.09it/s, training_loss=0.162]\u001b[A\n",
            "Epoch 4:   3%|▎         | 35/1271 [00:11<06:41,  3.08it/s, training_loss=0.162]\u001b[A\n",
            "Epoch 4:   3%|▎         | 35/1271 [00:11<06:41,  3.08it/s, training_loss=0.080]\u001b[A\n",
            "Epoch 4:   3%|▎         | 36/1271 [00:11<06:41,  3.07it/s, training_loss=0.080]\u001b[A\n",
            "Epoch 4:   3%|▎         | 36/1271 [00:12<06:41,  3.07it/s, training_loss=0.067]\u001b[A\n",
            "Epoch 4:   3%|▎         | 37/1271 [00:12<06:42,  3.06it/s, training_loss=0.067]\u001b[A\n",
            "Epoch 4:   3%|▎         | 37/1271 [00:12<06:42,  3.06it/s, training_loss=0.363]\u001b[A\n",
            "Epoch 4:   3%|▎         | 38/1271 [00:12<06:46,  3.03it/s, training_loss=0.363]\u001b[A\n",
            "Epoch 4:   3%|▎         | 38/1271 [00:12<06:46,  3.03it/s, training_loss=0.326]\u001b[A\n",
            "Epoch 4:   3%|▎         | 39/1271 [00:12<06:43,  3.05it/s, training_loss=0.326]\u001b[A\n",
            "Epoch 4:   3%|▎         | 39/1271 [00:12<06:43,  3.05it/s, training_loss=0.314]\u001b[A\n",
            "Epoch 4:   3%|▎         | 40/1271 [00:12<06:41,  3.06it/s, training_loss=0.314]\u001b[A\n",
            "Epoch 4:   3%|▎         | 40/1271 [00:13<06:41,  3.06it/s, training_loss=0.243]\u001b[A\n",
            "Epoch 4:   3%|▎         | 41/1271 [00:13<06:39,  3.08it/s, training_loss=0.243]\u001b[A\n",
            "Epoch 4:   3%|▎         | 41/1271 [00:13<06:39,  3.08it/s, training_loss=0.071]\u001b[A\n",
            "Epoch 4:   3%|▎         | 42/1271 [00:13<06:37,  3.09it/s, training_loss=0.071]\u001b[A\n",
            "Epoch 4:   3%|▎         | 42/1271 [00:13<06:37,  3.09it/s, training_loss=0.213]\u001b[A\n",
            "Epoch 4:   3%|▎         | 43/1271 [00:13<06:37,  3.09it/s, training_loss=0.213]\u001b[A\n",
            "Epoch 4:   3%|▎         | 43/1271 [00:14<06:37,  3.09it/s, training_loss=0.400]\u001b[A\n",
            "Epoch 4:   3%|▎         | 44/1271 [00:14<06:38,  3.08it/s, training_loss=0.400]\u001b[A\n",
            "Epoch 4:   3%|▎         | 44/1271 [00:14<06:38,  3.08it/s, training_loss=0.942]\u001b[A\n",
            "Epoch 4:   4%|▎         | 45/1271 [00:14<06:37,  3.08it/s, training_loss=0.942]\u001b[A\n",
            "Epoch 4:   4%|▎         | 45/1271 [00:14<06:37,  3.08it/s, training_loss=0.103]\u001b[A\n",
            "Epoch 4:   4%|▎         | 46/1271 [00:14<06:37,  3.09it/s, training_loss=0.103]\u001b[A\n",
            "Epoch 4:   4%|▎         | 46/1271 [00:15<06:37,  3.09it/s, training_loss=0.239]\u001b[A\n",
            "Epoch 4:   4%|▎         | 47/1271 [00:15<06:38,  3.07it/s, training_loss=0.239]\u001b[A\n",
            "Epoch 4:   4%|▎         | 47/1271 [00:15<06:38,  3.07it/s, training_loss=0.130]\u001b[A\n",
            "Epoch 4:   4%|▍         | 48/1271 [00:15<06:40,  3.06it/s, training_loss=0.130]\u001b[A\n",
            "Epoch 4:   4%|▍         | 48/1271 [00:15<06:40,  3.06it/s, training_loss=0.074]\u001b[A\n",
            "Epoch 4:   4%|▍         | 49/1271 [00:15<06:37,  3.07it/s, training_loss=0.074]\u001b[A\n",
            "Epoch 4:   4%|▍         | 49/1271 [00:16<06:37,  3.07it/s, training_loss=0.848]\u001b[A\n",
            "Epoch 4:   4%|▍         | 50/1271 [00:16<06:36,  3.08it/s, training_loss=0.848]\u001b[A\n",
            "Epoch 4:   4%|▍         | 50/1271 [00:16<06:36,  3.08it/s, training_loss=0.393]\u001b[A\n",
            "Epoch 4:   4%|▍         | 51/1271 [00:16<06:34,  3.09it/s, training_loss=0.393]\u001b[A\n",
            "Epoch 4:   4%|▍         | 51/1271 [00:16<06:34,  3.09it/s, training_loss=0.148]\u001b[A\n",
            "Epoch 4:   4%|▍         | 52/1271 [00:16<06:37,  3.07it/s, training_loss=0.148]\u001b[A\n",
            "Epoch 4:   4%|▍         | 52/1271 [00:17<06:37,  3.07it/s, training_loss=0.107]\u001b[A\n",
            "Epoch 4:   4%|▍         | 53/1271 [00:17<06:36,  3.08it/s, training_loss=0.107]\u001b[A\n",
            "Epoch 4:   4%|▍         | 53/1271 [00:17<06:36,  3.08it/s, training_loss=0.150]\u001b[A\n",
            "Epoch 4:   4%|▍         | 54/1271 [00:17<06:35,  3.08it/s, training_loss=0.150]\u001b[A\n",
            "Epoch 4:   4%|▍         | 54/1271 [00:17<06:35,  3.08it/s, training_loss=0.657]\u001b[A\n",
            "Epoch 4:   4%|▍         | 55/1271 [00:17<06:34,  3.08it/s, training_loss=0.657]\u001b[A\n",
            "Epoch 4:   4%|▍         | 55/1271 [00:18<06:34,  3.08it/s, training_loss=0.219]\u001b[A\n",
            "Epoch 4:   4%|▍         | 56/1271 [00:18<06:36,  3.06it/s, training_loss=0.219]\u001b[A\n",
            "Epoch 4:   4%|▍         | 56/1271 [00:18<06:36,  3.06it/s, training_loss=0.311]\u001b[A\n",
            "Epoch 4:   4%|▍         | 57/1271 [00:18<06:37,  3.05it/s, training_loss=0.311]\u001b[A\n",
            "Epoch 4:   4%|▍         | 57/1271 [00:18<06:37,  3.05it/s, training_loss=0.148]\u001b[A\n",
            "Epoch 4:   5%|▍         | 58/1271 [00:18<06:35,  3.07it/s, training_loss=0.148]\u001b[A\n",
            "Epoch 4:   5%|▍         | 58/1271 [00:19<06:35,  3.07it/s, training_loss=0.132]\u001b[A\n",
            "Epoch 4:   5%|▍         | 59/1271 [00:19<06:33,  3.08it/s, training_loss=0.132]\u001b[A\n",
            "Epoch 4:   5%|▍         | 59/1271 [00:19<06:33,  3.08it/s, training_loss=0.085]\u001b[A\n",
            "Epoch 4:   5%|▍         | 60/1271 [00:19<06:32,  3.08it/s, training_loss=0.085]\u001b[A\n",
            "Epoch 4:   5%|▍         | 60/1271 [00:19<06:32,  3.08it/s, training_loss=0.313]\u001b[A\n",
            "Epoch 4:   5%|▍         | 61/1271 [00:19<06:31,  3.09it/s, training_loss=0.313]\u001b[A\n",
            "Epoch 4:   5%|▍         | 61/1271 [00:20<06:31,  3.09it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 4:   5%|▍         | 62/1271 [00:20<06:31,  3.09it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 4:   5%|▍         | 62/1271 [00:20<06:31,  3.09it/s, training_loss=0.271]\u001b[A\n",
            "Epoch 4:   5%|▍         | 63/1271 [00:20<06:32,  3.08it/s, training_loss=0.271]\u001b[A\n",
            "Epoch 4:   5%|▍         | 63/1271 [00:20<06:32,  3.08it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 4:   5%|▌         | 64/1271 [00:20<06:32,  3.08it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 4:   5%|▌         | 64/1271 [00:21<06:32,  3.08it/s, training_loss=0.597]\u001b[A\n",
            "Epoch 4:   5%|▌         | 65/1271 [00:21<06:31,  3.08it/s, training_loss=0.597]\u001b[A\n",
            "Epoch 4:   5%|▌         | 65/1271 [00:21<06:31,  3.08it/s, training_loss=0.366]\u001b[A\n",
            "Epoch 4:   5%|▌         | 66/1271 [00:21<06:31,  3.08it/s, training_loss=0.366]\u001b[A\n",
            "Epoch 4:   5%|▌         | 66/1271 [00:21<06:31,  3.08it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 4:   5%|▌         | 67/1271 [00:21<06:29,  3.09it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 4:   5%|▌         | 67/1271 [00:22<06:29,  3.09it/s, training_loss=0.619]\u001b[A\n",
            "Epoch 4:   5%|▌         | 68/1271 [00:22<06:29,  3.09it/s, training_loss=0.619]\u001b[A\n",
            "Epoch 4:   5%|▌         | 68/1271 [00:22<06:29,  3.09it/s, training_loss=0.354]\u001b[A\n",
            "Epoch 4:   5%|▌         | 69/1271 [00:22<06:29,  3.09it/s, training_loss=0.354]\u001b[A\n",
            "Epoch 4:   5%|▌         | 69/1271 [00:22<06:29,  3.09it/s, training_loss=0.126]\u001b[A\n",
            "Epoch 4:   6%|▌         | 70/1271 [00:22<06:29,  3.09it/s, training_loss=0.126]\u001b[A\n",
            "Epoch 4:   6%|▌         | 70/1271 [00:23<06:29,  3.09it/s, training_loss=0.579]\u001b[A\n",
            "Epoch 4:   6%|▌         | 71/1271 [00:23<06:27,  3.10it/s, training_loss=0.579]\u001b[A\n",
            "Epoch 4:   6%|▌         | 71/1271 [00:23<06:27,  3.10it/s, training_loss=0.061]\u001b[A\n",
            "Epoch 4:   6%|▌         | 72/1271 [00:23<06:28,  3.08it/s, training_loss=0.061]\u001b[A\n",
            "Epoch 4:   6%|▌         | 72/1271 [00:23<06:28,  3.08it/s, training_loss=0.202]\u001b[A\n",
            "Epoch 4:   6%|▌         | 73/1271 [00:23<06:26,  3.10it/s, training_loss=0.202]\u001b[A\n",
            "Epoch 4:   6%|▌         | 73/1271 [00:24<06:26,  3.10it/s, training_loss=0.371]\u001b[A\n",
            "Epoch 4:   6%|▌         | 74/1271 [00:24<06:27,  3.09it/s, training_loss=0.371]\u001b[A\n",
            "Epoch 4:   6%|▌         | 74/1271 [00:24<06:27,  3.09it/s, training_loss=0.069]\u001b[A\n",
            "Epoch 4:   6%|▌         | 75/1271 [00:24<06:27,  3.09it/s, training_loss=0.069]\u001b[A\n",
            "Epoch 4:   6%|▌         | 75/1271 [00:24<06:27,  3.09it/s, training_loss=0.021]\u001b[A\n",
            "Epoch 4:   6%|▌         | 76/1271 [00:24<06:26,  3.09it/s, training_loss=0.021]\u001b[A\n",
            "Epoch 4:   6%|▌         | 76/1271 [00:24<06:26,  3.09it/s, training_loss=0.142]\u001b[A\n",
            "Epoch 4:   6%|▌         | 77/1271 [00:25<06:29,  3.07it/s, training_loss=0.142]\u001b[A\n",
            "Epoch 4:   6%|▌         | 77/1271 [00:25<06:29,  3.07it/s, training_loss=0.231]\u001b[A\n",
            "Epoch 4:   6%|▌         | 78/1271 [00:25<06:28,  3.07it/s, training_loss=0.231]\u001b[A\n",
            "Epoch 4:   6%|▌         | 78/1271 [00:25<06:28,  3.07it/s, training_loss=0.171]\u001b[A\n",
            "Epoch 4:   6%|▌         | 79/1271 [00:25<06:28,  3.06it/s, training_loss=0.171]\u001b[A\n",
            "Epoch 4:   6%|▌         | 79/1271 [00:25<06:28,  3.06it/s, training_loss=0.190]\u001b[A\n",
            "Epoch 4:   6%|▋         | 80/1271 [00:25<06:26,  3.08it/s, training_loss=0.190]\u001b[A\n",
            "Epoch 4:   6%|▋         | 80/1271 [00:26<06:26,  3.08it/s, training_loss=0.076]\u001b[A\n",
            "Epoch 4:   6%|▋         | 81/1271 [00:26<06:24,  3.10it/s, training_loss=0.076]\u001b[A\n",
            "Epoch 4:   6%|▋         | 81/1271 [00:26<06:24,  3.10it/s, training_loss=0.232]\u001b[A\n",
            "Epoch 4:   6%|▋         | 82/1271 [00:26<06:23,  3.10it/s, training_loss=0.232]\u001b[A\n",
            "Epoch 4:   6%|▋         | 82/1271 [00:26<06:23,  3.10it/s, training_loss=0.106]\u001b[A\n",
            "Epoch 4:   7%|▋         | 83/1271 [00:26<06:23,  3.10it/s, training_loss=0.106]\u001b[A\n",
            "Epoch 4:   7%|▋         | 83/1271 [00:27<06:23,  3.10it/s, training_loss=0.120]\u001b[A\n",
            "Epoch 4:   7%|▋         | 84/1271 [00:27<06:25,  3.08it/s, training_loss=0.120]\u001b[A\n",
            "Epoch 4:   7%|▋         | 84/1271 [00:27<06:25,  3.08it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 4:   7%|▋         | 85/1271 [00:27<06:22,  3.10it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 4:   7%|▋         | 85/1271 [00:27<06:22,  3.10it/s, training_loss=0.224]\u001b[A\n",
            "Epoch 4:   7%|▋         | 86/1271 [00:27<06:23,  3.09it/s, training_loss=0.224]\u001b[A\n",
            "Epoch 4:   7%|▋         | 86/1271 [00:28<06:23,  3.09it/s, training_loss=0.349]\u001b[A\n",
            "Epoch 4:   7%|▋         | 87/1271 [00:28<06:26,  3.06it/s, training_loss=0.349]\u001b[A\n",
            "Epoch 4:   7%|▋         | 87/1271 [00:28<06:26,  3.06it/s, training_loss=0.162]\u001b[A\n",
            "Epoch 4:   7%|▋         | 88/1271 [00:28<06:23,  3.09it/s, training_loss=0.162]\u001b[A\n",
            "Epoch 4:   7%|▋         | 88/1271 [00:28<06:23,  3.09it/s, training_loss=0.441]\u001b[A\n",
            "Epoch 4:   7%|▋         | 89/1271 [00:28<06:21,  3.10it/s, training_loss=0.441]\u001b[A\n",
            "Epoch 4:   7%|▋         | 89/1271 [00:29<06:21,  3.10it/s, training_loss=0.050]\u001b[A\n",
            "Epoch 4:   7%|▋         | 90/1271 [00:29<06:23,  3.08it/s, training_loss=0.050]\u001b[A\n",
            "Epoch 4:   7%|▋         | 90/1271 [00:29<06:23,  3.08it/s, training_loss=0.226]\u001b[A\n",
            "Epoch 4:   7%|▋         | 91/1271 [00:29<06:21,  3.09it/s, training_loss=0.226]\u001b[A\n",
            "Epoch 4:   7%|▋         | 91/1271 [00:29<06:21,  3.09it/s, training_loss=0.041]\u001b[A\n",
            "Epoch 4:   7%|▋         | 92/1271 [00:29<06:21,  3.09it/s, training_loss=0.041]\u001b[A\n",
            "Epoch 4:   7%|▋         | 92/1271 [00:30<06:21,  3.09it/s, training_loss=0.175]\u001b[A\n",
            "Epoch 4:   7%|▋         | 93/1271 [00:30<06:19,  3.10it/s, training_loss=0.175]\u001b[A\n",
            "Epoch 4:   7%|▋         | 93/1271 [00:30<06:19,  3.10it/s, training_loss=0.007]\u001b[A\n",
            "Epoch 4:   7%|▋         | 94/1271 [00:30<06:18,  3.11it/s, training_loss=0.007]\u001b[A\n",
            "Epoch 4:   7%|▋         | 94/1271 [00:30<06:18,  3.11it/s, training_loss=0.026]\u001b[A\n",
            "Epoch 4:   7%|▋         | 95/1271 [00:30<06:19,  3.10it/s, training_loss=0.026]\u001b[A\n",
            "Epoch 4:   7%|▋         | 95/1271 [00:31<06:19,  3.10it/s, training_loss=0.221]\u001b[A\n",
            "Epoch 4:   8%|▊         | 96/1271 [00:31<06:20,  3.09it/s, training_loss=0.221]\u001b[A\n",
            "Epoch 4:   8%|▊         | 96/1271 [00:31<06:20,  3.09it/s, training_loss=0.214]\u001b[A\n",
            "Epoch 4:   8%|▊         | 97/1271 [00:31<06:25,  3.04it/s, training_loss=0.214]\u001b[A\n",
            "Epoch 4:   8%|▊         | 97/1271 [00:31<06:25,  3.04it/s, training_loss=0.296]\u001b[A\n",
            "Epoch 4:   8%|▊         | 98/1271 [00:31<06:26,  3.03it/s, training_loss=0.296]\u001b[A\n",
            "Epoch 4:   8%|▊         | 98/1271 [00:32<06:26,  3.03it/s, training_loss=0.475]\u001b[A\n",
            "Epoch 4:   8%|▊         | 99/1271 [00:32<06:25,  3.04it/s, training_loss=0.475]\u001b[A\n",
            "Epoch 4:   8%|▊         | 99/1271 [00:32<06:25,  3.04it/s, training_loss=0.052]\u001b[A\n",
            "Epoch 4:   8%|▊         | 100/1271 [00:32<06:21,  3.07it/s, training_loss=0.052]\u001b[A\n",
            "Epoch 4:   8%|▊         | 100/1271 [00:32<06:21,  3.07it/s, training_loss=0.155]\u001b[A\n",
            "Epoch 4:   8%|▊         | 101/1271 [00:32<06:18,  3.09it/s, training_loss=0.155]\u001b[A\n",
            "Epoch 4:   8%|▊         | 101/1271 [00:33<06:18,  3.09it/s, training_loss=0.569]\u001b[A\n",
            "Epoch 4:   8%|▊         | 102/1271 [00:33<06:15,  3.11it/s, training_loss=0.569]\u001b[A\n",
            "Epoch 4:   8%|▊         | 102/1271 [00:33<06:15,  3.11it/s, training_loss=0.325]\u001b[A\n",
            "Epoch 4:   8%|▊         | 103/1271 [00:33<06:16,  3.10it/s, training_loss=0.325]\u001b[A\n",
            "Epoch 4:   8%|▊         | 103/1271 [00:33<06:16,  3.10it/s, training_loss=0.007]\u001b[A\n",
            "Epoch 4:   8%|▊         | 104/1271 [00:33<06:16,  3.10it/s, training_loss=0.007]\u001b[A\n",
            "Epoch 4:   8%|▊         | 104/1271 [00:34<06:16,  3.10it/s, training_loss=0.474]\u001b[A\n",
            "Epoch 4:   8%|▊         | 105/1271 [00:34<06:13,  3.12it/s, training_loss=0.474]\u001b[A\n",
            "Epoch 4:   8%|▊         | 105/1271 [00:34<06:13,  3.12it/s, training_loss=0.481]\u001b[A\n",
            "Epoch 4:   8%|▊         | 106/1271 [00:34<06:12,  3.13it/s, training_loss=0.481]\u001b[A\n",
            "Epoch 4:   8%|▊         | 106/1271 [00:34<06:12,  3.13it/s, training_loss=0.047]\u001b[A\n",
            "Epoch 4:   8%|▊         | 107/1271 [00:34<06:12,  3.13it/s, training_loss=0.047]\u001b[A\n",
            "Epoch 4:   8%|▊         | 107/1271 [00:35<06:12,  3.13it/s, training_loss=0.531]\u001b[A\n",
            "Epoch 4:   8%|▊         | 108/1271 [00:35<06:12,  3.13it/s, training_loss=0.531]\u001b[A\n",
            "Epoch 4:   8%|▊         | 108/1271 [00:35<06:12,  3.13it/s, training_loss=0.091]\u001b[A\n",
            "Epoch 4:   9%|▊         | 109/1271 [00:35<06:13,  3.11it/s, training_loss=0.091]\u001b[A\n",
            "Epoch 4:   9%|▊         | 109/1271 [00:35<06:13,  3.11it/s, training_loss=0.269]\u001b[A\n",
            "Epoch 4:   9%|▊         | 110/1271 [00:35<06:13,  3.11it/s, training_loss=0.269]\u001b[A\n",
            "Epoch 4:   9%|▊         | 110/1271 [00:35<06:13,  3.11it/s, training_loss=0.073]\u001b[A\n",
            "Epoch 4:   9%|▊         | 111/1271 [00:35<06:12,  3.11it/s, training_loss=0.073]\u001b[A\n",
            "Epoch 4:   9%|▊         | 111/1271 [00:36<06:12,  3.11it/s, training_loss=0.339]\u001b[A\n",
            "Epoch 4:   9%|▉         | 112/1271 [00:36<06:11,  3.12it/s, training_loss=0.339]\u001b[A\n",
            "Epoch 4:   9%|▉         | 112/1271 [00:36<06:11,  3.12it/s, training_loss=0.331]\u001b[A\n",
            "Epoch 4:   9%|▉         | 113/1271 [00:36<06:10,  3.13it/s, training_loss=0.331]\u001b[A\n",
            "Epoch 4:   9%|▉         | 113/1271 [00:36<06:10,  3.13it/s, training_loss=0.310]\u001b[A\n",
            "Epoch 4:   9%|▉         | 114/1271 [00:36<06:11,  3.12it/s, training_loss=0.310]\u001b[A\n",
            "Epoch 4:   9%|▉         | 114/1271 [00:37<06:11,  3.12it/s, training_loss=0.288]\u001b[A\n",
            "Epoch 4:   9%|▉         | 115/1271 [00:37<06:09,  3.13it/s, training_loss=0.288]\u001b[A\n",
            "Epoch 4:   9%|▉         | 115/1271 [00:37<06:09,  3.13it/s, training_loss=0.315]\u001b[A\n",
            "Epoch 4:   9%|▉         | 116/1271 [00:37<06:09,  3.12it/s, training_loss=0.315]\u001b[A\n",
            "Epoch 4:   9%|▉         | 116/1271 [00:37<06:09,  3.12it/s, training_loss=0.520]\u001b[A\n",
            "Epoch 4:   9%|▉         | 117/1271 [00:37<06:10,  3.11it/s, training_loss=0.520]\u001b[A\n",
            "Epoch 4:   9%|▉         | 117/1271 [00:38<06:10,  3.11it/s, training_loss=0.252]\u001b[A\n",
            "Epoch 4:   9%|▉         | 118/1271 [00:38<06:09,  3.12it/s, training_loss=0.252]\u001b[A\n",
            "Epoch 4:   9%|▉         | 118/1271 [00:38<06:09,  3.12it/s, training_loss=0.211]\u001b[A\n",
            "Epoch 4:   9%|▉         | 119/1271 [00:38<06:08,  3.13it/s, training_loss=0.211]\u001b[A\n",
            "Epoch 4:   9%|▉         | 119/1271 [00:38<06:08,  3.13it/s, training_loss=0.357]\u001b[A\n",
            "Epoch 4:   9%|▉         | 120/1271 [00:38<06:08,  3.13it/s, training_loss=0.357]\u001b[A\n",
            "Epoch 4:   9%|▉         | 120/1271 [00:39<06:08,  3.13it/s, training_loss=0.233]\u001b[A\n",
            "Epoch 4:  10%|▉         | 121/1271 [00:39<06:06,  3.14it/s, training_loss=0.233]\u001b[A\n",
            "Epoch 4:  10%|▉         | 121/1271 [00:39<06:06,  3.14it/s, training_loss=0.268]\u001b[A\n",
            "Epoch 4:  10%|▉         | 122/1271 [00:39<06:08,  3.12it/s, training_loss=0.268]\u001b[A\n",
            "Epoch 4:  10%|▉         | 122/1271 [00:39<06:08,  3.12it/s, training_loss=0.141]\u001b[A\n",
            "Epoch 4:  10%|▉         | 123/1271 [00:39<06:08,  3.11it/s, training_loss=0.141]\u001b[A\n",
            "Epoch 4:  10%|▉         | 123/1271 [00:40<06:08,  3.11it/s, training_loss=0.394]\u001b[A\n",
            "Epoch 4:  10%|▉         | 124/1271 [00:40<06:08,  3.12it/s, training_loss=0.394]\u001b[A\n",
            "Epoch 4:  10%|▉         | 124/1271 [00:40<06:08,  3.12it/s, training_loss=0.251]\u001b[A\n",
            "Epoch 4:  10%|▉         | 125/1271 [00:40<06:09,  3.10it/s, training_loss=0.251]\u001b[A\n",
            "Epoch 4:  10%|▉         | 125/1271 [00:40<06:09,  3.10it/s, training_loss=0.176]\u001b[A\n",
            "Epoch 4:  10%|▉         | 126/1271 [00:40<06:09,  3.10it/s, training_loss=0.176]\u001b[A\n",
            "Epoch 4:  10%|▉         | 126/1271 [00:41<06:09,  3.10it/s, training_loss=0.389]\u001b[A\n",
            "Epoch 4:  10%|▉         | 127/1271 [00:41<06:06,  3.12it/s, training_loss=0.389]\u001b[A\n",
            "Epoch 4:  10%|▉         | 127/1271 [00:41<06:06,  3.12it/s, training_loss=0.371]\u001b[A\n",
            "Epoch 4:  10%|█         | 128/1271 [00:41<06:06,  3.12it/s, training_loss=0.371]\u001b[A\n",
            "Epoch 4:  10%|█         | 128/1271 [00:41<06:06,  3.12it/s, training_loss=0.044]\u001b[A\n",
            "Epoch 4:  10%|█         | 129/1271 [00:41<06:05,  3.13it/s, training_loss=0.044]\u001b[A\n",
            "Epoch 4:  10%|█         | 129/1271 [00:42<06:05,  3.13it/s, training_loss=0.062]\u001b[A\n",
            "Epoch 4:  10%|█         | 130/1271 [00:42<06:05,  3.12it/s, training_loss=0.062]\u001b[A\n",
            "Epoch 4:  10%|█         | 130/1271 [00:42<06:05,  3.12it/s, training_loss=0.253]\u001b[A\n",
            "Epoch 4:  10%|█         | 131/1271 [00:42<06:05,  3.12it/s, training_loss=0.253]\u001b[A\n",
            "Epoch 4:  10%|█         | 131/1271 [00:42<06:05,  3.12it/s, training_loss=0.583]\u001b[A\n",
            "Epoch 4:  10%|█         | 132/1271 [00:42<06:03,  3.13it/s, training_loss=0.583]\u001b[A\n",
            "Epoch 4:  10%|█         | 132/1271 [00:43<06:03,  3.13it/s, training_loss=0.505]\u001b[A\n",
            "Epoch 4:  10%|█         | 133/1271 [00:43<06:04,  3.12it/s, training_loss=0.505]\u001b[A\n",
            "Epoch 4:  10%|█         | 133/1271 [00:43<06:04,  3.12it/s, training_loss=0.145]\u001b[A\n",
            "Epoch 4:  11%|█         | 134/1271 [00:43<06:05,  3.11it/s, training_loss=0.145]\u001b[A\n",
            "Epoch 4:  11%|█         | 134/1271 [00:43<06:05,  3.11it/s, training_loss=0.335]\u001b[A\n",
            "Epoch 4:  11%|█         | 135/1271 [00:43<06:05,  3.11it/s, training_loss=0.335]\u001b[A\n",
            "Epoch 4:  11%|█         | 135/1271 [00:44<06:05,  3.11it/s, training_loss=0.074]\u001b[A\n",
            "Epoch 4:  11%|█         | 136/1271 [00:44<06:09,  3.07it/s, training_loss=0.074]\u001b[A\n",
            "Epoch 4:  11%|█         | 136/1271 [00:44<06:09,  3.07it/s, training_loss=0.321]\u001b[A\n",
            "Epoch 4:  11%|█         | 137/1271 [00:44<06:05,  3.10it/s, training_loss=0.321]\u001b[A\n",
            "Epoch 4:  11%|█         | 137/1271 [00:44<06:05,  3.10it/s, training_loss=0.454]\u001b[A\n",
            "Epoch 4:  11%|█         | 138/1271 [00:44<06:03,  3.11it/s, training_loss=0.454]\u001b[A\n",
            "Epoch 4:  11%|█         | 138/1271 [00:44<06:03,  3.11it/s, training_loss=0.202]\u001b[A\n",
            "Epoch 4:  11%|█         | 139/1271 [00:44<06:03,  3.11it/s, training_loss=0.202]\u001b[A\n",
            "Epoch 4:  11%|█         | 139/1271 [00:45<06:03,  3.11it/s, training_loss=0.176]\u001b[A\n",
            "Epoch 4:  11%|█         | 140/1271 [00:45<06:05,  3.09it/s, training_loss=0.176]\u001b[A\n",
            "Epoch 4:  11%|█         | 140/1271 [00:45<06:05,  3.09it/s, training_loss=0.506]\u001b[A\n",
            "Epoch 4:  11%|█         | 141/1271 [00:45<06:05,  3.10it/s, training_loss=0.506]\u001b[A\n",
            "Epoch 4:  11%|█         | 141/1271 [00:45<06:05,  3.10it/s, training_loss=0.259]\u001b[A\n",
            "Epoch 4:  11%|█         | 142/1271 [00:45<06:04,  3.09it/s, training_loss=0.259]\u001b[A\n",
            "Epoch 4:  11%|█         | 142/1271 [00:46<06:04,  3.09it/s, training_loss=0.039]\u001b[A\n",
            "Epoch 4:  11%|█▏        | 143/1271 [00:46<06:05,  3.08it/s, training_loss=0.039]\u001b[A\n",
            "Epoch 4:  11%|█▏        | 143/1271 [00:46<06:05,  3.08it/s, training_loss=0.230]\u001b[A\n",
            "Epoch 4:  11%|█▏        | 144/1271 [00:46<06:02,  3.11it/s, training_loss=0.230]\u001b[A\n",
            "Epoch 4:  11%|█▏        | 144/1271 [00:46<06:02,  3.11it/s, training_loss=0.227]\u001b[A\n",
            "Epoch 4:  11%|█▏        | 145/1271 [00:46<06:02,  3.11it/s, training_loss=0.227]\u001b[A\n",
            "Epoch 4:  11%|█▏        | 145/1271 [00:47<06:02,  3.11it/s, training_loss=0.606]\u001b[A\n",
            "Epoch 4:  11%|█▏        | 146/1271 [00:47<06:01,  3.11it/s, training_loss=0.606]\u001b[A\n",
            "Epoch 4:  11%|█▏        | 146/1271 [00:47<06:01,  3.11it/s, training_loss=0.180]\u001b[A\n",
            "Epoch 4:  12%|█▏        | 147/1271 [00:47<06:01,  3.11it/s, training_loss=0.180]\u001b[A\n",
            "Epoch 4:  12%|█▏        | 147/1271 [00:47<06:01,  3.11it/s, training_loss=0.098]\u001b[A\n",
            "Epoch 4:  12%|█▏        | 148/1271 [00:47<06:00,  3.11it/s, training_loss=0.098]\u001b[A\n",
            "Epoch 4:  12%|█▏        | 148/1271 [00:48<06:00,  3.11it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 4:  12%|█▏        | 149/1271 [00:48<06:01,  3.10it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 4:  12%|█▏        | 149/1271 [00:48<06:01,  3.10it/s, training_loss=0.329]\u001b[A\n",
            "Epoch 4:  12%|█▏        | 150/1271 [00:48<06:00,  3.11it/s, training_loss=0.329]\u001b[A\n",
            "Epoch 4:  12%|█▏        | 150/1271 [00:48<06:00,  3.11it/s, training_loss=0.178]\u001b[A\n",
            "Epoch 4:  12%|█▏        | 151/1271 [00:48<06:01,  3.10it/s, training_loss=0.178]\u001b[A\n",
            "Epoch 4:  12%|█▏        | 151/1271 [00:49<06:01,  3.10it/s, training_loss=0.396]\u001b[A\n",
            "Epoch 4:  12%|█▏        | 152/1271 [00:49<06:02,  3.09it/s, training_loss=0.396]\u001b[A\n",
            "Epoch 4:  12%|█▏        | 152/1271 [00:49<06:02,  3.09it/s, training_loss=0.778]\u001b[A\n",
            "Epoch 4:  12%|█▏        | 153/1271 [00:49<06:00,  3.10it/s, training_loss=0.778]\u001b[A\n",
            "Epoch 4:  12%|█▏        | 153/1271 [00:49<06:00,  3.10it/s, training_loss=0.137]\u001b[A\n",
            "Epoch 4:  12%|█▏        | 154/1271 [00:49<05:58,  3.11it/s, training_loss=0.137]\u001b[A\n",
            "Epoch 4:  12%|█▏        | 154/1271 [00:50<05:58,  3.11it/s, training_loss=0.081]\u001b[A\n",
            "Epoch 4:  12%|█▏        | 155/1271 [00:50<05:57,  3.12it/s, training_loss=0.081]\u001b[A\n",
            "Epoch 4:  12%|█▏        | 155/1271 [00:50<05:57,  3.12it/s, training_loss=0.443]\u001b[A\n",
            "Epoch 4:  12%|█▏        | 156/1271 [00:50<05:57,  3.12it/s, training_loss=0.443]\u001b[A\n",
            "Epoch 4:  12%|█▏        | 156/1271 [00:50<05:57,  3.12it/s, training_loss=0.115]\u001b[A\n",
            "Epoch 4:  12%|█▏        | 157/1271 [00:50<05:57,  3.12it/s, training_loss=0.115]\u001b[A\n",
            "Epoch 4:  12%|█▏        | 157/1271 [00:51<05:57,  3.12it/s, training_loss=0.382]\u001b[A\n",
            "Epoch 4:  12%|█▏        | 158/1271 [00:51<05:59,  3.10it/s, training_loss=0.382]\u001b[A\n",
            "Epoch 4:  12%|█▏        | 158/1271 [00:51<05:59,  3.10it/s, training_loss=0.041]\u001b[A\n",
            "Epoch 4:  13%|█▎        | 159/1271 [00:51<05:57,  3.11it/s, training_loss=0.041]\u001b[A\n",
            "Epoch 4:  13%|█▎        | 159/1271 [00:51<05:57,  3.11it/s, training_loss=0.017]\u001b[A\n",
            "Epoch 4:  13%|█▎        | 160/1271 [00:51<05:57,  3.11it/s, training_loss=0.017]\u001b[A\n",
            "Epoch 4:  13%|█▎        | 160/1271 [00:52<05:57,  3.11it/s, training_loss=0.182]\u001b[A\n",
            "Epoch 4:  13%|█▎        | 161/1271 [00:52<05:58,  3.10it/s, training_loss=0.182]\u001b[A\n",
            "Epoch 4:  13%|█▎        | 161/1271 [00:52<05:58,  3.10it/s, training_loss=0.326]\u001b[A\n",
            "Epoch 4:  13%|█▎        | 162/1271 [00:52<05:59,  3.08it/s, training_loss=0.326]\u001b[A\n",
            "Epoch 4:  13%|█▎        | 162/1271 [00:52<05:59,  3.08it/s, training_loss=0.221]\u001b[A\n",
            "Epoch 4:  13%|█▎        | 163/1271 [00:52<06:02,  3.06it/s, training_loss=0.221]\u001b[A\n",
            "Epoch 4:  13%|█▎        | 163/1271 [00:53<06:02,  3.06it/s, training_loss=0.669]\u001b[A\n",
            "Epoch 4:  13%|█▎        | 164/1271 [00:53<05:59,  3.08it/s, training_loss=0.669]\u001b[A\n",
            "Epoch 4:  13%|█▎        | 164/1271 [00:53<05:59,  3.08it/s, training_loss=0.024]\u001b[A\n",
            "Epoch 4:  13%|█▎        | 165/1271 [00:53<05:58,  3.09it/s, training_loss=0.024]\u001b[A\n",
            "Epoch 4:  13%|█▎        | 165/1271 [00:53<05:58,  3.09it/s, training_loss=0.098]\u001b[A\n",
            "Epoch 4:  13%|█▎        | 166/1271 [00:53<05:59,  3.07it/s, training_loss=0.098]\u001b[A\n",
            "Epoch 4:  13%|█▎        | 166/1271 [00:54<05:59,  3.07it/s, training_loss=0.169]\u001b[A\n",
            "Epoch 4:  13%|█▎        | 167/1271 [00:54<05:57,  3.09it/s, training_loss=0.169]\u001b[A\n",
            "Epoch 4:  13%|█▎        | 167/1271 [00:54<05:57,  3.09it/s, training_loss=0.542]\u001b[A\n",
            "Epoch 4:  13%|█▎        | 168/1271 [00:54<05:57,  3.09it/s, training_loss=0.542]\u001b[A\n",
            "Epoch 4:  13%|█▎        | 168/1271 [00:54<05:57,  3.09it/s, training_loss=0.418]\u001b[A\n",
            "Epoch 4:  13%|█▎        | 169/1271 [00:54<05:54,  3.11it/s, training_loss=0.418]\u001b[A\n",
            "Epoch 4:  13%|█▎        | 169/1271 [00:54<05:54,  3.11it/s, training_loss=0.213]\u001b[A\n",
            "Epoch 4:  13%|█▎        | 170/1271 [00:54<05:52,  3.12it/s, training_loss=0.213]\u001b[A\n",
            "Epoch 4:  13%|█▎        | 170/1271 [00:55<05:52,  3.12it/s, training_loss=0.291]\u001b[A\n",
            "Epoch 4:  13%|█▎        | 171/1271 [00:55<05:50,  3.14it/s, training_loss=0.291]\u001b[A\n",
            "Epoch 4:  13%|█▎        | 171/1271 [00:55<05:50,  3.14it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 4:  14%|█▎        | 172/1271 [00:55<05:51,  3.13it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 4:  14%|█▎        | 172/1271 [00:55<05:51,  3.13it/s, training_loss=0.093]\u001b[A\n",
            "Epoch 4:  14%|█▎        | 173/1271 [00:55<05:52,  3.12it/s, training_loss=0.093]\u001b[A\n",
            "Epoch 4:  14%|█▎        | 173/1271 [00:56<05:52,  3.12it/s, training_loss=0.439]\u001b[A\n",
            "Epoch 4:  14%|█▎        | 174/1271 [00:56<05:51,  3.12it/s, training_loss=0.439]\u001b[A\n",
            "Epoch 4:  14%|█▎        | 174/1271 [00:56<05:51,  3.12it/s, training_loss=0.499]\u001b[A\n",
            "Epoch 4:  14%|█▍        | 175/1271 [00:56<05:50,  3.12it/s, training_loss=0.499]\u001b[A\n",
            "Epoch 4:  14%|█▍        | 175/1271 [00:56<05:50,  3.12it/s, training_loss=0.117]\u001b[A\n",
            "Epoch 4:  14%|█▍        | 176/1271 [00:56<05:49,  3.13it/s, training_loss=0.117]\u001b[A\n",
            "Epoch 4:  14%|█▍        | 176/1271 [00:57<05:49,  3.13it/s, training_loss=0.523]\u001b[A\n",
            "Epoch 4:  14%|█▍        | 177/1271 [00:57<05:47,  3.14it/s, training_loss=0.523]\u001b[A\n",
            "Epoch 4:  14%|█▍        | 177/1271 [00:57<05:47,  3.14it/s, training_loss=0.282]\u001b[A\n",
            "Epoch 4:  14%|█▍        | 178/1271 [00:57<05:50,  3.12it/s, training_loss=0.282]\u001b[A\n",
            "Epoch 4:  14%|█▍        | 178/1271 [00:57<05:50,  3.12it/s, training_loss=0.147]\u001b[A\n",
            "Epoch 4:  14%|█▍        | 179/1271 [00:57<05:48,  3.13it/s, training_loss=0.147]\u001b[A\n",
            "Epoch 4:  14%|█▍        | 179/1271 [00:58<05:48,  3.13it/s, training_loss=0.236]\u001b[A\n",
            "Epoch 4:  14%|█▍        | 180/1271 [00:58<05:47,  3.14it/s, training_loss=0.236]\u001b[A\n",
            "Epoch 4:  14%|█▍        | 180/1271 [00:58<05:47,  3.14it/s, training_loss=0.204]\u001b[A\n",
            "Epoch 4:  14%|█▍        | 181/1271 [00:58<05:49,  3.12it/s, training_loss=0.204]\u001b[A\n",
            "Epoch 4:  14%|█▍        | 181/1271 [00:58<05:49,  3.12it/s, training_loss=0.442]\u001b[A\n",
            "Epoch 4:  14%|█▍        | 182/1271 [00:58<05:51,  3.10it/s, training_loss=0.442]\u001b[A\n",
            "Epoch 4:  14%|█▍        | 182/1271 [00:59<05:51,  3.10it/s, training_loss=0.368]\u001b[A\n",
            "Epoch 4:  14%|█▍        | 183/1271 [00:59<05:48,  3.12it/s, training_loss=0.368]\u001b[A\n",
            "Epoch 4:  14%|█▍        | 183/1271 [00:59<05:48,  3.12it/s, training_loss=0.025]\u001b[A\n",
            "Epoch 4:  14%|█▍        | 184/1271 [00:59<05:47,  3.13it/s, training_loss=0.025]\u001b[A\n",
            "Epoch 4:  14%|█▍        | 184/1271 [00:59<05:47,  3.13it/s, training_loss=0.443]\u001b[A\n",
            "Epoch 4:  15%|█▍        | 185/1271 [00:59<05:47,  3.12it/s, training_loss=0.443]\u001b[A\n",
            "Epoch 4:  15%|█▍        | 185/1271 [01:00<05:47,  3.12it/s, training_loss=0.017]\u001b[A\n",
            "Epoch 4:  15%|█▍        | 186/1271 [01:00<05:46,  3.13it/s, training_loss=0.017]\u001b[A\n",
            "Epoch 4:  15%|█▍        | 186/1271 [01:00<05:46,  3.13it/s, training_loss=0.192]\u001b[A\n",
            "Epoch 4:  15%|█▍        | 187/1271 [01:00<05:46,  3.13it/s, training_loss=0.192]\u001b[A\n",
            "Epoch 4:  15%|█▍        | 187/1271 [01:00<05:46,  3.13it/s, training_loss=0.133]\u001b[A\n",
            "Epoch 4:  15%|█▍        | 188/1271 [01:00<05:45,  3.13it/s, training_loss=0.133]\u001b[A\n",
            "Epoch 4:  15%|█▍        | 188/1271 [01:01<05:45,  3.13it/s, training_loss=0.347]\u001b[A\n",
            "Epoch 4:  15%|█▍        | 189/1271 [01:01<05:45,  3.13it/s, training_loss=0.347]\u001b[A\n",
            "Epoch 4:  15%|█▍        | 189/1271 [01:01<05:45,  3.13it/s, training_loss=0.204]\u001b[A\n",
            "Epoch 4:  15%|█▍        | 190/1271 [01:01<05:44,  3.14it/s, training_loss=0.204]\u001b[A\n",
            "Epoch 4:  15%|█▍        | 190/1271 [01:01<05:44,  3.14it/s, training_loss=0.135]\u001b[A\n",
            "Epoch 4:  15%|█▌        | 191/1271 [01:01<05:44,  3.13it/s, training_loss=0.135]\u001b[A\n",
            "Epoch 4:  15%|█▌        | 191/1271 [01:01<05:44,  3.13it/s, training_loss=0.588]\u001b[A\n",
            "Epoch 4:  15%|█▌        | 192/1271 [01:02<05:44,  3.13it/s, training_loss=0.588]\u001b[A\n",
            "Epoch 4:  15%|█▌        | 192/1271 [01:02<05:44,  3.13it/s, training_loss=0.163]\u001b[A\n",
            "Epoch 4:  15%|█▌        | 193/1271 [01:02<05:47,  3.10it/s, training_loss=0.163]\u001b[A\n",
            "Epoch 4:  15%|█▌        | 193/1271 [01:02<05:47,  3.10it/s, training_loss=0.149]\u001b[A\n",
            "Epoch 4:  15%|█▌        | 194/1271 [01:02<05:46,  3.11it/s, training_loss=0.149]\u001b[A\n",
            "Epoch 4:  15%|█▌        | 194/1271 [01:02<05:46,  3.11it/s, training_loss=0.052]\u001b[A\n",
            "Epoch 4:  15%|█▌        | 195/1271 [01:02<05:45,  3.12it/s, training_loss=0.052]\u001b[A\n",
            "Epoch 4:  15%|█▌        | 195/1271 [01:03<05:45,  3.12it/s, training_loss=0.183]\u001b[A\n",
            "Epoch 4:  15%|█▌        | 196/1271 [01:03<05:42,  3.14it/s, training_loss=0.183]\u001b[A\n",
            "Epoch 4:  15%|█▌        | 196/1271 [01:03<05:42,  3.14it/s, training_loss=0.681]\u001b[A\n",
            "Epoch 4:  15%|█▌        | 197/1271 [01:03<05:40,  3.15it/s, training_loss=0.681]\u001b[A\n",
            "Epoch 4:  15%|█▌        | 197/1271 [01:03<05:40,  3.15it/s, training_loss=0.058]\u001b[A\n",
            "Epoch 4:  16%|█▌        | 198/1271 [01:03<05:40,  3.15it/s, training_loss=0.058]\u001b[A\n",
            "Epoch 4:  16%|█▌        | 198/1271 [01:04<05:40,  3.15it/s, training_loss=0.558]\u001b[A\n",
            "Epoch 4:  16%|█▌        | 199/1271 [01:04<05:41,  3.14it/s, training_loss=0.558]\u001b[A\n",
            "Epoch 4:  16%|█▌        | 199/1271 [01:04<05:41,  3.14it/s, training_loss=0.158]\u001b[A\n",
            "Epoch 4:  16%|█▌        | 200/1271 [01:04<05:40,  3.14it/s, training_loss=0.158]\u001b[A\n",
            "Epoch 4:  16%|█▌        | 200/1271 [01:04<05:40,  3.14it/s, training_loss=0.220]\u001b[A\n",
            "Epoch 4:  16%|█▌        | 201/1271 [01:04<05:39,  3.15it/s, training_loss=0.220]\u001b[A\n",
            "Epoch 4:  16%|█▌        | 201/1271 [01:05<05:39,  3.15it/s, training_loss=0.266]\u001b[A\n",
            "Epoch 4:  16%|█▌        | 202/1271 [01:05<05:40,  3.14it/s, training_loss=0.266]\u001b[A\n",
            "Epoch 4:  16%|█▌        | 202/1271 [01:05<05:40,  3.14it/s, training_loss=0.265]\u001b[A\n",
            "Epoch 4:  16%|█▌        | 203/1271 [01:05<05:41,  3.13it/s, training_loss=0.265]\u001b[A\n",
            "Epoch 4:  16%|█▌        | 203/1271 [01:05<05:41,  3.13it/s, training_loss=0.038]\u001b[A\n",
            "Epoch 4:  16%|█▌        | 204/1271 [01:05<05:40,  3.14it/s, training_loss=0.038]\u001b[A\n",
            "Epoch 4:  16%|█▌        | 204/1271 [01:06<05:40,  3.14it/s, training_loss=0.184]\u001b[A\n",
            "Epoch 4:  16%|█▌        | 205/1271 [01:06<05:41,  3.12it/s, training_loss=0.184]\u001b[A\n",
            "Epoch 4:  16%|█▌        | 205/1271 [01:06<05:41,  3.12it/s, training_loss=0.180]\u001b[A\n",
            "Epoch 4:  16%|█▌        | 206/1271 [01:06<05:41,  3.12it/s, training_loss=0.180]\u001b[A\n",
            "Epoch 4:  16%|█▌        | 206/1271 [01:06<05:41,  3.12it/s, training_loss=0.324]\u001b[A\n",
            "Epoch 4:  16%|█▋        | 207/1271 [01:06<05:39,  3.13it/s, training_loss=0.324]\u001b[A\n",
            "Epoch 4:  16%|█▋        | 207/1271 [01:07<05:39,  3.13it/s, training_loss=0.451]\u001b[A\n",
            "Epoch 4:  16%|█▋        | 208/1271 [01:07<05:39,  3.13it/s, training_loss=0.451]\u001b[A\n",
            "Epoch 4:  16%|█▋        | 208/1271 [01:07<05:39,  3.13it/s, training_loss=0.208]\u001b[A\n",
            "Epoch 4:  16%|█▋        | 209/1271 [01:07<05:39,  3.12it/s, training_loss=0.208]\u001b[A\n",
            "Epoch 4:  16%|█▋        | 209/1271 [01:07<05:39,  3.12it/s, training_loss=0.483]\u001b[A\n",
            "Epoch 4:  17%|█▋        | 210/1271 [01:07<05:38,  3.13it/s, training_loss=0.483]\u001b[A\n",
            "Epoch 4:  17%|█▋        | 210/1271 [01:08<05:38,  3.13it/s, training_loss=0.369]\u001b[A\n",
            "Epoch 4:  17%|█▋        | 211/1271 [01:08<05:38,  3.13it/s, training_loss=0.369]\u001b[A\n",
            "Epoch 4:  17%|█▋        | 211/1271 [01:08<05:38,  3.13it/s, training_loss=0.696]\u001b[A\n",
            "Epoch 4:  17%|█▋        | 212/1271 [01:08<05:39,  3.11it/s, training_loss=0.696]\u001b[A\n",
            "Epoch 4:  17%|█▋        | 212/1271 [01:08<05:39,  3.11it/s, training_loss=0.204]\u001b[A\n",
            "Epoch 4:  17%|█▋        | 213/1271 [01:08<05:40,  3.11it/s, training_loss=0.204]\u001b[A\n",
            "Epoch 4:  17%|█▋        | 213/1271 [01:09<05:40,  3.11it/s, training_loss=0.073]\u001b[A\n",
            "Epoch 4:  17%|█▋        | 214/1271 [01:09<05:40,  3.11it/s, training_loss=0.073]\u001b[A\n",
            "Epoch 4:  17%|█▋        | 214/1271 [01:09<05:40,  3.11it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 4:  17%|█▋        | 215/1271 [01:09<05:38,  3.12it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 4:  17%|█▋        | 215/1271 [01:09<05:38,  3.12it/s, training_loss=0.094]\u001b[A\n",
            "Epoch 4:  17%|█▋        | 216/1271 [01:09<05:38,  3.12it/s, training_loss=0.094]\u001b[A\n",
            "Epoch 4:  17%|█▋        | 216/1271 [01:09<05:38,  3.12it/s, training_loss=0.028]\u001b[A\n",
            "Epoch 4:  17%|█▋        | 217/1271 [01:09<05:37,  3.12it/s, training_loss=0.028]\u001b[A\n",
            "Epoch 4:  17%|█▋        | 217/1271 [01:10<05:37,  3.12it/s, training_loss=0.209]\u001b[A\n",
            "Epoch 4:  17%|█▋        | 218/1271 [01:10<05:35,  3.14it/s, training_loss=0.209]\u001b[A\n",
            "Epoch 4:  17%|█▋        | 218/1271 [01:10<05:35,  3.14it/s, training_loss=0.260]\u001b[A\n",
            "Epoch 4:  17%|█▋        | 219/1271 [01:10<05:36,  3.13it/s, training_loss=0.260]\u001b[A\n",
            "Epoch 4:  17%|█▋        | 219/1271 [01:10<05:36,  3.13it/s, training_loss=0.019]\u001b[A\n",
            "Epoch 4:  17%|█▋        | 220/1271 [01:10<05:36,  3.12it/s, training_loss=0.019]\u001b[A\n",
            "Epoch 4:  17%|█▋        | 220/1271 [01:11<05:36,  3.12it/s, training_loss=0.020]\u001b[A\n",
            "Epoch 4:  17%|█▋        | 221/1271 [01:11<05:35,  3.13it/s, training_loss=0.020]\u001b[A\n",
            "Epoch 4:  17%|█▋        | 221/1271 [01:11<05:35,  3.13it/s, training_loss=0.134]\u001b[A\n",
            "Epoch 4:  17%|█▋        | 222/1271 [01:11<05:35,  3.12it/s, training_loss=0.134]\u001b[A\n",
            "Epoch 4:  17%|█▋        | 222/1271 [01:11<05:35,  3.12it/s, training_loss=0.453]\u001b[A\n",
            "Epoch 4:  18%|█▊        | 223/1271 [01:11<05:35,  3.12it/s, training_loss=0.453]\u001b[A\n",
            "Epoch 4:  18%|█▊        | 223/1271 [01:12<05:35,  3.12it/s, training_loss=0.087]\u001b[A\n",
            "Epoch 4:  18%|█▊        | 224/1271 [01:12<05:35,  3.12it/s, training_loss=0.087]\u001b[A\n",
            "Epoch 4:  18%|█▊        | 224/1271 [01:12<05:35,  3.12it/s, training_loss=0.309]\u001b[A\n",
            "Epoch 4:  18%|█▊        | 225/1271 [01:12<05:34,  3.13it/s, training_loss=0.309]\u001b[A\n",
            "Epoch 4:  18%|█▊        | 225/1271 [01:12<05:34,  3.13it/s, training_loss=0.107]\u001b[A\n",
            "Epoch 4:  18%|█▊        | 226/1271 [01:12<05:33,  3.13it/s, training_loss=0.107]\u001b[A\n",
            "Epoch 4:  18%|█▊        | 226/1271 [01:13<05:33,  3.13it/s, training_loss=0.104]\u001b[A\n",
            "Epoch 4:  18%|█▊        | 227/1271 [01:13<05:33,  3.13it/s, training_loss=0.104]\u001b[A\n",
            "Epoch 4:  18%|█▊        | 227/1271 [01:13<05:33,  3.13it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 4:  18%|█▊        | 228/1271 [01:13<05:35,  3.11it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 4:  18%|█▊        | 228/1271 [01:13<05:35,  3.11it/s, training_loss=0.170]\u001b[A\n",
            "Epoch 4:  18%|█▊        | 229/1271 [01:13<05:35,  3.11it/s, training_loss=0.170]\u001b[A\n",
            "Epoch 4:  18%|█▊        | 229/1271 [01:14<05:35,  3.11it/s, training_loss=0.127]\u001b[A\n",
            "Epoch 4:  18%|█▊        | 230/1271 [01:14<05:34,  3.11it/s, training_loss=0.127]\u001b[A\n",
            "Epoch 4:  18%|█▊        | 230/1271 [01:14<05:34,  3.11it/s, training_loss=0.428]\u001b[A\n",
            "Epoch 4:  18%|█▊        | 231/1271 [01:14<05:33,  3.12it/s, training_loss=0.428]\u001b[A\n",
            "Epoch 4:  18%|█▊        | 231/1271 [01:14<05:33,  3.12it/s, training_loss=0.023]\u001b[A\n",
            "Epoch 4:  18%|█▊        | 232/1271 [01:14<05:31,  3.13it/s, training_loss=0.023]\u001b[A\n",
            "Epoch 4:  18%|█▊        | 232/1271 [01:15<05:31,  3.13it/s, training_loss=0.144]\u001b[A\n",
            "Epoch 4:  18%|█▊        | 233/1271 [01:15<05:31,  3.13it/s, training_loss=0.144]\u001b[A\n",
            "Epoch 4:  18%|█▊        | 233/1271 [01:15<05:31,  3.13it/s, training_loss=0.294]\u001b[A\n",
            "Epoch 4:  18%|█▊        | 234/1271 [01:15<05:30,  3.14it/s, training_loss=0.294]\u001b[A\n",
            "Epoch 4:  18%|█▊        | 234/1271 [01:15<05:30,  3.14it/s, training_loss=0.436]\u001b[A\n",
            "Epoch 4:  18%|█▊        | 235/1271 [01:15<05:29,  3.14it/s, training_loss=0.436]\u001b[A\n",
            "Epoch 4:  18%|█▊        | 235/1271 [01:16<05:29,  3.14it/s, training_loss=0.206]\u001b[A\n",
            "Epoch 4:  19%|█▊        | 236/1271 [01:16<05:30,  3.13it/s, training_loss=0.206]\u001b[A\n",
            "Epoch 4:  19%|█▊        | 236/1271 [01:16<05:30,  3.13it/s, training_loss=0.202]\u001b[A\n",
            "Epoch 4:  19%|█▊        | 237/1271 [01:16<05:32,  3.11it/s, training_loss=0.202]\u001b[A\n",
            "Epoch 4:  19%|█▊        | 237/1271 [01:16<05:32,  3.11it/s, training_loss=0.206]\u001b[A\n",
            "Epoch 4:  19%|█▊        | 238/1271 [01:16<05:32,  3.10it/s, training_loss=0.206]\u001b[A\n",
            "Epoch 4:  19%|█▊        | 238/1271 [01:17<05:32,  3.10it/s, training_loss=0.900]\u001b[A\n",
            "Epoch 4:  19%|█▉        | 239/1271 [01:17<05:31,  3.12it/s, training_loss=0.900]\u001b[A\n",
            "Epoch 4:  19%|█▉        | 239/1271 [01:17<05:31,  3.12it/s, training_loss=0.379]\u001b[A\n",
            "Epoch 4:  19%|█▉        | 240/1271 [01:17<05:30,  3.12it/s, training_loss=0.379]\u001b[A\n",
            "Epoch 4:  19%|█▉        | 240/1271 [01:17<05:30,  3.12it/s, training_loss=0.144]\u001b[A\n",
            "Epoch 4:  19%|█▉        | 241/1271 [01:17<05:31,  3.10it/s, training_loss=0.144]\u001b[A\n",
            "Epoch 4:  19%|█▉        | 241/1271 [01:18<05:31,  3.10it/s, training_loss=0.233]\u001b[A\n",
            "Epoch 4:  19%|█▉        | 242/1271 [01:18<05:31,  3.10it/s, training_loss=0.233]\u001b[A\n",
            "Epoch 4:  19%|█▉        | 242/1271 [01:18<05:31,  3.10it/s, training_loss=0.708]\u001b[A\n",
            "Epoch 4:  19%|█▉        | 243/1271 [01:18<05:29,  3.12it/s, training_loss=0.708]\u001b[A\n",
            "Epoch 4:  19%|█▉        | 243/1271 [01:18<05:29,  3.12it/s, training_loss=0.441]\u001b[A\n",
            "Epoch 4:  19%|█▉        | 244/1271 [01:18<05:33,  3.08it/s, training_loss=0.441]\u001b[A\n",
            "Epoch 4:  19%|█▉        | 244/1271 [01:18<05:33,  3.08it/s, training_loss=0.645]\u001b[A\n",
            "Epoch 4:  19%|█▉        | 245/1271 [01:18<05:31,  3.09it/s, training_loss=0.645]\u001b[A\n",
            "Epoch 4:  19%|█▉        | 245/1271 [01:19<05:31,  3.09it/s, training_loss=0.403]\u001b[A\n",
            "Epoch 4:  19%|█▉        | 246/1271 [01:19<05:29,  3.11it/s, training_loss=0.403]\u001b[A\n",
            "Epoch 4:  19%|█▉        | 246/1271 [01:19<05:29,  3.11it/s, training_loss=0.522]\u001b[A\n",
            "Epoch 4:  19%|█▉        | 247/1271 [01:19<05:28,  3.11it/s, training_loss=0.522]\u001b[A\n",
            "Epoch 4:  19%|█▉        | 247/1271 [01:19<05:28,  3.11it/s, training_loss=0.071]\u001b[A\n",
            "Epoch 4:  20%|█▉        | 248/1271 [01:19<05:30,  3.09it/s, training_loss=0.071]\u001b[A\n",
            "Epoch 4:  20%|█▉        | 248/1271 [01:20<05:30,  3.09it/s, training_loss=0.013]\u001b[A\n",
            "Epoch 4:  20%|█▉        | 249/1271 [01:20<05:28,  3.11it/s, training_loss=0.013]\u001b[A\n",
            "Epoch 4:  20%|█▉        | 249/1271 [01:20<05:28,  3.11it/s, training_loss=0.426]\u001b[A\n",
            "Epoch 4:  20%|█▉        | 250/1271 [01:20<05:28,  3.11it/s, training_loss=0.426]\u001b[A\n",
            "Epoch 4:  20%|█▉        | 250/1271 [01:20<05:28,  3.11it/s, training_loss=0.199]\u001b[A\n",
            "Epoch 4:  20%|█▉        | 251/1271 [01:20<05:29,  3.10it/s, training_loss=0.199]\u001b[A\n",
            "Epoch 4:  20%|█▉        | 251/1271 [01:21<05:29,  3.10it/s, training_loss=0.144]\u001b[A\n",
            "Epoch 4:  20%|█▉        | 252/1271 [01:21<05:31,  3.08it/s, training_loss=0.144]\u001b[A\n",
            "Epoch 4:  20%|█▉        | 252/1271 [01:21<05:31,  3.08it/s, training_loss=0.348]\u001b[A\n",
            "Epoch 4:  20%|█▉        | 253/1271 [01:21<05:30,  3.08it/s, training_loss=0.348]\u001b[A\n",
            "Epoch 4:  20%|█▉        | 253/1271 [01:21<05:30,  3.08it/s, training_loss=0.354]\u001b[A\n",
            "Epoch 4:  20%|█▉        | 254/1271 [01:21<05:28,  3.09it/s, training_loss=0.354]\u001b[A\n",
            "Epoch 4:  20%|█▉        | 254/1271 [01:22<05:28,  3.09it/s, training_loss=0.495]\u001b[A\n",
            "Epoch 4:  20%|██        | 255/1271 [01:22<05:28,  3.09it/s, training_loss=0.495]\u001b[A\n",
            "Epoch 4:  20%|██        | 255/1271 [01:22<05:28,  3.09it/s, training_loss=0.354]\u001b[A\n",
            "Epoch 4:  20%|██        | 256/1271 [01:22<05:28,  3.09it/s, training_loss=0.354]\u001b[A\n",
            "Epoch 4:  20%|██        | 256/1271 [01:22<05:28,  3.09it/s, training_loss=0.220]\u001b[A\n",
            "Epoch 4:  20%|██        | 257/1271 [01:22<05:27,  3.10it/s, training_loss=0.220]\u001b[A\n",
            "Epoch 4:  20%|██        | 257/1271 [01:23<05:27,  3.10it/s, training_loss=0.075]\u001b[A\n",
            "Epoch 4:  20%|██        | 258/1271 [01:23<05:25,  3.12it/s, training_loss=0.075]\u001b[A\n",
            "Epoch 4:  20%|██        | 258/1271 [01:23<05:25,  3.12it/s, training_loss=0.250]\u001b[A\n",
            "Epoch 4:  20%|██        | 259/1271 [01:23<05:23,  3.13it/s, training_loss=0.250]\u001b[A\n",
            "Epoch 4:  20%|██        | 259/1271 [01:23<05:23,  3.13it/s, training_loss=0.260]\u001b[A\n",
            "Epoch 4:  20%|██        | 260/1271 [01:23<05:23,  3.13it/s, training_loss=0.260]\u001b[A\n",
            "Epoch 4:  20%|██        | 260/1271 [01:24<05:23,  3.13it/s, training_loss=0.271]\u001b[A\n",
            "Epoch 4:  21%|██        | 261/1271 [01:24<05:26,  3.10it/s, training_loss=0.271]\u001b[A\n",
            "Epoch 4:  21%|██        | 261/1271 [01:24<05:26,  3.10it/s, training_loss=0.010]\u001b[A\n",
            "Epoch 4:  21%|██        | 262/1271 [01:24<05:23,  3.11it/s, training_loss=0.010]\u001b[A\n",
            "Epoch 4:  21%|██        | 262/1271 [01:24<05:23,  3.11it/s, training_loss=0.256]\u001b[A\n",
            "Epoch 4:  21%|██        | 263/1271 [01:24<05:22,  3.12it/s, training_loss=0.256]\u001b[A\n",
            "Epoch 4:  21%|██        | 263/1271 [01:25<05:22,  3.12it/s, training_loss=0.439]\u001b[A\n",
            "Epoch 4:  21%|██        | 264/1271 [01:25<05:26,  3.09it/s, training_loss=0.439]\u001b[A\n",
            "Epoch 4:  21%|██        | 264/1271 [01:25<05:26,  3.09it/s, training_loss=0.377]\u001b[A\n",
            "Epoch 4:  21%|██        | 265/1271 [01:25<05:25,  3.09it/s, training_loss=0.377]\u001b[A\n",
            "Epoch 4:  21%|██        | 265/1271 [01:25<05:25,  3.09it/s, training_loss=0.070]\u001b[A\n",
            "Epoch 4:  21%|██        | 266/1271 [01:25<05:25,  3.09it/s, training_loss=0.070]\u001b[A\n",
            "Epoch 4:  21%|██        | 266/1271 [01:26<05:25,  3.09it/s, training_loss=0.011]\u001b[A\n",
            "Epoch 4:  21%|██        | 267/1271 [01:26<05:26,  3.08it/s, training_loss=0.011]\u001b[A\n",
            "Epoch 4:  21%|██        | 267/1271 [01:26<05:26,  3.08it/s, training_loss=0.079]\u001b[A\n",
            "Epoch 4:  21%|██        | 268/1271 [01:26<05:24,  3.09it/s, training_loss=0.079]\u001b[A\n",
            "Epoch 4:  21%|██        | 268/1271 [01:26<05:24,  3.09it/s, training_loss=0.177]\u001b[A\n",
            "Epoch 4:  21%|██        | 269/1271 [01:26<05:23,  3.10it/s, training_loss=0.177]\u001b[A\n",
            "Epoch 4:  21%|██        | 269/1271 [01:27<05:23,  3.10it/s, training_loss=0.652]\u001b[A\n",
            "Epoch 4:  21%|██        | 270/1271 [01:27<05:20,  3.12it/s, training_loss=0.652]\u001b[A\n",
            "Epoch 4:  21%|██        | 270/1271 [01:27<05:20,  3.12it/s, training_loss=0.399]\u001b[A\n",
            "Epoch 4:  21%|██▏       | 271/1271 [01:27<05:20,  3.12it/s, training_loss=0.399]\u001b[A\n",
            "Epoch 4:  21%|██▏       | 271/1271 [01:27<05:20,  3.12it/s, training_loss=0.475]\u001b[A\n",
            "Epoch 4:  21%|██▏       | 272/1271 [01:27<05:20,  3.12it/s, training_loss=0.475]\u001b[A\n",
            "Epoch 4:  21%|██▏       | 272/1271 [01:27<05:20,  3.12it/s, training_loss=0.010]\u001b[A\n",
            "Epoch 4:  21%|██▏       | 273/1271 [01:27<05:19,  3.13it/s, training_loss=0.010]\u001b[A\n",
            "Epoch 4:  21%|██▏       | 273/1271 [01:28<05:19,  3.13it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 4:  22%|██▏       | 274/1271 [01:28<05:19,  3.12it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 4:  22%|██▏       | 274/1271 [01:28<05:19,  3.12it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 4:  22%|██▏       | 275/1271 [01:28<05:19,  3.12it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 4:  22%|██▏       | 275/1271 [01:28<05:19,  3.12it/s, training_loss=0.186]\u001b[A\n",
            "Epoch 4:  22%|██▏       | 276/1271 [01:28<05:19,  3.12it/s, training_loss=0.186]\u001b[A\n",
            "Epoch 4:  22%|██▏       | 276/1271 [01:29<05:19,  3.12it/s, training_loss=0.192]\u001b[A\n",
            "Epoch 4:  22%|██▏       | 277/1271 [01:29<05:19,  3.11it/s, training_loss=0.192]\u001b[A\n",
            "Epoch 4:  22%|██▏       | 277/1271 [01:29<05:19,  3.11it/s, training_loss=0.056]\u001b[A\n",
            "Epoch 4:  22%|██▏       | 278/1271 [01:29<05:20,  3.10it/s, training_loss=0.056]\u001b[A\n",
            "Epoch 4:  22%|██▏       | 278/1271 [01:29<05:20,  3.10it/s, training_loss=0.202]\u001b[A\n",
            "Epoch 4:  22%|██▏       | 279/1271 [01:29<05:21,  3.08it/s, training_loss=0.202]\u001b[A\n",
            "Epoch 4:  22%|██▏       | 279/1271 [01:30<05:21,  3.08it/s, training_loss=0.445]\u001b[A\n",
            "Epoch 4:  22%|██▏       | 280/1271 [01:30<05:18,  3.11it/s, training_loss=0.445]\u001b[A\n",
            "Epoch 4:  22%|██▏       | 280/1271 [01:30<05:18,  3.11it/s, training_loss=0.074]\u001b[A\n",
            "Epoch 4:  22%|██▏       | 281/1271 [01:30<05:17,  3.12it/s, training_loss=0.074]\u001b[A\n",
            "Epoch 4:  22%|██▏       | 281/1271 [01:30<05:17,  3.12it/s, training_loss=0.039]\u001b[A\n",
            "Epoch 4:  22%|██▏       | 282/1271 [01:30<05:18,  3.10it/s, training_loss=0.039]\u001b[A\n",
            "Epoch 4:  22%|██▏       | 282/1271 [01:31<05:18,  3.10it/s, training_loss=0.302]\u001b[A\n",
            "Epoch 4:  22%|██▏       | 283/1271 [01:31<05:18,  3.10it/s, training_loss=0.302]\u001b[A\n",
            "Epoch 4:  22%|██▏       | 283/1271 [01:31<05:18,  3.10it/s, training_loss=0.087]\u001b[A\n",
            "Epoch 4:  22%|██▏       | 284/1271 [01:31<05:18,  3.10it/s, training_loss=0.087]\u001b[A\n",
            "Epoch 4:  22%|██▏       | 284/1271 [01:31<05:18,  3.10it/s, training_loss=0.377]\u001b[A\n",
            "Epoch 4:  22%|██▏       | 285/1271 [01:31<05:18,  3.10it/s, training_loss=0.377]\u001b[A\n",
            "Epoch 4:  22%|██▏       | 285/1271 [01:32<05:18,  3.10it/s, training_loss=0.432]\u001b[A\n",
            "Epoch 4:  23%|██▎       | 286/1271 [01:32<05:16,  3.11it/s, training_loss=0.432]\u001b[A\n",
            "Epoch 4:  23%|██▎       | 286/1271 [01:32<05:16,  3.11it/s, training_loss=0.063]\u001b[A\n",
            "Epoch 4:  23%|██▎       | 287/1271 [01:32<05:18,  3.09it/s, training_loss=0.063]\u001b[A\n",
            "Epoch 4:  23%|██▎       | 287/1271 [01:32<05:18,  3.09it/s, training_loss=0.280]\u001b[A\n",
            "Epoch 4:  23%|██▎       | 288/1271 [01:32<05:18,  3.09it/s, training_loss=0.280]\u001b[A\n",
            "Epoch 4:  23%|██▎       | 288/1271 [01:33<05:18,  3.09it/s, training_loss=0.417]\u001b[A\n",
            "Epoch 4:  23%|██▎       | 289/1271 [01:33<05:18,  3.08it/s, training_loss=0.417]\u001b[A\n",
            "Epoch 4:  23%|██▎       | 289/1271 [01:33<05:18,  3.08it/s, training_loss=0.180]\u001b[A\n",
            "Epoch 4:  23%|██▎       | 290/1271 [01:33<05:18,  3.08it/s, training_loss=0.180]\u001b[A\n",
            "Epoch 4:  23%|██▎       | 290/1271 [01:33<05:18,  3.08it/s, training_loss=0.199]\u001b[A\n",
            "Epoch 4:  23%|██▎       | 291/1271 [01:33<05:18,  3.07it/s, training_loss=0.199]\u001b[A\n",
            "Epoch 4:  23%|██▎       | 291/1271 [01:34<05:18,  3.07it/s, training_loss=0.895]\u001b[A\n",
            "Epoch 4:  23%|██▎       | 292/1271 [01:34<05:18,  3.08it/s, training_loss=0.895]\u001b[A\n",
            "Epoch 4:  23%|██▎       | 292/1271 [01:34<05:18,  3.08it/s, training_loss=0.745]\u001b[A\n",
            "Epoch 4:  23%|██▎       | 293/1271 [01:34<05:17,  3.08it/s, training_loss=0.745]\u001b[A\n",
            "Epoch 4:  23%|██▎       | 293/1271 [01:34<05:17,  3.08it/s, training_loss=0.336]\u001b[A\n",
            "Epoch 4:  23%|██▎       | 294/1271 [01:34<05:16,  3.08it/s, training_loss=0.336]\u001b[A\n",
            "Epoch 4:  23%|██▎       | 294/1271 [01:35<05:16,  3.08it/s, training_loss=0.150]\u001b[A\n",
            "Epoch 4:  23%|██▎       | 295/1271 [01:35<05:16,  3.08it/s, training_loss=0.150]\u001b[A\n",
            "Epoch 4:  23%|██▎       | 295/1271 [01:35<05:16,  3.08it/s, training_loss=0.249]\u001b[A\n",
            "Epoch 4:  23%|██▎       | 296/1271 [01:35<05:16,  3.08it/s, training_loss=0.249]\u001b[A\n",
            "Epoch 4:  23%|██▎       | 296/1271 [01:35<05:16,  3.08it/s, training_loss=0.226]\u001b[A\n",
            "Epoch 4:  23%|██▎       | 297/1271 [01:35<05:16,  3.07it/s, training_loss=0.226]\u001b[A\n",
            "Epoch 4:  23%|██▎       | 297/1271 [01:36<05:16,  3.07it/s, training_loss=0.315]\u001b[A\n",
            "Epoch 4:  23%|██▎       | 298/1271 [01:36<05:14,  3.09it/s, training_loss=0.315]\u001b[A\n",
            "Epoch 4:  23%|██▎       | 298/1271 [01:36<05:14,  3.09it/s, training_loss=0.154]\u001b[A\n",
            "Epoch 4:  24%|██▎       | 299/1271 [01:36<05:14,  3.09it/s, training_loss=0.154]\u001b[A\n",
            "Epoch 4:  24%|██▎       | 299/1271 [01:36<05:14,  3.09it/s, training_loss=0.013]\u001b[A\n",
            "Epoch 4:  24%|██▎       | 300/1271 [01:36<05:14,  3.09it/s, training_loss=0.013]\u001b[A\n",
            "Epoch 4:  24%|██▎       | 300/1271 [01:37<05:14,  3.09it/s, training_loss=0.647]\u001b[A\n",
            "Epoch 4:  24%|██▎       | 301/1271 [01:37<05:13,  3.10it/s, training_loss=0.647]\u001b[A\n",
            "Epoch 4:  24%|██▎       | 301/1271 [01:37<05:13,  3.10it/s, training_loss=0.200]\u001b[A\n",
            "Epoch 4:  24%|██▍       | 302/1271 [01:37<05:15,  3.07it/s, training_loss=0.200]\u001b[A\n",
            "Epoch 4:  24%|██▍       | 302/1271 [01:37<05:15,  3.07it/s, training_loss=0.126]\u001b[A\n",
            "Epoch 4:  24%|██▍       | 303/1271 [01:37<05:12,  3.10it/s, training_loss=0.126]\u001b[A\n",
            "Epoch 4:  24%|██▍       | 303/1271 [01:38<05:12,  3.10it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 4:  24%|██▍       | 304/1271 [01:38<05:11,  3.10it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 4:  24%|██▍       | 304/1271 [01:38<05:11,  3.10it/s, training_loss=0.840]\u001b[A\n",
            "Epoch 4:  24%|██▍       | 305/1271 [01:38<05:12,  3.09it/s, training_loss=0.840]\u001b[A\n",
            "Epoch 4:  24%|██▍       | 305/1271 [01:38<05:12,  3.09it/s, training_loss=0.474]\u001b[A\n",
            "Epoch 4:  24%|██▍       | 306/1271 [01:38<05:13,  3.08it/s, training_loss=0.474]\u001b[A\n",
            "Epoch 4:  24%|██▍       | 306/1271 [01:38<05:13,  3.08it/s, training_loss=0.690]\u001b[A\n",
            "Epoch 4:  24%|██▍       | 307/1271 [01:38<05:13,  3.07it/s, training_loss=0.690]\u001b[A\n",
            "Epoch 4:  24%|██▍       | 307/1271 [01:39<05:13,  3.07it/s, training_loss=0.161]\u001b[A\n",
            "Epoch 4:  24%|██▍       | 308/1271 [01:39<05:12,  3.08it/s, training_loss=0.161]\u001b[A\n",
            "Epoch 4:  24%|██▍       | 308/1271 [01:39<05:12,  3.08it/s, training_loss=0.122]\u001b[A\n",
            "Epoch 4:  24%|██▍       | 309/1271 [01:39<05:11,  3.09it/s, training_loss=0.122]\u001b[A\n",
            "Epoch 4:  24%|██▍       | 309/1271 [01:39<05:11,  3.09it/s, training_loss=0.251]\u001b[A\n",
            "Epoch 4:  24%|██▍       | 310/1271 [01:39<05:13,  3.07it/s, training_loss=0.251]\u001b[A\n",
            "Epoch 4:  24%|██▍       | 310/1271 [01:40<05:13,  3.07it/s, training_loss=0.328]\u001b[A\n",
            "Epoch 4:  24%|██▍       | 311/1271 [01:40<05:11,  3.08it/s, training_loss=0.328]\u001b[A\n",
            "Epoch 4:  24%|██▍       | 311/1271 [01:40<05:11,  3.08it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 4:  25%|██▍       | 312/1271 [01:40<05:09,  3.10it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 4:  25%|██▍       | 312/1271 [01:40<05:09,  3.10it/s, training_loss=0.148]\u001b[A\n",
            "Epoch 4:  25%|██▍       | 313/1271 [01:40<05:08,  3.11it/s, training_loss=0.148]\u001b[A\n",
            "Epoch 4:  25%|██▍       | 313/1271 [01:41<05:08,  3.11it/s, training_loss=0.182]\u001b[A\n",
            "Epoch 4:  25%|██▍       | 314/1271 [01:41<05:06,  3.12it/s, training_loss=0.182]\u001b[A\n",
            "Epoch 4:  25%|██▍       | 314/1271 [01:41<05:06,  3.12it/s, training_loss=0.350]\u001b[A\n",
            "Epoch 4:  25%|██▍       | 315/1271 [01:41<05:05,  3.13it/s, training_loss=0.350]\u001b[A\n",
            "Epoch 4:  25%|██▍       | 315/1271 [01:41<05:05,  3.13it/s, training_loss=0.365]\u001b[A\n",
            "Epoch 4:  25%|██▍       | 316/1271 [01:41<05:05,  3.13it/s, training_loss=0.365]\u001b[A\n",
            "Epoch 4:  25%|██▍       | 316/1271 [01:42<05:05,  3.13it/s, training_loss=0.284]\u001b[A\n",
            "Epoch 4:  25%|██▍       | 317/1271 [01:42<05:07,  3.11it/s, training_loss=0.284]\u001b[A\n",
            "Epoch 4:  25%|██▍       | 317/1271 [01:42<05:07,  3.11it/s, training_loss=0.671]\u001b[A\n",
            "Epoch 4:  25%|██▌       | 318/1271 [01:42<05:06,  3.11it/s, training_loss=0.671]\u001b[A\n",
            "Epoch 4:  25%|██▌       | 318/1271 [01:42<05:06,  3.11it/s, training_loss=0.414]\u001b[A\n",
            "Epoch 4:  25%|██▌       | 319/1271 [01:42<05:07,  3.10it/s, training_loss=0.414]\u001b[A\n",
            "Epoch 4:  25%|██▌       | 319/1271 [01:43<05:07,  3.10it/s, training_loss=0.167]\u001b[A\n",
            "Epoch 4:  25%|██▌       | 320/1271 [01:43<05:05,  3.11it/s, training_loss=0.167]\u001b[A\n",
            "Epoch 4:  25%|██▌       | 320/1271 [01:43<05:05,  3.11it/s, training_loss=0.580]\u001b[A\n",
            "Epoch 4:  25%|██▌       | 321/1271 [01:43<05:05,  3.11it/s, training_loss=0.580]\u001b[A\n",
            "Epoch 4:  25%|██▌       | 321/1271 [01:43<05:05,  3.11it/s, training_loss=0.266]\u001b[A\n",
            "Epoch 4:  25%|██▌       | 322/1271 [01:43<05:05,  3.10it/s, training_loss=0.266]\u001b[A\n",
            "Epoch 4:  25%|██▌       | 322/1271 [01:44<05:05,  3.10it/s, training_loss=0.321]\u001b[A\n",
            "Epoch 4:  25%|██▌       | 323/1271 [01:44<05:05,  3.10it/s, training_loss=0.321]\u001b[A\n",
            "Epoch 4:  25%|██▌       | 323/1271 [01:44<05:05,  3.10it/s, training_loss=0.791]\u001b[A\n",
            "Epoch 4:  25%|██▌       | 324/1271 [01:44<05:04,  3.11it/s, training_loss=0.791]\u001b[A\n",
            "Epoch 4:  25%|██▌       | 324/1271 [01:44<05:04,  3.11it/s, training_loss=0.027]\u001b[A\n",
            "Epoch 4:  26%|██▌       | 325/1271 [01:44<05:03,  3.12it/s, training_loss=0.027]\u001b[A\n",
            "Epoch 4:  26%|██▌       | 325/1271 [01:45<05:03,  3.12it/s, training_loss=0.276]\u001b[A\n",
            "Epoch 4:  26%|██▌       | 326/1271 [01:45<05:03,  3.12it/s, training_loss=0.276]\u001b[A\n",
            "Epoch 4:  26%|██▌       | 326/1271 [01:45<05:03,  3.12it/s, training_loss=0.132]\u001b[A\n",
            "Epoch 4:  26%|██▌       | 327/1271 [01:45<05:04,  3.10it/s, training_loss=0.132]\u001b[A\n",
            "Epoch 4:  26%|██▌       | 327/1271 [01:45<05:04,  3.10it/s, training_loss=0.359]\u001b[A\n",
            "Epoch 4:  26%|██▌       | 328/1271 [01:45<05:03,  3.11it/s, training_loss=0.359]\u001b[A\n",
            "Epoch 4:  26%|██▌       | 328/1271 [01:46<05:03,  3.11it/s, training_loss=0.126]\u001b[A\n",
            "Epoch 4:  26%|██▌       | 329/1271 [01:46<05:02,  3.11it/s, training_loss=0.126]\u001b[A\n",
            "Epoch 4:  26%|██▌       | 329/1271 [01:46<05:02,  3.11it/s, training_loss=0.153]\u001b[A\n",
            "Epoch 4:  26%|██▌       | 330/1271 [01:46<05:00,  3.13it/s, training_loss=0.153]\u001b[A\n",
            "Epoch 4:  26%|██▌       | 330/1271 [01:46<05:00,  3.13it/s, training_loss=0.416]\u001b[A\n",
            "Epoch 4:  26%|██▌       | 331/1271 [01:46<05:00,  3.13it/s, training_loss=0.416]\u001b[A\n",
            "Epoch 4:  26%|██▌       | 331/1271 [01:47<05:00,  3.13it/s, training_loss=0.387]\u001b[A\n",
            "Epoch 4:  26%|██▌       | 332/1271 [01:47<05:00,  3.12it/s, training_loss=0.387]\u001b[A\n",
            "Epoch 4:  26%|██▌       | 332/1271 [01:47<05:00,  3.12it/s, training_loss=0.200]\u001b[A\n",
            "Epoch 4:  26%|██▌       | 333/1271 [01:47<05:00,  3.12it/s, training_loss=0.200]\u001b[A\n",
            "Epoch 4:  26%|██▌       | 333/1271 [01:47<05:00,  3.12it/s, training_loss=0.246]\u001b[A\n",
            "Epoch 4:  26%|██▋       | 334/1271 [01:47<04:59,  3.12it/s, training_loss=0.246]\u001b[A\n",
            "Epoch 4:  26%|██▋       | 334/1271 [01:47<04:59,  3.12it/s, training_loss=0.007]\u001b[A\n",
            "Epoch 4:  26%|██▋       | 335/1271 [01:47<04:59,  3.12it/s, training_loss=0.007]\u001b[A\n",
            "Epoch 4:  26%|██▋       | 335/1271 [01:48<04:59,  3.12it/s, training_loss=0.156]\u001b[A\n",
            "Epoch 4:  26%|██▋       | 336/1271 [01:48<05:00,  3.11it/s, training_loss=0.156]\u001b[A\n",
            "Epoch 4:  26%|██▋       | 336/1271 [01:48<05:00,  3.11it/s, training_loss=0.329]\u001b[A\n",
            "Epoch 4:  27%|██▋       | 337/1271 [01:48<05:00,  3.11it/s, training_loss=0.329]\u001b[A\n",
            "Epoch 4:  27%|██▋       | 337/1271 [01:48<05:00,  3.11it/s, training_loss=0.464]\u001b[A\n",
            "Epoch 4:  27%|██▋       | 338/1271 [01:48<05:00,  3.10it/s, training_loss=0.464]\u001b[A\n",
            "Epoch 4:  27%|██▋       | 338/1271 [01:49<05:00,  3.10it/s, training_loss=0.236]\u001b[A\n",
            "Epoch 4:  27%|██▋       | 339/1271 [01:49<05:01,  3.09it/s, training_loss=0.236]\u001b[A\n",
            "Epoch 4:  27%|██▋       | 339/1271 [01:49<05:01,  3.09it/s, training_loss=0.117]\u001b[A\n",
            "Epoch 4:  27%|██▋       | 340/1271 [01:49<05:00,  3.09it/s, training_loss=0.117]\u001b[A\n",
            "Epoch 4:  27%|██▋       | 340/1271 [01:49<05:00,  3.09it/s, training_loss=0.076]\u001b[A\n",
            "Epoch 4:  27%|██▋       | 341/1271 [01:49<04:59,  3.10it/s, training_loss=0.076]\u001b[A\n",
            "Epoch 4:  27%|██▋       | 341/1271 [01:50<04:59,  3.10it/s, training_loss=0.063]\u001b[A\n",
            "Epoch 4:  27%|██▋       | 342/1271 [01:50<04:58,  3.12it/s, training_loss=0.063]\u001b[A\n",
            "Epoch 4:  27%|██▋       | 342/1271 [01:50<04:58,  3.12it/s, training_loss=0.341]\u001b[A\n",
            "Epoch 4:  27%|██▋       | 343/1271 [01:50<04:58,  3.11it/s, training_loss=0.341]\u001b[A\n",
            "Epoch 4:  27%|██▋       | 343/1271 [01:50<04:58,  3.11it/s, training_loss=0.357]\u001b[A\n",
            "Epoch 4:  27%|██▋       | 344/1271 [01:50<04:58,  3.11it/s, training_loss=0.357]\u001b[A\n",
            "Epoch 4:  27%|██▋       | 344/1271 [01:51<04:58,  3.11it/s, training_loss=0.424]\u001b[A\n",
            "Epoch 4:  27%|██▋       | 345/1271 [01:51<04:58,  3.11it/s, training_loss=0.424]\u001b[A\n",
            "Epoch 4:  27%|██▋       | 345/1271 [01:51<04:58,  3.11it/s, training_loss=0.257]\u001b[A\n",
            "Epoch 4:  27%|██▋       | 346/1271 [01:51<04:59,  3.09it/s, training_loss=0.257]\u001b[A\n",
            "Epoch 4:  27%|██▋       | 346/1271 [01:51<04:59,  3.09it/s, training_loss=0.192]\u001b[A\n",
            "Epoch 4:  27%|██▋       | 347/1271 [01:51<04:59,  3.09it/s, training_loss=0.192]\u001b[A\n",
            "Epoch 4:  27%|██▋       | 347/1271 [01:52<04:59,  3.09it/s, training_loss=0.225]\u001b[A\n",
            "Epoch 4:  27%|██▋       | 348/1271 [01:52<04:59,  3.08it/s, training_loss=0.225]\u001b[A\n",
            "Epoch 4:  27%|██▋       | 348/1271 [01:52<04:59,  3.08it/s, training_loss=0.094]\u001b[A\n",
            "Epoch 4:  27%|██▋       | 349/1271 [01:52<04:57,  3.10it/s, training_loss=0.094]\u001b[A\n",
            "Epoch 4:  27%|██▋       | 349/1271 [01:52<04:57,  3.10it/s, training_loss=0.212]\u001b[A\n",
            "Epoch 4:  28%|██▊       | 350/1271 [01:52<04:57,  3.10it/s, training_loss=0.212]\u001b[A\n",
            "Epoch 4:  28%|██▊       | 350/1271 [01:53<04:57,  3.10it/s, training_loss=0.356]\u001b[A\n",
            "Epoch 4:  28%|██▊       | 351/1271 [01:53<04:55,  3.11it/s, training_loss=0.356]\u001b[A\n",
            "Epoch 4:  28%|██▊       | 351/1271 [01:53<04:55,  3.11it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 4:  28%|██▊       | 352/1271 [01:53<04:54,  3.12it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 4:  28%|██▊       | 352/1271 [01:53<04:54,  3.12it/s, training_loss=0.310]\u001b[A\n",
            "Epoch 4:  28%|██▊       | 353/1271 [01:53<04:56,  3.10it/s, training_loss=0.310]\u001b[A\n",
            "Epoch 4:  28%|██▊       | 353/1271 [01:54<04:56,  3.10it/s, training_loss=0.088]\u001b[A\n",
            "Epoch 4:  28%|██▊       | 354/1271 [01:54<04:55,  3.10it/s, training_loss=0.088]\u001b[A\n",
            "Epoch 4:  28%|██▊       | 354/1271 [01:54<04:55,  3.10it/s, training_loss=0.269]\u001b[A\n",
            "Epoch 4:  28%|██▊       | 355/1271 [01:54<04:55,  3.10it/s, training_loss=0.269]\u001b[A\n",
            "Epoch 4:  28%|██▊       | 355/1271 [01:54<04:55,  3.10it/s, training_loss=0.495]\u001b[A\n",
            "Epoch 4:  28%|██▊       | 356/1271 [01:54<04:53,  3.11it/s, training_loss=0.495]\u001b[A\n",
            "Epoch 4:  28%|██▊       | 356/1271 [01:55<04:53,  3.11it/s, training_loss=0.520]\u001b[A\n",
            "Epoch 4:  28%|██▊       | 357/1271 [01:55<04:54,  3.10it/s, training_loss=0.520]\u001b[A\n",
            "Epoch 4:  28%|██▊       | 357/1271 [01:55<04:54,  3.10it/s, training_loss=0.019]\u001b[A\n",
            "Epoch 4:  28%|██▊       | 358/1271 [01:55<04:56,  3.08it/s, training_loss=0.019]\u001b[A\n",
            "Epoch 4:  28%|██▊       | 358/1271 [01:55<04:56,  3.08it/s, training_loss=0.136]\u001b[A\n",
            "Epoch 4:  28%|██▊       | 359/1271 [01:55<04:56,  3.07it/s, training_loss=0.136]\u001b[A\n",
            "Epoch 4:  28%|██▊       | 359/1271 [01:56<04:56,  3.07it/s, training_loss=0.156]\u001b[A\n",
            "Epoch 4:  28%|██▊       | 360/1271 [01:56<04:55,  3.08it/s, training_loss=0.156]\u001b[A\n",
            "Epoch 4:  28%|██▊       | 360/1271 [01:56<04:55,  3.08it/s, training_loss=0.182]\u001b[A\n",
            "Epoch 4:  28%|██▊       | 361/1271 [01:56<04:56,  3.07it/s, training_loss=0.182]\u001b[A\n",
            "Epoch 4:  28%|██▊       | 361/1271 [01:56<04:56,  3.07it/s, training_loss=0.292]\u001b[A\n",
            "Epoch 4:  28%|██▊       | 362/1271 [01:56<04:53,  3.10it/s, training_loss=0.292]\u001b[A\n",
            "Epoch 4:  28%|██▊       | 362/1271 [01:57<04:53,  3.10it/s, training_loss=0.157]\u001b[A\n",
            "Epoch 4:  29%|██▊       | 363/1271 [01:57<04:51,  3.11it/s, training_loss=0.157]\u001b[A\n",
            "Epoch 4:  29%|██▊       | 363/1271 [01:57<04:51,  3.11it/s, training_loss=0.028]\u001b[A\n",
            "Epoch 4:  29%|██▊       | 364/1271 [01:57<04:53,  3.09it/s, training_loss=0.028]\u001b[A\n",
            "Epoch 4:  29%|██▊       | 364/1271 [01:57<04:53,  3.09it/s, training_loss=0.567]\u001b[A\n",
            "Epoch 4:  29%|██▊       | 365/1271 [01:57<04:52,  3.09it/s, training_loss=0.567]\u001b[A\n",
            "Epoch 4:  29%|██▊       | 365/1271 [01:57<04:52,  3.09it/s, training_loss=0.141]\u001b[A\n",
            "Epoch 4:  29%|██▉       | 366/1271 [01:58<04:51,  3.10it/s, training_loss=0.141]\u001b[A\n",
            "Epoch 4:  29%|██▉       | 366/1271 [01:58<04:51,  3.10it/s, training_loss=0.129]\u001b[A\n",
            "Epoch 4:  29%|██▉       | 367/1271 [01:58<04:51,  3.10it/s, training_loss=0.129]\u001b[A\n",
            "Epoch 4:  29%|██▉       | 367/1271 [01:58<04:51,  3.10it/s, training_loss=0.131]\u001b[A\n",
            "Epoch 4:  29%|██▉       | 368/1271 [01:58<04:49,  3.12it/s, training_loss=0.131]\u001b[A\n",
            "Epoch 4:  29%|██▉       | 368/1271 [01:58<04:49,  3.12it/s, training_loss=0.247]\u001b[A\n",
            "Epoch 4:  29%|██▉       | 369/1271 [01:58<04:51,  3.09it/s, training_loss=0.247]\u001b[A\n",
            "Epoch 4:  29%|██▉       | 369/1271 [01:59<04:51,  3.09it/s, training_loss=0.010]\u001b[A\n",
            "Epoch 4:  29%|██▉       | 370/1271 [01:59<04:49,  3.11it/s, training_loss=0.010]\u001b[A\n",
            "Epoch 4:  29%|██▉       | 370/1271 [01:59<04:49,  3.11it/s, training_loss=0.375]\u001b[A\n",
            "Epoch 4:  29%|██▉       | 371/1271 [01:59<04:49,  3.11it/s, training_loss=0.375]\u001b[A\n",
            "Epoch 4:  29%|██▉       | 371/1271 [01:59<04:49,  3.11it/s, training_loss=0.138]\u001b[A\n",
            "Epoch 4:  29%|██▉       | 372/1271 [01:59<04:48,  3.12it/s, training_loss=0.138]\u001b[A\n",
            "Epoch 4:  29%|██▉       | 372/1271 [02:00<04:48,  3.12it/s, training_loss=0.198]\u001b[A\n",
            "Epoch 4:  29%|██▉       | 373/1271 [02:00<04:50,  3.10it/s, training_loss=0.198]\u001b[A\n",
            "Epoch 4:  29%|██▉       | 373/1271 [02:00<04:50,  3.10it/s, training_loss=0.338]\u001b[A\n",
            "Epoch 4:  29%|██▉       | 374/1271 [02:00<04:50,  3.09it/s, training_loss=0.338]\u001b[A\n",
            "Epoch 4:  29%|██▉       | 374/1271 [02:00<04:50,  3.09it/s, training_loss=0.538]\u001b[A\n",
            "Epoch 4:  30%|██▉       | 375/1271 [02:00<04:49,  3.10it/s, training_loss=0.538]\u001b[A\n",
            "Epoch 4:  30%|██▉       | 375/1271 [02:01<04:49,  3.10it/s, training_loss=0.501]\u001b[A\n",
            "Epoch 4:  30%|██▉       | 376/1271 [02:01<04:49,  3.09it/s, training_loss=0.501]\u001b[A\n",
            "Epoch 4:  30%|██▉       | 376/1271 [02:01<04:49,  3.09it/s, training_loss=0.092]\u001b[A\n",
            "Epoch 4:  30%|██▉       | 377/1271 [02:01<04:49,  3.09it/s, training_loss=0.092]\u001b[A\n",
            "Epoch 4:  30%|██▉       | 377/1271 [02:01<04:49,  3.09it/s, training_loss=0.262]\u001b[A\n",
            "Epoch 4:  30%|██▉       | 378/1271 [02:01<04:49,  3.08it/s, training_loss=0.262]\u001b[A\n",
            "Epoch 4:  30%|██▉       | 378/1271 [02:02<04:49,  3.08it/s, training_loss=0.317]\u001b[A\n",
            "Epoch 4:  30%|██▉       | 379/1271 [02:02<04:48,  3.09it/s, training_loss=0.317]\u001b[A\n",
            "Epoch 4:  30%|██▉       | 379/1271 [02:02<04:48,  3.09it/s, training_loss=0.470]\u001b[A\n",
            "Epoch 4:  30%|██▉       | 380/1271 [02:02<04:47,  3.10it/s, training_loss=0.470]\u001b[A\n",
            "Epoch 4:  30%|██▉       | 380/1271 [02:02<04:47,  3.10it/s, training_loss=0.123]\u001b[A\n",
            "Epoch 4:  30%|██▉       | 381/1271 [02:02<04:48,  3.08it/s, training_loss=0.123]\u001b[A\n",
            "Epoch 4:  30%|██▉       | 381/1271 [02:03<04:48,  3.08it/s, training_loss=0.167]\u001b[A\n",
            "Epoch 4:  30%|███       | 382/1271 [02:03<04:47,  3.09it/s, training_loss=0.167]\u001b[A\n",
            "Epoch 4:  30%|███       | 382/1271 [02:03<04:47,  3.09it/s, training_loss=0.266]\u001b[A\n",
            "Epoch 4:  30%|███       | 383/1271 [02:03<04:46,  3.10it/s, training_loss=0.266]\u001b[A\n",
            "Epoch 4:  30%|███       | 383/1271 [02:03<04:46,  3.10it/s, training_loss=0.632]\u001b[A\n",
            "Epoch 4:  30%|███       | 384/1271 [02:03<04:45,  3.11it/s, training_loss=0.632]\u001b[A\n",
            "Epoch 4:  30%|███       | 384/1271 [02:04<04:45,  3.11it/s, training_loss=0.203]\u001b[A\n",
            "Epoch 4:  30%|███       | 385/1271 [02:04<04:44,  3.12it/s, training_loss=0.203]\u001b[A\n",
            "Epoch 4:  30%|███       | 385/1271 [02:04<04:44,  3.12it/s, training_loss=0.318]\u001b[A\n",
            "Epoch 4:  30%|███       | 386/1271 [02:04<04:43,  3.12it/s, training_loss=0.318]\u001b[A\n",
            "Epoch 4:  30%|███       | 386/1271 [02:04<04:43,  3.12it/s, training_loss=0.188]\u001b[A\n",
            "Epoch 4:  30%|███       | 387/1271 [02:04<04:43,  3.12it/s, training_loss=0.188]\u001b[A\n",
            "Epoch 4:  30%|███       | 387/1271 [02:05<04:43,  3.12it/s, training_loss=0.185]\u001b[A\n",
            "Epoch 4:  31%|███       | 388/1271 [02:05<04:44,  3.10it/s, training_loss=0.185]\u001b[A\n",
            "Epoch 4:  31%|███       | 388/1271 [02:05<04:44,  3.10it/s, training_loss=0.484]\u001b[A\n",
            "Epoch 4:  31%|███       | 389/1271 [02:05<04:44,  3.11it/s, training_loss=0.484]\u001b[A\n",
            "Epoch 4:  31%|███       | 389/1271 [02:05<04:44,  3.11it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 4:  31%|███       | 390/1271 [02:05<04:43,  3.11it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 4:  31%|███       | 390/1271 [02:06<04:43,  3.11it/s, training_loss=0.288]\u001b[A\n",
            "Epoch 4:  31%|███       | 391/1271 [02:06<04:43,  3.10it/s, training_loss=0.288]\u001b[A\n",
            "Epoch 4:  31%|███       | 391/1271 [02:06<04:43,  3.10it/s, training_loss=0.186]\u001b[A\n",
            "Epoch 4:  31%|███       | 392/1271 [02:06<04:45,  3.08it/s, training_loss=0.186]\u001b[A\n",
            "Epoch 4:  31%|███       | 392/1271 [02:06<04:45,  3.08it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 4:  31%|███       | 393/1271 [02:06<04:44,  3.09it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 4:  31%|███       | 393/1271 [02:07<04:44,  3.09it/s, training_loss=0.259]\u001b[A\n",
            "Epoch 4:  31%|███       | 394/1271 [02:07<04:42,  3.10it/s, training_loss=0.259]\u001b[A\n",
            "Epoch 4:  31%|███       | 394/1271 [02:07<04:42,  3.10it/s, training_loss=0.176]\u001b[A\n",
            "Epoch 4:  31%|███       | 395/1271 [02:07<04:41,  3.12it/s, training_loss=0.176]\u001b[A\n",
            "Epoch 4:  31%|███       | 395/1271 [02:07<04:41,  3.12it/s, training_loss=0.183]\u001b[A\n",
            "Epoch 4:  31%|███       | 396/1271 [02:07<04:42,  3.10it/s, training_loss=0.183]\u001b[A\n",
            "Epoch 4:  31%|███       | 396/1271 [02:07<04:42,  3.10it/s, training_loss=0.536]\u001b[A\n",
            "Epoch 4:  31%|███       | 397/1271 [02:08<04:42,  3.10it/s, training_loss=0.536]\u001b[A\n",
            "Epoch 4:  31%|███       | 397/1271 [02:08<04:42,  3.10it/s, training_loss=0.285]\u001b[A\n",
            "Epoch 4:  31%|███▏      | 398/1271 [02:08<04:41,  3.11it/s, training_loss=0.285]\u001b[A\n",
            "Epoch 4:  31%|███▏      | 398/1271 [02:08<04:41,  3.11it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 4:  31%|███▏      | 399/1271 [02:08<04:41,  3.10it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 4:  31%|███▏      | 399/1271 [02:08<04:41,  3.10it/s, training_loss=0.216]\u001b[A\n",
            "Epoch 4:  31%|███▏      | 400/1271 [02:08<04:40,  3.10it/s, training_loss=0.216]\u001b[A\n",
            "Epoch 4:  31%|███▏      | 400/1271 [02:09<04:40,  3.10it/s, training_loss=0.082]\u001b[A\n",
            "Epoch 4:  32%|███▏      | 401/1271 [02:09<04:39,  3.11it/s, training_loss=0.082]\u001b[A\n",
            "Epoch 4:  32%|███▏      | 401/1271 [02:09<04:39,  3.11it/s, training_loss=0.208]\u001b[A\n",
            "Epoch 4:  32%|███▏      | 402/1271 [02:09<04:39,  3.11it/s, training_loss=0.208]\u001b[A\n",
            "Epoch 4:  32%|███▏      | 402/1271 [02:09<04:39,  3.11it/s, training_loss=0.266]\u001b[A\n",
            "Epoch 4:  32%|███▏      | 403/1271 [02:09<04:38,  3.12it/s, training_loss=0.266]\u001b[A\n",
            "Epoch 4:  32%|███▏      | 403/1271 [02:10<04:38,  3.12it/s, training_loss=0.166]\u001b[A\n",
            "Epoch 4:  32%|███▏      | 404/1271 [02:10<04:37,  3.13it/s, training_loss=0.166]\u001b[A\n",
            "Epoch 4:  32%|███▏      | 404/1271 [02:10<04:37,  3.13it/s, training_loss=0.364]\u001b[A\n",
            "Epoch 4:  32%|███▏      | 405/1271 [02:10<04:36,  3.13it/s, training_loss=0.364]\u001b[A\n",
            "Epoch 4:  32%|███▏      | 405/1271 [02:10<04:36,  3.13it/s, training_loss=0.122]\u001b[A\n",
            "Epoch 4:  32%|███▏      | 406/1271 [02:10<04:36,  3.12it/s, training_loss=0.122]\u001b[A\n",
            "Epoch 4:  32%|███▏      | 406/1271 [02:11<04:36,  3.12it/s, training_loss=0.376]\u001b[A\n",
            "Epoch 4:  32%|███▏      | 407/1271 [02:11<04:37,  3.12it/s, training_loss=0.376]\u001b[A\n",
            "Epoch 4:  32%|███▏      | 407/1271 [02:11<04:37,  3.12it/s, training_loss=0.300]\u001b[A\n",
            "Epoch 4:  32%|███▏      | 408/1271 [02:11<04:37,  3.11it/s, training_loss=0.300]\u001b[A\n",
            "Epoch 4:  32%|███▏      | 408/1271 [02:11<04:37,  3.11it/s, training_loss=0.317]\u001b[A\n",
            "Epoch 4:  32%|███▏      | 409/1271 [02:11<04:35,  3.12it/s, training_loss=0.317]\u001b[A\n",
            "Epoch 4:  32%|███▏      | 409/1271 [02:12<04:35,  3.12it/s, training_loss=0.422]\u001b[A\n",
            "Epoch 4:  32%|███▏      | 410/1271 [02:12<04:36,  3.11it/s, training_loss=0.422]\u001b[A\n",
            "Epoch 4:  32%|███▏      | 410/1271 [02:12<04:36,  3.11it/s, training_loss=0.172]\u001b[A\n",
            "Epoch 4:  32%|███▏      | 411/1271 [02:12<04:36,  3.11it/s, training_loss=0.172]\u001b[A\n",
            "Epoch 4:  32%|███▏      | 411/1271 [02:12<04:36,  3.11it/s, training_loss=0.153]\u001b[A\n",
            "Epoch 4:  32%|███▏      | 412/1271 [02:12<04:35,  3.12it/s, training_loss=0.153]\u001b[A\n",
            "Epoch 4:  32%|███▏      | 412/1271 [02:13<04:35,  3.12it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 4:  32%|███▏      | 413/1271 [02:13<04:35,  3.12it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 4:  32%|███▏      | 413/1271 [02:13<04:35,  3.12it/s, training_loss=0.145]\u001b[A\n",
            "Epoch 4:  33%|███▎      | 414/1271 [02:13<04:35,  3.11it/s, training_loss=0.145]\u001b[A\n",
            "Epoch 4:  33%|███▎      | 414/1271 [02:13<04:35,  3.11it/s, training_loss=0.531]\u001b[A\n",
            "Epoch 4:  33%|███▎      | 415/1271 [02:13<04:35,  3.10it/s, training_loss=0.531]\u001b[A\n",
            "Epoch 4:  33%|███▎      | 415/1271 [02:14<04:35,  3.10it/s, training_loss=0.376]\u001b[A\n",
            "Epoch 4:  33%|███▎      | 416/1271 [02:14<04:37,  3.08it/s, training_loss=0.376]\u001b[A\n",
            "Epoch 4:  33%|███▎      | 416/1271 [02:14<04:37,  3.08it/s, training_loss=0.159]\u001b[A\n",
            "Epoch 4:  33%|███▎      | 417/1271 [02:14<04:35,  3.10it/s, training_loss=0.159]\u001b[A\n",
            "Epoch 4:  33%|███▎      | 417/1271 [02:14<04:35,  3.10it/s, training_loss=0.073]\u001b[A\n",
            "Epoch 4:  33%|███▎      | 418/1271 [02:14<04:36,  3.09it/s, training_loss=0.073]\u001b[A\n",
            "Epoch 4:  33%|███▎      | 418/1271 [02:15<04:36,  3.09it/s, training_loss=0.260]\u001b[A\n",
            "Epoch 4:  33%|███▎      | 419/1271 [02:15<04:35,  3.10it/s, training_loss=0.260]\u001b[A\n",
            "Epoch 4:  33%|███▎      | 419/1271 [02:15<04:35,  3.10it/s, training_loss=0.064]\u001b[A\n",
            "Epoch 4:  33%|███▎      | 420/1271 [02:15<04:33,  3.11it/s, training_loss=0.064]\u001b[A\n",
            "Epoch 4:  33%|███▎      | 420/1271 [02:15<04:33,  3.11it/s, training_loss=0.298]\u001b[A\n",
            "Epoch 4:  33%|███▎      | 421/1271 [02:15<04:33,  3.11it/s, training_loss=0.298]\u001b[A\n",
            "Epoch 4:  33%|███▎      | 421/1271 [02:16<04:33,  3.11it/s, training_loss=0.285]\u001b[A\n",
            "Epoch 4:  33%|███▎      | 422/1271 [02:16<04:33,  3.10it/s, training_loss=0.285]\u001b[A\n",
            "Epoch 4:  33%|███▎      | 422/1271 [02:16<04:33,  3.10it/s, training_loss=0.259]\u001b[A\n",
            "Epoch 4:  33%|███▎      | 423/1271 [02:16<04:32,  3.11it/s, training_loss=0.259]\u001b[A\n",
            "Epoch 4:  33%|███▎      | 423/1271 [02:16<04:32,  3.11it/s, training_loss=0.076]\u001b[A\n",
            "Epoch 4:  33%|███▎      | 424/1271 [02:16<04:32,  3.11it/s, training_loss=0.076]\u001b[A\n",
            "Epoch 4:  33%|███▎      | 424/1271 [02:16<04:32,  3.11it/s, training_loss=0.311]\u001b[A\n",
            "Epoch 4:  33%|███▎      | 425/1271 [02:16<04:30,  3.12it/s, training_loss=0.311]\u001b[A\n",
            "Epoch 4:  33%|███▎      | 425/1271 [02:17<04:30,  3.12it/s, training_loss=0.472]\u001b[A\n",
            "Epoch 4:  34%|███▎      | 426/1271 [02:17<04:30,  3.13it/s, training_loss=0.472]\u001b[A\n",
            "Epoch 4:  34%|███▎      | 426/1271 [02:17<04:30,  3.13it/s, training_loss=0.221]\u001b[A\n",
            "Epoch 4:  34%|███▎      | 427/1271 [02:17<04:30,  3.13it/s, training_loss=0.221]\u001b[A\n",
            "Epoch 4:  34%|███▎      | 427/1271 [02:17<04:30,  3.13it/s, training_loss=0.240]\u001b[A\n",
            "Epoch 4:  34%|███▎      | 428/1271 [02:17<04:30,  3.12it/s, training_loss=0.240]\u001b[A\n",
            "Epoch 4:  34%|███▎      | 428/1271 [02:18<04:30,  3.12it/s, training_loss=0.034]\u001b[A\n",
            "Epoch 4:  34%|███▍      | 429/1271 [02:18<04:29,  3.12it/s, training_loss=0.034]\u001b[A\n",
            "Epoch 4:  34%|███▍      | 429/1271 [02:18<04:29,  3.12it/s, training_loss=0.265]\u001b[A\n",
            "Epoch 4:  34%|███▍      | 430/1271 [02:18<04:29,  3.12it/s, training_loss=0.265]\u001b[A\n",
            "Epoch 4:  34%|███▍      | 430/1271 [02:18<04:29,  3.12it/s, training_loss=0.479]\u001b[A\n",
            "Epoch 4:  34%|███▍      | 431/1271 [02:18<04:28,  3.13it/s, training_loss=0.479]\u001b[A\n",
            "Epoch 4:  34%|███▍      | 431/1271 [02:19<04:28,  3.13it/s, training_loss=0.143]\u001b[A\n",
            "Epoch 4:  34%|███▍      | 432/1271 [02:19<04:28,  3.12it/s, training_loss=0.143]\u001b[A\n",
            "Epoch 4:  34%|███▍      | 432/1271 [02:19<04:28,  3.12it/s, training_loss=0.513]\u001b[A\n",
            "Epoch 4:  34%|███▍      | 433/1271 [02:19<04:29,  3.11it/s, training_loss=0.513]\u001b[A\n",
            "Epoch 4:  34%|███▍      | 433/1271 [02:19<04:29,  3.11it/s, training_loss=0.103]\u001b[A\n",
            "Epoch 4:  34%|███▍      | 434/1271 [02:19<04:28,  3.12it/s, training_loss=0.103]\u001b[A\n",
            "Epoch 4:  34%|███▍      | 434/1271 [02:20<04:28,  3.12it/s, training_loss=0.047]\u001b[A\n",
            "Epoch 4:  34%|███▍      | 435/1271 [02:20<04:28,  3.11it/s, training_loss=0.047]\u001b[A\n",
            "Epoch 4:  34%|███▍      | 435/1271 [02:20<04:28,  3.11it/s, training_loss=0.238]\u001b[A\n",
            "Epoch 4:  34%|███▍      | 436/1271 [02:20<04:29,  3.09it/s, training_loss=0.238]\u001b[A\n",
            "Epoch 4:  34%|███▍      | 436/1271 [02:20<04:29,  3.09it/s, training_loss=0.159]\u001b[A\n",
            "Epoch 4:  34%|███▍      | 437/1271 [02:20<04:29,  3.09it/s, training_loss=0.159]\u001b[A\n",
            "Epoch 4:  34%|███▍      | 437/1271 [02:21<04:29,  3.09it/s, training_loss=0.079]\u001b[A\n",
            "Epoch 4:  34%|███▍      | 438/1271 [02:21<04:29,  3.09it/s, training_loss=0.079]\u001b[A\n",
            "Epoch 4:  34%|███▍      | 438/1271 [02:21<04:29,  3.09it/s, training_loss=0.010]\u001b[A\n",
            "Epoch 4:  35%|███▍      | 439/1271 [02:21<04:28,  3.09it/s, training_loss=0.010]\u001b[A\n",
            "Epoch 4:  35%|███▍      | 439/1271 [02:21<04:28,  3.09it/s, training_loss=0.525]\u001b[A\n",
            "Epoch 4:  35%|███▍      | 440/1271 [02:21<04:27,  3.10it/s, training_loss=0.525]\u001b[A\n",
            "Epoch 4:  35%|███▍      | 440/1271 [02:22<04:27,  3.10it/s, training_loss=0.367]\u001b[A\n",
            "Epoch 4:  35%|███▍      | 441/1271 [02:22<04:28,  3.10it/s, training_loss=0.367]\u001b[A\n",
            "Epoch 4:  35%|███▍      | 441/1271 [02:22<04:28,  3.10it/s, training_loss=0.322]\u001b[A\n",
            "Epoch 4:  35%|███▍      | 442/1271 [02:22<04:28,  3.09it/s, training_loss=0.322]\u001b[A\n",
            "Epoch 4:  35%|███▍      | 442/1271 [02:22<04:28,  3.09it/s, training_loss=0.138]\u001b[A\n",
            "Epoch 4:  35%|███▍      | 443/1271 [02:22<04:29,  3.08it/s, training_loss=0.138]\u001b[A\n",
            "Epoch 4:  35%|███▍      | 443/1271 [02:23<04:29,  3.08it/s, training_loss=0.473]\u001b[A\n",
            "Epoch 4:  35%|███▍      | 444/1271 [02:23<04:27,  3.09it/s, training_loss=0.473]\u001b[A\n",
            "Epoch 4:  35%|███▍      | 444/1271 [02:23<04:27,  3.09it/s, training_loss=0.109]\u001b[A\n",
            "Epoch 4:  35%|███▌      | 445/1271 [02:23<04:25,  3.11it/s, training_loss=0.109]\u001b[A\n",
            "Epoch 4:  35%|███▌      | 445/1271 [02:23<04:25,  3.11it/s, training_loss=0.356]\u001b[A\n",
            "Epoch 4:  35%|███▌      | 446/1271 [02:23<04:24,  3.12it/s, training_loss=0.356]\u001b[A\n",
            "Epoch 4:  35%|███▌      | 446/1271 [02:24<04:24,  3.12it/s, training_loss=0.039]\u001b[A\n",
            "Epoch 4:  35%|███▌      | 447/1271 [02:24<04:24,  3.12it/s, training_loss=0.039]\u001b[A\n",
            "Epoch 4:  35%|███▌      | 447/1271 [02:24<04:24,  3.12it/s, training_loss=0.253]\u001b[A\n",
            "Epoch 4:  35%|███▌      | 448/1271 [02:24<04:24,  3.11it/s, training_loss=0.253]\u001b[A\n",
            "Epoch 4:  35%|███▌      | 448/1271 [02:24<04:24,  3.11it/s, training_loss=0.437]\u001b[A\n",
            "Epoch 4:  35%|███▌      | 449/1271 [02:24<04:23,  3.12it/s, training_loss=0.437]\u001b[A\n",
            "Epoch 4:  35%|███▌      | 449/1271 [02:25<04:23,  3.12it/s, training_loss=0.121]\u001b[A\n",
            "Epoch 4:  35%|███▌      | 450/1271 [02:25<04:22,  3.13it/s, training_loss=0.121]\u001b[A\n",
            "Epoch 4:  35%|███▌      | 450/1271 [02:25<04:22,  3.13it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 4:  35%|███▌      | 451/1271 [02:25<04:23,  3.12it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 4:  35%|███▌      | 451/1271 [02:25<04:23,  3.12it/s, training_loss=0.231]\u001b[A\n",
            "Epoch 4:  36%|███▌      | 452/1271 [02:25<04:22,  3.12it/s, training_loss=0.231]\u001b[A\n",
            "Epoch 4:  36%|███▌      | 452/1271 [02:25<04:22,  3.12it/s, training_loss=0.164]\u001b[A\n",
            "Epoch 4:  36%|███▌      | 453/1271 [02:25<04:20,  3.13it/s, training_loss=0.164]\u001b[A\n",
            "Epoch 4:  36%|███▌      | 453/1271 [02:26<04:20,  3.13it/s, training_loss=0.297]\u001b[A\n",
            "Epoch 4:  36%|███▌      | 454/1271 [02:26<04:22,  3.11it/s, training_loss=0.297]\u001b[A\n",
            "Epoch 4:  36%|███▌      | 454/1271 [02:26<04:22,  3.11it/s, training_loss=0.429]\u001b[A\n",
            "Epoch 4:  36%|███▌      | 455/1271 [02:26<04:22,  3.11it/s, training_loss=0.429]\u001b[A\n",
            "Epoch 4:  36%|███▌      | 455/1271 [02:26<04:22,  3.11it/s, training_loss=0.181]\u001b[A\n",
            "Epoch 4:  36%|███▌      | 456/1271 [02:26<04:21,  3.12it/s, training_loss=0.181]\u001b[A\n",
            "Epoch 4:  36%|███▌      | 456/1271 [02:27<04:21,  3.12it/s, training_loss=0.252]\u001b[A\n",
            "Epoch 4:  36%|███▌      | 457/1271 [02:27<04:20,  3.12it/s, training_loss=0.252]\u001b[A\n",
            "Epoch 4:  36%|███▌      | 457/1271 [02:27<04:20,  3.12it/s, training_loss=0.802]\u001b[A\n",
            "Epoch 4:  36%|███▌      | 458/1271 [02:27<04:21,  3.11it/s, training_loss=0.802]\u001b[A\n",
            "Epoch 4:  36%|███▌      | 458/1271 [02:27<04:21,  3.11it/s, training_loss=0.359]\u001b[A\n",
            "Epoch 4:  36%|███▌      | 459/1271 [02:27<04:20,  3.12it/s, training_loss=0.359]\u001b[A\n",
            "Epoch 4:  36%|███▌      | 459/1271 [02:28<04:20,  3.12it/s, training_loss=0.076]\u001b[A\n",
            "Epoch 4:  36%|███▌      | 460/1271 [02:28<04:19,  3.13it/s, training_loss=0.076]\u001b[A\n",
            "Epoch 4:  36%|███▌      | 460/1271 [02:28<04:19,  3.13it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 4:  36%|███▋      | 461/1271 [02:28<04:19,  3.12it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 4:  36%|███▋      | 461/1271 [02:28<04:19,  3.12it/s, training_loss=0.286]\u001b[A\n",
            "Epoch 4:  36%|███▋      | 462/1271 [02:28<04:19,  3.11it/s, training_loss=0.286]\u001b[A\n",
            "Epoch 4:  36%|███▋      | 462/1271 [02:29<04:19,  3.11it/s, training_loss=0.330]\u001b[A\n",
            "Epoch 4:  36%|███▋      | 463/1271 [02:29<04:19,  3.11it/s, training_loss=0.330]\u001b[A\n",
            "Epoch 4:  36%|███▋      | 463/1271 [02:29<04:19,  3.11it/s, training_loss=0.157]\u001b[A\n",
            "Epoch 4:  37%|███▋      | 464/1271 [02:29<04:18,  3.12it/s, training_loss=0.157]\u001b[A\n",
            "Epoch 4:  37%|███▋      | 464/1271 [02:29<04:18,  3.12it/s, training_loss=0.124]\u001b[A\n",
            "Epoch 4:  37%|███▋      | 465/1271 [02:29<04:16,  3.14it/s, training_loss=0.124]\u001b[A\n",
            "Epoch 4:  37%|███▋      | 465/1271 [02:30<04:16,  3.14it/s, training_loss=0.666]\u001b[A\n",
            "Epoch 4:  37%|███▋      | 466/1271 [02:30<04:16,  3.14it/s, training_loss=0.666]\u001b[A\n",
            "Epoch 4:  37%|███▋      | 466/1271 [02:30<04:16,  3.14it/s, training_loss=0.646]\u001b[A\n",
            "Epoch 4:  37%|███▋      | 467/1271 [02:30<04:17,  3.12it/s, training_loss=0.646]\u001b[A\n",
            "Epoch 4:  37%|███▋      | 467/1271 [02:30<04:17,  3.12it/s, training_loss=0.047]\u001b[A\n",
            "Epoch 4:  37%|███▋      | 468/1271 [02:30<04:18,  3.10it/s, training_loss=0.047]\u001b[A\n",
            "Epoch 4:  37%|███▋      | 468/1271 [02:31<04:18,  3.10it/s, training_loss=0.044]\u001b[A\n",
            "Epoch 4:  37%|███▋      | 469/1271 [02:31<04:18,  3.10it/s, training_loss=0.044]\u001b[A\n",
            "Epoch 4:  37%|███▋      | 469/1271 [02:31<04:18,  3.10it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 4:  37%|███▋      | 470/1271 [02:31<04:17,  3.11it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 4:  37%|███▋      | 470/1271 [02:31<04:17,  3.11it/s, training_loss=0.350]\u001b[A\n",
            "Epoch 4:  37%|███▋      | 471/1271 [02:31<04:16,  3.12it/s, training_loss=0.350]\u001b[A\n",
            "Epoch 4:  37%|███▋      | 471/1271 [02:32<04:16,  3.12it/s, training_loss=0.700]\u001b[A\n",
            "Epoch 4:  37%|███▋      | 472/1271 [02:32<04:18,  3.09it/s, training_loss=0.700]\u001b[A\n",
            "Epoch 4:  37%|███▋      | 472/1271 [02:32<04:18,  3.09it/s, training_loss=0.113]\u001b[A\n",
            "Epoch 4:  37%|███▋      | 473/1271 [02:32<04:18,  3.09it/s, training_loss=0.113]\u001b[A\n",
            "Epoch 4:  37%|███▋      | 473/1271 [02:32<04:18,  3.09it/s, training_loss=0.470]\u001b[A\n",
            "Epoch 4:  37%|███▋      | 474/1271 [02:32<04:20,  3.06it/s, training_loss=0.470]\u001b[A\n",
            "Epoch 4:  37%|███▋      | 474/1271 [02:33<04:20,  3.06it/s, training_loss=0.369]\u001b[A\n",
            "Epoch 4:  37%|███▋      | 475/1271 [02:33<04:18,  3.08it/s, training_loss=0.369]\u001b[A\n",
            "Epoch 4:  37%|███▋      | 475/1271 [02:33<04:18,  3.08it/s, training_loss=0.018]\u001b[A\n",
            "Epoch 4:  37%|███▋      | 476/1271 [02:33<04:18,  3.08it/s, training_loss=0.018]\u001b[A\n",
            "Epoch 4:  37%|███▋      | 476/1271 [02:33<04:18,  3.08it/s, training_loss=0.599]\u001b[A\n",
            "Epoch 4:  38%|███▊      | 477/1271 [02:33<04:18,  3.07it/s, training_loss=0.599]\u001b[A\n",
            "Epoch 4:  38%|███▊      | 477/1271 [02:34<04:18,  3.07it/s, training_loss=0.704]\u001b[A\n",
            "Epoch 4:  38%|███▊      | 478/1271 [02:34<04:18,  3.06it/s, training_loss=0.704]\u001b[A\n",
            "Epoch 4:  38%|███▊      | 478/1271 [02:34<04:18,  3.06it/s, training_loss=0.121]\u001b[A\n",
            "Epoch 4:  38%|███▊      | 479/1271 [02:34<04:16,  3.09it/s, training_loss=0.121]\u001b[A\n",
            "Epoch 4:  38%|███▊      | 479/1271 [02:34<04:16,  3.09it/s, training_loss=0.043]\u001b[A\n",
            "Epoch 4:  38%|███▊      | 480/1271 [02:34<04:14,  3.11it/s, training_loss=0.043]\u001b[A\n",
            "Epoch 4:  38%|███▊      | 480/1271 [02:35<04:14,  3.11it/s, training_loss=0.082]\u001b[A\n",
            "Epoch 4:  38%|███▊      | 481/1271 [02:35<04:14,  3.10it/s, training_loss=0.082]\u001b[A\n",
            "Epoch 4:  38%|███▊      | 481/1271 [02:35<04:14,  3.10it/s, training_loss=0.016]\u001b[A\n",
            "Epoch 4:  38%|███▊      | 482/1271 [02:35<04:13,  3.11it/s, training_loss=0.016]\u001b[A\n",
            "Epoch 4:  38%|███▊      | 482/1271 [02:35<04:13,  3.11it/s, training_loss=0.269]\u001b[A\n",
            "Epoch 4:  38%|███▊      | 483/1271 [02:35<04:12,  3.12it/s, training_loss=0.269]\u001b[A\n",
            "Epoch 4:  38%|███▊      | 483/1271 [02:35<04:12,  3.12it/s, training_loss=0.137]\u001b[A\n",
            "Epoch 4:  38%|███▊      | 484/1271 [02:35<04:10,  3.14it/s, training_loss=0.137]\u001b[A\n",
            "Epoch 4:  38%|███▊      | 484/1271 [02:36<04:10,  3.14it/s, training_loss=0.578]\u001b[A\n",
            "Epoch 4:  38%|███▊      | 485/1271 [02:36<04:10,  3.14it/s, training_loss=0.578]\u001b[A\n",
            "Epoch 4:  38%|███▊      | 485/1271 [02:36<04:10,  3.14it/s, training_loss=0.239]\u001b[A\n",
            "Epoch 4:  38%|███▊      | 486/1271 [02:36<04:10,  3.14it/s, training_loss=0.239]\u001b[A\n",
            "Epoch 4:  38%|███▊      | 486/1271 [02:36<04:10,  3.14it/s, training_loss=0.197]\u001b[A\n",
            "Epoch 4:  38%|███▊      | 487/1271 [02:36<04:09,  3.14it/s, training_loss=0.197]\u001b[A\n",
            "Epoch 4:  38%|███▊      | 487/1271 [02:37<04:09,  3.14it/s, training_loss=0.624]\u001b[A\n",
            "Epoch 4:  38%|███▊      | 488/1271 [02:37<04:11,  3.11it/s, training_loss=0.624]\u001b[A\n",
            "Epoch 4:  38%|███▊      | 488/1271 [02:37<04:11,  3.11it/s, training_loss=0.100]\u001b[A\n",
            "Epoch 4:  38%|███▊      | 489/1271 [02:37<04:10,  3.12it/s, training_loss=0.100]\u001b[A\n",
            "Epoch 4:  38%|███▊      | 489/1271 [02:37<04:10,  3.12it/s, training_loss=0.194]\u001b[A\n",
            "Epoch 4:  39%|███▊      | 490/1271 [02:37<04:10,  3.11it/s, training_loss=0.194]\u001b[A\n",
            "Epoch 4:  39%|███▊      | 490/1271 [02:38<04:10,  3.11it/s, training_loss=0.505]\u001b[A\n",
            "Epoch 4:  39%|███▊      | 491/1271 [02:38<04:10,  3.11it/s, training_loss=0.505]\u001b[A\n",
            "Epoch 4:  39%|███▊      | 491/1271 [02:38<04:10,  3.11it/s, training_loss=0.067]\u001b[A\n",
            "Epoch 4:  39%|███▊      | 492/1271 [02:38<04:10,  3.11it/s, training_loss=0.067]\u001b[A\n",
            "Epoch 4:  39%|███▊      | 492/1271 [02:38<04:10,  3.11it/s, training_loss=0.081]\u001b[A\n",
            "Epoch 4:  39%|███▉      | 493/1271 [02:38<04:11,  3.09it/s, training_loss=0.081]\u001b[A\n",
            "Epoch 4:  39%|███▉      | 493/1271 [02:39<04:11,  3.09it/s, training_loss=0.027]\u001b[A\n",
            "Epoch 4:  39%|███▉      | 494/1271 [02:39<04:10,  3.10it/s, training_loss=0.027]\u001b[A\n",
            "Epoch 4:  39%|███▉      | 494/1271 [02:39<04:10,  3.10it/s, training_loss=0.523]\u001b[A\n",
            "Epoch 4:  39%|███▉      | 495/1271 [02:39<04:10,  3.09it/s, training_loss=0.523]\u001b[A\n",
            "Epoch 4:  39%|███▉      | 495/1271 [02:39<04:10,  3.09it/s, training_loss=0.099]\u001b[A\n",
            "Epoch 4:  39%|███▉      | 496/1271 [02:39<04:10,  3.09it/s, training_loss=0.099]\u001b[A\n",
            "Epoch 4:  39%|███▉      | 496/1271 [02:40<04:10,  3.09it/s, training_loss=0.007]\u001b[A\n",
            "Epoch 4:  39%|███▉      | 497/1271 [02:40<04:10,  3.09it/s, training_loss=0.007]\u001b[A\n",
            "Epoch 4:  39%|███▉      | 497/1271 [02:40<04:10,  3.09it/s, training_loss=0.077]\u001b[A\n",
            "Epoch 4:  39%|███▉      | 498/1271 [02:40<04:09,  3.10it/s, training_loss=0.077]\u001b[A\n",
            "Epoch 4:  39%|███▉      | 498/1271 [02:40<04:09,  3.10it/s, training_loss=0.152]\u001b[A\n",
            "Epoch 4:  39%|███▉      | 499/1271 [02:40<04:09,  3.10it/s, training_loss=0.152]\u001b[A\n",
            "Epoch 4:  39%|███▉      | 499/1271 [02:41<04:09,  3.10it/s, training_loss=0.388]\u001b[A\n",
            "Epoch 4:  39%|███▉      | 500/1271 [02:41<04:08,  3.11it/s, training_loss=0.388]\u001b[A\n",
            "Epoch 4:  39%|███▉      | 500/1271 [02:41<04:08,  3.11it/s, training_loss=0.054]\u001b[A\n",
            "Epoch 4:  39%|███▉      | 501/1271 [02:41<04:07,  3.12it/s, training_loss=0.054]\u001b[A\n",
            "Epoch 4:  39%|███▉      | 501/1271 [02:41<04:07,  3.12it/s, training_loss=0.417]\u001b[A\n",
            "Epoch 4:  39%|███▉      | 502/1271 [02:41<04:06,  3.12it/s, training_loss=0.417]\u001b[A\n",
            "Epoch 4:  39%|███▉      | 502/1271 [02:42<04:06,  3.12it/s, training_loss=0.655]\u001b[A\n",
            "Epoch 4:  40%|███▉      | 503/1271 [02:42<04:06,  3.12it/s, training_loss=0.655]\u001b[A\n",
            "Epoch 4:  40%|███▉      | 503/1271 [02:42<04:06,  3.12it/s, training_loss=0.497]\u001b[A\n",
            "Epoch 4:  40%|███▉      | 504/1271 [02:42<04:06,  3.11it/s, training_loss=0.497]\u001b[A\n",
            "Epoch 4:  40%|███▉      | 504/1271 [02:42<04:06,  3.11it/s, training_loss=0.189]\u001b[A\n",
            "Epoch 4:  40%|███▉      | 505/1271 [02:42<04:05,  3.12it/s, training_loss=0.189]\u001b[A\n",
            "Epoch 4:  40%|███▉      | 505/1271 [02:43<04:05,  3.12it/s, training_loss=0.007]\u001b[A\n",
            "Epoch 4:  40%|███▉      | 506/1271 [02:43<04:05,  3.11it/s, training_loss=0.007]\u001b[A\n",
            "Epoch 4:  40%|███▉      | 506/1271 [02:43<04:05,  3.11it/s, training_loss=0.362]\u001b[A\n",
            "Epoch 4:  40%|███▉      | 507/1271 [02:43<04:06,  3.10it/s, training_loss=0.362]\u001b[A\n",
            "Epoch 4:  40%|███▉      | 507/1271 [02:43<04:06,  3.10it/s, training_loss=0.304]\u001b[A\n",
            "Epoch 4:  40%|███▉      | 508/1271 [02:43<04:07,  3.09it/s, training_loss=0.304]\u001b[A\n",
            "Epoch 4:  40%|███▉      | 508/1271 [02:44<04:07,  3.09it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 4:  40%|████      | 509/1271 [02:44<04:05,  3.10it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 4:  40%|████      | 509/1271 [02:44<04:05,  3.10it/s, training_loss=0.257]\u001b[A\n",
            "Epoch 4:  40%|████      | 510/1271 [02:44<04:05,  3.09it/s, training_loss=0.257]\u001b[A\n",
            "Epoch 4:  40%|████      | 510/1271 [02:44<04:05,  3.09it/s, training_loss=0.140]\u001b[A\n",
            "Epoch 4:  40%|████      | 511/1271 [02:44<04:04,  3.11it/s, training_loss=0.140]\u001b[A\n",
            "Epoch 4:  40%|████      | 511/1271 [02:44<04:04,  3.11it/s, training_loss=0.398]\u001b[A\n",
            "Epoch 4:  40%|████      | 512/1271 [02:44<04:03,  3.12it/s, training_loss=0.398]\u001b[A\n",
            "Epoch 4:  40%|████      | 512/1271 [02:45<04:03,  3.12it/s, training_loss=0.660]\u001b[A\n",
            "Epoch 4:  40%|████      | 513/1271 [02:45<04:02,  3.13it/s, training_loss=0.660]\u001b[A\n",
            "Epoch 4:  40%|████      | 513/1271 [02:45<04:02,  3.13it/s, training_loss=0.120]\u001b[A\n",
            "Epoch 4:  40%|████      | 514/1271 [02:45<04:01,  3.13it/s, training_loss=0.120]\u001b[A\n",
            "Epoch 4:  40%|████      | 514/1271 [02:45<04:01,  3.13it/s, training_loss=0.549]\u001b[A\n",
            "Epoch 4:  41%|████      | 515/1271 [02:45<04:01,  3.13it/s, training_loss=0.549]\u001b[A\n",
            "Epoch 4:  41%|████      | 515/1271 [02:46<04:01,  3.13it/s, training_loss=0.345]\u001b[A\n",
            "Epoch 4:  41%|████      | 516/1271 [02:46<04:00,  3.13it/s, training_loss=0.345]\u001b[A\n",
            "Epoch 4:  41%|████      | 516/1271 [02:46<04:00,  3.13it/s, training_loss=0.382]\u001b[A\n",
            "Epoch 4:  41%|████      | 517/1271 [02:46<04:01,  3.12it/s, training_loss=0.382]\u001b[A\n",
            "Epoch 4:  41%|████      | 517/1271 [02:46<04:01,  3.12it/s, training_loss=0.252]\u001b[A\n",
            "Epoch 4:  41%|████      | 518/1271 [02:46<04:00,  3.13it/s, training_loss=0.252]\u001b[A\n",
            "Epoch 4:  41%|████      | 518/1271 [02:47<04:00,  3.13it/s, training_loss=0.529]\u001b[A\n",
            "Epoch 4:  41%|████      | 519/1271 [02:47<04:00,  3.13it/s, training_loss=0.529]\u001b[A\n",
            "Epoch 4:  41%|████      | 519/1271 [02:47<04:00,  3.13it/s, training_loss=0.017]\u001b[A\n",
            "Epoch 4:  41%|████      | 520/1271 [02:47<04:00,  3.13it/s, training_loss=0.017]\u001b[A\n",
            "Epoch 4:  41%|████      | 520/1271 [02:47<04:00,  3.13it/s, training_loss=0.518]\u001b[A\n",
            "Epoch 4:  41%|████      | 521/1271 [02:47<03:59,  3.14it/s, training_loss=0.518]\u001b[A\n",
            "Epoch 4:  41%|████      | 521/1271 [02:48<03:59,  3.14it/s, training_loss=0.321]\u001b[A\n",
            "Epoch 4:  41%|████      | 522/1271 [02:48<03:59,  3.13it/s, training_loss=0.321]\u001b[A\n",
            "Epoch 4:  41%|████      | 522/1271 [02:48<03:59,  3.13it/s, training_loss=0.064]\u001b[A\n",
            "Epoch 4:  41%|████      | 523/1271 [02:48<03:59,  3.13it/s, training_loss=0.064]\u001b[A\n",
            "Epoch 4:  41%|████      | 523/1271 [02:48<03:59,  3.13it/s, training_loss=0.083]\u001b[A\n",
            "Epoch 4:  41%|████      | 524/1271 [02:48<03:59,  3.12it/s, training_loss=0.083]\u001b[A\n",
            "Epoch 4:  41%|████      | 524/1271 [02:49<03:59,  3.12it/s, training_loss=0.207]\u001b[A\n",
            "Epoch 4:  41%|████▏     | 525/1271 [02:49<04:01,  3.09it/s, training_loss=0.207]\u001b[A\n",
            "Epoch 4:  41%|████▏     | 525/1271 [02:49<04:01,  3.09it/s, training_loss=0.314]\u001b[A\n",
            "Epoch 4:  41%|████▏     | 526/1271 [02:49<04:00,  3.10it/s, training_loss=0.314]\u001b[A\n",
            "Epoch 4:  41%|████▏     | 526/1271 [02:49<04:00,  3.10it/s, training_loss=0.410]\u001b[A\n",
            "Epoch 4:  41%|████▏     | 527/1271 [02:49<03:58,  3.12it/s, training_loss=0.410]\u001b[A\n",
            "Epoch 4:  41%|████▏     | 527/1271 [02:50<03:58,  3.12it/s, training_loss=0.066]\u001b[A\n",
            "Epoch 4:  42%|████▏     | 528/1271 [02:50<03:58,  3.12it/s, training_loss=0.066]\u001b[A\n",
            "Epoch 4:  42%|████▏     | 528/1271 [02:50<03:58,  3.12it/s, training_loss=0.089]\u001b[A\n",
            "Epoch 4:  42%|████▏     | 529/1271 [02:50<03:57,  3.12it/s, training_loss=0.089]\u001b[A\n",
            "Epoch 4:  42%|████▏     | 529/1271 [02:50<03:57,  3.12it/s, training_loss=0.059]\u001b[A\n",
            "Epoch 4:  42%|████▏     | 530/1271 [02:50<03:57,  3.11it/s, training_loss=0.059]\u001b[A\n",
            "Epoch 4:  42%|████▏     | 530/1271 [02:51<03:57,  3.11it/s, training_loss=0.182]\u001b[A\n",
            "Epoch 4:  42%|████▏     | 531/1271 [02:51<03:57,  3.12it/s, training_loss=0.182]\u001b[A\n",
            "Epoch 4:  42%|████▏     | 531/1271 [02:51<03:57,  3.12it/s, training_loss=0.224]\u001b[A\n",
            "Epoch 4:  42%|████▏     | 532/1271 [02:51<03:58,  3.10it/s, training_loss=0.224]\u001b[A\n",
            "Epoch 4:  42%|████▏     | 532/1271 [02:51<03:58,  3.10it/s, training_loss=0.683]\u001b[A\n",
            "Epoch 4:  42%|████▏     | 533/1271 [02:51<03:57,  3.10it/s, training_loss=0.683]\u001b[A\n",
            "Epoch 4:  42%|████▏     | 533/1271 [02:52<03:57,  3.10it/s, training_loss=0.044]\u001b[A\n",
            "Epoch 4:  42%|████▏     | 534/1271 [02:52<03:56,  3.12it/s, training_loss=0.044]\u001b[A\n",
            "Epoch 4:  42%|████▏     | 534/1271 [02:52<03:56,  3.12it/s, training_loss=0.355]\u001b[A\n",
            "Epoch 4:  42%|████▏     | 535/1271 [02:52<03:55,  3.13it/s, training_loss=0.355]\u001b[A\n",
            "Epoch 4:  42%|████▏     | 535/1271 [02:52<03:55,  3.13it/s, training_loss=0.718]\u001b[A\n",
            "Epoch 4:  42%|████▏     | 536/1271 [02:52<03:55,  3.12it/s, training_loss=0.718]\u001b[A\n",
            "Epoch 4:  42%|████▏     | 536/1271 [02:52<03:55,  3.12it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 4:  42%|████▏     | 537/1271 [02:52<03:55,  3.12it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 4:  42%|████▏     | 537/1271 [02:53<03:55,  3.12it/s, training_loss=0.539]\u001b[A\n",
            "Epoch 4:  42%|████▏     | 538/1271 [02:53<03:55,  3.11it/s, training_loss=0.539]\u001b[A\n",
            "Epoch 4:  42%|████▏     | 538/1271 [02:53<03:55,  3.11it/s, training_loss=0.320]\u001b[A\n",
            "Epoch 4:  42%|████▏     | 539/1271 [02:53<03:54,  3.12it/s, training_loss=0.320]\u001b[A\n",
            "Epoch 4:  42%|████▏     | 539/1271 [02:53<03:54,  3.12it/s, training_loss=0.293]\u001b[A\n",
            "Epoch 4:  42%|████▏     | 540/1271 [02:53<03:54,  3.12it/s, training_loss=0.293]\u001b[A\n",
            "Epoch 4:  42%|████▏     | 540/1271 [02:54<03:54,  3.12it/s, training_loss=0.121]\u001b[A\n",
            "Epoch 4:  43%|████▎     | 541/1271 [02:54<03:52,  3.14it/s, training_loss=0.121]\u001b[A\n",
            "Epoch 4:  43%|████▎     | 541/1271 [02:54<03:52,  3.14it/s, training_loss=0.069]\u001b[A\n",
            "Epoch 4:  43%|████▎     | 542/1271 [02:54<03:52,  3.13it/s, training_loss=0.069]\u001b[A\n",
            "Epoch 4:  43%|████▎     | 542/1271 [02:54<03:52,  3.13it/s, training_loss=0.056]\u001b[A\n",
            "Epoch 4:  43%|████▎     | 543/1271 [02:54<03:51,  3.15it/s, training_loss=0.056]\u001b[A\n",
            "Epoch 4:  43%|████▎     | 543/1271 [02:55<03:51,  3.15it/s, training_loss=0.102]\u001b[A\n",
            "Epoch 4:  43%|████▎     | 544/1271 [02:55<03:51,  3.15it/s, training_loss=0.102]\u001b[A\n",
            "Epoch 4:  43%|████▎     | 544/1271 [02:55<03:51,  3.15it/s, training_loss=0.105]\u001b[A\n",
            "Epoch 4:  43%|████▎     | 545/1271 [02:55<03:52,  3.13it/s, training_loss=0.105]\u001b[A\n",
            "Epoch 4:  43%|████▎     | 545/1271 [02:55<03:52,  3.13it/s, training_loss=0.532]\u001b[A\n",
            "Epoch 4:  43%|████▎     | 546/1271 [02:55<03:51,  3.13it/s, training_loss=0.532]\u001b[A\n",
            "Epoch 4:  43%|████▎     | 546/1271 [02:56<03:51,  3.13it/s, training_loss=0.105]\u001b[A\n",
            "Epoch 4:  43%|████▎     | 547/1271 [02:56<03:51,  3.13it/s, training_loss=0.105]\u001b[A\n",
            "Epoch 4:  43%|████▎     | 547/1271 [02:56<03:51,  3.13it/s, training_loss=0.338]\u001b[A\n",
            "Epoch 4:  43%|████▎     | 548/1271 [02:56<03:51,  3.13it/s, training_loss=0.338]\u001b[A\n",
            "Epoch 4:  43%|████▎     | 548/1271 [02:56<03:51,  3.13it/s, training_loss=0.234]\u001b[A\n",
            "Epoch 4:  43%|████▎     | 549/1271 [02:56<03:50,  3.13it/s, training_loss=0.234]\u001b[A\n",
            "Epoch 4:  43%|████▎     | 549/1271 [02:57<03:50,  3.13it/s, training_loss=0.190]\u001b[A\n",
            "Epoch 4:  43%|████▎     | 550/1271 [02:57<03:50,  3.13it/s, training_loss=0.190]\u001b[A\n",
            "Epoch 4:  43%|████▎     | 550/1271 [02:57<03:50,  3.13it/s, training_loss=0.335]\u001b[A\n",
            "Epoch 4:  43%|████▎     | 551/1271 [02:57<03:50,  3.12it/s, training_loss=0.335]\u001b[A\n",
            "Epoch 4:  43%|████▎     | 551/1271 [02:57<03:50,  3.12it/s, training_loss=0.527]\u001b[A\n",
            "Epoch 4:  43%|████▎     | 552/1271 [02:57<03:50,  3.12it/s, training_loss=0.527]\u001b[A\n",
            "Epoch 4:  43%|████▎     | 552/1271 [02:58<03:50,  3.12it/s, training_loss=0.130]\u001b[A\n",
            "Epoch 4:  44%|████▎     | 553/1271 [02:58<03:50,  3.12it/s, training_loss=0.130]\u001b[A\n",
            "Epoch 4:  44%|████▎     | 553/1271 [02:58<03:50,  3.12it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 4:  44%|████▎     | 554/1271 [02:58<03:49,  3.12it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 4:  44%|████▎     | 554/1271 [02:58<03:49,  3.12it/s, training_loss=0.342]\u001b[A\n",
            "Epoch 4:  44%|████▎     | 555/1271 [02:58<03:50,  3.10it/s, training_loss=0.342]\u001b[A\n",
            "Epoch 4:  44%|████▎     | 555/1271 [02:59<03:50,  3.10it/s, training_loss=0.197]\u001b[A\n",
            "Epoch 4:  44%|████▎     | 556/1271 [02:59<03:50,  3.10it/s, training_loss=0.197]\u001b[A\n",
            "Epoch 4:  44%|████▎     | 556/1271 [02:59<03:50,  3.10it/s, training_loss=0.234]\u001b[A\n",
            "Epoch 4:  44%|████▍     | 557/1271 [02:59<03:49,  3.11it/s, training_loss=0.234]\u001b[A\n",
            "Epoch 4:  44%|████▍     | 557/1271 [02:59<03:49,  3.11it/s, training_loss=0.158]\u001b[A\n",
            "Epoch 4:  44%|████▍     | 558/1271 [02:59<03:48,  3.11it/s, training_loss=0.158]\u001b[A\n",
            "Epoch 4:  44%|████▍     | 558/1271 [03:00<03:48,  3.11it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 4:  44%|████▍     | 559/1271 [03:00<03:49,  3.10it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 4:  44%|████▍     | 559/1271 [03:00<03:49,  3.10it/s, training_loss=0.298]\u001b[A\n",
            "Epoch 4:  44%|████▍     | 560/1271 [03:00<03:47,  3.12it/s, training_loss=0.298]\u001b[A\n",
            "Epoch 4:  44%|████▍     | 560/1271 [03:00<03:47,  3.12it/s, training_loss=0.080]\u001b[A\n",
            "Epoch 4:  44%|████▍     | 561/1271 [03:00<03:49,  3.10it/s, training_loss=0.080]\u001b[A\n",
            "Epoch 4:  44%|████▍     | 561/1271 [03:01<03:49,  3.10it/s, training_loss=0.288]\u001b[A\n",
            "Epoch 4:  44%|████▍     | 562/1271 [03:01<03:48,  3.10it/s, training_loss=0.288]\u001b[A\n",
            "Epoch 4:  44%|████▍     | 562/1271 [03:01<03:48,  3.10it/s, training_loss=0.169]\u001b[A\n",
            "Epoch 4:  44%|████▍     | 563/1271 [03:01<03:49,  3.09it/s, training_loss=0.169]\u001b[A\n",
            "Epoch 4:  44%|████▍     | 563/1271 [03:01<03:49,  3.09it/s, training_loss=0.291]\u001b[A\n",
            "Epoch 4:  44%|████▍     | 564/1271 [03:01<03:48,  3.09it/s, training_loss=0.291]\u001b[A\n",
            "Epoch 4:  44%|████▍     | 564/1271 [03:01<03:48,  3.09it/s, training_loss=1.018]\u001b[A\n",
            "Epoch 4:  44%|████▍     | 565/1271 [03:01<03:47,  3.11it/s, training_loss=1.018]\u001b[A\n",
            "Epoch 4:  44%|████▍     | 565/1271 [03:02<03:47,  3.11it/s, training_loss=0.278]\u001b[A\n",
            "Epoch 4:  45%|████▍     | 566/1271 [03:02<03:46,  3.11it/s, training_loss=0.278]\u001b[A\n",
            "Epoch 4:  45%|████▍     | 566/1271 [03:02<03:46,  3.11it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 4:  45%|████▍     | 567/1271 [03:02<03:47,  3.09it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 4:  45%|████▍     | 567/1271 [03:02<03:47,  3.09it/s, training_loss=0.040]\u001b[A\n",
            "Epoch 4:  45%|████▍     | 568/1271 [03:02<03:46,  3.10it/s, training_loss=0.040]\u001b[A\n",
            "Epoch 4:  45%|████▍     | 568/1271 [03:03<03:46,  3.10it/s, training_loss=0.381]\u001b[A\n",
            "Epoch 4:  45%|████▍     | 569/1271 [03:03<03:45,  3.11it/s, training_loss=0.381]\u001b[A\n",
            "Epoch 4:  45%|████▍     | 569/1271 [03:03<03:45,  3.11it/s, training_loss=0.160]\u001b[A\n",
            "Epoch 4:  45%|████▍     | 570/1271 [03:03<03:44,  3.12it/s, training_loss=0.160]\u001b[A\n",
            "Epoch 4:  45%|████▍     | 570/1271 [03:03<03:44,  3.12it/s, training_loss=0.108]\u001b[A\n",
            "Epoch 4:  45%|████▍     | 571/1271 [03:03<03:44,  3.11it/s, training_loss=0.108]\u001b[A\n",
            "Epoch 4:  45%|████▍     | 571/1271 [03:04<03:44,  3.11it/s, training_loss=0.794]\u001b[A\n",
            "Epoch 4:  45%|████▌     | 572/1271 [03:04<03:44,  3.11it/s, training_loss=0.794]\u001b[A\n",
            "Epoch 4:  45%|████▌     | 572/1271 [03:04<03:44,  3.11it/s, training_loss=0.109]\u001b[A\n",
            "Epoch 4:  45%|████▌     | 573/1271 [03:04<03:44,  3.11it/s, training_loss=0.109]\u001b[A\n",
            "Epoch 4:  45%|████▌     | 573/1271 [03:04<03:44,  3.11it/s, training_loss=0.089]\u001b[A\n",
            "Epoch 4:  45%|████▌     | 574/1271 [03:04<03:43,  3.11it/s, training_loss=0.089]\u001b[A\n",
            "Epoch 4:  45%|████▌     | 574/1271 [03:05<03:43,  3.11it/s, training_loss=0.065]\u001b[A\n",
            "Epoch 4:  45%|████▌     | 575/1271 [03:05<03:43,  3.11it/s, training_loss=0.065]\u001b[A\n",
            "Epoch 4:  45%|████▌     | 575/1271 [03:05<03:43,  3.11it/s, training_loss=0.449]\u001b[A\n",
            "Epoch 4:  45%|████▌     | 576/1271 [03:05<03:42,  3.13it/s, training_loss=0.449]\u001b[A\n",
            "Epoch 4:  45%|████▌     | 576/1271 [03:05<03:42,  3.13it/s, training_loss=0.326]\u001b[A\n",
            "Epoch 4:  45%|████▌     | 577/1271 [03:05<03:42,  3.12it/s, training_loss=0.326]\u001b[A\n",
            "Epoch 4:  45%|████▌     | 577/1271 [03:06<03:42,  3.12it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 4:  45%|████▌     | 578/1271 [03:06<03:41,  3.13it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 4:  45%|████▌     | 578/1271 [03:06<03:41,  3.13it/s, training_loss=0.070]\u001b[A\n",
            "Epoch 4:  46%|████▌     | 579/1271 [03:06<03:42,  3.10it/s, training_loss=0.070]\u001b[A\n",
            "Epoch 4:  46%|████▌     | 579/1271 [03:06<03:42,  3.10it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 4:  46%|████▌     | 580/1271 [03:06<03:42,  3.10it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 4:  46%|████▌     | 580/1271 [03:07<03:42,  3.10it/s, training_loss=0.225]\u001b[A\n",
            "Epoch 4:  46%|████▌     | 581/1271 [03:07<03:42,  3.11it/s, training_loss=0.225]\u001b[A\n",
            "Epoch 4:  46%|████▌     | 581/1271 [03:07<03:42,  3.11it/s, training_loss=0.176]\u001b[A\n",
            "Epoch 4:  46%|████▌     | 582/1271 [03:07<03:40,  3.12it/s, training_loss=0.176]\u001b[A\n",
            "Epoch 4:  46%|████▌     | 582/1271 [03:07<03:40,  3.12it/s, training_loss=0.800]\u001b[A\n",
            "Epoch 4:  46%|████▌     | 583/1271 [03:07<03:41,  3.10it/s, training_loss=0.800]\u001b[A\n",
            "Epoch 4:  46%|████▌     | 583/1271 [03:08<03:41,  3.10it/s, training_loss=0.133]\u001b[A\n",
            "Epoch 4:  46%|████▌     | 584/1271 [03:08<03:43,  3.07it/s, training_loss=0.133]\u001b[A\n",
            "Epoch 4:  46%|████▌     | 584/1271 [03:08<03:43,  3.07it/s, training_loss=0.022]\u001b[A\n",
            "Epoch 4:  46%|████▌     | 585/1271 [03:08<03:42,  3.08it/s, training_loss=0.022]\u001b[A\n",
            "Epoch 4:  46%|████▌     | 585/1271 [03:08<03:42,  3.08it/s, training_loss=0.595]\u001b[A\n",
            "Epoch 4:  46%|████▌     | 586/1271 [03:08<03:42,  3.08it/s, training_loss=0.595]\u001b[A\n",
            "Epoch 4:  46%|████▌     | 586/1271 [03:09<03:42,  3.08it/s, training_loss=0.265]\u001b[A\n",
            "Epoch 4:  46%|████▌     | 587/1271 [03:09<03:41,  3.09it/s, training_loss=0.265]\u001b[A\n",
            "Epoch 4:  46%|████▌     | 587/1271 [03:09<03:41,  3.09it/s, training_loss=0.128]\u001b[A\n",
            "Epoch 4:  46%|████▋     | 588/1271 [03:09<03:40,  3.09it/s, training_loss=0.128]\u001b[A\n",
            "Epoch 4:  46%|████▋     | 588/1271 [03:09<03:40,  3.09it/s, training_loss=0.541]\u001b[A\n",
            "Epoch 4:  46%|████▋     | 589/1271 [03:09<03:39,  3.10it/s, training_loss=0.541]\u001b[A\n",
            "Epoch 4:  46%|████▋     | 589/1271 [03:10<03:39,  3.10it/s, training_loss=0.235]\u001b[A\n",
            "Epoch 4:  46%|████▋     | 590/1271 [03:10<03:38,  3.11it/s, training_loss=0.235]\u001b[A\n",
            "Epoch 4:  46%|████▋     | 590/1271 [03:10<03:38,  3.11it/s, training_loss=0.529]\u001b[A\n",
            "Epoch 4:  46%|████▋     | 591/1271 [03:10<03:38,  3.12it/s, training_loss=0.529]\u001b[A\n",
            "Epoch 4:  46%|████▋     | 591/1271 [03:10<03:38,  3.12it/s, training_loss=0.063]\u001b[A\n",
            "Epoch 4:  47%|████▋     | 592/1271 [03:10<03:37,  3.13it/s, training_loss=0.063]\u001b[A\n",
            "Epoch 4:  47%|████▋     | 592/1271 [03:10<03:37,  3.13it/s, training_loss=0.075]\u001b[A\n",
            "Epoch 4:  47%|████▋     | 593/1271 [03:10<03:36,  3.13it/s, training_loss=0.075]\u001b[A\n",
            "Epoch 4:  47%|████▋     | 593/1271 [03:11<03:36,  3.13it/s, training_loss=0.313]\u001b[A\n",
            "Epoch 4:  47%|████▋     | 594/1271 [03:11<03:37,  3.11it/s, training_loss=0.313]\u001b[A\n",
            "Epoch 4:  47%|████▋     | 594/1271 [03:11<03:37,  3.11it/s, training_loss=0.243]\u001b[A\n",
            "Epoch 4:  47%|████▋     | 595/1271 [03:11<03:36,  3.12it/s, training_loss=0.243]\u001b[A\n",
            "Epoch 4:  47%|████▋     | 595/1271 [03:11<03:36,  3.12it/s, training_loss=0.103]\u001b[A\n",
            "Epoch 4:  47%|████▋     | 596/1271 [03:11<03:36,  3.12it/s, training_loss=0.103]\u001b[A\n",
            "Epoch 4:  47%|████▋     | 596/1271 [03:12<03:36,  3.12it/s, training_loss=0.185]\u001b[A\n",
            "Epoch 4:  47%|████▋     | 597/1271 [03:12<03:36,  3.11it/s, training_loss=0.185]\u001b[A\n",
            "Epoch 4:  47%|████▋     | 597/1271 [03:12<03:36,  3.11it/s, training_loss=0.546]\u001b[A\n",
            "Epoch 4:  47%|████▋     | 598/1271 [03:12<03:38,  3.08it/s, training_loss=0.546]\u001b[A\n",
            "Epoch 4:  47%|████▋     | 598/1271 [03:12<03:38,  3.08it/s, training_loss=0.275]\u001b[A\n",
            "Epoch 4:  47%|████▋     | 599/1271 [03:12<03:37,  3.10it/s, training_loss=0.275]\u001b[A\n",
            "Epoch 4:  47%|████▋     | 599/1271 [03:13<03:37,  3.10it/s, training_loss=0.054]\u001b[A\n",
            "Epoch 4:  47%|████▋     | 600/1271 [03:13<03:35,  3.11it/s, training_loss=0.054]\u001b[A\n",
            "Epoch 4:  47%|████▋     | 600/1271 [03:13<03:35,  3.11it/s, training_loss=0.282]\u001b[A\n",
            "Epoch 4:  47%|████▋     | 601/1271 [03:13<03:36,  3.10it/s, training_loss=0.282]\u001b[A\n",
            "Epoch 4:  47%|████▋     | 601/1271 [03:13<03:36,  3.10it/s, training_loss=0.196]\u001b[A\n",
            "Epoch 4:  47%|████▋     | 602/1271 [03:13<03:35,  3.10it/s, training_loss=0.196]\u001b[A\n",
            "Epoch 4:  47%|████▋     | 602/1271 [03:14<03:35,  3.10it/s, training_loss=0.577]\u001b[A\n",
            "Epoch 4:  47%|████▋     | 603/1271 [03:14<03:35,  3.11it/s, training_loss=0.577]\u001b[A\n",
            "Epoch 4:  47%|████▋     | 603/1271 [03:14<03:35,  3.11it/s, training_loss=0.179]\u001b[A\n",
            "Epoch 4:  48%|████▊     | 604/1271 [03:14<03:35,  3.10it/s, training_loss=0.179]\u001b[A\n",
            "Epoch 4:  48%|████▊     | 604/1271 [03:14<03:35,  3.10it/s, training_loss=0.230]\u001b[A\n",
            "Epoch 4:  48%|████▊     | 605/1271 [03:14<03:34,  3.11it/s, training_loss=0.230]\u001b[A\n",
            "Epoch 4:  48%|████▊     | 605/1271 [03:15<03:34,  3.11it/s, training_loss=0.071]\u001b[A\n",
            "Epoch 4:  48%|████▊     | 606/1271 [03:15<03:34,  3.10it/s, training_loss=0.071]\u001b[A\n",
            "Epoch 4:  48%|████▊     | 606/1271 [03:15<03:34,  3.10it/s, training_loss=0.190]\u001b[A\n",
            "Epoch 4:  48%|████▊     | 607/1271 [03:15<03:34,  3.09it/s, training_loss=0.190]\u001b[A\n",
            "Epoch 4:  48%|████▊     | 607/1271 [03:15<03:34,  3.09it/s, training_loss=0.265]\u001b[A\n",
            "Epoch 4:  48%|████▊     | 608/1271 [03:15<03:34,  3.09it/s, training_loss=0.265]\u001b[A\n",
            "Epoch 4:  48%|████▊     | 608/1271 [03:16<03:34,  3.09it/s, training_loss=0.250]\u001b[A\n",
            "Epoch 4:  48%|████▊     | 609/1271 [03:16<03:34,  3.08it/s, training_loss=0.250]\u001b[A\n",
            "Epoch 4:  48%|████▊     | 609/1271 [03:16<03:34,  3.08it/s, training_loss=0.028]\u001b[A\n",
            "Epoch 4:  48%|████▊     | 610/1271 [03:16<03:33,  3.09it/s, training_loss=0.028]\u001b[A\n",
            "Epoch 4:  48%|████▊     | 610/1271 [03:16<03:33,  3.09it/s, training_loss=0.173]\u001b[A\n",
            "Epoch 4:  48%|████▊     | 611/1271 [03:16<03:33,  3.10it/s, training_loss=0.173]\u001b[A\n",
            "Epoch 4:  48%|████▊     | 611/1271 [03:17<03:33,  3.10it/s, training_loss=0.115]\u001b[A\n",
            "Epoch 4:  48%|████▊     | 612/1271 [03:17<03:31,  3.12it/s, training_loss=0.115]\u001b[A\n",
            "Epoch 4:  48%|████▊     | 612/1271 [03:17<03:31,  3.12it/s, training_loss=0.258]\u001b[A\n",
            "Epoch 4:  48%|████▊     | 613/1271 [03:17<03:31,  3.10it/s, training_loss=0.258]\u001b[A\n",
            "Epoch 4:  48%|████▊     | 613/1271 [03:17<03:31,  3.10it/s, training_loss=0.110]\u001b[A\n",
            "Epoch 4:  48%|████▊     | 614/1271 [03:17<03:31,  3.10it/s, training_loss=0.110]\u001b[A\n",
            "Epoch 4:  48%|████▊     | 614/1271 [03:18<03:31,  3.10it/s, training_loss=0.276]\u001b[A\n",
            "Epoch 4:  48%|████▊     | 615/1271 [03:18<03:31,  3.10it/s, training_loss=0.276]\u001b[A\n",
            "Epoch 4:  48%|████▊     | 615/1271 [03:18<03:31,  3.10it/s, training_loss=0.398]\u001b[A\n",
            "Epoch 4:  48%|████▊     | 616/1271 [03:18<03:31,  3.10it/s, training_loss=0.398]\u001b[A\n",
            "Epoch 4:  48%|████▊     | 616/1271 [03:18<03:31,  3.10it/s, training_loss=0.423]\u001b[A\n",
            "Epoch 4:  49%|████▊     | 617/1271 [03:18<03:30,  3.10it/s, training_loss=0.423]\u001b[A\n",
            "Epoch 4:  49%|████▊     | 617/1271 [03:19<03:30,  3.10it/s, training_loss=0.451]\u001b[A\n",
            "Epoch 4:  49%|████▊     | 618/1271 [03:19<03:30,  3.11it/s, training_loss=0.451]\u001b[A\n",
            "Epoch 4:  49%|████▊     | 618/1271 [03:19<03:30,  3.11it/s, training_loss=0.140]\u001b[A\n",
            "Epoch 4:  49%|████▊     | 619/1271 [03:19<03:30,  3.10it/s, training_loss=0.140]\u001b[A\n",
            "Epoch 4:  49%|████▊     | 619/1271 [03:19<03:30,  3.10it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 4:  49%|████▉     | 620/1271 [03:19<03:29,  3.11it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 4:  49%|████▉     | 620/1271 [03:20<03:29,  3.11it/s, training_loss=0.049]\u001b[A\n",
            "Epoch 4:  49%|████▉     | 621/1271 [03:20<03:29,  3.11it/s, training_loss=0.049]\u001b[A\n",
            "Epoch 4:  49%|████▉     | 621/1271 [03:20<03:29,  3.11it/s, training_loss=0.229]\u001b[A\n",
            "Epoch 4:  49%|████▉     | 622/1271 [03:20<03:28,  3.12it/s, training_loss=0.229]\u001b[A\n",
            "Epoch 4:  49%|████▉     | 622/1271 [03:20<03:28,  3.12it/s, training_loss=0.523]\u001b[A\n",
            "Epoch 4:  49%|████▉     | 623/1271 [03:20<03:27,  3.13it/s, training_loss=0.523]\u001b[A\n",
            "Epoch 4:  49%|████▉     | 623/1271 [03:20<03:27,  3.13it/s, training_loss=0.132]\u001b[A\n",
            "Epoch 4:  49%|████▉     | 624/1271 [03:20<03:28,  3.10it/s, training_loss=0.132]\u001b[A\n",
            "Epoch 4:  49%|████▉     | 624/1271 [03:21<03:28,  3.10it/s, training_loss=0.372]\u001b[A\n",
            "Epoch 4:  49%|████▉     | 625/1271 [03:21<03:28,  3.09it/s, training_loss=0.372]\u001b[A\n",
            "Epoch 4:  49%|████▉     | 625/1271 [03:21<03:28,  3.09it/s, training_loss=0.300]\u001b[A\n",
            "Epoch 4:  49%|████▉     | 626/1271 [03:21<03:27,  3.11it/s, training_loss=0.300]\u001b[A\n",
            "Epoch 4:  49%|████▉     | 626/1271 [03:21<03:27,  3.11it/s, training_loss=0.371]\u001b[A\n",
            "Epoch 4:  49%|████▉     | 627/1271 [03:21<03:27,  3.10it/s, training_loss=0.371]\u001b[A\n",
            "Epoch 4:  49%|████▉     | 627/1271 [03:22<03:27,  3.10it/s, training_loss=0.330]\u001b[A\n",
            "Epoch 4:  49%|████▉     | 628/1271 [03:22<03:27,  3.09it/s, training_loss=0.330]\u001b[A\n",
            "Epoch 4:  49%|████▉     | 628/1271 [03:22<03:27,  3.09it/s, training_loss=0.377]\u001b[A\n",
            "Epoch 4:  49%|████▉     | 629/1271 [03:22<03:28,  3.08it/s, training_loss=0.377]\u001b[A\n",
            "Epoch 4:  49%|████▉     | 629/1271 [03:22<03:28,  3.08it/s, training_loss=0.291]\u001b[A\n",
            "Epoch 4:  50%|████▉     | 630/1271 [03:22<03:28,  3.08it/s, training_loss=0.291]\u001b[A\n",
            "Epoch 4:  50%|████▉     | 630/1271 [03:23<03:28,  3.08it/s, training_loss=0.871]\u001b[A\n",
            "Epoch 4:  50%|████▉     | 631/1271 [03:23<03:28,  3.07it/s, training_loss=0.871]\u001b[A\n",
            "Epoch 4:  50%|████▉     | 631/1271 [03:23<03:28,  3.07it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 4:  50%|████▉     | 632/1271 [03:23<03:28,  3.07it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 4:  50%|████▉     | 632/1271 [03:23<03:28,  3.07it/s, training_loss=0.262]\u001b[A\n",
            "Epoch 4:  50%|████▉     | 633/1271 [03:23<03:26,  3.09it/s, training_loss=0.262]\u001b[A\n",
            "Epoch 4:  50%|████▉     | 633/1271 [03:24<03:26,  3.09it/s, training_loss=0.137]\u001b[A\n",
            "Epoch 4:  50%|████▉     | 634/1271 [03:24<03:27,  3.07it/s, training_loss=0.137]\u001b[A\n",
            "Epoch 4:  50%|████▉     | 634/1271 [03:24<03:27,  3.07it/s, training_loss=0.037]\u001b[A\n",
            "Epoch 4:  50%|████▉     | 635/1271 [03:24<03:26,  3.08it/s, training_loss=0.037]\u001b[A\n",
            "Epoch 4:  50%|████▉     | 635/1271 [03:24<03:26,  3.08it/s, training_loss=0.072]\u001b[A\n",
            "Epoch 4:  50%|█████     | 636/1271 [03:24<03:25,  3.09it/s, training_loss=0.072]\u001b[A\n",
            "Epoch 4:  50%|█████     | 636/1271 [03:25<03:25,  3.09it/s, training_loss=0.168]\u001b[A\n",
            "Epoch 4:  50%|█████     | 637/1271 [03:25<03:24,  3.11it/s, training_loss=0.168]\u001b[A\n",
            "Epoch 4:  50%|█████     | 637/1271 [03:25<03:24,  3.11it/s, training_loss=0.235]\u001b[A\n",
            "Epoch 4:  50%|█████     | 638/1271 [03:25<03:23,  3.12it/s, training_loss=0.235]\u001b[A\n",
            "Epoch 4:  50%|█████     | 638/1271 [03:25<03:23,  3.12it/s, training_loss=0.238]\u001b[A\n",
            "Epoch 4:  50%|█████     | 639/1271 [03:25<03:22,  3.12it/s, training_loss=0.238]\u001b[A\n",
            "Epoch 4:  50%|█████     | 639/1271 [03:26<03:22,  3.12it/s, training_loss=0.714]\u001b[A\n",
            "Epoch 4:  50%|█████     | 640/1271 [03:26<03:22,  3.12it/s, training_loss=0.714]\u001b[A\n",
            "Epoch 4:  50%|█████     | 640/1271 [03:26<03:22,  3.12it/s, training_loss=0.286]\u001b[A\n",
            "Epoch 4:  50%|█████     | 641/1271 [03:26<03:22,  3.12it/s, training_loss=0.286]\u001b[A\n",
            "Epoch 4:  50%|█████     | 641/1271 [03:26<03:22,  3.12it/s, training_loss=0.142]\u001b[A\n",
            "Epoch 4:  51%|█████     | 642/1271 [03:26<03:22,  3.11it/s, training_loss=0.142]\u001b[A\n",
            "Epoch 4:  51%|█████     | 642/1271 [03:27<03:22,  3.11it/s, training_loss=0.383]\u001b[A\n",
            "Epoch 4:  51%|█████     | 643/1271 [03:27<03:21,  3.12it/s, training_loss=0.383]\u001b[A\n",
            "Epoch 4:  51%|█████     | 643/1271 [03:27<03:21,  3.12it/s, training_loss=0.578]\u001b[A\n",
            "Epoch 4:  51%|█████     | 644/1271 [03:27<03:20,  3.13it/s, training_loss=0.578]\u001b[A\n",
            "Epoch 4:  51%|█████     | 644/1271 [03:27<03:20,  3.13it/s, training_loss=0.270]\u001b[A\n",
            "Epoch 4:  51%|█████     | 645/1271 [03:27<03:20,  3.12it/s, training_loss=0.270]\u001b[A\n",
            "Epoch 4:  51%|█████     | 645/1271 [03:28<03:20,  3.12it/s, training_loss=0.128]\u001b[A\n",
            "Epoch 4:  51%|█████     | 646/1271 [03:28<03:20,  3.12it/s, training_loss=0.128]\u001b[A\n",
            "Epoch 4:  51%|█████     | 646/1271 [03:28<03:20,  3.12it/s, training_loss=0.422]\u001b[A\n",
            "Epoch 4:  51%|█████     | 647/1271 [03:28<03:20,  3.12it/s, training_loss=0.422]\u001b[A\n",
            "Epoch 4:  51%|█████     | 647/1271 [03:28<03:20,  3.12it/s, training_loss=0.768]\u001b[A\n",
            "Epoch 4:  51%|█████     | 648/1271 [03:28<03:20,  3.11it/s, training_loss=0.768]\u001b[A\n",
            "Epoch 4:  51%|█████     | 648/1271 [03:29<03:20,  3.11it/s, training_loss=0.076]\u001b[A\n",
            "Epoch 4:  51%|█████     | 649/1271 [03:29<03:19,  3.11it/s, training_loss=0.076]\u001b[A\n",
            "Epoch 4:  51%|█████     | 649/1271 [03:29<03:19,  3.11it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 4:  51%|█████     | 650/1271 [03:29<03:18,  3.12it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 4:  51%|█████     | 650/1271 [03:29<03:18,  3.12it/s, training_loss=0.142]\u001b[A\n",
            "Epoch 4:  51%|█████     | 651/1271 [03:29<03:19,  3.11it/s, training_loss=0.142]\u001b[A\n",
            "Epoch 4:  51%|█████     | 651/1271 [03:29<03:19,  3.11it/s, training_loss=0.115]\u001b[A\n",
            "Epoch 4:  51%|█████▏    | 652/1271 [03:30<03:19,  3.10it/s, training_loss=0.115]\u001b[A\n",
            "Epoch 4:  51%|█████▏    | 652/1271 [03:30<03:19,  3.10it/s, training_loss=0.108]\u001b[A\n",
            "Epoch 4:  51%|█████▏    | 653/1271 [03:30<03:19,  3.10it/s, training_loss=0.108]\u001b[A\n",
            "Epoch 4:  51%|█████▏    | 653/1271 [03:30<03:19,  3.10it/s, training_loss=0.125]\u001b[A\n",
            "Epoch 4:  51%|█████▏    | 654/1271 [03:30<03:18,  3.11it/s, training_loss=0.125]\u001b[A\n",
            "Epoch 4:  51%|█████▏    | 654/1271 [03:30<03:18,  3.11it/s, training_loss=0.624]\u001b[A\n",
            "Epoch 4:  52%|█████▏    | 655/1271 [03:30<03:18,  3.11it/s, training_loss=0.624]\u001b[A\n",
            "Epoch 4:  52%|█████▏    | 655/1271 [03:31<03:18,  3.11it/s, training_loss=0.095]\u001b[A\n",
            "Epoch 4:  52%|█████▏    | 656/1271 [03:31<03:17,  3.11it/s, training_loss=0.095]\u001b[A\n",
            "Epoch 4:  52%|█████▏    | 656/1271 [03:31<03:17,  3.11it/s, training_loss=0.529]\u001b[A\n",
            "Epoch 4:  52%|█████▏    | 657/1271 [03:31<03:16,  3.12it/s, training_loss=0.529]\u001b[A\n",
            "Epoch 4:  52%|█████▏    | 657/1271 [03:31<03:16,  3.12it/s, training_loss=0.319]\u001b[A\n",
            "Epoch 4:  52%|█████▏    | 658/1271 [03:31<03:17,  3.11it/s, training_loss=0.319]\u001b[A\n",
            "Epoch 4:  52%|█████▏    | 658/1271 [03:32<03:17,  3.11it/s, training_loss=0.475]\u001b[A\n",
            "Epoch 4:  52%|█████▏    | 659/1271 [03:32<03:16,  3.11it/s, training_loss=0.475]\u001b[A\n",
            "Epoch 4:  52%|█████▏    | 659/1271 [03:32<03:16,  3.11it/s, training_loss=0.293]\u001b[A\n",
            "Epoch 4:  52%|█████▏    | 660/1271 [03:32<03:16,  3.10it/s, training_loss=0.293]\u001b[A\n",
            "Epoch 4:  52%|█████▏    | 660/1271 [03:32<03:16,  3.10it/s, training_loss=0.317]\u001b[A\n",
            "Epoch 4:  52%|█████▏    | 661/1271 [03:32<03:16,  3.11it/s, training_loss=0.317]\u001b[A\n",
            "Epoch 4:  52%|█████▏    | 661/1271 [03:33<03:16,  3.11it/s, training_loss=0.589]\u001b[A\n",
            "Epoch 4:  52%|█████▏    | 662/1271 [03:33<03:15,  3.11it/s, training_loss=0.589]\u001b[A\n",
            "Epoch 4:  52%|█████▏    | 662/1271 [03:33<03:15,  3.11it/s, training_loss=0.428]\u001b[A\n",
            "Epoch 4:  52%|█████▏    | 663/1271 [03:33<03:16,  3.09it/s, training_loss=0.428]\u001b[A\n",
            "Epoch 4:  52%|█████▏    | 663/1271 [03:33<03:16,  3.09it/s, training_loss=0.087]\u001b[A\n",
            "Epoch 4:  52%|█████▏    | 664/1271 [03:33<03:17,  3.08it/s, training_loss=0.087]\u001b[A\n",
            "Epoch 4:  52%|█████▏    | 664/1271 [03:34<03:17,  3.08it/s, training_loss=0.208]\u001b[A\n",
            "Epoch 4:  52%|█████▏    | 665/1271 [03:34<03:16,  3.09it/s, training_loss=0.208]\u001b[A\n",
            "Epoch 4:  52%|█████▏    | 665/1271 [03:34<03:16,  3.09it/s, training_loss=0.363]\u001b[A\n",
            "Epoch 4:  52%|█████▏    | 666/1271 [03:34<03:15,  3.09it/s, training_loss=0.363]\u001b[A\n",
            "Epoch 4:  52%|█████▏    | 666/1271 [03:34<03:15,  3.09it/s, training_loss=0.019]\u001b[A\n",
            "Epoch 4:  52%|█████▏    | 667/1271 [03:34<03:16,  3.08it/s, training_loss=0.019]\u001b[A\n",
            "Epoch 4:  52%|█████▏    | 667/1271 [03:35<03:16,  3.08it/s, training_loss=0.137]\u001b[A\n",
            "Epoch 4:  53%|█████▎    | 668/1271 [03:35<03:15,  3.09it/s, training_loss=0.137]\u001b[A\n",
            "Epoch 4:  53%|█████▎    | 668/1271 [03:35<03:15,  3.09it/s, training_loss=0.230]\u001b[A\n",
            "Epoch 4:  53%|█████▎    | 669/1271 [03:35<03:14,  3.09it/s, training_loss=0.230]\u001b[A\n",
            "Epoch 4:  53%|█████▎    | 669/1271 [03:35<03:14,  3.09it/s, training_loss=0.339]\u001b[A\n",
            "Epoch 4:  53%|█████▎    | 670/1271 [03:35<03:14,  3.08it/s, training_loss=0.339]\u001b[A\n",
            "Epoch 4:  53%|█████▎    | 670/1271 [03:36<03:14,  3.08it/s, training_loss=0.130]\u001b[A\n",
            "Epoch 4:  53%|█████▎    | 671/1271 [03:36<03:14,  3.09it/s, training_loss=0.130]\u001b[A\n",
            "Epoch 4:  53%|█████▎    | 671/1271 [03:36<03:14,  3.09it/s, training_loss=0.088]\u001b[A\n",
            "Epoch 4:  53%|█████▎    | 672/1271 [03:36<03:12,  3.11it/s, training_loss=0.088]\u001b[A\n",
            "Epoch 4:  53%|█████▎    | 672/1271 [03:36<03:12,  3.11it/s, training_loss=0.145]\u001b[A\n",
            "Epoch 4:  53%|█████▎    | 673/1271 [03:36<03:12,  3.11it/s, training_loss=0.145]\u001b[A\n",
            "Epoch 4:  53%|█████▎    | 673/1271 [03:37<03:12,  3.11it/s, training_loss=0.422]\u001b[A\n",
            "Epoch 4:  53%|█████▎    | 674/1271 [03:37<03:12,  3.10it/s, training_loss=0.422]\u001b[A\n",
            "Epoch 4:  53%|█████▎    | 674/1271 [03:37<03:12,  3.10it/s, training_loss=0.477]\u001b[A\n",
            "Epoch 4:  53%|█████▎    | 675/1271 [03:37<03:11,  3.11it/s, training_loss=0.477]\u001b[A\n",
            "Epoch 4:  53%|█████▎    | 675/1271 [03:37<03:11,  3.11it/s, training_loss=0.174]\u001b[A\n",
            "Epoch 4:  53%|█████▎    | 676/1271 [03:37<03:11,  3.11it/s, training_loss=0.174]\u001b[A\n",
            "Epoch 4:  53%|█████▎    | 676/1271 [03:38<03:11,  3.11it/s, training_loss=0.340]\u001b[A\n",
            "Epoch 4:  53%|█████▎    | 677/1271 [03:38<03:11,  3.11it/s, training_loss=0.340]\u001b[A\n",
            "Epoch 4:  53%|█████▎    | 677/1271 [03:38<03:11,  3.11it/s, training_loss=0.337]\u001b[A\n",
            "Epoch 4:  53%|█████▎    | 678/1271 [03:38<03:10,  3.11it/s, training_loss=0.337]\u001b[A\n",
            "Epoch 4:  53%|█████▎    | 678/1271 [03:38<03:10,  3.11it/s, training_loss=0.326]\u001b[A\n",
            "Epoch 4:  53%|█████▎    | 679/1271 [03:38<03:10,  3.10it/s, training_loss=0.326]\u001b[A\n",
            "Epoch 4:  53%|█████▎    | 679/1271 [03:39<03:10,  3.10it/s, training_loss=0.283]\u001b[A\n",
            "Epoch 4:  54%|█████▎    | 680/1271 [03:39<03:10,  3.11it/s, training_loss=0.283]\u001b[A\n",
            "Epoch 4:  54%|█████▎    | 680/1271 [03:39<03:10,  3.11it/s, training_loss=0.253]\u001b[A\n",
            "Epoch 4:  54%|█████▎    | 681/1271 [03:39<03:10,  3.10it/s, training_loss=0.253]\u001b[A\n",
            "Epoch 4:  54%|█████▎    | 681/1271 [03:39<03:10,  3.10it/s, training_loss=0.093]\u001b[A\n",
            "Epoch 4:  54%|█████▎    | 682/1271 [03:39<03:08,  3.12it/s, training_loss=0.093]\u001b[A\n",
            "Epoch 4:  54%|█████▎    | 682/1271 [03:39<03:08,  3.12it/s, training_loss=0.131]\u001b[A\n",
            "Epoch 4:  54%|█████▎    | 683/1271 [03:39<03:08,  3.13it/s, training_loss=0.131]\u001b[A\n",
            "Epoch 4:  54%|█████▎    | 683/1271 [03:40<03:08,  3.13it/s, training_loss=0.496]\u001b[A\n",
            "Epoch 4:  54%|█████▍    | 684/1271 [03:40<03:08,  3.11it/s, training_loss=0.496]\u001b[A\n",
            "Epoch 4:  54%|█████▍    | 684/1271 [03:40<03:08,  3.11it/s, training_loss=0.208]\u001b[A\n",
            "Epoch 4:  54%|█████▍    | 685/1271 [03:40<03:09,  3.10it/s, training_loss=0.208]\u001b[A\n",
            "Epoch 4:  54%|█████▍    | 685/1271 [03:40<03:09,  3.10it/s, training_loss=0.601]\u001b[A\n",
            "Epoch 4:  54%|█████▍    | 686/1271 [03:40<03:08,  3.11it/s, training_loss=0.601]\u001b[A\n",
            "Epoch 4:  54%|█████▍    | 686/1271 [03:41<03:08,  3.11it/s, training_loss=0.078]\u001b[A\n",
            "Epoch 4:  54%|█████▍    | 687/1271 [03:41<03:08,  3.10it/s, training_loss=0.078]\u001b[A\n",
            "Epoch 4:  54%|█████▍    | 687/1271 [03:41<03:08,  3.10it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 4:  54%|█████▍    | 688/1271 [03:41<03:08,  3.10it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 4:  54%|█████▍    | 688/1271 [03:41<03:08,  3.10it/s, training_loss=0.068]\u001b[A\n",
            "Epoch 4:  54%|█████▍    | 689/1271 [03:41<03:07,  3.11it/s, training_loss=0.068]\u001b[A\n",
            "Epoch 4:  54%|█████▍    | 689/1271 [03:42<03:07,  3.11it/s, training_loss=0.478]\u001b[A\n",
            "Epoch 4:  54%|█████▍    | 690/1271 [03:42<03:07,  3.10it/s, training_loss=0.478]\u001b[A\n",
            "Epoch 4:  54%|█████▍    | 690/1271 [03:42<03:07,  3.10it/s, training_loss=0.108]\u001b[A\n",
            "Epoch 4:  54%|█████▍    | 691/1271 [03:42<03:08,  3.08it/s, training_loss=0.108]\u001b[A\n",
            "Epoch 4:  54%|█████▍    | 691/1271 [03:42<03:08,  3.08it/s, training_loss=0.338]\u001b[A\n",
            "Epoch 4:  54%|█████▍    | 692/1271 [03:42<03:08,  3.07it/s, training_loss=0.338]\u001b[A\n",
            "Epoch 4:  54%|█████▍    | 692/1271 [03:43<03:08,  3.07it/s, training_loss=0.133]\u001b[A\n",
            "Epoch 4:  55%|█████▍    | 693/1271 [03:43<03:06,  3.10it/s, training_loss=0.133]\u001b[A\n",
            "Epoch 4:  55%|█████▍    | 693/1271 [03:43<03:06,  3.10it/s, training_loss=0.061]\u001b[A\n",
            "Epoch 4:  55%|█████▍    | 694/1271 [03:43<03:05,  3.10it/s, training_loss=0.061]\u001b[A\n",
            "Epoch 4:  55%|█████▍    | 694/1271 [03:43<03:05,  3.10it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 4:  55%|█████▍    | 695/1271 [03:43<03:05,  3.10it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 4:  55%|█████▍    | 695/1271 [03:44<03:05,  3.10it/s, training_loss=0.479]\u001b[A\n",
            "Epoch 4:  55%|█████▍    | 696/1271 [03:44<03:05,  3.10it/s, training_loss=0.479]\u001b[A\n",
            "Epoch 4:  55%|█████▍    | 696/1271 [03:44<03:05,  3.10it/s, training_loss=0.178]\u001b[A\n",
            "Epoch 4:  55%|█████▍    | 697/1271 [03:44<03:04,  3.12it/s, training_loss=0.178]\u001b[A\n",
            "Epoch 4:  55%|█████▍    | 697/1271 [03:44<03:04,  3.12it/s, training_loss=0.172]\u001b[A\n",
            "Epoch 4:  55%|█████▍    | 698/1271 [03:44<03:05,  3.09it/s, training_loss=0.172]\u001b[A\n",
            "Epoch 4:  55%|█████▍    | 698/1271 [03:45<03:05,  3.09it/s, training_loss=0.351]\u001b[A\n",
            "Epoch 4:  55%|█████▍    | 699/1271 [03:45<03:04,  3.10it/s, training_loss=0.351]\u001b[A\n",
            "Epoch 4:  55%|█████▍    | 699/1271 [03:45<03:04,  3.10it/s, training_loss=0.098]\u001b[A\n",
            "Epoch 4:  55%|█████▌    | 700/1271 [03:45<03:05,  3.08it/s, training_loss=0.098]\u001b[A\n",
            "Epoch 4:  55%|█████▌    | 700/1271 [03:45<03:05,  3.08it/s, training_loss=0.408]\u001b[A\n",
            "Epoch 4:  55%|█████▌    | 701/1271 [03:45<03:04,  3.10it/s, training_loss=0.408]\u001b[A\n",
            "Epoch 4:  55%|█████▌    | 701/1271 [03:46<03:04,  3.10it/s, training_loss=0.015]\u001b[A\n",
            "Epoch 4:  55%|█████▌    | 702/1271 [03:46<03:03,  3.10it/s, training_loss=0.015]\u001b[A\n",
            "Epoch 4:  55%|█████▌    | 702/1271 [03:46<03:03,  3.10it/s, training_loss=0.350]\u001b[A\n",
            "Epoch 4:  55%|█████▌    | 703/1271 [03:46<03:04,  3.09it/s, training_loss=0.350]\u001b[A\n",
            "Epoch 4:  55%|█████▌    | 703/1271 [03:46<03:04,  3.09it/s, training_loss=0.182]\u001b[A\n",
            "Epoch 4:  55%|█████▌    | 704/1271 [03:46<03:02,  3.10it/s, training_loss=0.182]\u001b[A\n",
            "Epoch 4:  55%|█████▌    | 704/1271 [03:47<03:02,  3.10it/s, training_loss=0.083]\u001b[A\n",
            "Epoch 4:  55%|█████▌    | 705/1271 [03:47<03:02,  3.11it/s, training_loss=0.083]\u001b[A\n",
            "Epoch 4:  55%|█████▌    | 705/1271 [03:47<03:02,  3.11it/s, training_loss=0.031]\u001b[A\n",
            "Epoch 4:  56%|█████▌    | 706/1271 [03:47<03:02,  3.10it/s, training_loss=0.031]\u001b[A\n",
            "Epoch 4:  56%|█████▌    | 706/1271 [03:47<03:02,  3.10it/s, training_loss=0.058]\u001b[A\n",
            "Epoch 4:  56%|█████▌    | 707/1271 [03:47<03:02,  3.09it/s, training_loss=0.058]\u001b[A\n",
            "Epoch 4:  56%|█████▌    | 707/1271 [03:48<03:02,  3.09it/s, training_loss=0.377]\u001b[A\n",
            "Epoch 4:  56%|█████▌    | 708/1271 [03:48<03:02,  3.09it/s, training_loss=0.377]\u001b[A\n",
            "Epoch 4:  56%|█████▌    | 708/1271 [03:48<03:02,  3.09it/s, training_loss=0.128]\u001b[A\n",
            "Epoch 4:  56%|█████▌    | 709/1271 [03:48<03:00,  3.11it/s, training_loss=0.128]\u001b[A\n",
            "Epoch 4:  56%|█████▌    | 709/1271 [03:48<03:00,  3.11it/s, training_loss=0.268]\u001b[A\n",
            "Epoch 4:  56%|█████▌    | 710/1271 [03:48<03:00,  3.11it/s, training_loss=0.268]\u001b[A\n",
            "Epoch 4:  56%|█████▌    | 710/1271 [03:49<03:00,  3.11it/s, training_loss=0.150]\u001b[A\n",
            "Epoch 4:  56%|█████▌    | 711/1271 [03:49<03:00,  3.10it/s, training_loss=0.150]\u001b[A\n",
            "Epoch 4:  56%|█████▌    | 711/1271 [03:49<03:00,  3.10it/s, training_loss=0.128]\u001b[A\n",
            "Epoch 4:  56%|█████▌    | 712/1271 [03:49<02:59,  3.11it/s, training_loss=0.128]\u001b[A\n",
            "Epoch 4:  56%|█████▌    | 712/1271 [03:49<02:59,  3.11it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 4:  56%|█████▌    | 713/1271 [03:49<02:58,  3.12it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 4:  56%|█████▌    | 713/1271 [03:49<02:58,  3.12it/s, training_loss=0.329]\u001b[A\n",
            "Epoch 4:  56%|█████▌    | 714/1271 [03:49<02:58,  3.11it/s, training_loss=0.329]\u001b[A\n",
            "Epoch 4:  56%|█████▌    | 714/1271 [03:50<02:58,  3.11it/s, training_loss=0.135]\u001b[A\n",
            "Epoch 4:  56%|█████▋    | 715/1271 [03:50<02:59,  3.10it/s, training_loss=0.135]\u001b[A\n",
            "Epoch 4:  56%|█████▋    | 715/1271 [03:50<02:59,  3.10it/s, training_loss=0.047]\u001b[A\n",
            "Epoch 4:  56%|█████▋    | 716/1271 [03:50<02:58,  3.10it/s, training_loss=0.047]\u001b[A\n",
            "Epoch 4:  56%|█████▋    | 716/1271 [03:50<02:58,  3.10it/s, training_loss=0.168]\u001b[A\n",
            "Epoch 4:  56%|█████▋    | 717/1271 [03:50<02:58,  3.10it/s, training_loss=0.168]\u001b[A\n",
            "Epoch 4:  56%|█████▋    | 717/1271 [03:51<02:58,  3.10it/s, training_loss=0.600]\u001b[A\n",
            "Epoch 4:  56%|█████▋    | 718/1271 [03:51<02:58,  3.10it/s, training_loss=0.600]\u001b[A\n",
            "Epoch 4:  56%|█████▋    | 718/1271 [03:51<02:58,  3.10it/s, training_loss=0.377]\u001b[A\n",
            "Epoch 4:  57%|█████▋    | 719/1271 [03:51<02:57,  3.10it/s, training_loss=0.377]\u001b[A\n",
            "Epoch 4:  57%|█████▋    | 719/1271 [03:51<02:57,  3.10it/s, training_loss=0.219]\u001b[A\n",
            "Epoch 4:  57%|█████▋    | 720/1271 [03:51<02:57,  3.11it/s, training_loss=0.219]\u001b[A\n",
            "Epoch 4:  57%|█████▋    | 720/1271 [03:52<02:57,  3.11it/s, training_loss=0.082]\u001b[A\n",
            "Epoch 4:  57%|█████▋    | 721/1271 [03:52<02:56,  3.11it/s, training_loss=0.082]\u001b[A\n",
            "Epoch 4:  57%|█████▋    | 721/1271 [03:52<02:56,  3.11it/s, training_loss=0.279]\u001b[A\n",
            "Epoch 4:  57%|█████▋    | 722/1271 [03:52<02:55,  3.12it/s, training_loss=0.279]\u001b[A\n",
            "Epoch 4:  57%|█████▋    | 722/1271 [03:52<02:55,  3.12it/s, training_loss=0.056]\u001b[A\n",
            "Epoch 4:  57%|█████▋    | 723/1271 [03:52<02:55,  3.12it/s, training_loss=0.056]\u001b[A\n",
            "Epoch 4:  57%|█████▋    | 723/1271 [03:53<02:55,  3.12it/s, training_loss=0.437]\u001b[A\n",
            "Epoch 4:  57%|█████▋    | 724/1271 [03:53<02:55,  3.11it/s, training_loss=0.437]\u001b[A\n",
            "Epoch 4:  57%|█████▋    | 724/1271 [03:53<02:55,  3.11it/s, training_loss=0.440]\u001b[A\n",
            "Epoch 4:  57%|█████▋    | 725/1271 [03:53<02:55,  3.10it/s, training_loss=0.440]\u001b[A\n",
            "Epoch 4:  57%|█████▋    | 725/1271 [03:53<02:55,  3.10it/s, training_loss=0.090]\u001b[A\n",
            "Epoch 4:  57%|█████▋    | 726/1271 [03:53<02:55,  3.11it/s, training_loss=0.090]\u001b[A\n",
            "Epoch 4:  57%|█████▋    | 726/1271 [03:54<02:55,  3.11it/s, training_loss=0.485]\u001b[A\n",
            "Epoch 4:  57%|█████▋    | 727/1271 [03:54<02:55,  3.11it/s, training_loss=0.485]\u001b[A\n",
            "Epoch 4:  57%|█████▋    | 727/1271 [03:54<02:55,  3.11it/s, training_loss=0.173]\u001b[A\n",
            "Epoch 4:  57%|█████▋    | 728/1271 [03:54<02:55,  3.09it/s, training_loss=0.173]\u001b[A\n",
            "Epoch 4:  57%|█████▋    | 728/1271 [03:54<02:55,  3.09it/s, training_loss=0.352]\u001b[A\n",
            "Epoch 4:  57%|█████▋    | 729/1271 [03:54<02:55,  3.09it/s, training_loss=0.352]\u001b[A\n",
            "Epoch 4:  57%|█████▋    | 729/1271 [03:55<02:55,  3.09it/s, training_loss=0.368]\u001b[A\n",
            "Epoch 4:  57%|█████▋    | 730/1271 [03:55<02:55,  3.09it/s, training_loss=0.368]\u001b[A\n",
            "Epoch 4:  57%|█████▋    | 730/1271 [03:55<02:55,  3.09it/s, training_loss=0.386]\u001b[A\n",
            "Epoch 4:  58%|█████▊    | 731/1271 [03:55<02:54,  3.09it/s, training_loss=0.386]\u001b[A\n",
            "Epoch 4:  58%|█████▊    | 731/1271 [03:55<02:54,  3.09it/s, training_loss=0.309]\u001b[A\n",
            "Epoch 4:  58%|█████▊    | 732/1271 [03:55<02:53,  3.10it/s, training_loss=0.309]\u001b[A\n",
            "Epoch 4:  58%|█████▊    | 732/1271 [03:56<02:53,  3.10it/s, training_loss=0.102]\u001b[A\n",
            "Epoch 4:  58%|█████▊    | 733/1271 [03:56<02:52,  3.12it/s, training_loss=0.102]\u001b[A\n",
            "Epoch 4:  58%|█████▊    | 733/1271 [03:56<02:52,  3.12it/s, training_loss=0.099]\u001b[A\n",
            "Epoch 4:  58%|█████▊    | 734/1271 [03:56<02:51,  3.12it/s, training_loss=0.099]\u001b[A\n",
            "Epoch 4:  58%|█████▊    | 734/1271 [03:56<02:51,  3.12it/s, training_loss=0.291]\u001b[A\n",
            "Epoch 4:  58%|█████▊    | 735/1271 [03:56<02:52,  3.11it/s, training_loss=0.291]\u001b[A\n",
            "Epoch 4:  58%|█████▊    | 735/1271 [03:57<02:52,  3.11it/s, training_loss=0.091]\u001b[A\n",
            "Epoch 4:  58%|█████▊    | 736/1271 [03:57<02:51,  3.11it/s, training_loss=0.091]\u001b[A\n",
            "Epoch 4:  58%|█████▊    | 736/1271 [03:57<02:51,  3.11it/s, training_loss=0.666]\u001b[A\n",
            "Epoch 4:  58%|█████▊    | 737/1271 [03:57<02:51,  3.11it/s, training_loss=0.666]\u001b[A\n",
            "Epoch 4:  58%|█████▊    | 737/1271 [03:57<02:51,  3.11it/s, training_loss=0.544]\u001b[A\n",
            "Epoch 4:  58%|█████▊    | 738/1271 [03:57<02:51,  3.10it/s, training_loss=0.544]\u001b[A\n",
            "Epoch 4:  58%|█████▊    | 738/1271 [03:58<02:51,  3.10it/s, training_loss=0.238]\u001b[A\n",
            "Epoch 4:  58%|█████▊    | 739/1271 [03:58<02:52,  3.08it/s, training_loss=0.238]\u001b[A\n",
            "Epoch 4:  58%|█████▊    | 739/1271 [03:58<02:52,  3.08it/s, training_loss=0.067]\u001b[A\n",
            "Epoch 4:  58%|█████▊    | 740/1271 [03:58<02:51,  3.09it/s, training_loss=0.067]\u001b[A\n",
            "Epoch 4:  58%|█████▊    | 740/1271 [03:58<02:51,  3.09it/s, training_loss=0.268]\u001b[A\n",
            "Epoch 4:  58%|█████▊    | 741/1271 [03:58<02:51,  3.09it/s, training_loss=0.268]\u001b[A\n",
            "Epoch 4:  58%|█████▊    | 741/1271 [03:59<02:51,  3.09it/s, training_loss=0.092]\u001b[A\n",
            "Epoch 4:  58%|█████▊    | 742/1271 [03:59<02:50,  3.11it/s, training_loss=0.092]\u001b[A\n",
            "Epoch 4:  58%|█████▊    | 742/1271 [03:59<02:50,  3.11it/s, training_loss=0.291]\u001b[A\n",
            "Epoch 4:  58%|█████▊    | 743/1271 [03:59<02:49,  3.11it/s, training_loss=0.291]\u001b[A\n",
            "Epoch 4:  58%|█████▊    | 743/1271 [03:59<02:49,  3.11it/s, training_loss=0.257]\u001b[A\n",
            "Epoch 4:  59%|█████▊    | 744/1271 [03:59<02:48,  3.12it/s, training_loss=0.257]\u001b[A\n",
            "Epoch 4:  59%|█████▊    | 744/1271 [03:59<02:48,  3.12it/s, training_loss=0.473]\u001b[A\n",
            "Epoch 4:  59%|█████▊    | 745/1271 [03:59<02:48,  3.12it/s, training_loss=0.473]\u001b[A\n",
            "Epoch 4:  59%|█████▊    | 745/1271 [04:00<02:48,  3.12it/s, training_loss=0.112]\u001b[A\n",
            "Epoch 4:  59%|█████▊    | 746/1271 [04:00<02:48,  3.12it/s, training_loss=0.112]\u001b[A\n",
            "Epoch 4:  59%|█████▊    | 746/1271 [04:00<02:48,  3.12it/s, training_loss=0.149]\u001b[A\n",
            "Epoch 4:  59%|█████▉    | 747/1271 [04:00<02:47,  3.12it/s, training_loss=0.149]\u001b[A\n",
            "Epoch 4:  59%|█████▉    | 747/1271 [04:00<02:47,  3.12it/s, training_loss=0.059]\u001b[A\n",
            "Epoch 4:  59%|█████▉    | 748/1271 [04:00<02:47,  3.12it/s, training_loss=0.059]\u001b[A\n",
            "Epoch 4:  59%|█████▉    | 748/1271 [04:01<02:47,  3.12it/s, training_loss=0.328]\u001b[A\n",
            "Epoch 4:  59%|█████▉    | 749/1271 [04:01<02:46,  3.14it/s, training_loss=0.328]\u001b[A\n",
            "Epoch 4:  59%|█████▉    | 749/1271 [04:01<02:46,  3.14it/s, training_loss=0.361]\u001b[A\n",
            "Epoch 4:  59%|█████▉    | 750/1271 [04:01<02:46,  3.13it/s, training_loss=0.361]\u001b[A\n",
            "Epoch 4:  59%|█████▉    | 750/1271 [04:01<02:46,  3.13it/s, training_loss=0.016]\u001b[A\n",
            "Epoch 4:  59%|█████▉    | 751/1271 [04:01<02:46,  3.12it/s, training_loss=0.016]\u001b[A\n",
            "Epoch 4:  59%|█████▉    | 751/1271 [04:02<02:46,  3.12it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 4:  59%|█████▉    | 752/1271 [04:02<02:46,  3.13it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 4:  59%|█████▉    | 752/1271 [04:02<02:46,  3.13it/s, training_loss=0.071]\u001b[A\n",
            "Epoch 4:  59%|█████▉    | 753/1271 [04:02<02:46,  3.11it/s, training_loss=0.071]\u001b[A\n",
            "Epoch 4:  59%|█████▉    | 753/1271 [04:02<02:46,  3.11it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 4:  59%|█████▉    | 754/1271 [04:02<02:45,  3.12it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 4:  59%|█████▉    | 754/1271 [04:03<02:45,  3.12it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 4:  59%|█████▉    | 755/1271 [04:03<02:44,  3.13it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 4:  59%|█████▉    | 755/1271 [04:03<02:44,  3.13it/s, training_loss=0.335]\u001b[A\n",
            "Epoch 4:  59%|█████▉    | 756/1271 [04:03<02:44,  3.12it/s, training_loss=0.335]\u001b[A\n",
            "Epoch 4:  59%|█████▉    | 756/1271 [04:03<02:44,  3.12it/s, training_loss=0.570]\u001b[A\n",
            "Epoch 4:  60%|█████▉    | 757/1271 [04:03<02:44,  3.12it/s, training_loss=0.570]\u001b[A\n",
            "Epoch 4:  60%|█████▉    | 757/1271 [04:04<02:44,  3.12it/s, training_loss=0.182]\u001b[A\n",
            "Epoch 4:  60%|█████▉    | 758/1271 [04:04<02:44,  3.11it/s, training_loss=0.182]\u001b[A\n",
            "Epoch 4:  60%|█████▉    | 758/1271 [04:04<02:44,  3.11it/s, training_loss=0.302]\u001b[A\n",
            "Epoch 4:  60%|█████▉    | 759/1271 [04:04<02:44,  3.12it/s, training_loss=0.302]\u001b[A\n",
            "Epoch 4:  60%|█████▉    | 759/1271 [04:04<02:44,  3.12it/s, training_loss=0.396]\u001b[A\n",
            "Epoch 4:  60%|█████▉    | 760/1271 [04:04<02:43,  3.12it/s, training_loss=0.396]\u001b[A\n",
            "Epoch 4:  60%|█████▉    | 760/1271 [04:05<02:43,  3.12it/s, training_loss=0.056]\u001b[A\n",
            "Epoch 4:  60%|█████▉    | 761/1271 [04:05<02:43,  3.11it/s, training_loss=0.056]\u001b[A\n",
            "Epoch 4:  60%|█████▉    | 761/1271 [04:05<02:43,  3.11it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 4:  60%|█████▉    | 762/1271 [04:05<02:43,  3.12it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 4:  60%|█████▉    | 762/1271 [04:05<02:43,  3.12it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 4:  60%|██████    | 763/1271 [04:05<02:42,  3.12it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 4:  60%|██████    | 763/1271 [04:06<02:42,  3.12it/s, training_loss=0.496]\u001b[A\n",
            "Epoch 4:  60%|██████    | 764/1271 [04:06<02:43,  3.10it/s, training_loss=0.496]\u001b[A\n",
            "Epoch 4:  60%|██████    | 764/1271 [04:06<02:43,  3.10it/s, training_loss=0.287]\u001b[A\n",
            "Epoch 4:  60%|██████    | 765/1271 [04:06<02:42,  3.11it/s, training_loss=0.287]\u001b[A\n",
            "Epoch 4:  60%|██████    | 765/1271 [04:06<02:42,  3.11it/s, training_loss=0.125]\u001b[A\n",
            "Epoch 4:  60%|██████    | 766/1271 [04:06<02:42,  3.10it/s, training_loss=0.125]\u001b[A\n",
            "Epoch 4:  60%|██████    | 766/1271 [04:07<02:42,  3.10it/s, training_loss=0.317]\u001b[A\n",
            "Epoch 4:  60%|██████    | 767/1271 [04:07<02:43,  3.09it/s, training_loss=0.317]\u001b[A\n",
            "Epoch 4:  60%|██████    | 767/1271 [04:07<02:43,  3.09it/s, training_loss=0.248]\u001b[A\n",
            "Epoch 4:  60%|██████    | 768/1271 [04:07<02:43,  3.08it/s, training_loss=0.248]\u001b[A\n",
            "Epoch 4:  60%|██████    | 768/1271 [04:07<02:43,  3.08it/s, training_loss=0.393]\u001b[A\n",
            "Epoch 4:  61%|██████    | 769/1271 [04:07<02:42,  3.09it/s, training_loss=0.393]\u001b[A\n",
            "Epoch 4:  61%|██████    | 769/1271 [04:08<02:42,  3.09it/s, training_loss=0.025]\u001b[A\n",
            "Epoch 4:  61%|██████    | 770/1271 [04:08<02:42,  3.09it/s, training_loss=0.025]\u001b[A\n",
            "Epoch 4:  61%|██████    | 770/1271 [04:08<02:42,  3.09it/s, training_loss=0.092]\u001b[A\n",
            "Epoch 4:  61%|██████    | 771/1271 [04:08<02:41,  3.09it/s, training_loss=0.092]\u001b[A\n",
            "Epoch 4:  61%|██████    | 771/1271 [04:08<02:41,  3.09it/s, training_loss=0.987]\u001b[A\n",
            "Epoch 4:  61%|██████    | 772/1271 [04:08<02:41,  3.09it/s, training_loss=0.987]\u001b[A\n",
            "Epoch 4:  61%|██████    | 772/1271 [04:08<02:41,  3.09it/s, training_loss=0.217]\u001b[A\n",
            "Epoch 4:  61%|██████    | 773/1271 [04:08<02:40,  3.10it/s, training_loss=0.217]\u001b[A\n",
            "Epoch 4:  61%|██████    | 773/1271 [04:09<02:40,  3.10it/s, training_loss=0.498]\u001b[A\n",
            "Epoch 4:  61%|██████    | 774/1271 [04:09<02:40,  3.10it/s, training_loss=0.498]\u001b[A\n",
            "Epoch 4:  61%|██████    | 774/1271 [04:09<02:40,  3.10it/s, training_loss=0.040]\u001b[A\n",
            "Epoch 4:  61%|██████    | 775/1271 [04:09<02:39,  3.11it/s, training_loss=0.040]\u001b[A\n",
            "Epoch 4:  61%|██████    | 775/1271 [04:09<02:39,  3.11it/s, training_loss=0.141]\u001b[A\n",
            "Epoch 4:  61%|██████    | 776/1271 [04:09<02:38,  3.12it/s, training_loss=0.141]\u001b[A\n",
            "Epoch 4:  61%|██████    | 776/1271 [04:10<02:38,  3.12it/s, training_loss=0.863]\u001b[A\n",
            "Epoch 4:  61%|██████    | 777/1271 [04:10<02:38,  3.11it/s, training_loss=0.863]\u001b[A\n",
            "Epoch 4:  61%|██████    | 777/1271 [04:10<02:38,  3.11it/s, training_loss=0.040]\u001b[A\n",
            "Epoch 4:  61%|██████    | 778/1271 [04:10<02:39,  3.10it/s, training_loss=0.040]\u001b[A\n",
            "Epoch 4:  61%|██████    | 778/1271 [04:10<02:39,  3.10it/s, training_loss=0.047]\u001b[A\n",
            "Epoch 4:  61%|██████▏   | 779/1271 [04:10<02:38,  3.10it/s, training_loss=0.047]\u001b[A\n",
            "Epoch 4:  61%|██████▏   | 779/1271 [04:11<02:38,  3.10it/s, training_loss=0.077]\u001b[A\n",
            "Epoch 4:  61%|██████▏   | 780/1271 [04:11<02:38,  3.10it/s, training_loss=0.077]\u001b[A\n",
            "Epoch 4:  61%|██████▏   | 780/1271 [04:11<02:38,  3.10it/s, training_loss=0.020]\u001b[A\n",
            "Epoch 4:  61%|██████▏   | 781/1271 [04:11<02:37,  3.10it/s, training_loss=0.020]\u001b[A\n",
            "Epoch 4:  61%|██████▏   | 781/1271 [04:11<02:37,  3.10it/s, training_loss=0.130]\u001b[A\n",
            "Epoch 4:  62%|██████▏   | 782/1271 [04:11<02:37,  3.11it/s, training_loss=0.130]\u001b[A\n",
            "Epoch 4:  62%|██████▏   | 782/1271 [04:12<02:37,  3.11it/s, training_loss=0.185]\u001b[A\n",
            "Epoch 4:  62%|██████▏   | 783/1271 [04:12<02:36,  3.12it/s, training_loss=0.185]\u001b[A\n",
            "Epoch 4:  62%|██████▏   | 783/1271 [04:12<02:36,  3.12it/s, training_loss=0.215]\u001b[A\n",
            "Epoch 4:  62%|██████▏   | 784/1271 [04:12<02:36,  3.12it/s, training_loss=0.215]\u001b[A\n",
            "Epoch 4:  62%|██████▏   | 784/1271 [04:12<02:36,  3.12it/s, training_loss=0.015]\u001b[A\n",
            "Epoch 4:  62%|██████▏   | 785/1271 [04:12<02:35,  3.12it/s, training_loss=0.015]\u001b[A\n",
            "Epoch 4:  62%|██████▏   | 785/1271 [04:13<02:35,  3.12it/s, training_loss=0.192]\u001b[A\n",
            "Epoch 4:  62%|██████▏   | 786/1271 [04:13<02:35,  3.12it/s, training_loss=0.192]\u001b[A\n",
            "Epoch 4:  62%|██████▏   | 786/1271 [04:13<02:35,  3.12it/s, training_loss=0.067]\u001b[A\n",
            "Epoch 4:  62%|██████▏   | 787/1271 [04:13<02:35,  3.12it/s, training_loss=0.067]\u001b[A\n",
            "Epoch 4:  62%|██████▏   | 787/1271 [04:13<02:35,  3.12it/s, training_loss=0.025]\u001b[A\n",
            "Epoch 4:  62%|██████▏   | 788/1271 [04:13<02:34,  3.13it/s, training_loss=0.025]\u001b[A\n",
            "Epoch 4:  62%|██████▏   | 788/1271 [04:14<02:34,  3.13it/s, training_loss=0.260]\u001b[A\n",
            "Epoch 4:  62%|██████▏   | 789/1271 [04:14<02:33,  3.13it/s, training_loss=0.260]\u001b[A\n",
            "Epoch 4:  62%|██████▏   | 789/1271 [04:14<02:33,  3.13it/s, training_loss=0.642]\u001b[A\n",
            "Epoch 4:  62%|██████▏   | 790/1271 [04:14<02:34,  3.12it/s, training_loss=0.642]\u001b[A\n",
            "Epoch 4:  62%|██████▏   | 790/1271 [04:14<02:34,  3.12it/s, training_loss=0.052]\u001b[A\n",
            "Epoch 4:  62%|██████▏   | 791/1271 [04:14<02:34,  3.11it/s, training_loss=0.052]\u001b[A\n",
            "Epoch 4:  62%|██████▏   | 791/1271 [04:15<02:34,  3.11it/s, training_loss=0.302]\u001b[A\n",
            "Epoch 4:  62%|██████▏   | 792/1271 [04:15<02:34,  3.11it/s, training_loss=0.302]\u001b[A\n",
            "Epoch 4:  62%|██████▏   | 792/1271 [04:15<02:34,  3.11it/s, training_loss=0.483]\u001b[A\n",
            "Epoch 4:  62%|██████▏   | 793/1271 [04:15<02:33,  3.12it/s, training_loss=0.483]\u001b[A\n",
            "Epoch 4:  62%|██████▏   | 793/1271 [04:15<02:33,  3.12it/s, training_loss=0.283]\u001b[A\n",
            "Epoch 4:  62%|██████▏   | 794/1271 [04:15<02:33,  3.10it/s, training_loss=0.283]\u001b[A\n",
            "Epoch 4:  62%|██████▏   | 794/1271 [04:16<02:33,  3.10it/s, training_loss=0.366]\u001b[A\n",
            "Epoch 4:  63%|██████▎   | 795/1271 [04:16<02:33,  3.09it/s, training_loss=0.366]\u001b[A\n",
            "Epoch 4:  63%|██████▎   | 795/1271 [04:16<02:33,  3.09it/s, training_loss=0.578]\u001b[A\n",
            "Epoch 4:  63%|██████▎   | 796/1271 [04:16<02:32,  3.11it/s, training_loss=0.578]\u001b[A\n",
            "Epoch 4:  63%|██████▎   | 796/1271 [04:16<02:32,  3.11it/s, training_loss=0.370]\u001b[A\n",
            "Epoch 4:  63%|██████▎   | 797/1271 [04:16<02:32,  3.10it/s, training_loss=0.370]\u001b[A\n",
            "Epoch 4:  63%|██████▎   | 797/1271 [04:17<02:32,  3.10it/s, training_loss=0.570]\u001b[A\n",
            "Epoch 4:  63%|██████▎   | 798/1271 [04:17<02:32,  3.11it/s, training_loss=0.570]\u001b[A\n",
            "Epoch 4:  63%|██████▎   | 798/1271 [04:17<02:32,  3.11it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 4:  63%|██████▎   | 799/1271 [04:17<02:31,  3.12it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 4:  63%|██████▎   | 799/1271 [04:17<02:31,  3.12it/s, training_loss=0.169]\u001b[A\n",
            "Epoch 4:  63%|██████▎   | 800/1271 [04:17<02:30,  3.12it/s, training_loss=0.169]\u001b[A\n",
            "Epoch 4:  63%|██████▎   | 800/1271 [04:17<02:30,  3.12it/s, training_loss=0.639]\u001b[A\n",
            "Epoch 4:  63%|██████▎   | 801/1271 [04:17<02:31,  3.11it/s, training_loss=0.639]\u001b[A\n",
            "Epoch 4:  63%|██████▎   | 801/1271 [04:18<02:31,  3.11it/s, training_loss=0.629]\u001b[A\n",
            "Epoch 4:  63%|██████▎   | 802/1271 [04:18<02:30,  3.11it/s, training_loss=0.629]\u001b[A\n",
            "Epoch 4:  63%|██████▎   | 802/1271 [04:18<02:30,  3.11it/s, training_loss=0.531]\u001b[A\n",
            "Epoch 4:  63%|██████▎   | 803/1271 [04:18<02:30,  3.11it/s, training_loss=0.531]\u001b[A\n",
            "Epoch 4:  63%|██████▎   | 803/1271 [04:18<02:30,  3.11it/s, training_loss=0.167]\u001b[A\n",
            "Epoch 4:  63%|██████▎   | 804/1271 [04:18<02:30,  3.10it/s, training_loss=0.167]\u001b[A\n",
            "Epoch 4:  63%|██████▎   | 804/1271 [04:19<02:30,  3.10it/s, training_loss=0.029]\u001b[A\n",
            "Epoch 4:  63%|██████▎   | 805/1271 [04:19<02:29,  3.11it/s, training_loss=0.029]\u001b[A\n",
            "Epoch 4:  63%|██████▎   | 805/1271 [04:19<02:29,  3.11it/s, training_loss=0.352]\u001b[A\n",
            "Epoch 4:  63%|██████▎   | 806/1271 [04:19<02:28,  3.13it/s, training_loss=0.352]\u001b[A\n",
            "Epoch 4:  63%|██████▎   | 806/1271 [04:19<02:28,  3.13it/s, training_loss=0.329]\u001b[A\n",
            "Epoch 4:  63%|██████▎   | 807/1271 [04:19<02:28,  3.12it/s, training_loss=0.329]\u001b[A\n",
            "Epoch 4:  63%|██████▎   | 807/1271 [04:20<02:28,  3.12it/s, training_loss=0.139]\u001b[A\n",
            "Epoch 4:  64%|██████▎   | 808/1271 [04:20<02:28,  3.12it/s, training_loss=0.139]\u001b[A\n",
            "Epoch 4:  64%|██████▎   | 808/1271 [04:20<02:28,  3.12it/s, training_loss=0.365]\u001b[A\n",
            "Epoch 4:  64%|██████▎   | 809/1271 [04:20<02:27,  3.12it/s, training_loss=0.365]\u001b[A\n",
            "Epoch 4:  64%|██████▎   | 809/1271 [04:20<02:27,  3.12it/s, training_loss=0.250]\u001b[A\n",
            "Epoch 4:  64%|██████▎   | 810/1271 [04:20<02:27,  3.12it/s, training_loss=0.250]\u001b[A\n",
            "Epoch 4:  64%|██████▎   | 810/1271 [04:21<02:27,  3.12it/s, training_loss=0.233]\u001b[A\n",
            "Epoch 4:  64%|██████▍   | 811/1271 [04:21<02:27,  3.11it/s, training_loss=0.233]\u001b[A\n",
            "Epoch 4:  64%|██████▍   | 811/1271 [04:21<02:27,  3.11it/s, training_loss=0.553]\u001b[A\n",
            "Epoch 4:  64%|██████▍   | 812/1271 [04:21<02:26,  3.12it/s, training_loss=0.553]\u001b[A\n",
            "Epoch 4:  64%|██████▍   | 812/1271 [04:21<02:26,  3.12it/s, training_loss=0.598]\u001b[A\n",
            "Epoch 4:  64%|██████▍   | 813/1271 [04:21<02:26,  3.12it/s, training_loss=0.598]\u001b[A\n",
            "Epoch 4:  64%|██████▍   | 813/1271 [04:22<02:26,  3.12it/s, training_loss=0.239]\u001b[A\n",
            "Epoch 4:  64%|██████▍   | 814/1271 [04:22<02:26,  3.12it/s, training_loss=0.239]\u001b[A\n",
            "Epoch 4:  64%|██████▍   | 814/1271 [04:22<02:26,  3.12it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 4:  64%|██████▍   | 815/1271 [04:22<02:25,  3.13it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 4:  64%|██████▍   | 815/1271 [04:22<02:25,  3.13it/s, training_loss=0.239]\u001b[A\n",
            "Epoch 4:  64%|██████▍   | 816/1271 [04:22<02:24,  3.14it/s, training_loss=0.239]\u001b[A\n",
            "Epoch 4:  64%|██████▍   | 816/1271 [04:23<02:24,  3.14it/s, training_loss=0.067]\u001b[A\n",
            "Epoch 4:  64%|██████▍   | 817/1271 [04:23<02:25,  3.13it/s, training_loss=0.067]\u001b[A\n",
            "Epoch 4:  64%|██████▍   | 817/1271 [04:23<02:25,  3.13it/s, training_loss=0.094]\u001b[A\n",
            "Epoch 4:  64%|██████▍   | 818/1271 [04:23<02:24,  3.13it/s, training_loss=0.094]\u001b[A\n",
            "Epoch 4:  64%|██████▍   | 818/1271 [04:23<02:24,  3.13it/s, training_loss=0.206]\u001b[A\n",
            "Epoch 4:  64%|██████▍   | 819/1271 [04:23<02:24,  3.13it/s, training_loss=0.206]\u001b[A\n",
            "Epoch 4:  64%|██████▍   | 819/1271 [04:24<02:24,  3.13it/s, training_loss=0.182]\u001b[A\n",
            "Epoch 4:  65%|██████▍   | 820/1271 [04:24<02:25,  3.11it/s, training_loss=0.182]\u001b[A\n",
            "Epoch 4:  65%|██████▍   | 820/1271 [04:24<02:25,  3.11it/s, training_loss=0.438]\u001b[A\n",
            "Epoch 4:  65%|██████▍   | 821/1271 [04:24<02:24,  3.10it/s, training_loss=0.438]\u001b[A\n",
            "Epoch 4:  65%|██████▍   | 821/1271 [04:24<02:24,  3.10it/s, training_loss=0.366]\u001b[A\n",
            "Epoch 4:  65%|██████▍   | 822/1271 [04:24<02:24,  3.11it/s, training_loss=0.366]\u001b[A\n",
            "Epoch 4:  65%|██████▍   | 822/1271 [04:25<02:24,  3.11it/s, training_loss=0.641]\u001b[A\n",
            "Epoch 4:  65%|██████▍   | 823/1271 [04:25<02:24,  3.11it/s, training_loss=0.641]\u001b[A\n",
            "Epoch 4:  65%|██████▍   | 823/1271 [04:25<02:24,  3.11it/s, training_loss=0.214]\u001b[A\n",
            "Epoch 4:  65%|██████▍   | 824/1271 [04:25<02:23,  3.11it/s, training_loss=0.214]\u001b[A\n",
            "Epoch 4:  65%|██████▍   | 824/1271 [04:25<02:23,  3.11it/s, training_loss=0.145]\u001b[A\n",
            "Epoch 4:  65%|██████▍   | 825/1271 [04:25<02:22,  3.12it/s, training_loss=0.145]\u001b[A\n",
            "Epoch 4:  65%|██████▍   | 825/1271 [04:25<02:22,  3.12it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 4:  65%|██████▍   | 826/1271 [04:25<02:22,  3.13it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 4:  65%|██████▍   | 826/1271 [04:26<02:22,  3.13it/s, training_loss=0.061]\u001b[A\n",
            "Epoch 4:  65%|██████▌   | 827/1271 [04:26<02:22,  3.13it/s, training_loss=0.061]\u001b[A\n",
            "Epoch 4:  65%|██████▌   | 827/1271 [04:26<02:22,  3.13it/s, training_loss=0.082]\u001b[A\n",
            "Epoch 4:  65%|██████▌   | 828/1271 [04:26<02:21,  3.13it/s, training_loss=0.082]\u001b[A\n",
            "Epoch 4:  65%|██████▌   | 828/1271 [04:26<02:21,  3.13it/s, training_loss=0.710]\u001b[A\n",
            "Epoch 4:  65%|██████▌   | 829/1271 [04:26<02:21,  3.12it/s, training_loss=0.710]\u001b[A\n",
            "Epoch 4:  65%|██████▌   | 829/1271 [04:27<02:21,  3.12it/s, training_loss=0.113]\u001b[A\n",
            "Epoch 4:  65%|██████▌   | 830/1271 [04:27<02:21,  3.11it/s, training_loss=0.113]\u001b[A\n",
            "Epoch 4:  65%|██████▌   | 830/1271 [04:27<02:21,  3.11it/s, training_loss=0.067]\u001b[A\n",
            "Epoch 4:  65%|██████▌   | 831/1271 [04:27<02:20,  3.12it/s, training_loss=0.067]\u001b[A\n",
            "Epoch 4:  65%|██████▌   | 831/1271 [04:27<02:20,  3.12it/s, training_loss=0.120]\u001b[A\n",
            "Epoch 4:  65%|██████▌   | 832/1271 [04:27<02:20,  3.13it/s, training_loss=0.120]\u001b[A\n",
            "Epoch 4:  65%|██████▌   | 832/1271 [04:28<02:20,  3.13it/s, training_loss=0.506]\u001b[A\n",
            "Epoch 4:  66%|██████▌   | 833/1271 [04:28<02:19,  3.14it/s, training_loss=0.506]\u001b[A\n",
            "Epoch 4:  66%|██████▌   | 833/1271 [04:28<02:19,  3.14it/s, training_loss=0.069]\u001b[A\n",
            "Epoch 4:  66%|██████▌   | 834/1271 [04:28<02:19,  3.12it/s, training_loss=0.069]\u001b[A\n",
            "Epoch 4:  66%|██████▌   | 834/1271 [04:28<02:19,  3.12it/s, training_loss=0.061]\u001b[A\n",
            "Epoch 4:  66%|██████▌   | 835/1271 [04:28<02:19,  3.12it/s, training_loss=0.061]\u001b[A\n",
            "Epoch 4:  66%|██████▌   | 835/1271 [04:29<02:19,  3.12it/s, training_loss=0.176]\u001b[A\n",
            "Epoch 4:  66%|██████▌   | 836/1271 [04:29<02:19,  3.12it/s, training_loss=0.176]\u001b[A\n",
            "Epoch 4:  66%|██████▌   | 836/1271 [04:29<02:19,  3.12it/s, training_loss=0.279]\u001b[A\n",
            "Epoch 4:  66%|██████▌   | 837/1271 [04:29<02:19,  3.11it/s, training_loss=0.279]\u001b[A\n",
            "Epoch 4:  66%|██████▌   | 837/1271 [04:29<02:19,  3.11it/s, training_loss=0.078]\u001b[A\n",
            "Epoch 4:  66%|██████▌   | 838/1271 [04:29<02:19,  3.10it/s, training_loss=0.078]\u001b[A\n",
            "Epoch 4:  66%|██████▌   | 838/1271 [04:30<02:19,  3.10it/s, training_loss=0.398]\u001b[A\n",
            "Epoch 4:  66%|██████▌   | 839/1271 [04:30<02:19,  3.10it/s, training_loss=0.398]\u001b[A\n",
            "Epoch 4:  66%|██████▌   | 839/1271 [04:30<02:19,  3.10it/s, training_loss=0.612]\u001b[A\n",
            "Epoch 4:  66%|██████▌   | 840/1271 [04:30<02:18,  3.11it/s, training_loss=0.612]\u001b[A\n",
            "Epoch 4:  66%|██████▌   | 840/1271 [04:30<02:18,  3.11it/s, training_loss=0.365]\u001b[A\n",
            "Epoch 4:  66%|██████▌   | 841/1271 [04:30<02:18,  3.11it/s, training_loss=0.365]\u001b[A\n",
            "Epoch 4:  66%|██████▌   | 841/1271 [04:31<02:18,  3.11it/s, training_loss=0.131]\u001b[A\n",
            "Epoch 4:  66%|██████▌   | 842/1271 [04:31<02:17,  3.13it/s, training_loss=0.131]\u001b[A\n",
            "Epoch 4:  66%|██████▌   | 842/1271 [04:31<02:17,  3.13it/s, training_loss=0.352]\u001b[A\n",
            "Epoch 4:  66%|██████▋   | 843/1271 [04:31<02:17,  3.12it/s, training_loss=0.352]\u001b[A\n",
            "Epoch 4:  66%|██████▋   | 843/1271 [04:31<02:17,  3.12it/s, training_loss=0.249]\u001b[A\n",
            "Epoch 4:  66%|██████▋   | 844/1271 [04:31<02:16,  3.12it/s, training_loss=0.249]\u001b[A\n",
            "Epoch 4:  66%|██████▋   | 844/1271 [04:32<02:16,  3.12it/s, training_loss=0.416]\u001b[A\n",
            "Epoch 4:  66%|██████▋   | 845/1271 [04:32<02:16,  3.12it/s, training_loss=0.416]\u001b[A\n",
            "Epoch 4:  66%|██████▋   | 845/1271 [04:32<02:16,  3.12it/s, training_loss=0.336]\u001b[A\n",
            "Epoch 4:  67%|██████▋   | 846/1271 [04:32<02:16,  3.11it/s, training_loss=0.336]\u001b[A\n",
            "Epoch 4:  67%|██████▋   | 846/1271 [04:32<02:16,  3.11it/s, training_loss=0.385]\u001b[A\n",
            "Epoch 4:  67%|██████▋   | 847/1271 [04:32<02:16,  3.10it/s, training_loss=0.385]\u001b[A\n",
            "Epoch 4:  67%|██████▋   | 847/1271 [04:33<02:16,  3.10it/s, training_loss=0.270]\u001b[A\n",
            "Epoch 4:  67%|██████▋   | 848/1271 [04:33<02:15,  3.12it/s, training_loss=0.270]\u001b[A\n",
            "Epoch 4:  67%|██████▋   | 848/1271 [04:33<02:15,  3.12it/s, training_loss=0.068]\u001b[A\n",
            "Epoch 4:  67%|██████▋   | 849/1271 [04:33<02:14,  3.13it/s, training_loss=0.068]\u001b[A\n",
            "Epoch 4:  67%|██████▋   | 849/1271 [04:33<02:14,  3.13it/s, training_loss=0.204]\u001b[A\n",
            "Epoch 4:  67%|██████▋   | 850/1271 [04:33<02:14,  3.12it/s, training_loss=0.204]\u001b[A\n",
            "Epoch 4:  67%|██████▋   | 850/1271 [04:33<02:14,  3.12it/s, training_loss=0.279]\u001b[A\n",
            "Epoch 4:  67%|██████▋   | 851/1271 [04:34<02:14,  3.11it/s, training_loss=0.279]\u001b[A\n",
            "Epoch 4:  67%|██████▋   | 851/1271 [04:34<02:14,  3.11it/s, training_loss=0.352]\u001b[A\n",
            "Epoch 4:  67%|██████▋   | 852/1271 [04:34<02:15,  3.10it/s, training_loss=0.352]\u001b[A\n",
            "Epoch 4:  67%|██████▋   | 852/1271 [04:34<02:15,  3.10it/s, training_loss=0.591]\u001b[A\n",
            "Epoch 4:  67%|██████▋   | 853/1271 [04:34<02:15,  3.10it/s, training_loss=0.591]\u001b[A\n",
            "Epoch 4:  67%|██████▋   | 853/1271 [04:34<02:15,  3.10it/s, training_loss=0.345]\u001b[A\n",
            "Epoch 4:  67%|██████▋   | 854/1271 [04:34<02:14,  3.11it/s, training_loss=0.345]\u001b[A\n",
            "Epoch 4:  67%|██████▋   | 854/1271 [04:35<02:14,  3.11it/s, training_loss=0.232]\u001b[A\n",
            "Epoch 4:  67%|██████▋   | 855/1271 [04:35<02:13,  3.12it/s, training_loss=0.232]\u001b[A\n",
            "Epoch 4:  67%|██████▋   | 855/1271 [04:35<02:13,  3.12it/s, training_loss=0.226]\u001b[A\n",
            "Epoch 4:  67%|██████▋   | 856/1271 [04:35<02:14,  3.09it/s, training_loss=0.226]\u001b[A\n",
            "Epoch 4:  67%|██████▋   | 856/1271 [04:35<02:14,  3.09it/s, training_loss=0.758]\u001b[A\n",
            "Epoch 4:  67%|██████▋   | 857/1271 [04:35<02:13,  3.10it/s, training_loss=0.758]\u001b[A\n",
            "Epoch 4:  67%|██████▋   | 857/1271 [04:36<02:13,  3.10it/s, training_loss=0.331]\u001b[A\n",
            "Epoch 4:  68%|██████▊   | 858/1271 [04:36<02:12,  3.11it/s, training_loss=0.331]\u001b[A\n",
            "Epoch 4:  68%|██████▊   | 858/1271 [04:36<02:12,  3.11it/s, training_loss=0.285]\u001b[A\n",
            "Epoch 4:  68%|██████▊   | 859/1271 [04:36<02:12,  3.10it/s, training_loss=0.285]\u001b[A\n",
            "Epoch 4:  68%|██████▊   | 859/1271 [04:36<02:12,  3.10it/s, training_loss=0.540]\u001b[A\n",
            "Epoch 4:  68%|██████▊   | 860/1271 [04:36<02:11,  3.12it/s, training_loss=0.540]\u001b[A\n",
            "Epoch 4:  68%|██████▊   | 860/1271 [04:37<02:11,  3.12it/s, training_loss=0.083]\u001b[A\n",
            "Epoch 4:  68%|██████▊   | 861/1271 [04:37<02:11,  3.12it/s, training_loss=0.083]\u001b[A\n",
            "Epoch 4:  68%|██████▊   | 861/1271 [04:37<02:11,  3.12it/s, training_loss=0.124]\u001b[A\n",
            "Epoch 4:  68%|██████▊   | 862/1271 [04:37<02:11,  3.12it/s, training_loss=0.124]\u001b[A\n",
            "Epoch 4:  68%|██████▊   | 862/1271 [04:37<02:11,  3.12it/s, training_loss=0.123]\u001b[A\n",
            "Epoch 4:  68%|██████▊   | 863/1271 [04:37<02:11,  3.10it/s, training_loss=0.123]\u001b[A\n",
            "Epoch 4:  68%|██████▊   | 863/1271 [04:38<02:11,  3.10it/s, training_loss=0.255]\u001b[A\n",
            "Epoch 4:  68%|██████▊   | 864/1271 [04:38<02:11,  3.10it/s, training_loss=0.255]\u001b[A\n",
            "Epoch 4:  68%|██████▊   | 864/1271 [04:38<02:11,  3.10it/s, training_loss=0.756]\u001b[A\n",
            "Epoch 4:  68%|██████▊   | 865/1271 [04:38<02:11,  3.08it/s, training_loss=0.756]\u001b[A\n",
            "Epoch 4:  68%|██████▊   | 865/1271 [04:38<02:11,  3.08it/s, training_loss=0.238]\u001b[A\n",
            "Epoch 4:  68%|██████▊   | 866/1271 [04:38<02:10,  3.10it/s, training_loss=0.238]\u001b[A\n",
            "Epoch 4:  68%|██████▊   | 866/1271 [04:39<02:10,  3.10it/s, training_loss=0.529]\u001b[A\n",
            "Epoch 4:  68%|██████▊   | 867/1271 [04:39<02:09,  3.11it/s, training_loss=0.529]\u001b[A\n",
            "Epoch 4:  68%|██████▊   | 867/1271 [04:39<02:09,  3.11it/s, training_loss=0.134]\u001b[A\n",
            "Epoch 4:  68%|██████▊   | 868/1271 [04:39<02:09,  3.12it/s, training_loss=0.134]\u001b[A\n",
            "Epoch 4:  68%|██████▊   | 868/1271 [04:39<02:09,  3.12it/s, training_loss=0.145]\u001b[A\n",
            "Epoch 4:  68%|██████▊   | 869/1271 [04:39<02:09,  3.09it/s, training_loss=0.145]\u001b[A\n",
            "Epoch 4:  68%|██████▊   | 869/1271 [04:40<02:09,  3.09it/s, training_loss=0.169]\u001b[A\n",
            "Epoch 4:  68%|██████▊   | 870/1271 [04:40<02:09,  3.09it/s, training_loss=0.169]\u001b[A\n",
            "Epoch 4:  68%|██████▊   | 870/1271 [04:40<02:09,  3.09it/s, training_loss=0.424]\u001b[A\n",
            "Epoch 4:  69%|██████▊   | 871/1271 [04:40<02:09,  3.10it/s, training_loss=0.424]\u001b[A\n",
            "Epoch 4:  69%|██████▊   | 871/1271 [04:40<02:09,  3.10it/s, training_loss=0.281]\u001b[A\n",
            "Epoch 4:  69%|██████▊   | 872/1271 [04:40<02:08,  3.10it/s, training_loss=0.281]\u001b[A\n",
            "Epoch 4:  69%|██████▊   | 872/1271 [04:41<02:08,  3.10it/s, training_loss=0.611]\u001b[A\n",
            "Epoch 4:  69%|██████▊   | 873/1271 [04:41<02:07,  3.12it/s, training_loss=0.611]\u001b[A\n",
            "Epoch 4:  69%|██████▊   | 873/1271 [04:41<02:07,  3.12it/s, training_loss=0.140]\u001b[A\n",
            "Epoch 4:  69%|██████▉   | 874/1271 [04:41<02:06,  3.13it/s, training_loss=0.140]\u001b[A\n",
            "Epoch 4:  69%|██████▉   | 874/1271 [04:41<02:06,  3.13it/s, training_loss=0.436]\u001b[A\n",
            "Epoch 4:  69%|██████▉   | 875/1271 [04:41<02:07,  3.10it/s, training_loss=0.436]\u001b[A\n",
            "Epoch 4:  69%|██████▉   | 875/1271 [04:42<02:07,  3.10it/s, training_loss=0.142]\u001b[A\n",
            "Epoch 4:  69%|██████▉   | 876/1271 [04:42<02:07,  3.11it/s, training_loss=0.142]\u001b[A\n",
            "Epoch 4:  69%|██████▉   | 876/1271 [04:42<02:07,  3.11it/s, training_loss=0.196]\u001b[A\n",
            "Epoch 4:  69%|██████▉   | 877/1271 [04:42<02:06,  3.11it/s, training_loss=0.196]\u001b[A\n",
            "Epoch 4:  69%|██████▉   | 877/1271 [04:42<02:06,  3.11it/s, training_loss=0.360]\u001b[A\n",
            "Epoch 4:  69%|██████▉   | 878/1271 [04:42<02:05,  3.12it/s, training_loss=0.360]\u001b[A\n",
            "Epoch 4:  69%|██████▉   | 878/1271 [04:43<02:05,  3.12it/s, training_loss=0.207]\u001b[A\n",
            "Epoch 4:  69%|██████▉   | 879/1271 [04:43<02:05,  3.13it/s, training_loss=0.207]\u001b[A\n",
            "Epoch 4:  69%|██████▉   | 879/1271 [04:43<02:05,  3.13it/s, training_loss=0.197]\u001b[A\n",
            "Epoch 4:  69%|██████▉   | 880/1271 [04:43<02:05,  3.11it/s, training_loss=0.197]\u001b[A\n",
            "Epoch 4:  69%|██████▉   | 880/1271 [04:43<02:05,  3.11it/s, training_loss=0.361]\u001b[A\n",
            "Epoch 4:  69%|██████▉   | 881/1271 [04:43<02:05,  3.12it/s, training_loss=0.361]\u001b[A\n",
            "Epoch 4:  69%|██████▉   | 881/1271 [04:43<02:05,  3.12it/s, training_loss=0.285]\u001b[A\n",
            "Epoch 4:  69%|██████▉   | 882/1271 [04:43<02:04,  3.12it/s, training_loss=0.285]\u001b[A\n",
            "Epoch 4:  69%|██████▉   | 882/1271 [04:44<02:04,  3.12it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 4:  69%|██████▉   | 883/1271 [04:44<02:03,  3.13it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 4:  69%|██████▉   | 883/1271 [04:44<02:03,  3.13it/s, training_loss=0.344]\u001b[A\n",
            "Epoch 4:  70%|██████▉   | 884/1271 [04:44<02:03,  3.13it/s, training_loss=0.344]\u001b[A\n",
            "Epoch 4:  70%|██████▉   | 884/1271 [04:44<02:03,  3.13it/s, training_loss=0.143]\u001b[A\n",
            "Epoch 4:  70%|██████▉   | 885/1271 [04:44<02:04,  3.10it/s, training_loss=0.143]\u001b[A\n",
            "Epoch 4:  70%|██████▉   | 885/1271 [04:45<02:04,  3.10it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 4:  70%|██████▉   | 886/1271 [04:45<02:03,  3.13it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 4:  70%|██████▉   | 886/1271 [04:45<02:03,  3.13it/s, training_loss=0.048]\u001b[A\n",
            "Epoch 4:  70%|██████▉   | 887/1271 [04:45<02:03,  3.10it/s, training_loss=0.048]\u001b[A\n",
            "Epoch 4:  70%|██████▉   | 887/1271 [04:45<02:03,  3.10it/s, training_loss=0.125]\u001b[A\n",
            "Epoch 4:  70%|██████▉   | 888/1271 [04:45<02:03,  3.11it/s, training_loss=0.125]\u001b[A\n",
            "Epoch 4:  70%|██████▉   | 888/1271 [04:46<02:03,  3.11it/s, training_loss=0.347]\u001b[A\n",
            "Epoch 4:  70%|██████▉   | 889/1271 [04:46<02:02,  3.12it/s, training_loss=0.347]\u001b[A\n",
            "Epoch 4:  70%|██████▉   | 889/1271 [04:46<02:02,  3.12it/s, training_loss=0.261]\u001b[A\n",
            "Epoch 4:  70%|███████   | 890/1271 [04:46<02:02,  3.12it/s, training_loss=0.261]\u001b[A\n",
            "Epoch 4:  70%|███████   | 890/1271 [04:46<02:02,  3.12it/s, training_loss=0.281]\u001b[A\n",
            "Epoch 4:  70%|███████   | 891/1271 [04:46<02:01,  3.13it/s, training_loss=0.281]\u001b[A\n",
            "Epoch 4:  70%|███████   | 891/1271 [04:47<02:01,  3.13it/s, training_loss=0.109]\u001b[A\n",
            "Epoch 4:  70%|███████   | 892/1271 [04:47<02:01,  3.11it/s, training_loss=0.109]\u001b[A\n",
            "Epoch 4:  70%|███████   | 892/1271 [04:47<02:01,  3.11it/s, training_loss=0.273]\u001b[A\n",
            "Epoch 4:  70%|███████   | 893/1271 [04:47<02:01,  3.12it/s, training_loss=0.273]\u001b[A\n",
            "Epoch 4:  70%|███████   | 893/1271 [04:47<02:01,  3.12it/s, training_loss=0.187]\u001b[A\n",
            "Epoch 4:  70%|███████   | 894/1271 [04:47<02:00,  3.13it/s, training_loss=0.187]\u001b[A\n",
            "Epoch 4:  70%|███████   | 894/1271 [04:48<02:00,  3.13it/s, training_loss=0.195]\u001b[A\n",
            "Epoch 4:  70%|███████   | 895/1271 [04:48<02:00,  3.13it/s, training_loss=0.195]\u001b[A\n",
            "Epoch 4:  70%|███████   | 895/1271 [04:48<02:00,  3.13it/s, training_loss=0.091]\u001b[A\n",
            "Epoch 4:  70%|███████   | 896/1271 [04:48<02:00,  3.12it/s, training_loss=0.091]\u001b[A\n",
            "Epoch 4:  70%|███████   | 896/1271 [04:48<02:00,  3.12it/s, training_loss=0.134]\u001b[A\n",
            "Epoch 4:  71%|███████   | 897/1271 [04:48<02:00,  3.10it/s, training_loss=0.134]\u001b[A\n",
            "Epoch 4:  71%|███████   | 897/1271 [04:49<02:00,  3.10it/s, training_loss=0.032]\u001b[A\n",
            "Epoch 4:  71%|███████   | 898/1271 [04:49<02:00,  3.10it/s, training_loss=0.032]\u001b[A\n",
            "Epoch 4:  71%|███████   | 898/1271 [04:49<02:00,  3.10it/s, training_loss=0.234]\u001b[A\n",
            "Epoch 4:  71%|███████   | 899/1271 [04:49<02:00,  3.09it/s, training_loss=0.234]\u001b[A\n",
            "Epoch 4:  71%|███████   | 899/1271 [04:49<02:00,  3.09it/s, training_loss=0.237]\u001b[A\n",
            "Epoch 4:  71%|███████   | 900/1271 [04:49<01:59,  3.09it/s, training_loss=0.237]\u001b[A\n",
            "Epoch 4:  71%|███████   | 900/1271 [04:50<01:59,  3.09it/s, training_loss=0.221]\u001b[A\n",
            "Epoch 4:  71%|███████   | 901/1271 [04:50<01:59,  3.09it/s, training_loss=0.221]\u001b[A\n",
            "Epoch 4:  71%|███████   | 901/1271 [04:50<01:59,  3.09it/s, training_loss=0.316]\u001b[A\n",
            "Epoch 4:  71%|███████   | 902/1271 [04:50<01:59,  3.09it/s, training_loss=0.316]\u001b[A\n",
            "Epoch 4:  71%|███████   | 902/1271 [04:50<01:59,  3.09it/s, training_loss=0.266]\u001b[A\n",
            "Epoch 4:  71%|███████   | 903/1271 [04:50<01:58,  3.09it/s, training_loss=0.266]\u001b[A\n",
            "Epoch 4:  71%|███████   | 903/1271 [04:51<01:58,  3.09it/s, training_loss=0.037]\u001b[A\n",
            "Epoch 4:  71%|███████   | 904/1271 [04:51<01:57,  3.11it/s, training_loss=0.037]\u001b[A\n",
            "Epoch 4:  71%|███████   | 904/1271 [04:51<01:57,  3.11it/s, training_loss=0.618]\u001b[A\n",
            "Epoch 4:  71%|███████   | 905/1271 [04:51<01:57,  3.12it/s, training_loss=0.618]\u001b[A\n",
            "Epoch 4:  71%|███████   | 905/1271 [04:51<01:57,  3.12it/s, training_loss=0.349]\u001b[A\n",
            "Epoch 4:  71%|███████▏  | 906/1271 [04:51<01:57,  3.11it/s, training_loss=0.349]\u001b[A\n",
            "Epoch 4:  71%|███████▏  | 906/1271 [04:52<01:57,  3.11it/s, training_loss=0.392]\u001b[A\n",
            "Epoch 4:  71%|███████▏  | 907/1271 [04:52<01:57,  3.10it/s, training_loss=0.392]\u001b[A\n",
            "Epoch 4:  71%|███████▏  | 907/1271 [04:52<01:57,  3.10it/s, training_loss=0.137]\u001b[A\n",
            "Epoch 4:  71%|███████▏  | 908/1271 [04:52<01:57,  3.09it/s, training_loss=0.137]\u001b[A\n",
            "Epoch 4:  71%|███████▏  | 908/1271 [04:52<01:57,  3.09it/s, training_loss=0.121]\u001b[A\n",
            "Epoch 4:  72%|███████▏  | 909/1271 [04:52<01:56,  3.10it/s, training_loss=0.121]\u001b[A\n",
            "Epoch 4:  72%|███████▏  | 909/1271 [04:52<01:56,  3.10it/s, training_loss=0.412]\u001b[A\n",
            "Epoch 4:  72%|███████▏  | 910/1271 [04:52<01:56,  3.09it/s, training_loss=0.412]\u001b[A\n",
            "Epoch 4:  72%|███████▏  | 910/1271 [04:53<01:56,  3.09it/s, training_loss=0.321]\u001b[A\n",
            "Epoch 4:  72%|███████▏  | 911/1271 [04:53<01:55,  3.11it/s, training_loss=0.321]\u001b[A\n",
            "Epoch 4:  72%|███████▏  | 911/1271 [04:53<01:55,  3.11it/s, training_loss=0.147]\u001b[A\n",
            "Epoch 4:  72%|███████▏  | 912/1271 [04:53<01:55,  3.12it/s, training_loss=0.147]\u001b[A\n",
            "Epoch 4:  72%|███████▏  | 912/1271 [04:53<01:55,  3.12it/s, training_loss=0.015]\u001b[A\n",
            "Epoch 4:  72%|███████▏  | 913/1271 [04:53<01:54,  3.11it/s, training_loss=0.015]\u001b[A\n",
            "Epoch 4:  72%|███████▏  | 913/1271 [04:54<01:54,  3.11it/s, training_loss=0.294]\u001b[A\n",
            "Epoch 4:  72%|███████▏  | 914/1271 [04:54<01:54,  3.12it/s, training_loss=0.294]\u001b[A\n",
            "Epoch 4:  72%|███████▏  | 914/1271 [04:54<01:54,  3.12it/s, training_loss=0.257]\u001b[A\n",
            "Epoch 4:  72%|███████▏  | 915/1271 [04:54<01:53,  3.13it/s, training_loss=0.257]\u001b[A\n",
            "Epoch 4:  72%|███████▏  | 915/1271 [04:54<01:53,  3.13it/s, training_loss=0.133]\u001b[A\n",
            "Epoch 4:  72%|███████▏  | 916/1271 [04:54<01:53,  3.13it/s, training_loss=0.133]\u001b[A\n",
            "Epoch 4:  72%|███████▏  | 916/1271 [04:55<01:53,  3.13it/s, training_loss=0.036]\u001b[A\n",
            "Epoch 4:  72%|███████▏  | 917/1271 [04:55<01:53,  3.12it/s, training_loss=0.036]\u001b[A\n",
            "Epoch 4:  72%|███████▏  | 917/1271 [04:55<01:53,  3.12it/s, training_loss=0.326]\u001b[A\n",
            "Epoch 4:  72%|███████▏  | 918/1271 [04:55<01:53,  3.12it/s, training_loss=0.326]\u001b[A\n",
            "Epoch 4:  72%|███████▏  | 918/1271 [04:55<01:53,  3.12it/s, training_loss=0.196]\u001b[A\n",
            "Epoch 4:  72%|███████▏  | 919/1271 [04:55<01:52,  3.13it/s, training_loss=0.196]\u001b[A\n",
            "Epoch 4:  72%|███████▏  | 919/1271 [04:56<01:52,  3.13it/s, training_loss=0.655]\u001b[A\n",
            "Epoch 4:  72%|███████▏  | 920/1271 [04:56<01:51,  3.13it/s, training_loss=0.655]\u001b[A\n",
            "Epoch 4:  72%|███████▏  | 920/1271 [04:56<01:51,  3.13it/s, training_loss=0.144]\u001b[A\n",
            "Epoch 4:  72%|███████▏  | 921/1271 [04:56<01:52,  3.12it/s, training_loss=0.144]\u001b[A\n",
            "Epoch 4:  72%|███████▏  | 921/1271 [04:56<01:52,  3.12it/s, training_loss=0.348]\u001b[A\n",
            "Epoch 4:  73%|███████▎  | 922/1271 [04:56<01:52,  3.11it/s, training_loss=0.348]\u001b[A\n",
            "Epoch 4:  73%|███████▎  | 922/1271 [04:57<01:52,  3.11it/s, training_loss=0.180]\u001b[A\n",
            "Epoch 4:  73%|███████▎  | 923/1271 [04:57<01:51,  3.12it/s, training_loss=0.180]\u001b[A\n",
            "Epoch 4:  73%|███████▎  | 923/1271 [04:57<01:51,  3.12it/s, training_loss=0.570]\u001b[A\n",
            "Epoch 4:  73%|███████▎  | 924/1271 [04:57<01:51,  3.11it/s, training_loss=0.570]\u001b[A\n",
            "Epoch 4:  73%|███████▎  | 924/1271 [04:57<01:51,  3.11it/s, training_loss=0.305]\u001b[A\n",
            "Epoch 4:  73%|███████▎  | 925/1271 [04:57<01:50,  3.12it/s, training_loss=0.305]\u001b[A\n",
            "Epoch 4:  73%|███████▎  | 925/1271 [04:58<01:50,  3.12it/s, training_loss=0.048]\u001b[A\n",
            "Epoch 4:  73%|███████▎  | 926/1271 [04:58<01:50,  3.13it/s, training_loss=0.048]\u001b[A\n",
            "Epoch 4:  73%|███████▎  | 926/1271 [04:58<01:50,  3.13it/s, training_loss=0.506]\u001b[A\n",
            "Epoch 4:  73%|███████▎  | 927/1271 [04:58<01:50,  3.12it/s, training_loss=0.506]\u001b[A\n",
            "Epoch 4:  73%|███████▎  | 927/1271 [04:58<01:50,  3.12it/s, training_loss=0.065]\u001b[A\n",
            "Epoch 4:  73%|███████▎  | 928/1271 [04:58<01:50,  3.10it/s, training_loss=0.065]\u001b[A\n",
            "Epoch 4:  73%|███████▎  | 928/1271 [04:59<01:50,  3.10it/s, training_loss=0.060]\u001b[A\n",
            "Epoch 4:  73%|███████▎  | 929/1271 [04:59<01:49,  3.11it/s, training_loss=0.060]\u001b[A\n",
            "Epoch 4:  73%|███████▎  | 929/1271 [04:59<01:49,  3.11it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 4:  73%|███████▎  | 930/1271 [04:59<01:49,  3.13it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 4:  73%|███████▎  | 930/1271 [04:59<01:49,  3.13it/s, training_loss=0.387]\u001b[A\n",
            "Epoch 4:  73%|███████▎  | 931/1271 [04:59<01:48,  3.13it/s, training_loss=0.387]\u001b[A\n",
            "Epoch 4:  73%|███████▎  | 931/1271 [05:00<01:48,  3.13it/s, training_loss=0.252]\u001b[A\n",
            "Epoch 4:  73%|███████▎  | 932/1271 [05:00<01:48,  3.13it/s, training_loss=0.252]\u001b[A\n",
            "Epoch 4:  73%|███████▎  | 932/1271 [05:00<01:48,  3.13it/s, training_loss=0.123]\u001b[A\n",
            "Epoch 4:  73%|███████▎  | 933/1271 [05:00<01:47,  3.13it/s, training_loss=0.123]\u001b[A\n",
            "Epoch 4:  73%|███████▎  | 933/1271 [05:00<01:47,  3.13it/s, training_loss=0.515]\u001b[A\n",
            "Epoch 4:  73%|███████▎  | 934/1271 [05:00<01:48,  3.12it/s, training_loss=0.515]\u001b[A\n",
            "Epoch 4:  73%|███████▎  | 934/1271 [05:00<01:48,  3.12it/s, training_loss=0.209]\u001b[A\n",
            "Epoch 4:  74%|███████▎  | 935/1271 [05:00<01:47,  3.12it/s, training_loss=0.209]\u001b[A\n",
            "Epoch 4:  74%|███████▎  | 935/1271 [05:01<01:47,  3.12it/s, training_loss=0.328]\u001b[A\n",
            "Epoch 4:  74%|███████▎  | 936/1271 [05:01<01:47,  3.12it/s, training_loss=0.328]\u001b[A\n",
            "Epoch 4:  74%|███████▎  | 936/1271 [05:01<01:47,  3.12it/s, training_loss=0.130]\u001b[A\n",
            "Epoch 4:  74%|███████▎  | 937/1271 [05:01<01:46,  3.12it/s, training_loss=0.130]\u001b[A\n",
            "Epoch 4:  74%|███████▎  | 937/1271 [05:01<01:46,  3.12it/s, training_loss=0.447]\u001b[A\n",
            "Epoch 4:  74%|███████▍  | 938/1271 [05:01<01:46,  3.12it/s, training_loss=0.447]\u001b[A\n",
            "Epoch 4:  74%|███████▍  | 938/1271 [05:02<01:46,  3.12it/s, training_loss=0.441]\u001b[A\n",
            "Epoch 4:  74%|███████▍  | 939/1271 [05:02<01:47,  3.10it/s, training_loss=0.441]\u001b[A\n",
            "Epoch 4:  74%|███████▍  | 939/1271 [05:02<01:47,  3.10it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 4:  74%|███████▍  | 940/1271 [05:02<01:46,  3.11it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 4:  74%|███████▍  | 940/1271 [05:02<01:46,  3.11it/s, training_loss=0.600]\u001b[A\n",
            "Epoch 4:  74%|███████▍  | 941/1271 [05:02<01:46,  3.10it/s, training_loss=0.600]\u001b[A\n",
            "Epoch 4:  74%|███████▍  | 941/1271 [05:03<01:46,  3.10it/s, training_loss=0.381]\u001b[A\n",
            "Epoch 4:  74%|███████▍  | 942/1271 [05:03<01:46,  3.10it/s, training_loss=0.381]\u001b[A\n",
            "Epoch 4:  74%|███████▍  | 942/1271 [05:03<01:46,  3.10it/s, training_loss=0.794]\u001b[A\n",
            "Epoch 4:  74%|███████▍  | 943/1271 [05:03<01:45,  3.11it/s, training_loss=0.794]\u001b[A\n",
            "Epoch 4:  74%|███████▍  | 943/1271 [05:03<01:45,  3.11it/s, training_loss=0.178]\u001b[A\n",
            "Epoch 4:  74%|███████▍  | 944/1271 [05:03<01:45,  3.10it/s, training_loss=0.178]\u001b[A\n",
            "Epoch 4:  74%|███████▍  | 944/1271 [05:04<01:45,  3.10it/s, training_loss=0.085]\u001b[A\n",
            "Epoch 4:  74%|███████▍  | 945/1271 [05:04<01:44,  3.11it/s, training_loss=0.085]\u001b[A\n",
            "Epoch 4:  74%|███████▍  | 945/1271 [05:04<01:44,  3.11it/s, training_loss=0.213]\u001b[A\n",
            "Epoch 4:  74%|███████▍  | 946/1271 [05:04<01:44,  3.11it/s, training_loss=0.213]\u001b[A\n",
            "Epoch 4:  74%|███████▍  | 946/1271 [05:04<01:44,  3.11it/s, training_loss=0.170]\u001b[A\n",
            "Epoch 4:  75%|███████▍  | 947/1271 [05:04<01:44,  3.11it/s, training_loss=0.170]\u001b[A\n",
            "Epoch 4:  75%|███████▍  | 947/1271 [05:05<01:44,  3.11it/s, training_loss=0.121]\u001b[A\n",
            "Epoch 4:  75%|███████▍  | 948/1271 [05:05<01:43,  3.12it/s, training_loss=0.121]\u001b[A\n",
            "Epoch 4:  75%|███████▍  | 948/1271 [05:05<01:43,  3.12it/s, training_loss=0.298]\u001b[A\n",
            "Epoch 4:  75%|███████▍  | 949/1271 [05:05<01:43,  3.12it/s, training_loss=0.298]\u001b[A\n",
            "Epoch 4:  75%|███████▍  | 949/1271 [05:05<01:43,  3.12it/s, training_loss=0.514]\u001b[A\n",
            "Epoch 4:  75%|███████▍  | 950/1271 [05:05<01:43,  3.09it/s, training_loss=0.514]\u001b[A\n",
            "Epoch 4:  75%|███████▍  | 950/1271 [05:06<01:43,  3.09it/s, training_loss=0.040]\u001b[A\n",
            "Epoch 4:  75%|███████▍  | 951/1271 [05:06<01:42,  3.11it/s, training_loss=0.040]\u001b[A\n",
            "Epoch 4:  75%|███████▍  | 951/1271 [05:06<01:42,  3.11it/s, training_loss=0.234]\u001b[A\n",
            "Epoch 4:  75%|███████▍  | 952/1271 [05:06<01:42,  3.10it/s, training_loss=0.234]\u001b[A\n",
            "Epoch 4:  75%|███████▍  | 952/1271 [05:06<01:42,  3.10it/s, training_loss=0.499]\u001b[A\n",
            "Epoch 4:  75%|███████▍  | 953/1271 [05:06<01:42,  3.10it/s, training_loss=0.499]\u001b[A\n",
            "Epoch 4:  75%|███████▍  | 953/1271 [05:07<01:42,  3.10it/s, training_loss=0.189]\u001b[A\n",
            "Epoch 4:  75%|███████▌  | 954/1271 [05:07<01:42,  3.09it/s, training_loss=0.189]\u001b[A\n",
            "Epoch 4:  75%|███████▌  | 954/1271 [05:07<01:42,  3.09it/s, training_loss=0.067]\u001b[A\n",
            "Epoch 4:  75%|███████▌  | 955/1271 [05:07<01:41,  3.10it/s, training_loss=0.067]\u001b[A\n",
            "Epoch 4:  75%|███████▌  | 955/1271 [05:07<01:41,  3.10it/s, training_loss=0.156]\u001b[A\n",
            "Epoch 4:  75%|███████▌  | 956/1271 [05:07<01:41,  3.10it/s, training_loss=0.156]\u001b[A\n",
            "Epoch 4:  75%|███████▌  | 956/1271 [05:08<01:41,  3.10it/s, training_loss=0.441]\u001b[A\n",
            "Epoch 4:  75%|███████▌  | 957/1271 [05:08<01:41,  3.10it/s, training_loss=0.441]\u001b[A\n",
            "Epoch 4:  75%|███████▌  | 957/1271 [05:08<01:41,  3.10it/s, training_loss=0.063]\u001b[A\n",
            "Epoch 4:  75%|███████▌  | 958/1271 [05:08<01:41,  3.09it/s, training_loss=0.063]\u001b[A\n",
            "Epoch 4:  75%|███████▌  | 958/1271 [05:08<01:41,  3.09it/s, training_loss=0.058]\u001b[A\n",
            "Epoch 4:  75%|███████▌  | 959/1271 [05:08<01:40,  3.10it/s, training_loss=0.058]\u001b[A\n",
            "Epoch 4:  75%|███████▌  | 959/1271 [05:09<01:40,  3.10it/s, training_loss=0.679]\u001b[A\n",
            "Epoch 4:  76%|███████▌  | 960/1271 [05:09<01:41,  3.08it/s, training_loss=0.679]\u001b[A\n",
            "Epoch 4:  76%|███████▌  | 960/1271 [05:09<01:41,  3.08it/s, training_loss=0.379]\u001b[A\n",
            "Epoch 4:  76%|███████▌  | 961/1271 [05:09<01:40,  3.09it/s, training_loss=0.379]\u001b[A\n",
            "Epoch 4:  76%|███████▌  | 961/1271 [05:09<01:40,  3.09it/s, training_loss=0.143]\u001b[A\n",
            "Epoch 4:  76%|███████▌  | 962/1271 [05:09<01:39,  3.11it/s, training_loss=0.143]\u001b[A\n",
            "Epoch 4:  76%|███████▌  | 962/1271 [05:10<01:39,  3.11it/s, training_loss=0.312]\u001b[A\n",
            "Epoch 4:  76%|███████▌  | 963/1271 [05:10<01:38,  3.13it/s, training_loss=0.312]\u001b[A\n",
            "Epoch 4:  76%|███████▌  | 963/1271 [05:10<01:38,  3.13it/s, training_loss=0.286]\u001b[A\n",
            "Epoch 4:  76%|███████▌  | 964/1271 [05:10<01:38,  3.11it/s, training_loss=0.286]\u001b[A\n",
            "Epoch 4:  76%|███████▌  | 964/1271 [05:10<01:38,  3.11it/s, training_loss=0.053]\u001b[A\n",
            "Epoch 4:  76%|███████▌  | 965/1271 [05:10<01:38,  3.11it/s, training_loss=0.053]\u001b[A\n",
            "Epoch 4:  76%|███████▌  | 965/1271 [05:10<01:38,  3.11it/s, training_loss=0.314]\u001b[A\n",
            "Epoch 4:  76%|███████▌  | 966/1271 [05:10<01:37,  3.12it/s, training_loss=0.314]\u001b[A\n",
            "Epoch 4:  76%|███████▌  | 966/1271 [05:11<01:37,  3.12it/s, training_loss=0.374]\u001b[A\n",
            "Epoch 4:  76%|███████▌  | 967/1271 [05:11<01:37,  3.13it/s, training_loss=0.374]\u001b[A\n",
            "Epoch 4:  76%|███████▌  | 967/1271 [05:11<01:37,  3.13it/s, training_loss=0.253]\u001b[A\n",
            "Epoch 4:  76%|███████▌  | 968/1271 [05:11<01:36,  3.13it/s, training_loss=0.253]\u001b[A\n",
            "Epoch 4:  76%|███████▌  | 968/1271 [05:11<01:36,  3.13it/s, training_loss=0.414]\u001b[A\n",
            "Epoch 4:  76%|███████▌  | 969/1271 [05:11<01:36,  3.13it/s, training_loss=0.414]\u001b[A\n",
            "Epoch 4:  76%|███████▌  | 969/1271 [05:12<01:36,  3.13it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 4:  76%|███████▋  | 970/1271 [05:12<01:36,  3.10it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 4:  76%|███████▋  | 970/1271 [05:12<01:36,  3.10it/s, training_loss=0.650]\u001b[A\n",
            "Epoch 4:  76%|███████▋  | 971/1271 [05:12<01:36,  3.10it/s, training_loss=0.650]\u001b[A\n",
            "Epoch 4:  76%|███████▋  | 971/1271 [05:12<01:36,  3.10it/s, training_loss=0.430]\u001b[A\n",
            "Epoch 4:  76%|███████▋  | 972/1271 [05:12<01:35,  3.12it/s, training_loss=0.430]\u001b[A\n",
            "Epoch 4:  76%|███████▋  | 972/1271 [05:13<01:35,  3.12it/s, training_loss=0.041]\u001b[A\n",
            "Epoch 4:  77%|███████▋  | 973/1271 [05:13<01:35,  3.12it/s, training_loss=0.041]\u001b[A\n",
            "Epoch 4:  77%|███████▋  | 973/1271 [05:13<01:35,  3.12it/s, training_loss=0.015]\u001b[A\n",
            "Epoch 4:  77%|███████▋  | 974/1271 [05:13<01:35,  3.10it/s, training_loss=0.015]\u001b[A\n",
            "Epoch 4:  77%|███████▋  | 974/1271 [05:13<01:35,  3.10it/s, training_loss=0.040]\u001b[A\n",
            "Epoch 4:  77%|███████▋  | 975/1271 [05:13<01:35,  3.09it/s, training_loss=0.040]\u001b[A\n",
            "Epoch 4:  77%|███████▋  | 975/1271 [05:14<01:35,  3.09it/s, training_loss=0.302]\u001b[A\n",
            "Epoch 4:  77%|███████▋  | 976/1271 [05:14<01:35,  3.09it/s, training_loss=0.302]\u001b[A\n",
            "Epoch 4:  77%|███████▋  | 976/1271 [05:14<01:35,  3.09it/s, training_loss=0.109]\u001b[A\n",
            "Epoch 4:  77%|███████▋  | 977/1271 [05:14<01:35,  3.09it/s, training_loss=0.109]\u001b[A\n",
            "Epoch 4:  77%|███████▋  | 977/1271 [05:14<01:35,  3.09it/s, training_loss=0.449]\u001b[A\n",
            "Epoch 4:  77%|███████▋  | 978/1271 [05:14<01:34,  3.09it/s, training_loss=0.449]\u001b[A\n",
            "Epoch 4:  77%|███████▋  | 978/1271 [05:15<01:34,  3.09it/s, training_loss=0.249]\u001b[A\n",
            "Epoch 4:  77%|███████▋  | 979/1271 [05:15<01:33,  3.12it/s, training_loss=0.249]\u001b[A\n",
            "Epoch 4:  77%|███████▋  | 979/1271 [05:15<01:33,  3.12it/s, training_loss=0.758]\u001b[A\n",
            "Epoch 4:  77%|███████▋  | 980/1271 [05:15<01:33,  3.11it/s, training_loss=0.758]\u001b[A\n",
            "Epoch 4:  77%|███████▋  | 980/1271 [05:15<01:33,  3.11it/s, training_loss=0.310]\u001b[A\n",
            "Epoch 4:  77%|███████▋  | 981/1271 [05:15<01:33,  3.12it/s, training_loss=0.310]\u001b[A\n",
            "Epoch 4:  77%|███████▋  | 981/1271 [05:16<01:33,  3.12it/s, training_loss=0.074]\u001b[A\n",
            "Epoch 4:  77%|███████▋  | 982/1271 [05:16<01:32,  3.12it/s, training_loss=0.074]\u001b[A\n",
            "Epoch 4:  77%|███████▋  | 982/1271 [05:16<01:32,  3.12it/s, training_loss=0.419]\u001b[A\n",
            "Epoch 4:  77%|███████▋  | 983/1271 [05:16<01:32,  3.13it/s, training_loss=0.419]\u001b[A\n",
            "Epoch 4:  77%|███████▋  | 983/1271 [05:16<01:32,  3.13it/s, training_loss=0.275]\u001b[A\n",
            "Epoch 4:  77%|███████▋  | 984/1271 [05:16<01:32,  3.11it/s, training_loss=0.275]\u001b[A\n",
            "Epoch 4:  77%|███████▋  | 984/1271 [05:17<01:32,  3.11it/s, training_loss=0.127]\u001b[A\n",
            "Epoch 4:  77%|███████▋  | 985/1271 [05:17<01:31,  3.11it/s, training_loss=0.127]\u001b[A\n",
            "Epoch 4:  77%|███████▋  | 985/1271 [05:17<01:31,  3.11it/s, training_loss=0.208]\u001b[A\n",
            "Epoch 4:  78%|███████▊  | 986/1271 [05:17<01:31,  3.11it/s, training_loss=0.208]\u001b[A\n",
            "Epoch 4:  78%|███████▊  | 986/1271 [05:17<01:31,  3.11it/s, training_loss=0.355]\u001b[A\n",
            "Epoch 4:  78%|███████▊  | 987/1271 [05:17<01:30,  3.12it/s, training_loss=0.355]\u001b[A\n",
            "Epoch 4:  78%|███████▊  | 987/1271 [05:18<01:30,  3.12it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 4:  78%|███████▊  | 988/1271 [05:18<01:30,  3.12it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 4:  78%|███████▊  | 988/1271 [05:18<01:30,  3.12it/s, training_loss=0.029]\u001b[A\n",
            "Epoch 4:  78%|███████▊  | 989/1271 [05:18<01:30,  3.12it/s, training_loss=0.029]\u001b[A\n",
            "Epoch 4:  78%|███████▊  | 989/1271 [05:18<01:30,  3.12it/s, training_loss=0.762]\u001b[A\n",
            "Epoch 4:  78%|███████▊  | 990/1271 [05:18<01:29,  3.13it/s, training_loss=0.762]\u001b[A\n",
            "Epoch 4:  78%|███████▊  | 990/1271 [05:18<01:29,  3.13it/s, training_loss=0.294]\u001b[A\n",
            "Epoch 4:  78%|███████▊  | 991/1271 [05:18<01:29,  3.13it/s, training_loss=0.294]\u001b[A\n",
            "Epoch 4:  78%|███████▊  | 991/1271 [05:19<01:29,  3.13it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 4:  78%|███████▊  | 992/1271 [05:19<01:29,  3.13it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 4:  78%|███████▊  | 992/1271 [05:19<01:29,  3.13it/s, training_loss=0.143]\u001b[A\n",
            "Epoch 4:  78%|███████▊  | 993/1271 [05:19<01:29,  3.12it/s, training_loss=0.143]\u001b[A\n",
            "Epoch 4:  78%|███████▊  | 993/1271 [05:19<01:29,  3.12it/s, training_loss=0.447]\u001b[A\n",
            "Epoch 4:  78%|███████▊  | 994/1271 [05:19<01:28,  3.12it/s, training_loss=0.447]\u001b[A\n",
            "Epoch 4:  78%|███████▊  | 994/1271 [05:20<01:28,  3.12it/s, training_loss=0.334]\u001b[A\n",
            "Epoch 4:  78%|███████▊  | 995/1271 [05:20<01:29,  3.10it/s, training_loss=0.334]\u001b[A\n",
            "Epoch 4:  78%|███████▊  | 995/1271 [05:20<01:29,  3.10it/s, training_loss=0.485]\u001b[A\n",
            "Epoch 4:  78%|███████▊  | 996/1271 [05:20<01:28,  3.11it/s, training_loss=0.485]\u001b[A\n",
            "Epoch 4:  78%|███████▊  | 996/1271 [05:20<01:28,  3.11it/s, training_loss=0.310]\u001b[A\n",
            "Epoch 4:  78%|███████▊  | 997/1271 [05:20<01:27,  3.12it/s, training_loss=0.310]\u001b[A\n",
            "Epoch 4:  78%|███████▊  | 997/1271 [05:21<01:27,  3.12it/s, training_loss=0.054]\u001b[A\n",
            "Epoch 4:  79%|███████▊  | 998/1271 [05:21<01:27,  3.12it/s, training_loss=0.054]\u001b[A\n",
            "Epoch 4:  79%|███████▊  | 998/1271 [05:21<01:27,  3.12it/s, training_loss=0.132]\u001b[A\n",
            "Epoch 4:  79%|███████▊  | 999/1271 [05:21<01:27,  3.11it/s, training_loss=0.132]\u001b[A\n",
            "Epoch 4:  79%|███████▊  | 999/1271 [05:21<01:27,  3.11it/s, training_loss=0.516]\u001b[A\n",
            "Epoch 4:  79%|███████▊  | 1000/1271 [05:21<01:27,  3.11it/s, training_loss=0.516]\u001b[A\n",
            "Epoch 4:  79%|███████▊  | 1000/1271 [05:22<01:27,  3.11it/s, training_loss=0.013]\u001b[A\n",
            "Epoch 4:  79%|███████▉  | 1001/1271 [05:22<01:27,  3.10it/s, training_loss=0.013]\u001b[A\n",
            "Epoch 4:  79%|███████▉  | 1001/1271 [05:22<01:27,  3.10it/s, training_loss=0.253]\u001b[A\n",
            "Epoch 4:  79%|███████▉  | 1002/1271 [05:22<01:26,  3.11it/s, training_loss=0.253]\u001b[A\n",
            "Epoch 4:  79%|███████▉  | 1002/1271 [05:22<01:26,  3.11it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 4:  79%|███████▉  | 1003/1271 [05:22<01:26,  3.10it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 4:  79%|███████▉  | 1003/1271 [05:23<01:26,  3.10it/s, training_loss=0.108]\u001b[A\n",
            "Epoch 4:  79%|███████▉  | 1004/1271 [05:23<01:26,  3.09it/s, training_loss=0.108]\u001b[A\n",
            "Epoch 4:  79%|███████▉  | 1004/1271 [05:23<01:26,  3.09it/s, training_loss=0.156]\u001b[A\n",
            "Epoch 4:  79%|███████▉  | 1005/1271 [05:23<01:26,  3.09it/s, training_loss=0.156]\u001b[A\n",
            "Epoch 4:  79%|███████▉  | 1005/1271 [05:23<01:26,  3.09it/s, training_loss=0.420]\u001b[A\n",
            "Epoch 4:  79%|███████▉  | 1006/1271 [05:23<01:25,  3.09it/s, training_loss=0.420]\u001b[A\n",
            "Epoch 4:  79%|███████▉  | 1006/1271 [05:24<01:25,  3.09it/s, training_loss=0.302]\u001b[A\n",
            "Epoch 4:  79%|███████▉  | 1007/1271 [05:24<01:25,  3.10it/s, training_loss=0.302]\u001b[A\n",
            "Epoch 4:  79%|███████▉  | 1007/1271 [05:24<01:25,  3.10it/s, training_loss=0.096]\u001b[A\n",
            "Epoch 4:  79%|███████▉  | 1008/1271 [05:24<01:24,  3.11it/s, training_loss=0.096]\u001b[A\n",
            "Epoch 4:  79%|███████▉  | 1008/1271 [05:24<01:24,  3.11it/s, training_loss=0.090]\u001b[A\n",
            "Epoch 4:  79%|███████▉  | 1009/1271 [05:24<01:23,  3.12it/s, training_loss=0.090]\u001b[A\n",
            "Epoch 4:  79%|███████▉  | 1009/1271 [05:25<01:23,  3.12it/s, training_loss=0.233]\u001b[A\n",
            "Epoch 4:  79%|███████▉  | 1010/1271 [05:25<01:23,  3.13it/s, training_loss=0.233]\u001b[A\n",
            "Epoch 4:  79%|███████▉  | 1010/1271 [05:25<01:23,  3.13it/s, training_loss=0.081]\u001b[A\n",
            "Epoch 4:  80%|███████▉  | 1011/1271 [05:25<01:23,  3.13it/s, training_loss=0.081]\u001b[A\n",
            "Epoch 4:  80%|███████▉  | 1011/1271 [05:25<01:23,  3.13it/s, training_loss=0.465]\u001b[A\n",
            "Epoch 4:  80%|███████▉  | 1012/1271 [05:25<01:23,  3.11it/s, training_loss=0.465]\u001b[A\n",
            "Epoch 4:  80%|███████▉  | 1012/1271 [05:26<01:23,  3.11it/s, training_loss=0.075]\u001b[A\n",
            "Epoch 4:  80%|███████▉  | 1013/1271 [05:26<01:23,  3.10it/s, training_loss=0.075]\u001b[A\n",
            "Epoch 4:  80%|███████▉  | 1013/1271 [05:26<01:23,  3.10it/s, training_loss=0.893]\u001b[A\n",
            "Epoch 4:  80%|███████▉  | 1014/1271 [05:26<01:22,  3.10it/s, training_loss=0.893]\u001b[A\n",
            "Epoch 4:  80%|███████▉  | 1014/1271 [05:26<01:22,  3.10it/s, training_loss=0.327]\u001b[A\n",
            "Epoch 4:  80%|███████▉  | 1015/1271 [05:26<01:22,  3.10it/s, training_loss=0.327]\u001b[A\n",
            "Epoch 4:  80%|███████▉  | 1015/1271 [05:27<01:22,  3.10it/s, training_loss=0.171]\u001b[A\n",
            "Epoch 4:  80%|███████▉  | 1016/1271 [05:27<01:21,  3.12it/s, training_loss=0.171]\u001b[A\n",
            "Epoch 4:  80%|███████▉  | 1016/1271 [05:27<01:21,  3.12it/s, training_loss=0.078]\u001b[A\n",
            "Epoch 4:  80%|████████  | 1017/1271 [05:27<01:21,  3.13it/s, training_loss=0.078]\u001b[A\n",
            "Epoch 4:  80%|████████  | 1017/1271 [05:27<01:21,  3.13it/s, training_loss=0.193]\u001b[A\n",
            "Epoch 4:  80%|████████  | 1018/1271 [05:27<01:21,  3.12it/s, training_loss=0.193]\u001b[A\n",
            "Epoch 4:  80%|████████  | 1018/1271 [05:27<01:21,  3.12it/s, training_loss=0.059]\u001b[A\n",
            "Epoch 4:  80%|████████  | 1019/1271 [05:27<01:20,  3.12it/s, training_loss=0.059]\u001b[A\n",
            "Epoch 4:  80%|████████  | 1019/1271 [05:28<01:20,  3.12it/s, training_loss=0.448]\u001b[A\n",
            "Epoch 4:  80%|████████  | 1020/1271 [05:28<01:20,  3.14it/s, training_loss=0.448]\u001b[A\n",
            "Epoch 4:  80%|████████  | 1020/1271 [05:28<01:20,  3.14it/s, training_loss=0.030]\u001b[A\n",
            "Epoch 4:  80%|████████  | 1021/1271 [05:28<01:19,  3.13it/s, training_loss=0.030]\u001b[A\n",
            "Epoch 4:  80%|████████  | 1021/1271 [05:28<01:19,  3.13it/s, training_loss=0.223]\u001b[A\n",
            "Epoch 4:  80%|████████  | 1022/1271 [05:28<01:19,  3.12it/s, training_loss=0.223]\u001b[A\n",
            "Epoch 4:  80%|████████  | 1022/1271 [05:29<01:19,  3.12it/s, training_loss=0.252]\u001b[A\n",
            "Epoch 4:  80%|████████  | 1023/1271 [05:29<01:19,  3.12it/s, training_loss=0.252]\u001b[A\n",
            "Epoch 4:  80%|████████  | 1023/1271 [05:29<01:19,  3.12it/s, training_loss=0.033]\u001b[A\n",
            "Epoch 4:  81%|████████  | 1024/1271 [05:29<01:19,  3.13it/s, training_loss=0.033]\u001b[A\n",
            "Epoch 4:  81%|████████  | 1024/1271 [05:29<01:19,  3.13it/s, training_loss=0.261]\u001b[A\n",
            "Epoch 4:  81%|████████  | 1025/1271 [05:29<01:18,  3.13it/s, training_loss=0.261]\u001b[A\n",
            "Epoch 4:  81%|████████  | 1025/1271 [05:30<01:18,  3.13it/s, training_loss=0.061]\u001b[A\n",
            "Epoch 4:  81%|████████  | 1026/1271 [05:30<01:18,  3.12it/s, training_loss=0.061]\u001b[A\n",
            "Epoch 4:  81%|████████  | 1026/1271 [05:30<01:18,  3.12it/s, training_loss=0.066]\u001b[A\n",
            "Epoch 4:  81%|████████  | 1027/1271 [05:30<01:18,  3.12it/s, training_loss=0.066]\u001b[A\n",
            "Epoch 4:  81%|████████  | 1027/1271 [05:30<01:18,  3.12it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 4:  81%|████████  | 1028/1271 [05:30<01:17,  3.12it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 4:  81%|████████  | 1028/1271 [05:31<01:17,  3.12it/s, training_loss=0.528]\u001b[A\n",
            "Epoch 4:  81%|████████  | 1029/1271 [05:31<01:17,  3.12it/s, training_loss=0.528]\u001b[A\n",
            "Epoch 4:  81%|████████  | 1029/1271 [05:31<01:17,  3.12it/s, training_loss=0.145]\u001b[A\n",
            "Epoch 4:  81%|████████  | 1030/1271 [05:31<01:16,  3.13it/s, training_loss=0.145]\u001b[A\n",
            "Epoch 4:  81%|████████  | 1030/1271 [05:31<01:16,  3.13it/s, training_loss=0.229]\u001b[A\n",
            "Epoch 4:  81%|████████  | 1031/1271 [05:31<01:16,  3.12it/s, training_loss=0.229]\u001b[A\n",
            "Epoch 4:  81%|████████  | 1031/1271 [05:32<01:16,  3.12it/s, training_loss=0.454]\u001b[A\n",
            "Epoch 4:  81%|████████  | 1032/1271 [05:32<01:16,  3.13it/s, training_loss=0.454]\u001b[A\n",
            "Epoch 4:  81%|████████  | 1032/1271 [05:32<01:16,  3.13it/s, training_loss=0.539]\u001b[A\n",
            "Epoch 4:  81%|████████▏ | 1033/1271 [05:32<01:16,  3.13it/s, training_loss=0.539]\u001b[A\n",
            "Epoch 4:  81%|████████▏ | 1033/1271 [05:32<01:16,  3.13it/s, training_loss=0.412]\u001b[A\n",
            "Epoch 4:  81%|████████▏ | 1034/1271 [05:32<01:15,  3.13it/s, training_loss=0.412]\u001b[A\n",
            "Epoch 4:  81%|████████▏ | 1034/1271 [05:33<01:15,  3.13it/s, training_loss=0.081]\u001b[A\n",
            "Epoch 4:  81%|████████▏ | 1035/1271 [05:33<01:15,  3.12it/s, training_loss=0.081]\u001b[A\n",
            "Epoch 4:  81%|████████▏ | 1035/1271 [05:33<01:15,  3.12it/s, training_loss=0.271]\u001b[A\n",
            "Epoch 4:  82%|████████▏ | 1036/1271 [05:33<01:15,  3.13it/s, training_loss=0.271]\u001b[A\n",
            "Epoch 4:  82%|████████▏ | 1036/1271 [05:33<01:15,  3.13it/s, training_loss=0.380]\u001b[A\n",
            "Epoch 4:  82%|████████▏ | 1037/1271 [05:33<01:14,  3.13it/s, training_loss=0.380]\u001b[A\n",
            "Epoch 4:  82%|████████▏ | 1037/1271 [05:34<01:14,  3.13it/s, training_loss=0.448]\u001b[A\n",
            "Epoch 4:  82%|████████▏ | 1038/1271 [05:34<01:14,  3.11it/s, training_loss=0.448]\u001b[A\n",
            "Epoch 4:  82%|████████▏ | 1038/1271 [05:34<01:14,  3.11it/s, training_loss=0.050]\u001b[A\n",
            "Epoch 4:  82%|████████▏ | 1039/1271 [05:34<01:15,  3.09it/s, training_loss=0.050]\u001b[A\n",
            "Epoch 4:  82%|████████▏ | 1039/1271 [05:34<01:15,  3.09it/s, training_loss=0.379]\u001b[A\n",
            "Epoch 4:  82%|████████▏ | 1040/1271 [05:34<01:14,  3.09it/s, training_loss=0.379]\u001b[A\n",
            "Epoch 4:  82%|████████▏ | 1040/1271 [05:35<01:14,  3.09it/s, training_loss=0.469]\u001b[A\n",
            "Epoch 4:  82%|████████▏ | 1041/1271 [05:35<01:14,  3.11it/s, training_loss=0.469]\u001b[A\n",
            "Epoch 4:  82%|████████▏ | 1041/1271 [05:35<01:14,  3.11it/s, training_loss=0.125]\u001b[A\n",
            "Epoch 4:  82%|████████▏ | 1042/1271 [05:35<01:13,  3.10it/s, training_loss=0.125]\u001b[A\n",
            "Epoch 4:  82%|████████▏ | 1042/1271 [05:35<01:13,  3.10it/s, training_loss=0.210]\u001b[A\n",
            "Epoch 4:  82%|████████▏ | 1043/1271 [05:35<01:13,  3.12it/s, training_loss=0.210]\u001b[A\n",
            "Epoch 4:  82%|████████▏ | 1043/1271 [05:36<01:13,  3.12it/s, training_loss=0.272]\u001b[A\n",
            "Epoch 4:  82%|████████▏ | 1044/1271 [05:36<01:12,  3.12it/s, training_loss=0.272]\u001b[A\n",
            "Epoch 4:  82%|████████▏ | 1044/1271 [05:36<01:12,  3.12it/s, training_loss=0.083]\u001b[A\n",
            "Epoch 4:  82%|████████▏ | 1045/1271 [05:36<01:12,  3.12it/s, training_loss=0.083]\u001b[A\n",
            "Epoch 4:  82%|████████▏ | 1045/1271 [05:36<01:12,  3.12it/s, training_loss=0.277]\u001b[A\n",
            "Epoch 4:  82%|████████▏ | 1046/1271 [05:36<01:12,  3.12it/s, training_loss=0.277]\u001b[A\n",
            "Epoch 4:  82%|████████▏ | 1046/1271 [05:36<01:12,  3.12it/s, training_loss=0.175]\u001b[A\n",
            "Epoch 4:  82%|████████▏ | 1047/1271 [05:36<01:11,  3.12it/s, training_loss=0.175]\u001b[A\n",
            "Epoch 4:  82%|████████▏ | 1047/1271 [05:37<01:11,  3.12it/s, training_loss=0.314]\u001b[A\n",
            "Epoch 4:  82%|████████▏ | 1048/1271 [05:37<01:11,  3.12it/s, training_loss=0.314]\u001b[A\n",
            "Epoch 4:  82%|████████▏ | 1048/1271 [05:37<01:11,  3.12it/s, training_loss=0.211]\u001b[A\n",
            "Epoch 4:  83%|████████▎ | 1049/1271 [05:37<01:11,  3.13it/s, training_loss=0.211]\u001b[A\n",
            "Epoch 4:  83%|████████▎ | 1049/1271 [05:37<01:11,  3.13it/s, training_loss=0.297]\u001b[A\n",
            "Epoch 4:  83%|████████▎ | 1050/1271 [05:37<01:10,  3.12it/s, training_loss=0.297]\u001b[A\n",
            "Epoch 4:  83%|████████▎ | 1050/1271 [05:38<01:10,  3.12it/s, training_loss=0.272]\u001b[A\n",
            "Epoch 4:  83%|████████▎ | 1051/1271 [05:38<01:10,  3.12it/s, training_loss=0.272]\u001b[A\n",
            "Epoch 4:  83%|████████▎ | 1051/1271 [05:38<01:10,  3.12it/s, training_loss=0.220]\u001b[A\n",
            "Epoch 4:  83%|████████▎ | 1052/1271 [05:38<01:10,  3.12it/s, training_loss=0.220]\u001b[A\n",
            "Epoch 4:  83%|████████▎ | 1052/1271 [05:38<01:10,  3.12it/s, training_loss=0.052]\u001b[A\n",
            "Epoch 4:  83%|████████▎ | 1053/1271 [05:38<01:10,  3.11it/s, training_loss=0.052]\u001b[A\n",
            "Epoch 4:  83%|████████▎ | 1053/1271 [05:39<01:10,  3.11it/s, training_loss=0.148]\u001b[A\n",
            "Epoch 4:  83%|████████▎ | 1054/1271 [05:39<01:09,  3.13it/s, training_loss=0.148]\u001b[A\n",
            "Epoch 4:  83%|████████▎ | 1054/1271 [05:39<01:09,  3.13it/s, training_loss=0.261]\u001b[A\n",
            "Epoch 4:  83%|████████▎ | 1055/1271 [05:39<01:09,  3.12it/s, training_loss=0.261]\u001b[A\n",
            "Epoch 4:  83%|████████▎ | 1055/1271 [05:39<01:09,  3.12it/s, training_loss=0.313]\u001b[A\n",
            "Epoch 4:  83%|████████▎ | 1056/1271 [05:39<01:08,  3.12it/s, training_loss=0.313]\u001b[A\n",
            "Epoch 4:  83%|████████▎ | 1056/1271 [05:40<01:08,  3.12it/s, training_loss=0.188]\u001b[A\n",
            "Epoch 4:  83%|████████▎ | 1057/1271 [05:40<01:08,  3.11it/s, training_loss=0.188]\u001b[A\n",
            "Epoch 4:  83%|████████▎ | 1057/1271 [05:40<01:08,  3.11it/s, training_loss=0.679]\u001b[A\n",
            "Epoch 4:  83%|████████▎ | 1058/1271 [05:40<01:08,  3.12it/s, training_loss=0.679]\u001b[A\n",
            "Epoch 4:  83%|████████▎ | 1058/1271 [05:40<01:08,  3.12it/s, training_loss=0.383]\u001b[A\n",
            "Epoch 4:  83%|████████▎ | 1059/1271 [05:40<01:07,  3.13it/s, training_loss=0.383]\u001b[A\n",
            "Epoch 4:  83%|████████▎ | 1059/1271 [05:41<01:07,  3.13it/s, training_loss=0.092]\u001b[A\n",
            "Epoch 4:  83%|████████▎ | 1060/1271 [05:41<01:07,  3.13it/s, training_loss=0.092]\u001b[A\n",
            "Epoch 4:  83%|████████▎ | 1060/1271 [05:41<01:07,  3.13it/s, training_loss=0.047]\u001b[A\n",
            "Epoch 4:  83%|████████▎ | 1061/1271 [05:41<01:07,  3.13it/s, training_loss=0.047]\u001b[A\n",
            "Epoch 4:  83%|████████▎ | 1061/1271 [05:41<01:07,  3.13it/s, training_loss=0.485]\u001b[A\n",
            "Epoch 4:  84%|████████▎ | 1062/1271 [05:41<01:06,  3.13it/s, training_loss=0.485]\u001b[A\n",
            "Epoch 4:  84%|████████▎ | 1062/1271 [05:42<01:06,  3.13it/s, training_loss=0.050]\u001b[A\n",
            "Epoch 4:  84%|████████▎ | 1063/1271 [05:42<01:06,  3.13it/s, training_loss=0.050]\u001b[A\n",
            "Epoch 4:  84%|████████▎ | 1063/1271 [05:42<01:06,  3.13it/s, training_loss=0.750]\u001b[A\n",
            "Epoch 4:  84%|████████▎ | 1064/1271 [05:42<01:06,  3.12it/s, training_loss=0.750]\u001b[A\n",
            "Epoch 4:  84%|████████▎ | 1064/1271 [05:42<01:06,  3.12it/s, training_loss=0.035]\u001b[A\n",
            "Epoch 4:  84%|████████▍ | 1065/1271 [05:42<01:06,  3.12it/s, training_loss=0.035]\u001b[A\n",
            "Epoch 4:  84%|████████▍ | 1065/1271 [05:43<01:06,  3.12it/s, training_loss=0.176]\u001b[A\n",
            "Epoch 4:  84%|████████▍ | 1066/1271 [05:43<01:05,  3.11it/s, training_loss=0.176]\u001b[A\n",
            "Epoch 4:  84%|████████▍ | 1066/1271 [05:43<01:05,  3.11it/s, training_loss=0.184]\u001b[A\n",
            "Epoch 4:  84%|████████▍ | 1067/1271 [05:43<01:05,  3.11it/s, training_loss=0.184]\u001b[A\n",
            "Epoch 4:  84%|████████▍ | 1067/1271 [05:43<01:05,  3.11it/s, training_loss=0.536]\u001b[A\n",
            "Epoch 4:  84%|████████▍ | 1068/1271 [05:43<01:05,  3.09it/s, training_loss=0.536]\u001b[A\n",
            "Epoch 4:  84%|████████▍ | 1068/1271 [05:44<01:05,  3.09it/s, training_loss=0.610]\u001b[A\n",
            "Epoch 4:  84%|████████▍ | 1069/1271 [05:44<01:05,  3.10it/s, training_loss=0.610]\u001b[A\n",
            "Epoch 4:  84%|████████▍ | 1069/1271 [05:44<01:05,  3.10it/s, training_loss=0.392]\u001b[A\n",
            "Epoch 4:  84%|████████▍ | 1070/1271 [05:44<01:04,  3.10it/s, training_loss=0.392]\u001b[A\n",
            "Epoch 4:  84%|████████▍ | 1070/1271 [05:44<01:04,  3.10it/s, training_loss=0.070]\u001b[A\n",
            "Epoch 4:  84%|████████▍ | 1071/1271 [05:44<01:04,  3.10it/s, training_loss=0.070]\u001b[A\n",
            "Epoch 4:  84%|████████▍ | 1071/1271 [05:44<01:04,  3.10it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 4:  84%|████████▍ | 1072/1271 [05:45<01:04,  3.09it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 4:  84%|████████▍ | 1072/1271 [05:45<01:04,  3.09it/s, training_loss=0.411]\u001b[A\n",
            "Epoch 4:  84%|████████▍ | 1073/1271 [05:45<01:04,  3.09it/s, training_loss=0.411]\u001b[A\n",
            "Epoch 4:  84%|████████▍ | 1073/1271 [05:45<01:04,  3.09it/s, training_loss=0.188]\u001b[A\n",
            "Epoch 4:  85%|████████▍ | 1074/1271 [05:45<01:03,  3.09it/s, training_loss=0.188]\u001b[A\n",
            "Epoch 4:  85%|████████▍ | 1074/1271 [05:45<01:03,  3.09it/s, training_loss=0.440]\u001b[A\n",
            "Epoch 4:  85%|████████▍ | 1075/1271 [05:45<01:03,  3.10it/s, training_loss=0.440]\u001b[A\n",
            "Epoch 4:  85%|████████▍ | 1075/1271 [05:46<01:03,  3.10it/s, training_loss=0.153]\u001b[A\n",
            "Epoch 4:  85%|████████▍ | 1076/1271 [05:46<01:02,  3.10it/s, training_loss=0.153]\u001b[A\n",
            "Epoch 4:  85%|████████▍ | 1076/1271 [05:46<01:02,  3.10it/s, training_loss=0.043]\u001b[A\n",
            "Epoch 4:  85%|████████▍ | 1077/1271 [05:46<01:02,  3.12it/s, training_loss=0.043]\u001b[A\n",
            "Epoch 4:  85%|████████▍ | 1077/1271 [05:46<01:02,  3.12it/s, training_loss=0.287]\u001b[A\n",
            "Epoch 4:  85%|████████▍ | 1078/1271 [05:46<01:01,  3.12it/s, training_loss=0.287]\u001b[A\n",
            "Epoch 4:  85%|████████▍ | 1078/1271 [05:47<01:01,  3.12it/s, training_loss=0.123]\u001b[A\n",
            "Epoch 4:  85%|████████▍ | 1079/1271 [05:47<01:01,  3.12it/s, training_loss=0.123]\u001b[A\n",
            "Epoch 4:  85%|████████▍ | 1079/1271 [05:47<01:01,  3.12it/s, training_loss=0.694]\u001b[A\n",
            "Epoch 4:  85%|████████▍ | 1080/1271 [05:47<01:01,  3.12it/s, training_loss=0.694]\u001b[A\n",
            "Epoch 4:  85%|████████▍ | 1080/1271 [05:47<01:01,  3.12it/s, training_loss=0.314]\u001b[A\n",
            "Epoch 4:  85%|████████▌ | 1081/1271 [05:47<01:00,  3.12it/s, training_loss=0.314]\u001b[A\n",
            "Epoch 4:  85%|████████▌ | 1081/1271 [05:48<01:00,  3.12it/s, training_loss=0.038]\u001b[A\n",
            "Epoch 4:  85%|████████▌ | 1082/1271 [05:48<01:00,  3.12it/s, training_loss=0.038]\u001b[A\n",
            "Epoch 4:  85%|████████▌ | 1082/1271 [05:48<01:00,  3.12it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 4:  85%|████████▌ | 1083/1271 [05:48<01:00,  3.12it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 4:  85%|████████▌ | 1083/1271 [05:48<01:00,  3.12it/s, training_loss=0.178]\u001b[A\n",
            "Epoch 4:  85%|████████▌ | 1084/1271 [05:48<01:00,  3.11it/s, training_loss=0.178]\u001b[A\n",
            "Epoch 4:  85%|████████▌ | 1084/1271 [05:49<01:00,  3.11it/s, training_loss=0.339]\u001b[A\n",
            "Epoch 4:  85%|████████▌ | 1085/1271 [05:49<00:59,  3.12it/s, training_loss=0.339]\u001b[A\n",
            "Epoch 4:  85%|████████▌ | 1085/1271 [05:49<00:59,  3.12it/s, training_loss=0.329]\u001b[A\n",
            "Epoch 4:  85%|████████▌ | 1086/1271 [05:49<00:59,  3.13it/s, training_loss=0.329]\u001b[A\n",
            "Epoch 4:  85%|████████▌ | 1086/1271 [05:49<00:59,  3.13it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 4:  86%|████████▌ | 1087/1271 [05:49<00:58,  3.13it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 4:  86%|████████▌ | 1087/1271 [05:50<00:58,  3.13it/s, training_loss=0.022]\u001b[A\n",
            "Epoch 4:  86%|████████▌ | 1088/1271 [05:50<00:58,  3.11it/s, training_loss=0.022]\u001b[A\n",
            "Epoch 4:  86%|████████▌ | 1088/1271 [05:50<00:58,  3.11it/s, training_loss=0.209]\u001b[A\n",
            "Epoch 4:  86%|████████▌ | 1089/1271 [05:50<00:58,  3.11it/s, training_loss=0.209]\u001b[A\n",
            "Epoch 4:  86%|████████▌ | 1089/1271 [05:50<00:58,  3.11it/s, training_loss=0.367]\u001b[A\n",
            "Epoch 4:  86%|████████▌ | 1090/1271 [05:50<00:58,  3.11it/s, training_loss=0.367]\u001b[A\n",
            "Epoch 4:  86%|████████▌ | 1090/1271 [05:51<00:58,  3.11it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 4:  86%|████████▌ | 1091/1271 [05:51<00:57,  3.12it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 4:  86%|████████▌ | 1091/1271 [05:51<00:57,  3.12it/s, training_loss=0.411]\u001b[A\n",
            "Epoch 4:  86%|████████▌ | 1092/1271 [05:51<00:57,  3.13it/s, training_loss=0.411]\u001b[A\n",
            "Epoch 4:  86%|████████▌ | 1092/1271 [05:51<00:57,  3.13it/s, training_loss=0.465]\u001b[A\n",
            "Epoch 4:  86%|████████▌ | 1093/1271 [05:51<00:57,  3.12it/s, training_loss=0.465]\u001b[A\n",
            "Epoch 4:  86%|████████▌ | 1093/1271 [05:52<00:57,  3.12it/s, training_loss=0.325]\u001b[A\n",
            "Epoch 4:  86%|████████▌ | 1094/1271 [05:52<00:56,  3.11it/s, training_loss=0.325]\u001b[A\n",
            "Epoch 4:  86%|████████▌ | 1094/1271 [05:52<00:56,  3.11it/s, training_loss=0.178]\u001b[A\n",
            "Epoch 4:  86%|████████▌ | 1095/1271 [05:52<00:56,  3.12it/s, training_loss=0.178]\u001b[A\n",
            "Epoch 4:  86%|████████▌ | 1095/1271 [05:52<00:56,  3.12it/s, training_loss=0.161]\u001b[A\n",
            "Epoch 4:  86%|████████▌ | 1096/1271 [05:52<00:56,  3.11it/s, training_loss=0.161]\u001b[A\n",
            "Epoch 4:  86%|████████▌ | 1096/1271 [05:53<00:56,  3.11it/s, training_loss=0.110]\u001b[A\n",
            "Epoch 4:  86%|████████▋ | 1097/1271 [05:53<00:55,  3.11it/s, training_loss=0.110]\u001b[A\n",
            "Epoch 4:  86%|████████▋ | 1097/1271 [05:53<00:55,  3.11it/s, training_loss=0.175]\u001b[A\n",
            "Epoch 4:  86%|████████▋ | 1098/1271 [05:53<00:56,  3.09it/s, training_loss=0.175]\u001b[A\n",
            "Epoch 4:  86%|████████▋ | 1098/1271 [05:53<00:56,  3.09it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 4:  86%|████████▋ | 1099/1271 [05:53<00:55,  3.11it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 4:  86%|████████▋ | 1099/1271 [05:53<00:55,  3.11it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 4:  87%|████████▋ | 1100/1271 [05:53<00:54,  3.12it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 4:  87%|████████▋ | 1100/1271 [05:54<00:54,  3.12it/s, training_loss=0.360]\u001b[A\n",
            "Epoch 4:  87%|████████▋ | 1101/1271 [05:54<00:54,  3.12it/s, training_loss=0.360]\u001b[A\n",
            "Epoch 4:  87%|████████▋ | 1101/1271 [05:54<00:54,  3.12it/s, training_loss=0.090]\u001b[A\n",
            "Epoch 4:  87%|████████▋ | 1102/1271 [05:54<00:53,  3.13it/s, training_loss=0.090]\u001b[A\n",
            "Epoch 4:  87%|████████▋ | 1102/1271 [05:54<00:53,  3.13it/s, training_loss=0.512]\u001b[A\n",
            "Epoch 4:  87%|████████▋ | 1103/1271 [05:54<00:53,  3.13it/s, training_loss=0.512]\u001b[A\n",
            "Epoch 4:  87%|████████▋ | 1103/1271 [05:55<00:53,  3.13it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 4:  87%|████████▋ | 1104/1271 [05:55<00:53,  3.12it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 4:  87%|████████▋ | 1104/1271 [05:55<00:53,  3.12it/s, training_loss=0.400]\u001b[A\n",
            "Epoch 4:  87%|████████▋ | 1105/1271 [05:55<00:52,  3.14it/s, training_loss=0.400]\u001b[A\n",
            "Epoch 4:  87%|████████▋ | 1105/1271 [05:55<00:52,  3.14it/s, training_loss=0.364]\u001b[A\n",
            "Epoch 4:  87%|████████▋ | 1106/1271 [05:55<00:52,  3.13it/s, training_loss=0.364]\u001b[A\n",
            "Epoch 4:  87%|████████▋ | 1106/1271 [05:56<00:52,  3.13it/s, training_loss=0.115]\u001b[A\n",
            "Epoch 4:  87%|████████▋ | 1107/1271 [05:56<00:52,  3.14it/s, training_loss=0.115]\u001b[A\n",
            "Epoch 4:  87%|████████▋ | 1107/1271 [05:56<00:52,  3.14it/s, training_loss=0.089]\u001b[A\n",
            "Epoch 4:  87%|████████▋ | 1108/1271 [05:56<00:52,  3.11it/s, training_loss=0.089]\u001b[A\n",
            "Epoch 4:  87%|████████▋ | 1108/1271 [05:56<00:52,  3.11it/s, training_loss=0.135]\u001b[A\n",
            "Epoch 4:  87%|████████▋ | 1109/1271 [05:56<00:52,  3.12it/s, training_loss=0.135]\u001b[A\n",
            "Epoch 4:  87%|████████▋ | 1109/1271 [05:57<00:52,  3.12it/s, training_loss=0.229]\u001b[A\n",
            "Epoch 4:  87%|████████▋ | 1110/1271 [05:57<00:51,  3.11it/s, training_loss=0.229]\u001b[A\n",
            "Epoch 4:  87%|████████▋ | 1110/1271 [05:57<00:51,  3.11it/s, training_loss=0.740]\u001b[A\n",
            "Epoch 4:  87%|████████▋ | 1111/1271 [05:57<00:51,  3.11it/s, training_loss=0.740]\u001b[A\n",
            "Epoch 4:  87%|████████▋ | 1111/1271 [05:57<00:51,  3.11it/s, training_loss=0.215]\u001b[A\n",
            "Epoch 4:  87%|████████▋ | 1112/1271 [05:57<00:51,  3.11it/s, training_loss=0.215]\u001b[A\n",
            "Epoch 4:  87%|████████▋ | 1112/1271 [05:58<00:51,  3.11it/s, training_loss=0.055]\u001b[A\n",
            "Epoch 4:  88%|████████▊ | 1113/1271 [05:58<00:50,  3.10it/s, training_loss=0.055]\u001b[A\n",
            "Epoch 4:  88%|████████▊ | 1113/1271 [05:58<00:50,  3.10it/s, training_loss=0.332]\u001b[A\n",
            "Epoch 4:  88%|████████▊ | 1114/1271 [05:58<00:50,  3.08it/s, training_loss=0.332]\u001b[A\n",
            "Epoch 4:  88%|████████▊ | 1114/1271 [05:58<00:50,  3.08it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 4:  88%|████████▊ | 1115/1271 [05:58<00:50,  3.10it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 4:  88%|████████▊ | 1115/1271 [05:59<00:50,  3.10it/s, training_loss=0.382]\u001b[A\n",
            "Epoch 4:  88%|████████▊ | 1116/1271 [05:59<00:50,  3.08it/s, training_loss=0.382]\u001b[A\n",
            "Epoch 4:  88%|████████▊ | 1116/1271 [05:59<00:50,  3.08it/s, training_loss=0.280]\u001b[A\n",
            "Epoch 4:  88%|████████▊ | 1117/1271 [05:59<00:49,  3.09it/s, training_loss=0.280]\u001b[A\n",
            "Epoch 4:  88%|████████▊ | 1117/1271 [05:59<00:49,  3.09it/s, training_loss=0.150]\u001b[A\n",
            "Epoch 4:  88%|████████▊ | 1118/1271 [05:59<00:49,  3.10it/s, training_loss=0.150]\u001b[A\n",
            "Epoch 4:  88%|████████▊ | 1118/1271 [06:00<00:49,  3.10it/s, training_loss=0.556]\u001b[A\n",
            "Epoch 4:  88%|████████▊ | 1119/1271 [06:00<00:48,  3.12it/s, training_loss=0.556]\u001b[A\n",
            "Epoch 4:  88%|████████▊ | 1119/1271 [06:00<00:48,  3.12it/s, training_loss=0.294]\u001b[A\n",
            "Epoch 4:  88%|████████▊ | 1120/1271 [06:00<00:48,  3.11it/s, training_loss=0.294]\u001b[A\n",
            "Epoch 4:  88%|████████▊ | 1120/1271 [06:00<00:48,  3.11it/s, training_loss=0.302]\u001b[A\n",
            "Epoch 4:  88%|████████▊ | 1121/1271 [06:00<00:48,  3.11it/s, training_loss=0.302]\u001b[A\n",
            "Epoch 4:  88%|████████▊ | 1121/1271 [06:01<00:48,  3.11it/s, training_loss=0.329]\u001b[A\n",
            "Epoch 4:  88%|████████▊ | 1122/1271 [06:01<00:47,  3.11it/s, training_loss=0.329]\u001b[A\n",
            "Epoch 4:  88%|████████▊ | 1122/1271 [06:01<00:47,  3.11it/s, training_loss=0.234]\u001b[A\n",
            "Epoch 4:  88%|████████▊ | 1123/1271 [06:01<00:47,  3.12it/s, training_loss=0.234]\u001b[A\n",
            "Epoch 4:  88%|████████▊ | 1123/1271 [06:01<00:47,  3.12it/s, training_loss=0.179]\u001b[A\n",
            "Epoch 4:  88%|████████▊ | 1124/1271 [06:01<00:47,  3.12it/s, training_loss=0.179]\u001b[A\n",
            "Epoch 4:  88%|████████▊ | 1124/1271 [06:02<00:47,  3.12it/s, training_loss=0.333]\u001b[A\n",
            "Epoch 4:  89%|████████▊ | 1125/1271 [06:02<00:46,  3.12it/s, training_loss=0.333]\u001b[A\n",
            "Epoch 4:  89%|████████▊ | 1125/1271 [06:02<00:46,  3.12it/s, training_loss=0.621]\u001b[A\n",
            "Epoch 4:  89%|████████▊ | 1126/1271 [06:02<00:46,  3.12it/s, training_loss=0.621]\u001b[A\n",
            "Epoch 4:  89%|████████▊ | 1126/1271 [06:02<00:46,  3.12it/s, training_loss=0.014]\u001b[A\n",
            "Epoch 4:  89%|████████▊ | 1127/1271 [06:02<00:46,  3.10it/s, training_loss=0.014]\u001b[A\n",
            "Epoch 4:  89%|████████▊ | 1127/1271 [06:02<00:46,  3.10it/s, training_loss=0.234]\u001b[A\n",
            "Epoch 4:  89%|████████▊ | 1128/1271 [06:02<00:46,  3.11it/s, training_loss=0.234]\u001b[A\n",
            "Epoch 4:  89%|████████▊ | 1128/1271 [06:03<00:46,  3.11it/s, training_loss=0.124]\u001b[A\n",
            "Epoch 4:  89%|████████▉ | 1129/1271 [06:03<00:45,  3.10it/s, training_loss=0.124]\u001b[A\n",
            "Epoch 4:  89%|████████▉ | 1129/1271 [06:03<00:45,  3.10it/s, training_loss=0.010]\u001b[A\n",
            "Epoch 4:  89%|████████▉ | 1130/1271 [06:03<00:45,  3.12it/s, training_loss=0.010]\u001b[A\n",
            "Epoch 4:  89%|████████▉ | 1130/1271 [06:03<00:45,  3.12it/s, training_loss=0.675]\u001b[A\n",
            "Epoch 4:  89%|████████▉ | 1131/1271 [06:03<00:44,  3.11it/s, training_loss=0.675]\u001b[A\n",
            "Epoch 4:  89%|████████▉ | 1131/1271 [06:04<00:44,  3.11it/s, training_loss=0.214]\u001b[A\n",
            "Epoch 4:  89%|████████▉ | 1132/1271 [06:04<00:44,  3.12it/s, training_loss=0.214]\u001b[A\n",
            "Epoch 4:  89%|████████▉ | 1132/1271 [06:04<00:44,  3.12it/s, training_loss=0.480]\u001b[A\n",
            "Epoch 4:  89%|████████▉ | 1133/1271 [06:04<00:44,  3.11it/s, training_loss=0.480]\u001b[A\n",
            "Epoch 4:  89%|████████▉ | 1133/1271 [06:04<00:44,  3.11it/s, training_loss=0.126]\u001b[A\n",
            "Epoch 4:  89%|████████▉ | 1134/1271 [06:04<00:43,  3.12it/s, training_loss=0.126]\u001b[A\n",
            "Epoch 4:  89%|████████▉ | 1134/1271 [06:05<00:43,  3.12it/s, training_loss=0.582]\u001b[A\n",
            "Epoch 4:  89%|████████▉ | 1135/1271 [06:05<00:43,  3.10it/s, training_loss=0.582]\u001b[A\n",
            "Epoch 4:  89%|████████▉ | 1135/1271 [06:05<00:43,  3.10it/s, training_loss=0.112]\u001b[A\n",
            "Epoch 4:  89%|████████▉ | 1136/1271 [06:05<00:43,  3.11it/s, training_loss=0.112]\u001b[A\n",
            "Epoch 4:  89%|████████▉ | 1136/1271 [06:05<00:43,  3.11it/s, training_loss=0.536]\u001b[A\n",
            "Epoch 4:  89%|████████▉ | 1137/1271 [06:05<00:43,  3.11it/s, training_loss=0.536]\u001b[A\n",
            "Epoch 4:  89%|████████▉ | 1137/1271 [06:06<00:43,  3.11it/s, training_loss=0.361]\u001b[A\n",
            "Epoch 4:  90%|████████▉ | 1138/1271 [06:06<00:42,  3.11it/s, training_loss=0.361]\u001b[A\n",
            "Epoch 4:  90%|████████▉ | 1138/1271 [06:06<00:42,  3.11it/s, training_loss=0.510]\u001b[A\n",
            "Epoch 4:  90%|████████▉ | 1139/1271 [06:06<00:42,  3.13it/s, training_loss=0.510]\u001b[A\n",
            "Epoch 4:  90%|████████▉ | 1139/1271 [06:06<00:42,  3.13it/s, training_loss=0.121]\u001b[A\n",
            "Epoch 4:  90%|████████▉ | 1140/1271 [06:06<00:41,  3.13it/s, training_loss=0.121]\u001b[A\n",
            "Epoch 4:  90%|████████▉ | 1140/1271 [06:07<00:41,  3.13it/s, training_loss=0.185]\u001b[A\n",
            "Epoch 4:  90%|████████▉ | 1141/1271 [06:07<00:41,  3.11it/s, training_loss=0.185]\u001b[A\n",
            "Epoch 4:  90%|████████▉ | 1141/1271 [06:07<00:41,  3.11it/s, training_loss=0.143]\u001b[A\n",
            "Epoch 4:  90%|████████▉ | 1142/1271 [06:07<00:41,  3.10it/s, training_loss=0.143]\u001b[A\n",
            "Epoch 4:  90%|████████▉ | 1142/1271 [06:07<00:41,  3.10it/s, training_loss=0.193]\u001b[A\n",
            "Epoch 4:  90%|████████▉ | 1143/1271 [06:07<00:41,  3.11it/s, training_loss=0.193]\u001b[A\n",
            "Epoch 4:  90%|████████▉ | 1143/1271 [06:08<00:41,  3.11it/s, training_loss=0.035]\u001b[A\n",
            "Epoch 4:  90%|█████████ | 1144/1271 [06:08<00:40,  3.10it/s, training_loss=0.035]\u001b[A\n",
            "Epoch 4:  90%|█████████ | 1144/1271 [06:08<00:40,  3.10it/s, training_loss=0.107]\u001b[A\n",
            "Epoch 4:  90%|█████████ | 1145/1271 [06:08<00:40,  3.09it/s, training_loss=0.107]\u001b[A\n",
            "Epoch 4:  90%|█████████ | 1145/1271 [06:08<00:40,  3.09it/s, training_loss=0.089]\u001b[A\n",
            "Epoch 4:  90%|█████████ | 1146/1271 [06:08<00:40,  3.09it/s, training_loss=0.089]\u001b[A\n",
            "Epoch 4:  90%|█████████ | 1146/1271 [06:09<00:40,  3.09it/s, training_loss=0.414]\u001b[A\n",
            "Epoch 4:  90%|█████████ | 1147/1271 [06:09<00:40,  3.10it/s, training_loss=0.414]\u001b[A\n",
            "Epoch 4:  90%|█████████ | 1147/1271 [06:09<00:40,  3.10it/s, training_loss=0.234]\u001b[A\n",
            "Epoch 4:  90%|█████████ | 1148/1271 [06:09<00:39,  3.11it/s, training_loss=0.234]\u001b[A\n",
            "Epoch 4:  90%|█████████ | 1148/1271 [06:09<00:39,  3.11it/s, training_loss=0.036]\u001b[A\n",
            "Epoch 4:  90%|█████████ | 1149/1271 [06:09<00:39,  3.10it/s, training_loss=0.036]\u001b[A\n",
            "Epoch 4:  90%|█████████ | 1149/1271 [06:10<00:39,  3.10it/s, training_loss=0.116]\u001b[A\n",
            "Epoch 4:  90%|█████████ | 1150/1271 [06:10<00:38,  3.10it/s, training_loss=0.116]\u001b[A\n",
            "Epoch 4:  90%|█████████ | 1150/1271 [06:10<00:38,  3.10it/s, training_loss=0.154]\u001b[A\n",
            "Epoch 4:  91%|█████████ | 1151/1271 [06:10<00:38,  3.11it/s, training_loss=0.154]\u001b[A\n",
            "Epoch 4:  91%|█████████ | 1151/1271 [06:10<00:38,  3.11it/s, training_loss=0.208]\u001b[A\n",
            "Epoch 4:  91%|█████████ | 1152/1271 [06:10<00:38,  3.11it/s, training_loss=0.208]\u001b[A\n",
            "Epoch 4:  91%|█████████ | 1152/1271 [06:11<00:38,  3.11it/s, training_loss=0.489]\u001b[A\n",
            "Epoch 4:  91%|█████████ | 1153/1271 [06:11<00:37,  3.12it/s, training_loss=0.489]\u001b[A\n",
            "Epoch 4:  91%|█████████ | 1153/1271 [06:11<00:37,  3.12it/s, training_loss=0.603]\u001b[A\n",
            "Epoch 4:  91%|█████████ | 1154/1271 [06:11<00:37,  3.11it/s, training_loss=0.603]\u001b[A\n",
            "Epoch 4:  91%|█████████ | 1154/1271 [06:11<00:37,  3.11it/s, training_loss=0.035]\u001b[A\n",
            "Epoch 4:  91%|█████████ | 1155/1271 [06:11<00:37,  3.11it/s, training_loss=0.035]\u001b[A\n",
            "Epoch 4:  91%|█████████ | 1155/1271 [06:11<00:37,  3.11it/s, training_loss=0.088]\u001b[A\n",
            "Epoch 4:  91%|█████████ | 1156/1271 [06:11<00:37,  3.11it/s, training_loss=0.088]\u001b[A\n",
            "Epoch 4:  91%|█████████ | 1156/1271 [06:12<00:37,  3.11it/s, training_loss=0.501]\u001b[A\n",
            "Epoch 4:  91%|█████████ | 1157/1271 [06:12<00:36,  3.10it/s, training_loss=0.501]\u001b[A\n",
            "Epoch 4:  91%|█████████ | 1157/1271 [06:12<00:36,  3.10it/s, training_loss=0.069]\u001b[A\n",
            "Epoch 4:  91%|█████████ | 1158/1271 [06:12<00:36,  3.10it/s, training_loss=0.069]\u001b[A\n",
            "Epoch 4:  91%|█████████ | 1158/1271 [06:12<00:36,  3.10it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 4:  91%|█████████ | 1159/1271 [06:12<00:36,  3.10it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 4:  91%|█████████ | 1159/1271 [06:13<00:36,  3.10it/s, training_loss=0.195]\u001b[A\n",
            "Epoch 4:  91%|█████████▏| 1160/1271 [06:13<00:35,  3.11it/s, training_loss=0.195]\u001b[A\n",
            "Epoch 4:  91%|█████████▏| 1160/1271 [06:13<00:35,  3.11it/s, training_loss=0.112]\u001b[A\n",
            "Epoch 4:  91%|█████████▏| 1161/1271 [06:13<00:35,  3.11it/s, training_loss=0.112]\u001b[A\n",
            "Epoch 4:  91%|█████████▏| 1161/1271 [06:13<00:35,  3.11it/s, training_loss=0.953]\u001b[A\n",
            "Epoch 4:  91%|█████████▏| 1162/1271 [06:13<00:35,  3.11it/s, training_loss=0.953]\u001b[A\n",
            "Epoch 4:  91%|█████████▏| 1162/1271 [06:14<00:35,  3.11it/s, training_loss=0.366]\u001b[A\n",
            "Epoch 4:  92%|█████████▏| 1163/1271 [06:14<00:34,  3.12it/s, training_loss=0.366]\u001b[A\n",
            "Epoch 4:  92%|█████████▏| 1163/1271 [06:14<00:34,  3.12it/s, training_loss=0.431]\u001b[A\n",
            "Epoch 4:  92%|█████████▏| 1164/1271 [06:14<00:34,  3.11it/s, training_loss=0.431]\u001b[A\n",
            "Epoch 4:  92%|█████████▏| 1164/1271 [06:14<00:34,  3.11it/s, training_loss=0.423]\u001b[A\n",
            "Epoch 4:  92%|█████████▏| 1165/1271 [06:14<00:34,  3.11it/s, training_loss=0.423]\u001b[A\n",
            "Epoch 4:  92%|█████████▏| 1165/1271 [06:15<00:34,  3.11it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 4:  92%|█████████▏| 1166/1271 [06:15<00:33,  3.12it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 4:  92%|█████████▏| 1166/1271 [06:15<00:33,  3.12it/s, training_loss=0.101]\u001b[A\n",
            "Epoch 4:  92%|█████████▏| 1167/1271 [06:15<00:33,  3.13it/s, training_loss=0.101]\u001b[A\n",
            "Epoch 4:  92%|█████████▏| 1167/1271 [06:15<00:33,  3.13it/s, training_loss=0.599]\u001b[A\n",
            "Epoch 4:  92%|█████████▏| 1168/1271 [06:15<00:32,  3.12it/s, training_loss=0.599]\u001b[A\n",
            "Epoch 4:  92%|█████████▏| 1168/1271 [06:16<00:32,  3.12it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 4:  92%|█████████▏| 1169/1271 [06:16<00:32,  3.13it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 4:  92%|█████████▏| 1169/1271 [06:16<00:32,  3.13it/s, training_loss=0.122]\u001b[A\n",
            "Epoch 4:  92%|█████████▏| 1170/1271 [06:16<00:32,  3.13it/s, training_loss=0.122]\u001b[A\n",
            "Epoch 4:  92%|█████████▏| 1170/1271 [06:16<00:32,  3.13it/s, training_loss=0.221]\u001b[A\n",
            "Epoch 4:  92%|█████████▏| 1171/1271 [06:16<00:32,  3.11it/s, training_loss=0.221]\u001b[A\n",
            "Epoch 4:  92%|█████████▏| 1171/1271 [06:17<00:32,  3.11it/s, training_loss=0.412]\u001b[A\n",
            "Epoch 4:  92%|█████████▏| 1172/1271 [06:17<00:31,  3.10it/s, training_loss=0.412]\u001b[A\n",
            "Epoch 4:  92%|█████████▏| 1172/1271 [06:17<00:31,  3.10it/s, training_loss=0.266]\u001b[A\n",
            "Epoch 4:  92%|█████████▏| 1173/1271 [06:17<00:31,  3.09it/s, training_loss=0.266]\u001b[A\n",
            "Epoch 4:  92%|█████████▏| 1173/1271 [06:17<00:31,  3.09it/s, training_loss=0.178]\u001b[A\n",
            "Epoch 4:  92%|█████████▏| 1174/1271 [06:17<00:31,  3.09it/s, training_loss=0.178]\u001b[A\n",
            "Epoch 4:  92%|█████████▏| 1174/1271 [06:18<00:31,  3.09it/s, training_loss=0.292]\u001b[A\n",
            "Epoch 4:  92%|█████████▏| 1175/1271 [06:18<00:31,  3.10it/s, training_loss=0.292]\u001b[A\n",
            "Epoch 4:  92%|█████████▏| 1175/1271 [06:18<00:31,  3.10it/s, training_loss=0.157]\u001b[A\n",
            "Epoch 4:  93%|█████████▎| 1176/1271 [06:18<00:30,  3.11it/s, training_loss=0.157]\u001b[A\n",
            "Epoch 4:  93%|█████████▎| 1176/1271 [06:18<00:30,  3.11it/s, training_loss=0.142]\u001b[A\n",
            "Epoch 4:  93%|█████████▎| 1177/1271 [06:18<00:30,  3.10it/s, training_loss=0.142]\u001b[A\n",
            "Epoch 4:  93%|█████████▎| 1177/1271 [06:19<00:30,  3.10it/s, training_loss=0.127]\u001b[A\n",
            "Epoch 4:  93%|█████████▎| 1178/1271 [06:19<00:29,  3.12it/s, training_loss=0.127]\u001b[A\n",
            "Epoch 4:  93%|█████████▎| 1178/1271 [06:19<00:29,  3.12it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 4:  93%|█████████▎| 1179/1271 [06:19<00:29,  3.13it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 4:  93%|█████████▎| 1179/1271 [06:19<00:29,  3.13it/s, training_loss=0.495]\u001b[A\n",
            "Epoch 4:  93%|█████████▎| 1180/1271 [06:19<00:29,  3.11it/s, training_loss=0.495]\u001b[A\n",
            "Epoch 4:  93%|█████████▎| 1180/1271 [06:20<00:29,  3.11it/s, training_loss=0.174]\u001b[A\n",
            "Epoch 4:  93%|█████████▎| 1181/1271 [06:20<00:28,  3.11it/s, training_loss=0.174]\u001b[A\n",
            "Epoch 4:  93%|█████████▎| 1181/1271 [06:20<00:28,  3.11it/s, training_loss=0.331]\u001b[A\n",
            "Epoch 4:  93%|█████████▎| 1182/1271 [06:20<00:28,  3.11it/s, training_loss=0.331]\u001b[A\n",
            "Epoch 4:  93%|█████████▎| 1182/1271 [06:20<00:28,  3.11it/s, training_loss=0.100]\u001b[A\n",
            "Epoch 4:  93%|█████████▎| 1183/1271 [06:20<00:28,  3.11it/s, training_loss=0.100]\u001b[A\n",
            "Epoch 4:  93%|█████████▎| 1183/1271 [06:20<00:28,  3.11it/s, training_loss=0.112]\u001b[A\n",
            "Epoch 4:  93%|█████████▎| 1184/1271 [06:21<00:28,  3.11it/s, training_loss=0.112]\u001b[A\n",
            "Epoch 4:  93%|█████████▎| 1184/1271 [06:21<00:28,  3.11it/s, training_loss=0.358]\u001b[A\n",
            "Epoch 4:  93%|█████████▎| 1185/1271 [06:21<00:27,  3.10it/s, training_loss=0.358]\u001b[A\n",
            "Epoch 4:  93%|█████████▎| 1185/1271 [06:21<00:27,  3.10it/s, training_loss=0.066]\u001b[A\n",
            "Epoch 4:  93%|█████████▎| 1186/1271 [06:21<00:27,  3.09it/s, training_loss=0.066]\u001b[A\n",
            "Epoch 4:  93%|█████████▎| 1186/1271 [06:21<00:27,  3.09it/s, training_loss=0.297]\u001b[A\n",
            "Epoch 4:  93%|█████████▎| 1187/1271 [06:21<00:27,  3.10it/s, training_loss=0.297]\u001b[A\n",
            "Epoch 4:  93%|█████████▎| 1187/1271 [06:22<00:27,  3.10it/s, training_loss=0.739]\u001b[A\n",
            "Epoch 4:  93%|█████████▎| 1188/1271 [06:22<00:26,  3.10it/s, training_loss=0.739]\u001b[A\n",
            "Epoch 4:  93%|█████████▎| 1188/1271 [06:22<00:26,  3.10it/s, training_loss=0.269]\u001b[A\n",
            "Epoch 4:  94%|█████████▎| 1189/1271 [06:22<00:26,  3.12it/s, training_loss=0.269]\u001b[A\n",
            "Epoch 4:  94%|█████████▎| 1189/1271 [06:22<00:26,  3.12it/s, training_loss=0.135]\u001b[A\n",
            "Epoch 4:  94%|█████████▎| 1190/1271 [06:22<00:25,  3.13it/s, training_loss=0.135]\u001b[A\n",
            "Epoch 4:  94%|█████████▎| 1190/1271 [06:23<00:25,  3.13it/s, training_loss=0.278]\u001b[A\n",
            "Epoch 4:  94%|█████████▎| 1191/1271 [06:23<00:25,  3.11it/s, training_loss=0.278]\u001b[A\n",
            "Epoch 4:  94%|█████████▎| 1191/1271 [06:23<00:25,  3.11it/s, training_loss=0.407]\u001b[A\n",
            "Epoch 4:  94%|█████████▍| 1192/1271 [06:23<00:25,  3.12it/s, training_loss=0.407]\u001b[A\n",
            "Epoch 4:  94%|█████████▍| 1192/1271 [06:23<00:25,  3.12it/s, training_loss=0.350]\u001b[A\n",
            "Epoch 4:  94%|█████████▍| 1193/1271 [06:23<00:24,  3.12it/s, training_loss=0.350]\u001b[A\n",
            "Epoch 4:  94%|█████████▍| 1193/1271 [06:24<00:24,  3.12it/s, training_loss=0.264]\u001b[A\n",
            "Epoch 4:  94%|█████████▍| 1194/1271 [06:24<00:24,  3.11it/s, training_loss=0.264]\u001b[A\n",
            "Epoch 4:  94%|█████████▍| 1194/1271 [06:24<00:24,  3.11it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 4:  94%|█████████▍| 1195/1271 [06:24<00:24,  3.11it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 4:  94%|█████████▍| 1195/1271 [06:24<00:24,  3.11it/s, training_loss=0.394]\u001b[A\n",
            "Epoch 4:  94%|█████████▍| 1196/1271 [06:24<00:24,  3.10it/s, training_loss=0.394]\u001b[A\n",
            "Epoch 4:  94%|█████████▍| 1196/1271 [06:25<00:24,  3.10it/s, training_loss=0.219]\u001b[A\n",
            "Epoch 4:  94%|█████████▍| 1197/1271 [06:25<00:23,  3.09it/s, training_loss=0.219]\u001b[A\n",
            "Epoch 4:  94%|█████████▍| 1197/1271 [06:25<00:23,  3.09it/s, training_loss=0.260]\u001b[A\n",
            "Epoch 4:  94%|█████████▍| 1198/1271 [06:25<00:23,  3.11it/s, training_loss=0.260]\u001b[A\n",
            "Epoch 4:  94%|█████████▍| 1198/1271 [06:25<00:23,  3.11it/s, training_loss=0.267]\u001b[A\n",
            "Epoch 4:  94%|█████████▍| 1199/1271 [06:25<00:23,  3.10it/s, training_loss=0.267]\u001b[A\n",
            "Epoch 4:  94%|█████████▍| 1199/1271 [06:26<00:23,  3.10it/s, training_loss=0.001]\u001b[A\n",
            "Epoch 4:  94%|█████████▍| 1200/1271 [06:26<00:22,  3.11it/s, training_loss=0.001]\u001b[A\n",
            "Epoch 4:  94%|█████████▍| 1200/1271 [06:26<00:22,  3.11it/s, training_loss=0.167]\u001b[A\n",
            "Epoch 4:  94%|█████████▍| 1201/1271 [06:26<00:22,  3.11it/s, training_loss=0.167]\u001b[A\n",
            "Epoch 4:  94%|█████████▍| 1201/1271 [06:26<00:22,  3.11it/s, training_loss=0.620]\u001b[A\n",
            "Epoch 4:  95%|█████████▍| 1202/1271 [06:26<00:22,  3.12it/s, training_loss=0.620]\u001b[A\n",
            "Epoch 4:  95%|█████████▍| 1202/1271 [06:27<00:22,  3.12it/s, training_loss=0.155]\u001b[A\n",
            "Epoch 4:  95%|█████████▍| 1203/1271 [06:27<00:21,  3.13it/s, training_loss=0.155]\u001b[A\n",
            "Epoch 4:  95%|█████████▍| 1203/1271 [06:27<00:21,  3.13it/s, training_loss=0.080]\u001b[A\n",
            "Epoch 4:  95%|█████████▍| 1204/1271 [06:27<00:21,  3.11it/s, training_loss=0.080]\u001b[A\n",
            "Epoch 4:  95%|█████████▍| 1204/1271 [06:27<00:21,  3.11it/s, training_loss=0.476]\u001b[A\n",
            "Epoch 4:  95%|█████████▍| 1205/1271 [06:27<00:21,  3.10it/s, training_loss=0.476]\u001b[A\n",
            "Epoch 4:  95%|█████████▍| 1205/1271 [06:28<00:21,  3.10it/s, training_loss=0.046]\u001b[A\n",
            "Epoch 4:  95%|█████████▍| 1206/1271 [06:28<00:20,  3.10it/s, training_loss=0.046]\u001b[A\n",
            "Epoch 4:  95%|█████████▍| 1206/1271 [06:28<00:20,  3.10it/s, training_loss=0.147]\u001b[A\n",
            "Epoch 4:  95%|█████████▍| 1207/1271 [06:28<00:20,  3.10it/s, training_loss=0.147]\u001b[A\n",
            "Epoch 4:  95%|█████████▍| 1207/1271 [06:28<00:20,  3.10it/s, training_loss=0.284]\u001b[A\n",
            "Epoch 4:  95%|█████████▌| 1208/1271 [06:28<00:20,  3.11it/s, training_loss=0.284]\u001b[A\n",
            "Epoch 4:  95%|█████████▌| 1208/1271 [06:29<00:20,  3.11it/s, training_loss=0.123]\u001b[A\n",
            "Epoch 4:  95%|█████████▌| 1209/1271 [06:29<00:19,  3.11it/s, training_loss=0.123]\u001b[A\n",
            "Epoch 4:  95%|█████████▌| 1209/1271 [06:29<00:19,  3.11it/s, training_loss=0.060]\u001b[A\n",
            "Epoch 4:  95%|█████████▌| 1210/1271 [06:29<00:19,  3.09it/s, training_loss=0.060]\u001b[A\n",
            "Epoch 4:  95%|█████████▌| 1210/1271 [06:29<00:19,  3.09it/s, training_loss=0.057]\u001b[A\n",
            "Epoch 4:  95%|█████████▌| 1211/1271 [06:29<00:19,  3.09it/s, training_loss=0.057]\u001b[A\n",
            "Epoch 4:  95%|█████████▌| 1211/1271 [06:30<00:19,  3.09it/s, training_loss=0.142]\u001b[A\n",
            "Epoch 4:  95%|█████████▌| 1212/1271 [06:30<00:19,  3.10it/s, training_loss=0.142]\u001b[A\n",
            "Epoch 4:  95%|█████████▌| 1212/1271 [06:30<00:19,  3.10it/s, training_loss=0.040]\u001b[A\n",
            "Epoch 4:  95%|█████████▌| 1213/1271 [06:30<00:18,  3.11it/s, training_loss=0.040]\u001b[A\n",
            "Epoch 4:  95%|█████████▌| 1213/1271 [06:30<00:18,  3.11it/s, training_loss=0.205]\u001b[A\n",
            "Epoch 4:  96%|█████████▌| 1214/1271 [06:30<00:18,  3.11it/s, training_loss=0.205]\u001b[A\n",
            "Epoch 4:  96%|█████████▌| 1214/1271 [06:30<00:18,  3.11it/s, training_loss=0.297]\u001b[A\n",
            "Epoch 4:  96%|█████████▌| 1215/1271 [06:30<00:17,  3.12it/s, training_loss=0.297]\u001b[A\n",
            "Epoch 4:  96%|█████████▌| 1215/1271 [06:31<00:17,  3.12it/s, training_loss=0.296]\u001b[A\n",
            "Epoch 4:  96%|█████████▌| 1216/1271 [06:31<00:17,  3.12it/s, training_loss=0.296]\u001b[A\n",
            "Epoch 4:  96%|█████████▌| 1216/1271 [06:31<00:17,  3.12it/s, training_loss=0.642]\u001b[A\n",
            "Epoch 4:  96%|█████████▌| 1217/1271 [06:31<00:17,  3.12it/s, training_loss=0.642]\u001b[A\n",
            "Epoch 4:  96%|█████████▌| 1217/1271 [06:31<00:17,  3.12it/s, training_loss=0.218]\u001b[A\n",
            "Epoch 4:  96%|█████████▌| 1218/1271 [06:31<00:17,  3.12it/s, training_loss=0.218]\u001b[A\n",
            "Epoch 4:  96%|█████████▌| 1218/1271 [06:32<00:17,  3.12it/s, training_loss=0.329]\u001b[A\n",
            "Epoch 4:  96%|█████████▌| 1219/1271 [06:32<00:16,  3.09it/s, training_loss=0.329]\u001b[A\n",
            "Epoch 4:  96%|█████████▌| 1219/1271 [06:32<00:16,  3.09it/s, training_loss=0.119]\u001b[A\n",
            "Epoch 4:  96%|█████████▌| 1220/1271 [06:32<00:16,  3.10it/s, training_loss=0.119]\u001b[A\n",
            "Epoch 4:  96%|█████████▌| 1220/1271 [06:32<00:16,  3.10it/s, training_loss=0.035]\u001b[A\n",
            "Epoch 4:  96%|█████████▌| 1221/1271 [06:32<00:16,  3.11it/s, training_loss=0.035]\u001b[A\n",
            "Epoch 4:  96%|█████████▌| 1221/1271 [06:33<00:16,  3.11it/s, training_loss=0.382]\u001b[A\n",
            "Epoch 4:  96%|█████████▌| 1222/1271 [06:33<00:15,  3.12it/s, training_loss=0.382]\u001b[A\n",
            "Epoch 4:  96%|█████████▌| 1222/1271 [06:33<00:15,  3.12it/s, training_loss=0.025]\u001b[A\n",
            "Epoch 4:  96%|█████████▌| 1223/1271 [06:33<00:15,  3.12it/s, training_loss=0.025]\u001b[A\n",
            "Epoch 4:  96%|█████████▌| 1223/1271 [06:33<00:15,  3.12it/s, training_loss=0.296]\u001b[A\n",
            "Epoch 4:  96%|█████████▋| 1224/1271 [06:33<00:15,  3.12it/s, training_loss=0.296]\u001b[A\n",
            "Epoch 4:  96%|█████████▋| 1224/1271 [06:34<00:15,  3.12it/s, training_loss=0.270]\u001b[A\n",
            "Epoch 4:  96%|█████████▋| 1225/1271 [06:34<00:14,  3.13it/s, training_loss=0.270]\u001b[A\n",
            "Epoch 4:  96%|█████████▋| 1225/1271 [06:34<00:14,  3.13it/s, training_loss=0.050]\u001b[A\n",
            "Epoch 4:  96%|█████████▋| 1226/1271 [06:34<00:14,  3.12it/s, training_loss=0.050]\u001b[A\n",
            "Epoch 4:  96%|█████████▋| 1226/1271 [06:34<00:14,  3.12it/s, training_loss=0.272]\u001b[A\n",
            "Epoch 4:  97%|█████████▋| 1227/1271 [06:34<00:14,  3.11it/s, training_loss=0.272]\u001b[A\n",
            "Epoch 4:  97%|█████████▋| 1227/1271 [06:35<00:14,  3.11it/s, training_loss=0.108]\u001b[A\n",
            "Epoch 4:  97%|█████████▋| 1228/1271 [06:35<00:13,  3.11it/s, training_loss=0.108]\u001b[A\n",
            "Epoch 4:  97%|█████████▋| 1228/1271 [06:35<00:13,  3.11it/s, training_loss=0.501]\u001b[A\n",
            "Epoch 4:  97%|█████████▋| 1229/1271 [06:35<00:13,  3.11it/s, training_loss=0.501]\u001b[A\n",
            "Epoch 4:  97%|█████████▋| 1229/1271 [06:35<00:13,  3.11it/s, training_loss=0.344]\u001b[A\n",
            "Epoch 4:  97%|█████████▋| 1230/1271 [06:35<00:13,  3.10it/s, training_loss=0.344]\u001b[A\n",
            "Epoch 4:  97%|█████████▋| 1230/1271 [06:36<00:13,  3.10it/s, training_loss=0.382]\u001b[A\n",
            "Epoch 4:  97%|█████████▋| 1231/1271 [06:36<00:12,  3.11it/s, training_loss=0.382]\u001b[A\n",
            "Epoch 4:  97%|█████████▋| 1231/1271 [06:36<00:12,  3.11it/s, training_loss=0.521]\u001b[A\n",
            "Epoch 4:  97%|█████████▋| 1232/1271 [06:36<00:12,  3.10it/s, training_loss=0.521]\u001b[A\n",
            "Epoch 4:  97%|█████████▋| 1232/1271 [06:36<00:12,  3.10it/s, training_loss=0.030]\u001b[A\n",
            "Epoch 4:  97%|█████████▋| 1233/1271 [06:36<00:12,  3.11it/s, training_loss=0.030]\u001b[A\n",
            "Epoch 4:  97%|█████████▋| 1233/1271 [06:37<00:12,  3.11it/s, training_loss=0.057]\u001b[A\n",
            "Epoch 4:  97%|█████████▋| 1234/1271 [06:37<00:11,  3.11it/s, training_loss=0.057]\u001b[A\n",
            "Epoch 4:  97%|█████████▋| 1234/1271 [06:37<00:11,  3.11it/s, training_loss=0.136]\u001b[A\n",
            "Epoch 4:  97%|█████████▋| 1235/1271 [06:37<00:11,  3.13it/s, training_loss=0.136]\u001b[A\n",
            "Epoch 4:  97%|█████████▋| 1235/1271 [06:37<00:11,  3.13it/s, training_loss=0.238]\u001b[A\n",
            "Epoch 4:  97%|█████████▋| 1236/1271 [06:37<00:11,  3.12it/s, training_loss=0.238]\u001b[A\n",
            "Epoch 4:  97%|█████████▋| 1236/1271 [06:38<00:11,  3.12it/s, training_loss=0.180]\u001b[A\n",
            "Epoch 4:  97%|█████████▋| 1237/1271 [06:38<00:10,  3.11it/s, training_loss=0.180]\u001b[A\n",
            "Epoch 4:  97%|█████████▋| 1237/1271 [06:38<00:10,  3.11it/s, training_loss=0.364]\u001b[A\n",
            "Epoch 4:  97%|█████████▋| 1238/1271 [06:38<00:10,  3.12it/s, training_loss=0.364]\u001b[A\n",
            "Epoch 4:  97%|█████████▋| 1238/1271 [06:38<00:10,  3.12it/s, training_loss=0.065]\u001b[A\n",
            "Epoch 4:  97%|█████████▋| 1239/1271 [06:38<00:10,  3.11it/s, training_loss=0.065]\u001b[A\n",
            "Epoch 4:  97%|█████████▋| 1239/1271 [06:39<00:10,  3.11it/s, training_loss=0.573]\u001b[A\n",
            "Epoch 4:  98%|█████████▊| 1240/1271 [06:39<00:09,  3.12it/s, training_loss=0.573]\u001b[A\n",
            "Epoch 4:  98%|█████████▊| 1240/1271 [06:39<00:09,  3.12it/s, training_loss=0.288]\u001b[A\n",
            "Epoch 4:  98%|█████████▊| 1241/1271 [06:39<00:09,  3.11it/s, training_loss=0.288]\u001b[A\n",
            "Epoch 4:  98%|█████████▊| 1241/1271 [06:39<00:09,  3.11it/s, training_loss=0.177]\u001b[A\n",
            "Epoch 4:  98%|█████████▊| 1242/1271 [06:39<00:09,  3.11it/s, training_loss=0.177]\u001b[A\n",
            "Epoch 4:  98%|█████████▊| 1242/1271 [06:39<00:09,  3.11it/s, training_loss=0.503]\u001b[A\n",
            "Epoch 4:  98%|█████████▊| 1243/1271 [06:39<00:09,  3.10it/s, training_loss=0.503]\u001b[A\n",
            "Epoch 4:  98%|█████████▊| 1243/1271 [06:40<00:09,  3.10it/s, training_loss=0.831]\u001b[A\n",
            "Epoch 4:  98%|█████████▊| 1244/1271 [06:40<00:08,  3.09it/s, training_loss=0.831]\u001b[A\n",
            "Epoch 4:  98%|█████████▊| 1244/1271 [06:40<00:08,  3.09it/s, training_loss=0.526]\u001b[A\n",
            "Epoch 4:  98%|█████████▊| 1245/1271 [06:40<00:08,  3.10it/s, training_loss=0.526]\u001b[A\n",
            "Epoch 4:  98%|█████████▊| 1245/1271 [06:40<00:08,  3.10it/s, training_loss=0.100]\u001b[A\n",
            "Epoch 4:  98%|█████████▊| 1246/1271 [06:40<00:08,  3.10it/s, training_loss=0.100]\u001b[A\n",
            "Epoch 4:  98%|█████████▊| 1246/1271 [06:41<00:08,  3.10it/s, training_loss=0.546]\u001b[A\n",
            "Epoch 4:  98%|█████████▊| 1247/1271 [06:41<00:07,  3.10it/s, training_loss=0.546]\u001b[A\n",
            "Epoch 4:  98%|█████████▊| 1247/1271 [06:41<00:07,  3.10it/s, training_loss=0.149]\u001b[A\n",
            "Epoch 4:  98%|█████████▊| 1248/1271 [06:41<00:07,  3.10it/s, training_loss=0.149]\u001b[A\n",
            "Epoch 4:  98%|█████████▊| 1248/1271 [06:41<00:07,  3.10it/s, training_loss=0.151]\u001b[A\n",
            "Epoch 4:  98%|█████████▊| 1249/1271 [06:41<00:07,  3.09it/s, training_loss=0.151]\u001b[A\n",
            "Epoch 4:  98%|█████████▊| 1249/1271 [06:42<00:07,  3.09it/s, training_loss=0.195]\u001b[A\n",
            "Epoch 4:  98%|█████████▊| 1250/1271 [06:42<00:06,  3.11it/s, training_loss=0.195]\u001b[A\n",
            "Epoch 4:  98%|█████████▊| 1250/1271 [06:42<00:06,  3.11it/s, training_loss=0.311]\u001b[A\n",
            "Epoch 4:  98%|█████████▊| 1251/1271 [06:42<00:06,  3.12it/s, training_loss=0.311]\u001b[A\n",
            "Epoch 4:  98%|█████████▊| 1251/1271 [06:42<00:06,  3.12it/s, training_loss=0.406]\u001b[A\n",
            "Epoch 4:  99%|█████████▊| 1252/1271 [06:42<00:06,  3.09it/s, training_loss=0.406]\u001b[A\n",
            "Epoch 4:  99%|█████████▊| 1252/1271 [06:43<00:06,  3.09it/s, training_loss=0.765]\u001b[A\n",
            "Epoch 4:  99%|█████████▊| 1253/1271 [06:43<00:05,  3.11it/s, training_loss=0.765]\u001b[A\n",
            "Epoch 4:  99%|█████████▊| 1253/1271 [06:43<00:05,  3.11it/s, training_loss=0.254]\u001b[A\n",
            "Epoch 4:  99%|█████████▊| 1254/1271 [06:43<00:05,  3.12it/s, training_loss=0.254]\u001b[A\n",
            "Epoch 4:  99%|█████████▊| 1254/1271 [06:43<00:05,  3.12it/s, training_loss=0.190]\u001b[A\n",
            "Epoch 4:  99%|█████████▊| 1255/1271 [06:43<00:05,  3.12it/s, training_loss=0.190]\u001b[A\n",
            "Epoch 4:  99%|█████████▊| 1255/1271 [06:44<00:05,  3.12it/s, training_loss=0.167]\u001b[A\n",
            "Epoch 4:  99%|█████████▉| 1256/1271 [06:44<00:04,  3.11it/s, training_loss=0.167]\u001b[A\n",
            "Epoch 4:  99%|█████████▉| 1256/1271 [06:44<00:04,  3.11it/s, training_loss=0.042]\u001b[A\n",
            "Epoch 4:  99%|█████████▉| 1257/1271 [06:44<00:04,  3.12it/s, training_loss=0.042]\u001b[A\n",
            "Epoch 4:  99%|█████████▉| 1257/1271 [06:44<00:04,  3.12it/s, training_loss=0.185]\u001b[A\n",
            "Epoch 4:  99%|█████████▉| 1258/1271 [06:44<00:04,  3.10it/s, training_loss=0.185]\u001b[A\n",
            "Epoch 4:  99%|█████████▉| 1258/1271 [06:45<00:04,  3.10it/s, training_loss=0.089]\u001b[A\n",
            "Epoch 4:  99%|█████████▉| 1259/1271 [06:45<00:03,  3.12it/s, training_loss=0.089]\u001b[A\n",
            "Epoch 4:  99%|█████████▉| 1259/1271 [06:45<00:03,  3.12it/s, training_loss=0.560]\u001b[A\n",
            "Epoch 4:  99%|█████████▉| 1260/1271 [06:45<00:03,  3.13it/s, training_loss=0.560]\u001b[A\n",
            "Epoch 4:  99%|█████████▉| 1260/1271 [06:45<00:03,  3.13it/s, training_loss=0.363]\u001b[A\n",
            "Epoch 4:  99%|█████████▉| 1261/1271 [06:45<00:03,  3.12it/s, training_loss=0.363]\u001b[A\n",
            "Epoch 4:  99%|█████████▉| 1261/1271 [06:46<00:03,  3.12it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 4:  99%|█████████▉| 1262/1271 [06:46<00:02,  3.11it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 4:  99%|█████████▉| 1262/1271 [06:46<00:02,  3.11it/s, training_loss=0.319]\u001b[A\n",
            "Epoch 4:  99%|█████████▉| 1263/1271 [06:46<00:02,  3.11it/s, training_loss=0.319]\u001b[A\n",
            "Epoch 4:  99%|█████████▉| 1263/1271 [06:46<00:02,  3.11it/s, training_loss=0.007]\u001b[A\n",
            "Epoch 4:  99%|█████████▉| 1264/1271 [06:46<00:02,  3.11it/s, training_loss=0.007]\u001b[A\n",
            "Epoch 4:  99%|█████████▉| 1264/1271 [06:47<00:02,  3.11it/s, training_loss=0.142]\u001b[A\n",
            "Epoch 4: 100%|█████████▉| 1265/1271 [06:47<00:01,  3.10it/s, training_loss=0.142]\u001b[A\n",
            "Epoch 4: 100%|█████████▉| 1265/1271 [06:47<00:01,  3.10it/s, training_loss=0.222]\u001b[A\n",
            "Epoch 4: 100%|█████████▉| 1266/1271 [06:47<00:01,  3.08it/s, training_loss=0.222]\u001b[A\n",
            "Epoch 4: 100%|█████████▉| 1266/1271 [06:47<00:01,  3.08it/s, training_loss=0.173]\u001b[A\n",
            "Epoch 4: 100%|█████████▉| 1267/1271 [06:47<00:01,  3.08it/s, training_loss=0.173]\u001b[A\n",
            "Epoch 4: 100%|█████████▉| 1267/1271 [06:48<00:01,  3.08it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 4: 100%|█████████▉| 1268/1271 [06:48<00:00,  3.09it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 4: 100%|█████████▉| 1268/1271 [06:48<00:00,  3.09it/s, training_loss=0.293]\u001b[A\n",
            "Epoch 4: 100%|█████████▉| 1269/1271 [06:48<00:00,  3.10it/s, training_loss=0.293]\u001b[A\n",
            "Epoch 4: 100%|█████████▉| 1269/1271 [06:48<00:00,  3.10it/s, training_loss=0.276]\u001b[A\n",
            "Epoch 4: 100%|█████████▉| 1270/1271 [06:48<00:00,  3.11it/s, training_loss=0.276]\u001b[A\n",
            "Epoch 4: 100%|█████████▉| 1270/1271 [06:48<00:00,  3.11it/s, training_loss=0.236]\u001b[A\n",
            "Epoch 4: 100%|██████████| 1271/1271 [06:48<00:00,  3.73it/s, training_loss=0.236]\u001b[A\n",
            " 60%|██████    | 3/5 [28:34<14:26, 433.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4\n",
            "Training loss: 0.762079257326989\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 4/5 [28:54<07:14, 434.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 0.7947616682439629\n",
            "F1 Score (Weighted): 0.7344331106199063\n",
            "Recall@5: 0.9791976225854383\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5:   0%|          | 0/1271 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5:   0%|          | 0/1271 [00:00<?, ?it/s, training_loss=0.200]\u001b[A\n",
            "Epoch 5:   0%|          | 1/1271 [00:00<06:50,  3.10it/s, training_loss=0.200]\u001b[A\n",
            "Epoch 5:   0%|          | 1/1271 [00:00<06:50,  3.10it/s, training_loss=0.116]\u001b[A\n",
            "Epoch 5:   0%|          | 2/1271 [00:00<06:51,  3.09it/s, training_loss=0.116]\u001b[A\n",
            "Epoch 5:   0%|          | 2/1271 [00:00<06:51,  3.09it/s, training_loss=0.563]\u001b[A\n",
            "Epoch 5:   0%|          | 3/1271 [00:00<06:50,  3.09it/s, training_loss=0.563]\u001b[A\n",
            "Epoch 5:   0%|          | 3/1271 [00:01<06:50,  3.09it/s, training_loss=0.267]\u001b[A\n",
            "Epoch 5:   0%|          | 4/1271 [00:01<06:52,  3.07it/s, training_loss=0.267]\u001b[A\n",
            "Epoch 5:   0%|          | 4/1271 [00:01<06:52,  3.07it/s, training_loss=0.520]\u001b[A\n",
            "Epoch 5:   0%|          | 5/1271 [00:01<06:51,  3.08it/s, training_loss=0.520]\u001b[A\n",
            "Epoch 5:   0%|          | 5/1271 [00:01<06:51,  3.08it/s, training_loss=0.129]\u001b[A\n",
            "Epoch 5:   0%|          | 6/1271 [00:01<06:47,  3.10it/s, training_loss=0.129]\u001b[A\n",
            "Epoch 5:   0%|          | 6/1271 [00:02<06:47,  3.10it/s, training_loss=0.196]\u001b[A\n",
            "Epoch 5:   1%|          | 7/1271 [00:02<06:53,  3.05it/s, training_loss=0.196]\u001b[A\n",
            "Epoch 5:   1%|          | 7/1271 [00:02<06:53,  3.05it/s, training_loss=0.170]\u001b[A\n",
            "Epoch 5:   1%|          | 8/1271 [00:02<06:55,  3.04it/s, training_loss=0.170]\u001b[A\n",
            "Epoch 5:   1%|          | 8/1271 [00:02<06:55,  3.04it/s, training_loss=0.454]\u001b[A\n",
            "Epoch 5:   1%|          | 9/1271 [00:02<06:55,  3.04it/s, training_loss=0.454]\u001b[A\n",
            "Epoch 5:   1%|          | 9/1271 [00:03<06:55,  3.04it/s, training_loss=0.520]\u001b[A\n",
            "Epoch 5:   1%|          | 10/1271 [00:03<06:53,  3.05it/s, training_loss=0.520]\u001b[A\n",
            "Epoch 5:   1%|          | 10/1271 [00:03<06:53,  3.05it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 5:   1%|          | 11/1271 [00:03<06:53,  3.05it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 5:   1%|          | 11/1271 [00:03<06:53,  3.05it/s, training_loss=0.066]\u001b[A\n",
            "Epoch 5:   1%|          | 12/1271 [00:03<06:50,  3.06it/s, training_loss=0.066]\u001b[A\n",
            "Epoch 5:   1%|          | 12/1271 [00:04<06:50,  3.06it/s, training_loss=0.219]\u001b[A\n",
            "Epoch 5:   1%|          | 13/1271 [00:04<06:48,  3.08it/s, training_loss=0.219]\u001b[A\n",
            "Epoch 5:   1%|          | 13/1271 [00:04<06:48,  3.08it/s, training_loss=0.074]\u001b[A\n",
            "Epoch 5:   1%|          | 14/1271 [00:04<06:50,  3.06it/s, training_loss=0.074]\u001b[A\n",
            "Epoch 5:   1%|          | 14/1271 [00:04<06:50,  3.06it/s, training_loss=0.387]\u001b[A\n",
            "Epoch 5:   1%|          | 15/1271 [00:04<06:50,  3.06it/s, training_loss=0.387]\u001b[A\n",
            "Epoch 5:   1%|          | 15/1271 [00:05<06:50,  3.06it/s, training_loss=0.792]\u001b[A\n",
            "Epoch 5:   1%|▏         | 16/1271 [00:05<06:45,  3.10it/s, training_loss=0.792]\u001b[A\n",
            "Epoch 5:   1%|▏         | 16/1271 [00:05<06:45,  3.10it/s, training_loss=0.108]\u001b[A\n",
            "Epoch 5:   1%|▏         | 17/1271 [00:05<06:44,  3.10it/s, training_loss=0.108]\u001b[A\n",
            "Epoch 5:   1%|▏         | 17/1271 [00:05<06:44,  3.10it/s, training_loss=0.186]\u001b[A\n",
            "Epoch 5:   1%|▏         | 18/1271 [00:05<06:45,  3.09it/s, training_loss=0.186]\u001b[A\n",
            "Epoch 5:   1%|▏         | 18/1271 [00:06<06:45,  3.09it/s, training_loss=0.265]\u001b[A\n",
            "Epoch 5:   1%|▏         | 19/1271 [00:06<06:46,  3.08it/s, training_loss=0.265]\u001b[A\n",
            "Epoch 5:   1%|▏         | 19/1271 [00:06<06:46,  3.08it/s, training_loss=0.219]\u001b[A\n",
            "Epoch 5:   2%|▏         | 20/1271 [00:06<06:47,  3.07it/s, training_loss=0.219]\u001b[A\n",
            "Epoch 5:   2%|▏         | 20/1271 [00:06<06:47,  3.07it/s, training_loss=0.018]\u001b[A\n",
            "Epoch 5:   2%|▏         | 21/1271 [00:06<06:47,  3.07it/s, training_loss=0.018]\u001b[A\n",
            "Epoch 5:   2%|▏         | 21/1271 [00:07<06:47,  3.07it/s, training_loss=0.410]\u001b[A\n",
            "Epoch 5:   2%|▏         | 22/1271 [00:07<06:45,  3.08it/s, training_loss=0.410]\u001b[A\n",
            "Epoch 5:   2%|▏         | 22/1271 [00:07<06:45,  3.08it/s, training_loss=0.242]\u001b[A\n",
            "Epoch 5:   2%|▏         | 23/1271 [00:07<06:45,  3.08it/s, training_loss=0.242]\u001b[A\n",
            "Epoch 5:   2%|▏         | 23/1271 [00:07<06:45,  3.08it/s, training_loss=0.028]\u001b[A\n",
            "Epoch 5:   2%|▏         | 24/1271 [00:07<06:46,  3.07it/s, training_loss=0.028]\u001b[A\n",
            "Epoch 5:   2%|▏         | 24/1271 [00:08<06:46,  3.07it/s, training_loss=0.074]\u001b[A\n",
            "Epoch 5:   2%|▏         | 25/1271 [00:08<06:45,  3.08it/s, training_loss=0.074]\u001b[A\n",
            "Epoch 5:   2%|▏         | 25/1271 [00:08<06:45,  3.08it/s, training_loss=0.191]\u001b[A\n",
            "Epoch 5:   2%|▏         | 26/1271 [00:08<06:53,  3.01it/s, training_loss=0.191]\u001b[A\n",
            "Epoch 5:   2%|▏         | 26/1271 [00:08<06:53,  3.01it/s, training_loss=0.172]\u001b[A\n",
            "Epoch 5:   2%|▏         | 27/1271 [00:08<06:56,  2.99it/s, training_loss=0.172]\u001b[A\n",
            "Epoch 5:   2%|▏         | 27/1271 [00:09<06:56,  2.99it/s, training_loss=0.095]\u001b[A\n",
            "Epoch 5:   2%|▏         | 28/1271 [00:09<06:56,  2.98it/s, training_loss=0.095]\u001b[A\n",
            "Epoch 5:   2%|▏         | 28/1271 [00:09<06:56,  2.98it/s, training_loss=0.228]\u001b[A\n",
            "Epoch 5:   2%|▏         | 29/1271 [00:09<06:52,  3.01it/s, training_loss=0.228]\u001b[A\n",
            "Epoch 5:   2%|▏         | 29/1271 [00:09<06:52,  3.01it/s, training_loss=0.079]\u001b[A\n",
            "Epoch 5:   2%|▏         | 30/1271 [00:09<06:49,  3.03it/s, training_loss=0.079]\u001b[A\n",
            "Epoch 5:   2%|▏         | 30/1271 [00:10<06:49,  3.03it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 5:   2%|▏         | 31/1271 [00:10<06:53,  3.00it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 5:   2%|▏         | 31/1271 [00:10<06:53,  3.00it/s, training_loss=0.045]\u001b[A\n",
            "Epoch 5:   3%|▎         | 32/1271 [00:10<06:48,  3.03it/s, training_loss=0.045]\u001b[A\n",
            "Epoch 5:   3%|▎         | 32/1271 [00:10<06:48,  3.03it/s, training_loss=0.201]\u001b[A\n",
            "Epoch 5:   3%|▎         | 33/1271 [00:10<06:48,  3.03it/s, training_loss=0.201]\u001b[A\n",
            "Epoch 5:   3%|▎         | 33/1271 [00:11<06:48,  3.03it/s, training_loss=0.254]\u001b[A\n",
            "Epoch 5:   3%|▎         | 34/1271 [00:11<06:45,  3.05it/s, training_loss=0.254]\u001b[A\n",
            "Epoch 5:   3%|▎         | 34/1271 [00:11<06:45,  3.05it/s, training_loss=0.118]\u001b[A\n",
            "Epoch 5:   3%|▎         | 35/1271 [00:11<06:48,  3.03it/s, training_loss=0.118]\u001b[A\n",
            "Epoch 5:   3%|▎         | 35/1271 [00:11<06:48,  3.03it/s, training_loss=0.337]\u001b[A\n",
            "Epoch 5:   3%|▎         | 36/1271 [00:11<06:45,  3.05it/s, training_loss=0.337]\u001b[A\n",
            "Epoch 5:   3%|▎         | 36/1271 [00:12<06:45,  3.05it/s, training_loss=0.401]\u001b[A\n",
            "Epoch 5:   3%|▎         | 37/1271 [00:12<06:43,  3.06it/s, training_loss=0.401]\u001b[A\n",
            "Epoch 5:   3%|▎         | 37/1271 [00:12<06:43,  3.06it/s, training_loss=0.109]\u001b[A\n",
            "Epoch 5:   3%|▎         | 38/1271 [00:12<06:44,  3.05it/s, training_loss=0.109]\u001b[A\n",
            "Epoch 5:   3%|▎         | 38/1271 [00:12<06:44,  3.05it/s, training_loss=0.335]\u001b[A\n",
            "Epoch 5:   3%|▎         | 39/1271 [00:12<06:41,  3.07it/s, training_loss=0.335]\u001b[A\n",
            "Epoch 5:   3%|▎         | 39/1271 [00:13<06:41,  3.07it/s, training_loss=0.813]\u001b[A\n",
            "Epoch 5:   3%|▎         | 40/1271 [00:13<06:40,  3.08it/s, training_loss=0.813]\u001b[A\n",
            "Epoch 5:   3%|▎         | 40/1271 [00:13<06:40,  3.08it/s, training_loss=0.193]\u001b[A\n",
            "Epoch 5:   3%|▎         | 41/1271 [00:13<06:38,  3.08it/s, training_loss=0.193]\u001b[A\n",
            "Epoch 5:   3%|▎         | 41/1271 [00:13<06:38,  3.08it/s, training_loss=0.034]\u001b[A\n",
            "Epoch 5:   3%|▎         | 42/1271 [00:13<06:39,  3.08it/s, training_loss=0.034]\u001b[A\n",
            "Epoch 5:   3%|▎         | 42/1271 [00:14<06:39,  3.08it/s, training_loss=0.263]\u001b[A\n",
            "Epoch 5:   3%|▎         | 43/1271 [00:14<06:40,  3.07it/s, training_loss=0.263]\u001b[A\n",
            "Epoch 5:   3%|▎         | 43/1271 [00:14<06:40,  3.07it/s, training_loss=0.238]\u001b[A\n",
            "Epoch 5:   3%|▎         | 44/1271 [00:14<06:39,  3.07it/s, training_loss=0.238]\u001b[A\n",
            "Epoch 5:   3%|▎         | 44/1271 [00:14<06:39,  3.07it/s, training_loss=0.054]\u001b[A\n",
            "Epoch 5:   4%|▎         | 45/1271 [00:14<06:40,  3.06it/s, training_loss=0.054]\u001b[A\n",
            "Epoch 5:   4%|▎         | 45/1271 [00:15<06:40,  3.06it/s, training_loss=0.182]\u001b[A\n",
            "Epoch 5:   4%|▎         | 46/1271 [00:15<06:39,  3.06it/s, training_loss=0.182]\u001b[A\n",
            "Epoch 5:   4%|▎         | 46/1271 [00:15<06:39,  3.06it/s, training_loss=0.171]\u001b[A\n",
            "Epoch 5:   4%|▎         | 47/1271 [00:15<06:39,  3.06it/s, training_loss=0.171]\u001b[A\n",
            "Epoch 5:   4%|▎         | 47/1271 [00:15<06:39,  3.06it/s, training_loss=0.205]\u001b[A\n",
            "Epoch 5:   4%|▍         | 48/1271 [00:15<06:39,  3.06it/s, training_loss=0.205]\u001b[A\n",
            "Epoch 5:   4%|▍         | 48/1271 [00:16<06:39,  3.06it/s, training_loss=0.405]\u001b[A\n",
            "Epoch 5:   4%|▍         | 49/1271 [00:16<06:37,  3.07it/s, training_loss=0.405]\u001b[A\n",
            "Epoch 5:   4%|▍         | 49/1271 [00:16<06:37,  3.07it/s, training_loss=0.534]\u001b[A\n",
            "Epoch 5:   4%|▍         | 50/1271 [00:16<06:38,  3.07it/s, training_loss=0.534]\u001b[A\n",
            "Epoch 5:   4%|▍         | 50/1271 [00:16<06:38,  3.07it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 5:   4%|▍         | 51/1271 [00:16<06:38,  3.06it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 5:   4%|▍         | 51/1271 [00:16<06:38,  3.06it/s, training_loss=0.072]\u001b[A\n",
            "Epoch 5:   4%|▍         | 52/1271 [00:16<06:36,  3.07it/s, training_loss=0.072]\u001b[A\n",
            "Epoch 5:   4%|▍         | 52/1271 [00:17<06:36,  3.07it/s, training_loss=0.049]\u001b[A\n",
            "Epoch 5:   4%|▍         | 53/1271 [00:17<06:34,  3.09it/s, training_loss=0.049]\u001b[A\n",
            "Epoch 5:   4%|▍         | 53/1271 [00:17<06:34,  3.09it/s, training_loss=0.316]\u001b[A\n",
            "Epoch 5:   4%|▍         | 54/1271 [00:17<06:34,  3.09it/s, training_loss=0.316]\u001b[A\n",
            "Epoch 5:   4%|▍         | 54/1271 [00:17<06:34,  3.09it/s, training_loss=0.151]\u001b[A\n",
            "Epoch 5:   4%|▍         | 55/1271 [00:17<06:34,  3.09it/s, training_loss=0.151]\u001b[A\n",
            "Epoch 5:   4%|▍         | 55/1271 [00:18<06:34,  3.09it/s, training_loss=0.159]\u001b[A\n",
            "Epoch 5:   4%|▍         | 56/1271 [00:18<06:34,  3.08it/s, training_loss=0.159]\u001b[A\n",
            "Epoch 5:   4%|▍         | 56/1271 [00:18<06:34,  3.08it/s, training_loss=0.143]\u001b[A\n",
            "Epoch 5:   4%|▍         | 57/1271 [00:18<06:33,  3.09it/s, training_loss=0.143]\u001b[A\n",
            "Epoch 5:   4%|▍         | 57/1271 [00:18<06:33,  3.09it/s, training_loss=0.549]\u001b[A\n",
            "Epoch 5:   5%|▍         | 58/1271 [00:18<06:33,  3.09it/s, training_loss=0.549]\u001b[A\n",
            "Epoch 5:   5%|▍         | 58/1271 [00:19<06:33,  3.09it/s, training_loss=0.311]\u001b[A\n",
            "Epoch 5:   5%|▍         | 59/1271 [00:19<06:31,  3.10it/s, training_loss=0.311]\u001b[A\n",
            "Epoch 5:   5%|▍         | 59/1271 [00:19<06:31,  3.10it/s, training_loss=0.294]\u001b[A\n",
            "Epoch 5:   5%|▍         | 60/1271 [00:19<06:32,  3.09it/s, training_loss=0.294]\u001b[A\n",
            "Epoch 5:   5%|▍         | 60/1271 [00:19<06:32,  3.09it/s, training_loss=0.079]\u001b[A\n",
            "Epoch 5:   5%|▍         | 61/1271 [00:19<06:32,  3.08it/s, training_loss=0.079]\u001b[A\n",
            "Epoch 5:   5%|▍         | 61/1271 [00:20<06:32,  3.08it/s, training_loss=0.031]\u001b[A\n",
            "Epoch 5:   5%|▍         | 62/1271 [00:20<06:33,  3.07it/s, training_loss=0.031]\u001b[A\n",
            "Epoch 5:   5%|▍         | 62/1271 [00:20<06:33,  3.07it/s, training_loss=0.567]\u001b[A\n",
            "Epoch 5:   5%|▍         | 63/1271 [00:20<06:31,  3.09it/s, training_loss=0.567]\u001b[A\n",
            "Epoch 5:   5%|▍         | 63/1271 [00:20<06:31,  3.09it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 5:   5%|▌         | 64/1271 [00:20<06:31,  3.09it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 5:   5%|▌         | 64/1271 [00:21<06:31,  3.09it/s, training_loss=0.257]\u001b[A\n",
            "Epoch 5:   5%|▌         | 65/1271 [00:21<06:31,  3.08it/s, training_loss=0.257]\u001b[A\n",
            "Epoch 5:   5%|▌         | 65/1271 [00:21<06:31,  3.08it/s, training_loss=0.128]\u001b[A\n",
            "Epoch 5:   5%|▌         | 66/1271 [00:21<06:29,  3.09it/s, training_loss=0.128]\u001b[A\n",
            "Epoch 5:   5%|▌         | 66/1271 [00:21<06:29,  3.09it/s, training_loss=0.317]\u001b[A\n",
            "Epoch 5:   5%|▌         | 67/1271 [00:21<06:27,  3.10it/s, training_loss=0.317]\u001b[A\n",
            "Epoch 5:   5%|▌         | 67/1271 [00:22<06:27,  3.10it/s, training_loss=0.288]\u001b[A\n",
            "Epoch 5:   5%|▌         | 68/1271 [00:22<06:27,  3.10it/s, training_loss=0.288]\u001b[A\n",
            "Epoch 5:   5%|▌         | 68/1271 [00:22<06:27,  3.10it/s, training_loss=0.184]\u001b[A\n",
            "Epoch 5:   5%|▌         | 69/1271 [00:22<06:26,  3.11it/s, training_loss=0.184]\u001b[A\n",
            "Epoch 5:   5%|▌         | 69/1271 [00:22<06:26,  3.11it/s, training_loss=0.199]\u001b[A\n",
            "Epoch 5:   6%|▌         | 70/1271 [00:22<06:26,  3.11it/s, training_loss=0.199]\u001b[A\n",
            "Epoch 5:   6%|▌         | 70/1271 [00:23<06:26,  3.11it/s, training_loss=0.512]\u001b[A\n",
            "Epoch 5:   6%|▌         | 71/1271 [00:23<06:24,  3.12it/s, training_loss=0.512]\u001b[A\n",
            "Epoch 5:   6%|▌         | 71/1271 [00:23<06:24,  3.12it/s, training_loss=0.059]\u001b[A\n",
            "Epoch 5:   6%|▌         | 72/1271 [00:23<06:24,  3.12it/s, training_loss=0.059]\u001b[A\n",
            "Epoch 5:   6%|▌         | 72/1271 [00:23<06:24,  3.12it/s, training_loss=0.618]\u001b[A\n",
            "Epoch 5:   6%|▌         | 73/1271 [00:23<06:24,  3.12it/s, training_loss=0.618]\u001b[A\n",
            "Epoch 5:   6%|▌         | 73/1271 [00:24<06:24,  3.12it/s, training_loss=0.251]\u001b[A\n",
            "Epoch 5:   6%|▌         | 74/1271 [00:24<06:25,  3.10it/s, training_loss=0.251]\u001b[A\n",
            "Epoch 5:   6%|▌         | 74/1271 [00:24<06:25,  3.10it/s, training_loss=0.247]\u001b[A\n",
            "Epoch 5:   6%|▌         | 75/1271 [00:24<06:23,  3.12it/s, training_loss=0.247]\u001b[A\n",
            "Epoch 5:   6%|▌         | 75/1271 [00:24<06:23,  3.12it/s, training_loss=0.288]\u001b[A\n",
            "Epoch 5:   6%|▌         | 76/1271 [00:24<06:22,  3.13it/s, training_loss=0.288]\u001b[A\n",
            "Epoch 5:   6%|▌         | 76/1271 [00:25<06:22,  3.13it/s, training_loss=0.059]\u001b[A\n",
            "Epoch 5:   6%|▌         | 77/1271 [00:25<06:29,  3.06it/s, training_loss=0.059]\u001b[A\n",
            "Epoch 5:   6%|▌         | 77/1271 [00:25<06:29,  3.06it/s, training_loss=0.015]\u001b[A\n",
            "Epoch 5:   6%|▌         | 78/1271 [00:25<06:30,  3.06it/s, training_loss=0.015]\u001b[A\n",
            "Epoch 5:   6%|▌         | 78/1271 [00:25<06:30,  3.06it/s, training_loss=0.370]\u001b[A\n",
            "Epoch 5:   6%|▌         | 79/1271 [00:25<06:29,  3.06it/s, training_loss=0.370]\u001b[A\n",
            "Epoch 5:   6%|▌         | 79/1271 [00:26<06:29,  3.06it/s, training_loss=0.150]\u001b[A\n",
            "Epoch 5:   6%|▋         | 80/1271 [00:26<06:26,  3.08it/s, training_loss=0.150]\u001b[A\n",
            "Epoch 5:   6%|▋         | 80/1271 [00:26<06:26,  3.08it/s, training_loss=0.622]\u001b[A\n",
            "Epoch 5:   6%|▋         | 81/1271 [00:26<06:26,  3.08it/s, training_loss=0.622]\u001b[A\n",
            "Epoch 5:   6%|▋         | 81/1271 [00:26<06:26,  3.08it/s, training_loss=0.141]\u001b[A\n",
            "Epoch 5:   6%|▋         | 82/1271 [00:26<06:25,  3.09it/s, training_loss=0.141]\u001b[A\n",
            "Epoch 5:   6%|▋         | 82/1271 [00:27<06:25,  3.09it/s, training_loss=0.543]\u001b[A\n",
            "Epoch 5:   7%|▋         | 83/1271 [00:27<06:23,  3.10it/s, training_loss=0.543]\u001b[A\n",
            "Epoch 5:   7%|▋         | 83/1271 [00:27<06:23,  3.10it/s, training_loss=0.062]\u001b[A\n",
            "Epoch 5:   7%|▋         | 84/1271 [00:27<06:22,  3.10it/s, training_loss=0.062]\u001b[A\n",
            "Epoch 5:   7%|▋         | 84/1271 [00:27<06:22,  3.10it/s, training_loss=0.472]\u001b[A\n",
            "Epoch 5:   7%|▋         | 85/1271 [00:27<06:21,  3.11it/s, training_loss=0.472]\u001b[A\n",
            "Epoch 5:   7%|▋         | 85/1271 [00:27<06:21,  3.11it/s, training_loss=0.360]\u001b[A\n",
            "Epoch 5:   7%|▋         | 86/1271 [00:27<06:20,  3.11it/s, training_loss=0.360]\u001b[A\n",
            "Epoch 5:   7%|▋         | 86/1271 [00:28<06:20,  3.11it/s, training_loss=0.138]\u001b[A\n",
            "Epoch 5:   7%|▋         | 87/1271 [00:28<06:21,  3.10it/s, training_loss=0.138]\u001b[A\n",
            "Epoch 5:   7%|▋         | 87/1271 [00:28<06:21,  3.10it/s, training_loss=0.026]\u001b[A\n",
            "Epoch 5:   7%|▋         | 88/1271 [00:28<06:20,  3.11it/s, training_loss=0.026]\u001b[A\n",
            "Epoch 5:   7%|▋         | 88/1271 [00:28<06:20,  3.11it/s, training_loss=0.030]\u001b[A\n",
            "Epoch 5:   7%|▋         | 89/1271 [00:28<06:21,  3.10it/s, training_loss=0.030]\u001b[A\n",
            "Epoch 5:   7%|▋         | 89/1271 [00:29<06:21,  3.10it/s, training_loss=0.246]\u001b[A\n",
            "Epoch 5:   7%|▋         | 90/1271 [00:29<06:19,  3.11it/s, training_loss=0.246]\u001b[A\n",
            "Epoch 5:   7%|▋         | 90/1271 [00:29<06:19,  3.11it/s, training_loss=0.016]\u001b[A\n",
            "Epoch 5:   7%|▋         | 91/1271 [00:29<06:19,  3.11it/s, training_loss=0.016]\u001b[A\n",
            "Epoch 5:   7%|▋         | 91/1271 [00:29<06:19,  3.11it/s, training_loss=0.110]\u001b[A\n",
            "Epoch 5:   7%|▋         | 92/1271 [00:29<06:18,  3.11it/s, training_loss=0.110]\u001b[A\n",
            "Epoch 5:   7%|▋         | 92/1271 [00:30<06:18,  3.11it/s, training_loss=0.151]\u001b[A\n",
            "Epoch 5:   7%|▋         | 93/1271 [00:30<06:19,  3.11it/s, training_loss=0.151]\u001b[A\n",
            "Epoch 5:   7%|▋         | 93/1271 [00:30<06:19,  3.11it/s, training_loss=0.136]\u001b[A\n",
            "Epoch 5:   7%|▋         | 94/1271 [00:30<06:18,  3.11it/s, training_loss=0.136]\u001b[A\n",
            "Epoch 5:   7%|▋         | 94/1271 [00:30<06:18,  3.11it/s, training_loss=0.284]\u001b[A\n",
            "Epoch 5:   7%|▋         | 95/1271 [00:30<06:20,  3.09it/s, training_loss=0.284]\u001b[A\n",
            "Epoch 5:   7%|▋         | 95/1271 [00:31<06:20,  3.09it/s, training_loss=1.094]\u001b[A\n",
            "Epoch 5:   8%|▊         | 96/1271 [00:31<06:19,  3.10it/s, training_loss=1.094]\u001b[A\n",
            "Epoch 5:   8%|▊         | 96/1271 [00:31<06:19,  3.10it/s, training_loss=0.052]\u001b[A\n",
            "Epoch 5:   8%|▊         | 97/1271 [00:31<06:17,  3.11it/s, training_loss=0.052]\u001b[A\n",
            "Epoch 5:   8%|▊         | 97/1271 [00:31<06:17,  3.11it/s, training_loss=0.148]\u001b[A\n",
            "Epoch 5:   8%|▊         | 98/1271 [00:31<06:15,  3.12it/s, training_loss=0.148]\u001b[A\n",
            "Epoch 5:   8%|▊         | 98/1271 [00:32<06:15,  3.12it/s, training_loss=0.355]\u001b[A\n",
            "Epoch 5:   8%|▊         | 99/1271 [00:32<06:15,  3.13it/s, training_loss=0.355]\u001b[A\n",
            "Epoch 5:   8%|▊         | 99/1271 [00:32<06:15,  3.13it/s, training_loss=0.264]\u001b[A\n",
            "Epoch 5:   8%|▊         | 100/1271 [00:32<06:15,  3.12it/s, training_loss=0.264]\u001b[A\n",
            "Epoch 5:   8%|▊         | 100/1271 [00:32<06:15,  3.12it/s, training_loss=0.282]\u001b[A\n",
            "Epoch 5:   8%|▊         | 101/1271 [00:32<06:15,  3.11it/s, training_loss=0.282]\u001b[A\n",
            "Epoch 5:   8%|▊         | 101/1271 [00:33<06:15,  3.11it/s, training_loss=0.255]\u001b[A\n",
            "Epoch 5:   8%|▊         | 102/1271 [00:33<06:15,  3.11it/s, training_loss=0.255]\u001b[A\n",
            "Epoch 5:   8%|▊         | 102/1271 [00:33<06:15,  3.11it/s, training_loss=0.240]\u001b[A\n",
            "Epoch 5:   8%|▊         | 103/1271 [00:33<06:16,  3.10it/s, training_loss=0.240]\u001b[A\n",
            "Epoch 5:   8%|▊         | 103/1271 [00:33<06:16,  3.10it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 5:   8%|▊         | 104/1271 [00:33<06:14,  3.12it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 5:   8%|▊         | 104/1271 [00:34<06:14,  3.12it/s, training_loss=0.389]\u001b[A\n",
            "Epoch 5:   8%|▊         | 105/1271 [00:34<06:14,  3.12it/s, training_loss=0.389]\u001b[A\n",
            "Epoch 5:   8%|▊         | 105/1271 [00:34<06:14,  3.12it/s, training_loss=0.130]\u001b[A\n",
            "Epoch 5:   8%|▊         | 106/1271 [00:34<06:13,  3.12it/s, training_loss=0.130]\u001b[A\n",
            "Epoch 5:   8%|▊         | 106/1271 [00:34<06:13,  3.12it/s, training_loss=0.364]\u001b[A\n",
            "Epoch 5:   8%|▊         | 107/1271 [00:34<06:12,  3.12it/s, training_loss=0.364]\u001b[A\n",
            "Epoch 5:   8%|▊         | 107/1271 [00:35<06:12,  3.12it/s, training_loss=0.313]\u001b[A\n",
            "Epoch 5:   8%|▊         | 108/1271 [00:35<06:10,  3.14it/s, training_loss=0.313]\u001b[A\n",
            "Epoch 5:   8%|▊         | 108/1271 [00:35<06:10,  3.14it/s, training_loss=0.051]\u001b[A\n",
            "Epoch 5:   9%|▊         | 109/1271 [00:35<06:10,  3.13it/s, training_loss=0.051]\u001b[A\n",
            "Epoch 5:   9%|▊         | 109/1271 [00:35<06:10,  3.13it/s, training_loss=0.323]\u001b[A\n",
            "Epoch 5:   9%|▊         | 110/1271 [00:35<06:09,  3.15it/s, training_loss=0.323]\u001b[A\n",
            "Epoch 5:   9%|▊         | 110/1271 [00:35<06:09,  3.15it/s, training_loss=0.256]\u001b[A\n",
            "Epoch 5:   9%|▊         | 111/1271 [00:36<06:09,  3.14it/s, training_loss=0.256]\u001b[A\n",
            "Epoch 5:   9%|▊         | 111/1271 [00:36<06:09,  3.14it/s, training_loss=0.352]\u001b[A\n",
            "Epoch 5:   9%|▉         | 112/1271 [00:36<06:08,  3.14it/s, training_loss=0.352]\u001b[A\n",
            "Epoch 5:   9%|▉         | 112/1271 [00:36<06:08,  3.14it/s, training_loss=0.208]\u001b[A\n",
            "Epoch 5:   9%|▉         | 113/1271 [00:36<06:07,  3.15it/s, training_loss=0.208]\u001b[A\n",
            "Epoch 5:   9%|▉         | 113/1271 [00:36<06:07,  3.15it/s, training_loss=0.065]\u001b[A\n",
            "Epoch 5:   9%|▉         | 114/1271 [00:36<06:06,  3.15it/s, training_loss=0.065]\u001b[A\n",
            "Epoch 5:   9%|▉         | 114/1271 [00:37<06:06,  3.15it/s, training_loss=0.167]\u001b[A\n",
            "Epoch 5:   9%|▉         | 115/1271 [00:37<06:05,  3.16it/s, training_loss=0.167]\u001b[A\n",
            "Epoch 5:   9%|▉         | 115/1271 [00:37<06:05,  3.16it/s, training_loss=0.184]\u001b[A\n",
            "Epoch 5:   9%|▉         | 116/1271 [00:37<06:05,  3.16it/s, training_loss=0.184]\u001b[A\n",
            "Epoch 5:   9%|▉         | 116/1271 [00:37<06:05,  3.16it/s, training_loss=0.432]\u001b[A\n",
            "Epoch 5:   9%|▉         | 117/1271 [00:37<06:05,  3.15it/s, training_loss=0.432]\u001b[A\n",
            "Epoch 5:   9%|▉         | 117/1271 [00:38<06:05,  3.15it/s, training_loss=0.182]\u001b[A\n",
            "Epoch 5:   9%|▉         | 118/1271 [00:38<06:06,  3.15it/s, training_loss=0.182]\u001b[A\n",
            "Epoch 5:   9%|▉         | 118/1271 [00:38<06:06,  3.15it/s, training_loss=0.451]\u001b[A\n",
            "Epoch 5:   9%|▉         | 119/1271 [00:38<06:06,  3.15it/s, training_loss=0.451]\u001b[A\n",
            "Epoch 5:   9%|▉         | 119/1271 [00:38<06:06,  3.15it/s, training_loss=0.015]\u001b[A\n",
            "Epoch 5:   9%|▉         | 120/1271 [00:38<06:06,  3.14it/s, training_loss=0.015]\u001b[A\n",
            "Epoch 5:   9%|▉         | 120/1271 [00:39<06:06,  3.14it/s, training_loss=0.099]\u001b[A\n",
            "Epoch 5:  10%|▉         | 121/1271 [00:39<06:06,  3.13it/s, training_loss=0.099]\u001b[A\n",
            "Epoch 5:  10%|▉         | 121/1271 [00:39<06:06,  3.13it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 5:  10%|▉         | 122/1271 [00:39<06:06,  3.14it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 5:  10%|▉         | 122/1271 [00:39<06:06,  3.14it/s, training_loss=0.130]\u001b[A\n",
            "Epoch 5:  10%|▉         | 123/1271 [00:39<06:04,  3.15it/s, training_loss=0.130]\u001b[A\n",
            "Epoch 5:  10%|▉         | 123/1271 [00:40<06:04,  3.15it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 5:  10%|▉         | 124/1271 [00:40<06:03,  3.16it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 5:  10%|▉         | 124/1271 [00:40<06:03,  3.16it/s, training_loss=0.209]\u001b[A\n",
            "Epoch 5:  10%|▉         | 125/1271 [00:40<06:03,  3.16it/s, training_loss=0.209]\u001b[A\n",
            "Epoch 5:  10%|▉         | 125/1271 [00:40<06:03,  3.16it/s, training_loss=0.174]\u001b[A\n",
            "Epoch 5:  10%|▉         | 126/1271 [00:40<06:04,  3.14it/s, training_loss=0.174]\u001b[A\n",
            "Epoch 5:  10%|▉         | 126/1271 [00:41<06:04,  3.14it/s, training_loss=0.058]\u001b[A\n",
            "Epoch 5:  10%|▉         | 127/1271 [00:41<06:03,  3.14it/s, training_loss=0.058]\u001b[A\n",
            "Epoch 5:  10%|▉         | 127/1271 [00:41<06:03,  3.14it/s, training_loss=0.347]\u001b[A\n",
            "Epoch 5:  10%|█         | 128/1271 [00:41<06:06,  3.12it/s, training_loss=0.347]\u001b[A\n",
            "Epoch 5:  10%|█         | 128/1271 [00:41<06:06,  3.12it/s, training_loss=0.229]\u001b[A\n",
            "Epoch 5:  10%|█         | 129/1271 [00:41<06:05,  3.12it/s, training_loss=0.229]\u001b[A\n",
            "Epoch 5:  10%|█         | 129/1271 [00:42<06:05,  3.12it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 5:  10%|█         | 130/1271 [00:42<06:04,  3.13it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 5:  10%|█         | 130/1271 [00:42<06:04,  3.13it/s, training_loss=0.056]\u001b[A\n",
            "Epoch 5:  10%|█         | 131/1271 [00:42<06:03,  3.13it/s, training_loss=0.056]\u001b[A\n",
            "Epoch 5:  10%|█         | 131/1271 [00:42<06:03,  3.13it/s, training_loss=0.456]\u001b[A\n",
            "Epoch 5:  10%|█         | 132/1271 [00:42<06:03,  3.13it/s, training_loss=0.456]\u001b[A\n",
            "Epoch 5:  10%|█         | 132/1271 [00:43<06:03,  3.13it/s, training_loss=0.193]\u001b[A\n",
            "Epoch 5:  10%|█         | 133/1271 [00:43<06:05,  3.11it/s, training_loss=0.193]\u001b[A\n",
            "Epoch 5:  10%|█         | 133/1271 [00:43<06:05,  3.11it/s, training_loss=0.307]\u001b[A\n",
            "Epoch 5:  11%|█         | 134/1271 [00:43<06:03,  3.13it/s, training_loss=0.307]\u001b[A\n",
            "Epoch 5:  11%|█         | 134/1271 [00:43<06:03,  3.13it/s, training_loss=0.142]\u001b[A\n",
            "Epoch 5:  11%|█         | 135/1271 [00:43<06:01,  3.14it/s, training_loss=0.142]\u001b[A\n",
            "Epoch 5:  11%|█         | 135/1271 [00:43<06:01,  3.14it/s, training_loss=0.119]\u001b[A\n",
            "Epoch 5:  11%|█         | 136/1271 [00:43<06:02,  3.13it/s, training_loss=0.119]\u001b[A\n",
            "Epoch 5:  11%|█         | 136/1271 [00:44<06:02,  3.13it/s, training_loss=0.311]\u001b[A\n",
            "Epoch 5:  11%|█         | 137/1271 [00:44<06:01,  3.13it/s, training_loss=0.311]\u001b[A\n",
            "Epoch 5:  11%|█         | 137/1271 [00:44<06:01,  3.13it/s, training_loss=0.535]\u001b[A\n",
            "Epoch 5:  11%|█         | 138/1271 [00:44<06:00,  3.15it/s, training_loss=0.535]\u001b[A\n",
            "Epoch 5:  11%|█         | 138/1271 [00:44<06:00,  3.15it/s, training_loss=0.348]\u001b[A\n",
            "Epoch 5:  11%|█         | 139/1271 [00:44<06:00,  3.14it/s, training_loss=0.348]\u001b[A\n",
            "Epoch 5:  11%|█         | 139/1271 [00:45<06:00,  3.14it/s, training_loss=0.145]\u001b[A\n",
            "Epoch 5:  11%|█         | 140/1271 [00:45<05:59,  3.14it/s, training_loss=0.145]\u001b[A\n",
            "Epoch 5:  11%|█         | 140/1271 [00:45<05:59,  3.14it/s, training_loss=0.045]\u001b[A\n",
            "Epoch 5:  11%|█         | 141/1271 [00:45<05:59,  3.15it/s, training_loss=0.045]\u001b[A\n",
            "Epoch 5:  11%|█         | 141/1271 [00:45<05:59,  3.15it/s, training_loss=0.135]\u001b[A\n",
            "Epoch 5:  11%|█         | 142/1271 [00:45<05:58,  3.15it/s, training_loss=0.135]\u001b[A\n",
            "Epoch 5:  11%|█         | 142/1271 [00:46<05:58,  3.15it/s, training_loss=0.282]\u001b[A\n",
            "Epoch 5:  11%|█▏        | 143/1271 [00:46<06:00,  3.13it/s, training_loss=0.282]\u001b[A\n",
            "Epoch 5:  11%|█▏        | 143/1271 [00:46<06:00,  3.13it/s, training_loss=0.192]\u001b[A\n",
            "Epoch 5:  11%|█▏        | 144/1271 [00:46<05:59,  3.14it/s, training_loss=0.192]\u001b[A\n",
            "Epoch 5:  11%|█▏        | 144/1271 [00:46<05:59,  3.14it/s, training_loss=0.611]\u001b[A\n",
            "Epoch 5:  11%|█▏        | 145/1271 [00:46<05:58,  3.14it/s, training_loss=0.611]\u001b[A\n",
            "Epoch 5:  11%|█▏        | 145/1271 [00:47<05:58,  3.14it/s, training_loss=0.371]\u001b[A\n",
            "Epoch 5:  11%|█▏        | 146/1271 [00:47<05:57,  3.14it/s, training_loss=0.371]\u001b[A\n",
            "Epoch 5:  11%|█▏        | 146/1271 [00:47<05:57,  3.14it/s, training_loss=0.076]\u001b[A\n",
            "Epoch 5:  12%|█▏        | 147/1271 [00:47<05:58,  3.13it/s, training_loss=0.076]\u001b[A\n",
            "Epoch 5:  12%|█▏        | 147/1271 [00:47<05:58,  3.13it/s, training_loss=0.048]\u001b[A\n",
            "Epoch 5:  12%|█▏        | 148/1271 [00:47<05:58,  3.14it/s, training_loss=0.048]\u001b[A\n",
            "Epoch 5:  12%|█▏        | 148/1271 [00:48<05:58,  3.14it/s, training_loss=0.454]\u001b[A\n",
            "Epoch 5:  12%|█▏        | 149/1271 [00:48<05:56,  3.15it/s, training_loss=0.454]\u001b[A\n",
            "Epoch 5:  12%|█▏        | 149/1271 [00:48<05:56,  3.15it/s, training_loss=0.274]\u001b[A\n",
            "Epoch 5:  12%|█▏        | 150/1271 [00:48<05:55,  3.15it/s, training_loss=0.274]\u001b[A\n",
            "Epoch 5:  12%|█▏        | 150/1271 [00:48<05:55,  3.15it/s, training_loss=0.276]\u001b[A\n",
            "Epoch 5:  12%|█▏        | 151/1271 [00:48<05:54,  3.16it/s, training_loss=0.276]\u001b[A\n",
            "Epoch 5:  12%|█▏        | 151/1271 [00:49<05:54,  3.16it/s, training_loss=0.820]\u001b[A\n",
            "Epoch 5:  12%|█▏        | 152/1271 [00:49<05:55,  3.15it/s, training_loss=0.820]\u001b[A\n",
            "Epoch 5:  12%|█▏        | 152/1271 [00:49<05:55,  3.15it/s, training_loss=0.465]\u001b[A\n",
            "Epoch 5:  12%|█▏        | 153/1271 [00:49<05:56,  3.14it/s, training_loss=0.465]\u001b[A\n",
            "Epoch 5:  12%|█▏        | 153/1271 [00:49<05:56,  3.14it/s, training_loss=0.088]\u001b[A\n",
            "Epoch 5:  12%|█▏        | 154/1271 [00:49<05:57,  3.13it/s, training_loss=0.088]\u001b[A\n",
            "Epoch 5:  12%|█▏        | 154/1271 [00:49<05:57,  3.13it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 5:  12%|█▏        | 155/1271 [00:50<05:54,  3.15it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 5:  12%|█▏        | 155/1271 [00:50<05:54,  3.15it/s, training_loss=0.184]\u001b[A\n",
            "Epoch 5:  12%|█▏        | 156/1271 [00:50<05:54,  3.15it/s, training_loss=0.184]\u001b[A\n",
            "Epoch 5:  12%|█▏        | 156/1271 [00:50<05:54,  3.15it/s, training_loss=0.001]\u001b[A\n",
            "Epoch 5:  12%|█▏        | 157/1271 [00:50<05:52,  3.16it/s, training_loss=0.001]\u001b[A\n",
            "Epoch 5:  12%|█▏        | 157/1271 [00:50<05:52,  3.16it/s, training_loss=0.014]\u001b[A\n",
            "Epoch 5:  12%|█▏        | 158/1271 [00:50<05:50,  3.17it/s, training_loss=0.014]\u001b[A\n",
            "Epoch 5:  12%|█▏        | 158/1271 [00:51<05:50,  3.17it/s, training_loss=0.364]\u001b[A\n",
            "Epoch 5:  13%|█▎        | 159/1271 [00:51<05:50,  3.17it/s, training_loss=0.364]\u001b[A\n",
            "Epoch 5:  13%|█▎        | 159/1271 [00:51<05:50,  3.17it/s, training_loss=0.310]\u001b[A\n",
            "Epoch 5:  13%|█▎        | 160/1271 [00:51<05:51,  3.16it/s, training_loss=0.310]\u001b[A\n",
            "Epoch 5:  13%|█▎        | 160/1271 [00:51<05:51,  3.16it/s, training_loss=0.575]\u001b[A\n",
            "Epoch 5:  13%|█▎        | 161/1271 [00:51<05:51,  3.16it/s, training_loss=0.575]\u001b[A\n",
            "Epoch 5:  13%|█▎        | 161/1271 [00:52<05:51,  3.16it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 5:  13%|█▎        | 162/1271 [00:52<05:50,  3.16it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 5:  13%|█▎        | 162/1271 [00:52<05:50,  3.16it/s, training_loss=0.051]\u001b[A\n",
            "Epoch 5:  13%|█▎        | 163/1271 [00:52<05:52,  3.15it/s, training_loss=0.051]\u001b[A\n",
            "Epoch 5:  13%|█▎        | 163/1271 [00:52<05:52,  3.15it/s, training_loss=0.106]\u001b[A\n",
            "Epoch 5:  13%|█▎        | 164/1271 [00:52<05:51,  3.15it/s, training_loss=0.106]\u001b[A\n",
            "Epoch 5:  13%|█▎        | 164/1271 [00:53<05:51,  3.15it/s, training_loss=0.178]\u001b[A\n",
            "Epoch 5:  13%|█▎        | 165/1271 [00:53<05:52,  3.14it/s, training_loss=0.178]\u001b[A\n",
            "Epoch 5:  13%|█▎        | 165/1271 [00:53<05:52,  3.14it/s, training_loss=0.073]\u001b[A\n",
            "Epoch 5:  13%|█▎        | 166/1271 [00:53<05:50,  3.16it/s, training_loss=0.073]\u001b[A\n",
            "Epoch 5:  13%|█▎        | 166/1271 [00:53<05:50,  3.16it/s, training_loss=0.182]\u001b[A\n",
            "Epoch 5:  13%|█▎        | 167/1271 [00:53<05:51,  3.14it/s, training_loss=0.182]\u001b[A\n",
            "Epoch 5:  13%|█▎        | 167/1271 [00:54<05:51,  3.14it/s, training_loss=0.174]\u001b[A\n",
            "Epoch 5:  13%|█▎        | 168/1271 [00:54<05:51,  3.14it/s, training_loss=0.174]\u001b[A\n",
            "Epoch 5:  13%|█▎        | 168/1271 [00:54<05:51,  3.14it/s, training_loss=0.033]\u001b[A\n",
            "Epoch 5:  13%|█▎        | 169/1271 [00:54<05:51,  3.13it/s, training_loss=0.033]\u001b[A\n",
            "Epoch 5:  13%|█▎        | 169/1271 [00:54<05:51,  3.13it/s, training_loss=0.455]\u001b[A\n",
            "Epoch 5:  13%|█▎        | 170/1271 [00:54<05:49,  3.15it/s, training_loss=0.455]\u001b[A\n",
            "Epoch 5:  13%|█▎        | 170/1271 [00:55<05:49,  3.15it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 5:  13%|█▎        | 171/1271 [00:55<05:50,  3.14it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 5:  13%|█▎        | 171/1271 [00:55<05:50,  3.14it/s, training_loss=0.359]\u001b[A\n",
            "Epoch 5:  14%|█▎        | 172/1271 [00:55<05:53,  3.11it/s, training_loss=0.359]\u001b[A\n",
            "Epoch 5:  14%|█▎        | 172/1271 [00:55<05:53,  3.11it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 5:  14%|█▎        | 173/1271 [00:55<05:51,  3.12it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 5:  14%|█▎        | 173/1271 [00:56<05:51,  3.12it/s, training_loss=0.437]\u001b[A\n",
            "Epoch 5:  14%|█▎        | 174/1271 [00:56<05:50,  3.13it/s, training_loss=0.437]\u001b[A\n",
            "Epoch 5:  14%|█▎        | 174/1271 [00:56<05:50,  3.13it/s, training_loss=0.485]\u001b[A\n",
            "Epoch 5:  14%|█▍        | 175/1271 [00:56<05:51,  3.12it/s, training_loss=0.485]\u001b[A\n",
            "Epoch 5:  14%|█▍        | 175/1271 [00:56<05:51,  3.12it/s, training_loss=0.482]\u001b[A\n",
            "Epoch 5:  14%|█▍        | 176/1271 [00:56<05:50,  3.13it/s, training_loss=0.482]\u001b[A\n",
            "Epoch 5:  14%|█▍        | 176/1271 [00:57<05:50,  3.13it/s, training_loss=0.489]\u001b[A\n",
            "Epoch 5:  14%|█▍        | 177/1271 [00:57<05:51,  3.11it/s, training_loss=0.489]\u001b[A\n",
            "Epoch 5:  14%|█▍        | 177/1271 [00:57<05:51,  3.11it/s, training_loss=0.368]\u001b[A\n",
            "Epoch 5:  14%|█▍        | 178/1271 [00:57<05:49,  3.13it/s, training_loss=0.368]\u001b[A\n",
            "Epoch 5:  14%|█▍        | 178/1271 [00:57<05:49,  3.13it/s, training_loss=0.056]\u001b[A\n",
            "Epoch 5:  14%|█▍        | 179/1271 [00:57<05:47,  3.14it/s, training_loss=0.056]\u001b[A\n",
            "Epoch 5:  14%|█▍        | 179/1271 [00:57<05:47,  3.14it/s, training_loss=0.285]\u001b[A\n",
            "Epoch 5:  14%|█▍        | 180/1271 [00:57<05:46,  3.15it/s, training_loss=0.285]\u001b[A\n",
            "Epoch 5:  14%|█▍        | 180/1271 [00:58<05:46,  3.15it/s, training_loss=0.467]\u001b[A\n",
            "Epoch 5:  14%|█▍        | 181/1271 [00:58<05:45,  3.15it/s, training_loss=0.467]\u001b[A\n",
            "Epoch 5:  14%|█▍        | 181/1271 [00:58<05:45,  3.15it/s, training_loss=0.193]\u001b[A\n",
            "Epoch 5:  14%|█▍        | 182/1271 [00:58<05:46,  3.14it/s, training_loss=0.193]\u001b[A\n",
            "Epoch 5:  14%|█▍        | 182/1271 [00:58<05:46,  3.14it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 5:  14%|█▍        | 183/1271 [00:58<05:46,  3.14it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 5:  14%|█▍        | 183/1271 [00:59<05:46,  3.14it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 5:  14%|█▍        | 184/1271 [00:59<05:45,  3.15it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 5:  14%|█▍        | 184/1271 [00:59<05:45,  3.15it/s, training_loss=0.056]\u001b[A\n",
            "Epoch 5:  15%|█▍        | 185/1271 [00:59<05:47,  3.13it/s, training_loss=0.056]\u001b[A\n",
            "Epoch 5:  15%|█▍        | 185/1271 [00:59<05:47,  3.13it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 5:  15%|█▍        | 186/1271 [00:59<05:45,  3.14it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 5:  15%|█▍        | 186/1271 [01:00<05:45,  3.14it/s, training_loss=0.121]\u001b[A\n",
            "Epoch 5:  15%|█▍        | 187/1271 [01:00<05:45,  3.13it/s, training_loss=0.121]\u001b[A\n",
            "Epoch 5:  15%|█▍        | 187/1271 [01:00<05:45,  3.13it/s, training_loss=0.182]\u001b[A\n",
            "Epoch 5:  15%|█▍        | 188/1271 [01:00<05:45,  3.14it/s, training_loss=0.182]\u001b[A\n",
            "Epoch 5:  15%|█▍        | 188/1271 [01:00<05:45,  3.14it/s, training_loss=0.258]\u001b[A\n",
            "Epoch 5:  15%|█▍        | 189/1271 [01:00<05:46,  3.12it/s, training_loss=0.258]\u001b[A\n",
            "Epoch 5:  15%|█▍        | 189/1271 [01:01<05:46,  3.12it/s, training_loss=0.249]\u001b[A\n",
            "Epoch 5:  15%|█▍        | 190/1271 [01:01<05:45,  3.13it/s, training_loss=0.249]\u001b[A\n",
            "Epoch 5:  15%|█▍        | 190/1271 [01:01<05:45,  3.13it/s, training_loss=0.027]\u001b[A\n",
            "Epoch 5:  15%|█▌        | 191/1271 [01:01<05:45,  3.12it/s, training_loss=0.027]\u001b[A\n",
            "Epoch 5:  15%|█▌        | 191/1271 [01:01<05:45,  3.12it/s, training_loss=0.144]\u001b[A\n",
            "Epoch 5:  15%|█▌        | 192/1271 [01:01<05:44,  3.13it/s, training_loss=0.144]\u001b[A\n",
            "Epoch 5:  15%|█▌        | 192/1271 [01:02<05:44,  3.13it/s, training_loss=0.381]\u001b[A\n",
            "Epoch 5:  15%|█▌        | 193/1271 [01:02<05:44,  3.13it/s, training_loss=0.381]\u001b[A\n",
            "Epoch 5:  15%|█▌        | 193/1271 [01:02<05:44,  3.13it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 5:  15%|█▌        | 194/1271 [01:02<05:43,  3.14it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 5:  15%|█▌        | 194/1271 [01:02<05:43,  3.14it/s, training_loss=0.654]\u001b[A\n",
            "Epoch 5:  15%|█▌        | 195/1271 [01:02<05:41,  3.15it/s, training_loss=0.654]\u001b[A\n",
            "Epoch 5:  15%|█▌        | 195/1271 [01:03<05:41,  3.15it/s, training_loss=0.343]\u001b[A\n",
            "Epoch 5:  15%|█▌        | 196/1271 [01:03<05:40,  3.15it/s, training_loss=0.343]\u001b[A\n",
            "Epoch 5:  15%|█▌        | 196/1271 [01:03<05:40,  3.15it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 5:  15%|█▌        | 197/1271 [01:03<05:40,  3.15it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 5:  15%|█▌        | 197/1271 [01:03<05:40,  3.15it/s, training_loss=0.117]\u001b[A\n",
            "Epoch 5:  16%|█▌        | 198/1271 [01:03<05:41,  3.14it/s, training_loss=0.117]\u001b[A\n",
            "Epoch 5:  16%|█▌        | 198/1271 [01:04<05:41,  3.14it/s, training_loss=0.039]\u001b[A\n",
            "Epoch 5:  16%|█▌        | 199/1271 [01:04<05:42,  3.13it/s, training_loss=0.039]\u001b[A\n",
            "Epoch 5:  16%|█▌        | 199/1271 [01:04<05:42,  3.13it/s, training_loss=0.739]\u001b[A\n",
            "Epoch 5:  16%|█▌        | 200/1271 [01:04<05:42,  3.13it/s, training_loss=0.739]\u001b[A\n",
            "Epoch 5:  16%|█▌        | 200/1271 [01:04<05:42,  3.13it/s, training_loss=0.513]\u001b[A\n",
            "Epoch 5:  16%|█▌        | 201/1271 [01:04<05:42,  3.12it/s, training_loss=0.513]\u001b[A\n",
            "Epoch 5:  16%|█▌        | 201/1271 [01:04<05:42,  3.12it/s, training_loss=0.446]\u001b[A\n",
            "Epoch 5:  16%|█▌        | 202/1271 [01:04<05:44,  3.10it/s, training_loss=0.446]\u001b[A\n",
            "Epoch 5:  16%|█▌        | 202/1271 [01:05<05:44,  3.10it/s, training_loss=0.218]\u001b[A\n",
            "Epoch 5:  16%|█▌        | 203/1271 [01:05<05:44,  3.10it/s, training_loss=0.218]\u001b[A\n",
            "Epoch 5:  16%|█▌        | 203/1271 [01:05<05:44,  3.10it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 5:  16%|█▌        | 204/1271 [01:05<05:44,  3.10it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 5:  16%|█▌        | 204/1271 [01:05<05:44,  3.10it/s, training_loss=0.393]\u001b[A\n",
            "Epoch 5:  16%|█▌        | 205/1271 [01:05<05:43,  3.10it/s, training_loss=0.393]\u001b[A\n",
            "Epoch 5:  16%|█▌        | 205/1271 [01:06<05:43,  3.10it/s, training_loss=0.051]\u001b[A\n",
            "Epoch 5:  16%|█▌        | 206/1271 [01:06<05:41,  3.12it/s, training_loss=0.051]\u001b[A\n",
            "Epoch 5:  16%|█▌        | 206/1271 [01:06<05:41,  3.12it/s, training_loss=0.112]\u001b[A\n",
            "Epoch 5:  16%|█▋        | 207/1271 [01:06<05:40,  3.13it/s, training_loss=0.112]\u001b[A\n",
            "Epoch 5:  16%|█▋        | 207/1271 [01:06<05:40,  3.13it/s, training_loss=0.049]\u001b[A\n",
            "Epoch 5:  16%|█▋        | 208/1271 [01:06<05:38,  3.14it/s, training_loss=0.049]\u001b[A\n",
            "Epoch 5:  16%|█▋        | 208/1271 [01:07<05:38,  3.14it/s, training_loss=0.115]\u001b[A\n",
            "Epoch 5:  16%|█▋        | 209/1271 [01:07<05:38,  3.14it/s, training_loss=0.115]\u001b[A\n",
            "Epoch 5:  16%|█▋        | 209/1271 [01:07<05:38,  3.14it/s, training_loss=0.415]\u001b[A\n",
            "Epoch 5:  17%|█▋        | 210/1271 [01:07<05:39,  3.13it/s, training_loss=0.415]\u001b[A\n",
            "Epoch 5:  17%|█▋        | 210/1271 [01:07<05:39,  3.13it/s, training_loss=0.268]\u001b[A\n",
            "Epoch 5:  17%|█▋        | 211/1271 [01:07<05:40,  3.12it/s, training_loss=0.268]\u001b[A\n",
            "Epoch 5:  17%|█▋        | 211/1271 [01:08<05:40,  3.12it/s, training_loss=0.032]\u001b[A\n",
            "Epoch 5:  17%|█▋        | 212/1271 [01:08<05:40,  3.11it/s, training_loss=0.032]\u001b[A\n",
            "Epoch 5:  17%|█▋        | 212/1271 [01:08<05:40,  3.11it/s, training_loss=0.300]\u001b[A\n",
            "Epoch 5:  17%|█▋        | 213/1271 [01:08<05:39,  3.11it/s, training_loss=0.300]\u001b[A\n",
            "Epoch 5:  17%|█▋        | 213/1271 [01:08<05:39,  3.11it/s, training_loss=0.099]\u001b[A\n",
            "Epoch 5:  17%|█▋        | 214/1271 [01:08<05:38,  3.12it/s, training_loss=0.099]\u001b[A\n",
            "Epoch 5:  17%|█▋        | 214/1271 [01:09<05:38,  3.12it/s, training_loss=0.081]\u001b[A\n",
            "Epoch 5:  17%|█▋        | 215/1271 [01:09<05:38,  3.12it/s, training_loss=0.081]\u001b[A\n",
            "Epoch 5:  17%|█▋        | 215/1271 [01:09<05:38,  3.12it/s, training_loss=0.381]\u001b[A\n",
            "Epoch 5:  17%|█▋        | 216/1271 [01:09<05:40,  3.10it/s, training_loss=0.381]\u001b[A\n",
            "Epoch 5:  17%|█▋        | 216/1271 [01:09<05:40,  3.10it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 5:  17%|█▋        | 217/1271 [01:09<05:40,  3.10it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 5:  17%|█▋        | 217/1271 [01:10<05:40,  3.10it/s, training_loss=0.323]\u001b[A\n",
            "Epoch 5:  17%|█▋        | 218/1271 [01:10<05:38,  3.11it/s, training_loss=0.323]\u001b[A\n",
            "Epoch 5:  17%|█▋        | 218/1271 [01:10<05:38,  3.11it/s, training_loss=0.314]\u001b[A\n",
            "Epoch 5:  17%|█▋        | 219/1271 [01:10<05:39,  3.10it/s, training_loss=0.314]\u001b[A\n",
            "Epoch 5:  17%|█▋        | 219/1271 [01:10<05:39,  3.10it/s, training_loss=0.192]\u001b[A\n",
            "Epoch 5:  17%|█▋        | 220/1271 [01:10<05:41,  3.08it/s, training_loss=0.192]\u001b[A\n",
            "Epoch 5:  17%|█▋        | 220/1271 [01:11<05:41,  3.08it/s, training_loss=0.194]\u001b[A\n",
            "Epoch 5:  17%|█▋        | 221/1271 [01:11<05:40,  3.09it/s, training_loss=0.194]\u001b[A\n",
            "Epoch 5:  17%|█▋        | 221/1271 [01:11<05:40,  3.09it/s, training_loss=0.140]\u001b[A\n",
            "Epoch 5:  17%|█▋        | 222/1271 [01:11<05:38,  3.10it/s, training_loss=0.140]\u001b[A\n",
            "Epoch 5:  17%|█▋        | 222/1271 [01:11<05:38,  3.10it/s, training_loss=0.064]\u001b[A\n",
            "Epoch 5:  18%|█▊        | 223/1271 [01:11<05:38,  3.10it/s, training_loss=0.064]\u001b[A\n",
            "Epoch 5:  18%|█▊        | 223/1271 [01:12<05:38,  3.10it/s, training_loss=0.022]\u001b[A\n",
            "Epoch 5:  18%|█▊        | 224/1271 [01:12<05:37,  3.11it/s, training_loss=0.022]\u001b[A\n",
            "Epoch 5:  18%|█▊        | 224/1271 [01:12<05:37,  3.11it/s, training_loss=0.537]\u001b[A\n",
            "Epoch 5:  18%|█▊        | 225/1271 [01:12<05:35,  3.12it/s, training_loss=0.537]\u001b[A\n",
            "Epoch 5:  18%|█▊        | 225/1271 [01:12<05:35,  3.12it/s, training_loss=0.083]\u001b[A\n",
            "Epoch 5:  18%|█▊        | 226/1271 [01:12<05:34,  3.12it/s, training_loss=0.083]\u001b[A\n",
            "Epoch 5:  18%|█▊        | 226/1271 [01:13<05:34,  3.12it/s, training_loss=0.052]\u001b[A\n",
            "Epoch 5:  18%|█▊        | 227/1271 [01:13<05:38,  3.09it/s, training_loss=0.052]\u001b[A\n",
            "Epoch 5:  18%|█▊        | 227/1271 [01:13<05:38,  3.09it/s, training_loss=0.307]\u001b[A\n",
            "Epoch 5:  18%|█▊        | 228/1271 [01:13<05:35,  3.11it/s, training_loss=0.307]\u001b[A\n",
            "Epoch 5:  18%|█▊        | 228/1271 [01:13<05:35,  3.11it/s, training_loss=0.063]\u001b[A\n",
            "Epoch 5:  18%|█▊        | 229/1271 [01:13<05:35,  3.11it/s, training_loss=0.063]\u001b[A\n",
            "Epoch 5:  18%|█▊        | 229/1271 [01:13<05:35,  3.11it/s, training_loss=0.287]\u001b[A\n",
            "Epoch 5:  18%|█▊        | 230/1271 [01:13<05:34,  3.11it/s, training_loss=0.287]\u001b[A\n",
            "Epoch 5:  18%|█▊        | 230/1271 [01:14<05:34,  3.11it/s, training_loss=0.392]\u001b[A\n",
            "Epoch 5:  18%|█▊        | 231/1271 [01:14<05:35,  3.10it/s, training_loss=0.392]\u001b[A\n",
            "Epoch 5:  18%|█▊        | 231/1271 [01:14<05:35,  3.10it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 5:  18%|█▊        | 232/1271 [01:14<05:35,  3.10it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 5:  18%|█▊        | 232/1271 [01:14<05:35,  3.10it/s, training_loss=0.233]\u001b[A\n",
            "Epoch 5:  18%|█▊        | 233/1271 [01:14<05:34,  3.10it/s, training_loss=0.233]\u001b[A\n",
            "Epoch 5:  18%|█▊        | 233/1271 [01:15<05:34,  3.10it/s, training_loss=0.053]\u001b[A\n",
            "Epoch 5:  18%|█▊        | 234/1271 [01:15<05:34,  3.10it/s, training_loss=0.053]\u001b[A\n",
            "Epoch 5:  18%|█▊        | 234/1271 [01:15<05:34,  3.10it/s, training_loss=0.395]\u001b[A\n",
            "Epoch 5:  18%|█▊        | 235/1271 [01:15<05:32,  3.12it/s, training_loss=0.395]\u001b[A\n",
            "Epoch 5:  18%|█▊        | 235/1271 [01:15<05:32,  3.12it/s, training_loss=0.374]\u001b[A\n",
            "Epoch 5:  19%|█▊        | 236/1271 [01:15<05:31,  3.12it/s, training_loss=0.374]\u001b[A\n",
            "Epoch 5:  19%|█▊        | 236/1271 [01:16<05:31,  3.12it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 5:  19%|█▊        | 237/1271 [01:16<05:31,  3.11it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 5:  19%|█▊        | 237/1271 [01:16<05:31,  3.11it/s, training_loss=0.628]\u001b[A\n",
            "Epoch 5:  19%|█▊        | 238/1271 [01:16<05:32,  3.10it/s, training_loss=0.628]\u001b[A\n",
            "Epoch 5:  19%|█▊        | 238/1271 [01:16<05:32,  3.10it/s, training_loss=0.504]\u001b[A\n",
            "Epoch 5:  19%|█▉        | 239/1271 [01:16<05:30,  3.12it/s, training_loss=0.504]\u001b[A\n",
            "Epoch 5:  19%|█▉        | 239/1271 [01:17<05:30,  3.12it/s, training_loss=0.322]\u001b[A\n",
            "Epoch 5:  19%|█▉        | 240/1271 [01:17<05:29,  3.13it/s, training_loss=0.322]\u001b[A\n",
            "Epoch 5:  19%|█▉        | 240/1271 [01:17<05:29,  3.13it/s, training_loss=0.142]\u001b[A\n",
            "Epoch 5:  19%|█▉        | 241/1271 [01:17<05:30,  3.12it/s, training_loss=0.142]\u001b[A\n",
            "Epoch 5:  19%|█▉        | 241/1271 [01:17<05:30,  3.12it/s, training_loss=0.300]\u001b[A\n",
            "Epoch 5:  19%|█▉        | 242/1271 [01:17<05:30,  3.12it/s, training_loss=0.300]\u001b[A\n",
            "Epoch 5:  19%|█▉        | 242/1271 [01:18<05:30,  3.12it/s, training_loss=0.159]\u001b[A\n",
            "Epoch 5:  19%|█▉        | 243/1271 [01:18<05:29,  3.12it/s, training_loss=0.159]\u001b[A\n",
            "Epoch 5:  19%|█▉        | 243/1271 [01:18<05:29,  3.12it/s, training_loss=0.182]\u001b[A\n",
            "Epoch 5:  19%|█▉        | 244/1271 [01:18<05:29,  3.12it/s, training_loss=0.182]\u001b[A\n",
            "Epoch 5:  19%|█▉        | 244/1271 [01:18<05:29,  3.12it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 5:  19%|█▉        | 245/1271 [01:18<05:32,  3.08it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 5:  19%|█▉        | 245/1271 [01:19<05:32,  3.08it/s, training_loss=0.261]\u001b[A\n",
            "Epoch 5:  19%|█▉        | 246/1271 [01:19<05:31,  3.10it/s, training_loss=0.261]\u001b[A\n",
            "Epoch 5:  19%|█▉        | 246/1271 [01:19<05:31,  3.10it/s, training_loss=0.268]\u001b[A\n",
            "Epoch 5:  19%|█▉        | 247/1271 [01:19<05:32,  3.08it/s, training_loss=0.268]\u001b[A\n",
            "Epoch 5:  19%|█▉        | 247/1271 [01:19<05:32,  3.08it/s, training_loss=0.639]\u001b[A\n",
            "Epoch 5:  20%|█▉        | 248/1271 [01:19<05:31,  3.08it/s, training_loss=0.639]\u001b[A\n",
            "Epoch 5:  20%|█▉        | 248/1271 [01:20<05:31,  3.08it/s, training_loss=0.339]\u001b[A\n",
            "Epoch 5:  20%|█▉        | 249/1271 [01:20<05:31,  3.09it/s, training_loss=0.339]\u001b[A\n",
            "Epoch 5:  20%|█▉        | 249/1271 [01:20<05:31,  3.09it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 5:  20%|█▉        | 250/1271 [01:20<05:31,  3.08it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 5:  20%|█▉        | 250/1271 [01:20<05:31,  3.08it/s, training_loss=0.303]\u001b[A\n",
            "Epoch 5:  20%|█▉        | 251/1271 [01:20<05:29,  3.09it/s, training_loss=0.303]\u001b[A\n",
            "Epoch 5:  20%|█▉        | 251/1271 [01:21<05:29,  3.09it/s, training_loss=0.055]\u001b[A\n",
            "Epoch 5:  20%|█▉        | 252/1271 [01:21<05:28,  3.11it/s, training_loss=0.055]\u001b[A\n",
            "Epoch 5:  20%|█▉        | 252/1271 [01:21<05:28,  3.11it/s, training_loss=0.328]\u001b[A\n",
            "Epoch 5:  20%|█▉        | 253/1271 [01:21<05:27,  3.11it/s, training_loss=0.328]\u001b[A\n",
            "Epoch 5:  20%|█▉        | 253/1271 [01:21<05:27,  3.11it/s, training_loss=0.239]\u001b[A\n",
            "Epoch 5:  20%|█▉        | 254/1271 [01:21<05:26,  3.11it/s, training_loss=0.239]\u001b[A\n",
            "Epoch 5:  20%|█▉        | 254/1271 [01:22<05:26,  3.11it/s, training_loss=0.048]\u001b[A\n",
            "Epoch 5:  20%|██        | 255/1271 [01:22<05:26,  3.12it/s, training_loss=0.048]\u001b[A\n",
            "Epoch 5:  20%|██        | 255/1271 [01:22<05:26,  3.12it/s, training_loss=0.295]\u001b[A\n",
            "Epoch 5:  20%|██        | 256/1271 [01:22<05:27,  3.10it/s, training_loss=0.295]\u001b[A\n",
            "Epoch 5:  20%|██        | 256/1271 [01:22<05:27,  3.10it/s, training_loss=0.230]\u001b[A\n",
            "Epoch 5:  20%|██        | 257/1271 [01:22<05:28,  3.09it/s, training_loss=0.230]\u001b[A\n",
            "Epoch 5:  20%|██        | 257/1271 [01:23<05:28,  3.09it/s, training_loss=0.586]\u001b[A\n",
            "Epoch 5:  20%|██        | 258/1271 [01:23<05:29,  3.08it/s, training_loss=0.586]\u001b[A\n",
            "Epoch 5:  20%|██        | 258/1271 [01:23<05:29,  3.08it/s, training_loss=0.122]\u001b[A\n",
            "Epoch 5:  20%|██        | 259/1271 [01:23<05:26,  3.10it/s, training_loss=0.122]\u001b[A\n",
            "Epoch 5:  20%|██        | 259/1271 [01:23<05:26,  3.10it/s, training_loss=0.120]\u001b[A\n",
            "Epoch 5:  20%|██        | 260/1271 [01:23<05:28,  3.08it/s, training_loss=0.120]\u001b[A\n",
            "Epoch 5:  20%|██        | 260/1271 [01:23<05:28,  3.08it/s, training_loss=0.021]\u001b[A\n",
            "Epoch 5:  21%|██        | 261/1271 [01:23<05:26,  3.09it/s, training_loss=0.021]\u001b[A\n",
            "Epoch 5:  21%|██        | 261/1271 [01:24<05:26,  3.09it/s, training_loss=0.538]\u001b[A\n",
            "Epoch 5:  21%|██        | 262/1271 [01:24<05:25,  3.10it/s, training_loss=0.538]\u001b[A\n",
            "Epoch 5:  21%|██        | 262/1271 [01:24<05:25,  3.10it/s, training_loss=0.359]\u001b[A\n",
            "Epoch 5:  21%|██        | 263/1271 [01:24<05:26,  3.09it/s, training_loss=0.359]\u001b[A\n",
            "Epoch 5:  21%|██        | 263/1271 [01:24<05:26,  3.09it/s, training_loss=0.213]\u001b[A\n",
            "Epoch 5:  21%|██        | 264/1271 [01:24<05:23,  3.11it/s, training_loss=0.213]\u001b[A\n",
            "Epoch 5:  21%|██        | 264/1271 [01:25<05:23,  3.11it/s, training_loss=0.359]\u001b[A\n",
            "Epoch 5:  21%|██        | 265/1271 [01:25<05:25,  3.09it/s, training_loss=0.359]\u001b[A\n",
            "Epoch 5:  21%|██        | 265/1271 [01:25<05:25,  3.09it/s, training_loss=0.316]\u001b[A\n",
            "Epoch 5:  21%|██        | 266/1271 [01:25<05:30,  3.04it/s, training_loss=0.316]\u001b[A\n",
            "Epoch 5:  21%|██        | 266/1271 [01:25<05:30,  3.04it/s, training_loss=0.105]\u001b[A\n",
            "Epoch 5:  21%|██        | 267/1271 [01:25<05:28,  3.06it/s, training_loss=0.105]\u001b[A\n",
            "Epoch 5:  21%|██        | 267/1271 [01:26<05:28,  3.06it/s, training_loss=0.035]\u001b[A\n",
            "Epoch 5:  21%|██        | 268/1271 [01:26<05:25,  3.08it/s, training_loss=0.035]\u001b[A\n",
            "Epoch 5:  21%|██        | 268/1271 [01:26<05:25,  3.08it/s, training_loss=0.289]\u001b[A\n",
            "Epoch 5:  21%|██        | 269/1271 [01:26<05:25,  3.08it/s, training_loss=0.289]\u001b[A\n",
            "Epoch 5:  21%|██        | 269/1271 [01:26<05:25,  3.08it/s, training_loss=0.017]\u001b[A\n",
            "Epoch 5:  21%|██        | 270/1271 [01:26<05:25,  3.07it/s, training_loss=0.017]\u001b[A\n",
            "Epoch 5:  21%|██        | 270/1271 [01:27<05:25,  3.07it/s, training_loss=0.588]\u001b[A\n",
            "Epoch 5:  21%|██▏       | 271/1271 [01:27<05:24,  3.08it/s, training_loss=0.588]\u001b[A\n",
            "Epoch 5:  21%|██▏       | 271/1271 [01:27<05:24,  3.08it/s, training_loss=0.264]\u001b[A\n",
            "Epoch 5:  21%|██▏       | 272/1271 [01:27<05:23,  3.08it/s, training_loss=0.264]\u001b[A\n",
            "Epoch 5:  21%|██▏       | 272/1271 [01:27<05:23,  3.08it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 5:  21%|██▏       | 273/1271 [01:27<05:24,  3.08it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 5:  21%|██▏       | 273/1271 [01:28<05:24,  3.08it/s, training_loss=0.531]\u001b[A\n",
            "Epoch 5:  22%|██▏       | 274/1271 [01:28<05:24,  3.07it/s, training_loss=0.531]\u001b[A\n",
            "Epoch 5:  22%|██▏       | 274/1271 [01:28<05:24,  3.07it/s, training_loss=0.118]\u001b[A\n",
            "Epoch 5:  22%|██▏       | 275/1271 [01:28<05:22,  3.09it/s, training_loss=0.118]\u001b[A\n",
            "Epoch 5:  22%|██▏       | 275/1271 [01:28<05:22,  3.09it/s, training_loss=0.506]\u001b[A\n",
            "Epoch 5:  22%|██▏       | 276/1271 [01:28<05:21,  3.09it/s, training_loss=0.506]\u001b[A\n",
            "Epoch 5:  22%|██▏       | 276/1271 [01:29<05:21,  3.09it/s, training_loss=0.064]\u001b[A\n",
            "Epoch 5:  22%|██▏       | 277/1271 [01:29<05:21,  3.09it/s, training_loss=0.064]\u001b[A\n",
            "Epoch 5:  22%|██▏       | 277/1271 [01:29<05:21,  3.09it/s, training_loss=0.258]\u001b[A\n",
            "Epoch 5:  22%|██▏       | 278/1271 [01:29<05:20,  3.10it/s, training_loss=0.258]\u001b[A\n",
            "Epoch 5:  22%|██▏       | 278/1271 [01:29<05:20,  3.10it/s, training_loss=0.481]\u001b[A\n",
            "Epoch 5:  22%|██▏       | 279/1271 [01:29<05:19,  3.10it/s, training_loss=0.481]\u001b[A\n",
            "Epoch 5:  22%|██▏       | 279/1271 [01:30<05:19,  3.10it/s, training_loss=0.075]\u001b[A\n",
            "Epoch 5:  22%|██▏       | 280/1271 [01:30<05:21,  3.08it/s, training_loss=0.075]\u001b[A\n",
            "Epoch 5:  22%|██▏       | 280/1271 [01:30<05:21,  3.08it/s, training_loss=0.192]\u001b[A\n",
            "Epoch 5:  22%|██▏       | 281/1271 [01:30<05:21,  3.08it/s, training_loss=0.192]\u001b[A\n",
            "Epoch 5:  22%|██▏       | 281/1271 [01:30<05:21,  3.08it/s, training_loss=0.020]\u001b[A\n",
            "Epoch 5:  22%|██▏       | 282/1271 [01:30<05:22,  3.06it/s, training_loss=0.020]\u001b[A\n",
            "Epoch 5:  22%|██▏       | 282/1271 [01:31<05:22,  3.06it/s, training_loss=0.231]\u001b[A\n",
            "Epoch 5:  22%|██▏       | 283/1271 [01:31<05:22,  3.06it/s, training_loss=0.231]\u001b[A\n",
            "Epoch 5:  22%|██▏       | 283/1271 [01:31<05:22,  3.06it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 5:  22%|██▏       | 284/1271 [01:31<05:21,  3.07it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 5:  22%|██▏       | 284/1271 [01:31<05:21,  3.07it/s, training_loss=0.322]\u001b[A\n",
            "Epoch 5:  22%|██▏       | 285/1271 [01:31<05:20,  3.07it/s, training_loss=0.322]\u001b[A\n",
            "Epoch 5:  22%|██▏       | 285/1271 [01:32<05:20,  3.07it/s, training_loss=0.289]\u001b[A\n",
            "Epoch 5:  23%|██▎       | 286/1271 [01:32<05:18,  3.09it/s, training_loss=0.289]\u001b[A\n",
            "Epoch 5:  23%|██▎       | 286/1271 [01:32<05:18,  3.09it/s, training_loss=0.231]\u001b[A\n",
            "Epoch 5:  23%|██▎       | 287/1271 [01:32<05:17,  3.10it/s, training_loss=0.231]\u001b[A\n",
            "Epoch 5:  23%|██▎       | 287/1271 [01:32<05:17,  3.10it/s, training_loss=0.114]\u001b[A\n",
            "Epoch 5:  23%|██▎       | 288/1271 [01:32<05:16,  3.11it/s, training_loss=0.114]\u001b[A\n",
            "Epoch 5:  23%|██▎       | 288/1271 [01:33<05:16,  3.11it/s, training_loss=0.122]\u001b[A\n",
            "Epoch 5:  23%|██▎       | 289/1271 [01:33<05:17,  3.10it/s, training_loss=0.122]\u001b[A\n",
            "Epoch 5:  23%|██▎       | 289/1271 [01:33<05:17,  3.10it/s, training_loss=0.242]\u001b[A\n",
            "Epoch 5:  23%|██▎       | 290/1271 [01:33<05:15,  3.11it/s, training_loss=0.242]\u001b[A\n",
            "Epoch 5:  23%|██▎       | 290/1271 [01:33<05:15,  3.11it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 5:  23%|██▎       | 291/1271 [01:33<05:14,  3.11it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 5:  23%|██▎       | 291/1271 [01:34<05:14,  3.11it/s, training_loss=0.284]\u001b[A\n",
            "Epoch 5:  23%|██▎       | 292/1271 [01:34<05:15,  3.10it/s, training_loss=0.284]\u001b[A\n",
            "Epoch 5:  23%|██▎       | 292/1271 [01:34<05:15,  3.10it/s, training_loss=0.139]\u001b[A\n",
            "Epoch 5:  23%|██▎       | 293/1271 [01:34<05:14,  3.11it/s, training_loss=0.139]\u001b[A\n",
            "Epoch 5:  23%|██▎       | 293/1271 [01:34<05:14,  3.11it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 5:  23%|██▎       | 294/1271 [01:34<05:12,  3.13it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 5:  23%|██▎       | 294/1271 [01:34<05:12,  3.13it/s, training_loss=0.043]\u001b[A\n",
            "Epoch 5:  23%|██▎       | 295/1271 [01:34<05:13,  3.11it/s, training_loss=0.043]\u001b[A\n",
            "Epoch 5:  23%|██▎       | 295/1271 [01:35<05:13,  3.11it/s, training_loss=0.123]\u001b[A\n",
            "Epoch 5:  23%|██▎       | 296/1271 [01:35<05:15,  3.09it/s, training_loss=0.123]\u001b[A\n",
            "Epoch 5:  23%|██▎       | 296/1271 [01:35<05:15,  3.09it/s, training_loss=0.313]\u001b[A\n",
            "Epoch 5:  23%|██▎       | 297/1271 [01:35<05:14,  3.10it/s, training_loss=0.313]\u001b[A\n",
            "Epoch 5:  23%|██▎       | 297/1271 [01:35<05:14,  3.10it/s, training_loss=0.074]\u001b[A\n",
            "Epoch 5:  23%|██▎       | 298/1271 [01:35<05:15,  3.09it/s, training_loss=0.074]\u001b[A\n",
            "Epoch 5:  23%|██▎       | 298/1271 [01:36<05:15,  3.09it/s, training_loss=0.222]\u001b[A\n",
            "Epoch 5:  24%|██▎       | 299/1271 [01:36<05:13,  3.10it/s, training_loss=0.222]\u001b[A\n",
            "Epoch 5:  24%|██▎       | 299/1271 [01:36<05:13,  3.10it/s, training_loss=0.530]\u001b[A\n",
            "Epoch 5:  24%|██▎       | 300/1271 [01:36<05:14,  3.09it/s, training_loss=0.530]\u001b[A\n",
            "Epoch 5:  24%|██▎       | 300/1271 [01:36<05:14,  3.09it/s, training_loss=0.021]\u001b[A\n",
            "Epoch 5:  24%|██▎       | 301/1271 [01:36<05:12,  3.11it/s, training_loss=0.021]\u001b[A\n",
            "Epoch 5:  24%|██▎       | 301/1271 [01:37<05:12,  3.11it/s, training_loss=0.478]\u001b[A\n",
            "Epoch 5:  24%|██▍       | 302/1271 [01:37<05:12,  3.10it/s, training_loss=0.478]\u001b[A\n",
            "Epoch 5:  24%|██▍       | 302/1271 [01:37<05:12,  3.10it/s, training_loss=0.200]\u001b[A\n",
            "Epoch 5:  24%|██▍       | 303/1271 [01:37<05:11,  3.10it/s, training_loss=0.200]\u001b[A\n",
            "Epoch 5:  24%|██▍       | 303/1271 [01:37<05:11,  3.10it/s, training_loss=0.189]\u001b[A\n",
            "Epoch 5:  24%|██▍       | 304/1271 [01:37<05:12,  3.09it/s, training_loss=0.189]\u001b[A\n",
            "Epoch 5:  24%|██▍       | 304/1271 [01:38<05:12,  3.09it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 5:  24%|██▍       | 305/1271 [01:38<05:10,  3.11it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 5:  24%|██▍       | 305/1271 [01:38<05:10,  3.11it/s, training_loss=0.040]\u001b[A\n",
            "Epoch 5:  24%|██▍       | 306/1271 [01:38<05:08,  3.12it/s, training_loss=0.040]\u001b[A\n",
            "Epoch 5:  24%|██▍       | 306/1271 [01:38<05:08,  3.12it/s, training_loss=0.307]\u001b[A\n",
            "Epoch 5:  24%|██▍       | 307/1271 [01:38<05:08,  3.12it/s, training_loss=0.307]\u001b[A\n",
            "Epoch 5:  24%|██▍       | 307/1271 [01:39<05:08,  3.12it/s, training_loss=0.742]\u001b[A\n",
            "Epoch 5:  24%|██▍       | 308/1271 [01:39<05:09,  3.11it/s, training_loss=0.742]\u001b[A\n",
            "Epoch 5:  24%|██▍       | 308/1271 [01:39<05:09,  3.11it/s, training_loss=0.381]\u001b[A\n",
            "Epoch 5:  24%|██▍       | 309/1271 [01:39<05:09,  3.11it/s, training_loss=0.381]\u001b[A\n",
            "Epoch 5:  24%|██▍       | 309/1271 [01:39<05:09,  3.11it/s, training_loss=0.285]\u001b[A\n",
            "Epoch 5:  24%|██▍       | 310/1271 [01:39<05:10,  3.09it/s, training_loss=0.285]\u001b[A\n",
            "Epoch 5:  24%|██▍       | 310/1271 [01:40<05:10,  3.09it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 5:  24%|██▍       | 311/1271 [01:40<05:10,  3.09it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 5:  24%|██▍       | 311/1271 [01:40<05:10,  3.09it/s, training_loss=0.186]\u001b[A\n",
            "Epoch 5:  25%|██▍       | 312/1271 [01:40<05:09,  3.10it/s, training_loss=0.186]\u001b[A\n",
            "Epoch 5:  25%|██▍       | 312/1271 [01:40<05:09,  3.10it/s, training_loss=0.694]\u001b[A\n",
            "Epoch 5:  25%|██▍       | 313/1271 [01:40<05:08,  3.10it/s, training_loss=0.694]\u001b[A\n",
            "Epoch 5:  25%|██▍       | 313/1271 [01:41<05:08,  3.10it/s, training_loss=0.133]\u001b[A\n",
            "Epoch 5:  25%|██▍       | 314/1271 [01:41<05:11,  3.07it/s, training_loss=0.133]\u001b[A\n",
            "Epoch 5:  25%|██▍       | 314/1271 [01:41<05:11,  3.07it/s, training_loss=0.042]\u001b[A\n",
            "Epoch 5:  25%|██▍       | 315/1271 [01:41<05:10,  3.07it/s, training_loss=0.042]\u001b[A\n",
            "Epoch 5:  25%|██▍       | 315/1271 [01:41<05:10,  3.07it/s, training_loss=0.163]\u001b[A\n",
            "Epoch 5:  25%|██▍       | 316/1271 [01:41<05:10,  3.08it/s, training_loss=0.163]\u001b[A\n",
            "Epoch 5:  25%|██▍       | 316/1271 [01:42<05:10,  3.08it/s, training_loss=0.287]\u001b[A\n",
            "Epoch 5:  25%|██▍       | 317/1271 [01:42<05:09,  3.08it/s, training_loss=0.287]\u001b[A\n",
            "Epoch 5:  25%|██▍       | 317/1271 [01:42<05:09,  3.08it/s, training_loss=0.225]\u001b[A\n",
            "Epoch 5:  25%|██▌       | 318/1271 [01:42<05:09,  3.08it/s, training_loss=0.225]\u001b[A\n",
            "Epoch 5:  25%|██▌       | 318/1271 [01:42<05:09,  3.08it/s, training_loss=0.133]\u001b[A\n",
            "Epoch 5:  25%|██▌       | 319/1271 [01:42<05:07,  3.10it/s, training_loss=0.133]\u001b[A\n",
            "Epoch 5:  25%|██▌       | 319/1271 [01:43<05:07,  3.10it/s, training_loss=0.458]\u001b[A\n",
            "Epoch 5:  25%|██▌       | 320/1271 [01:43<05:05,  3.11it/s, training_loss=0.458]\u001b[A\n",
            "Epoch 5:  25%|██▌       | 320/1271 [01:43<05:05,  3.11it/s, training_loss=0.123]\u001b[A\n",
            "Epoch 5:  25%|██▌       | 321/1271 [01:43<05:05,  3.11it/s, training_loss=0.123]\u001b[A\n",
            "Epoch 5:  25%|██▌       | 321/1271 [01:43<05:05,  3.11it/s, training_loss=0.055]\u001b[A\n",
            "Epoch 5:  25%|██▌       | 322/1271 [01:43<05:05,  3.10it/s, training_loss=0.055]\u001b[A\n",
            "Epoch 5:  25%|██▌       | 322/1271 [01:44<05:05,  3.10it/s, training_loss=0.476]\u001b[A\n",
            "Epoch 5:  25%|██▌       | 323/1271 [01:44<05:07,  3.08it/s, training_loss=0.476]\u001b[A\n",
            "Epoch 5:  25%|██▌       | 323/1271 [01:44<05:07,  3.08it/s, training_loss=0.200]\u001b[A\n",
            "Epoch 5:  25%|██▌       | 324/1271 [01:44<05:05,  3.10it/s, training_loss=0.200]\u001b[A\n",
            "Epoch 5:  25%|██▌       | 324/1271 [01:44<05:05,  3.10it/s, training_loss=0.437]\u001b[A\n",
            "Epoch 5:  26%|██▌       | 325/1271 [01:44<05:03,  3.11it/s, training_loss=0.437]\u001b[A\n",
            "Epoch 5:  26%|██▌       | 325/1271 [01:44<05:03,  3.11it/s, training_loss=0.346]\u001b[A\n",
            "Epoch 5:  26%|██▌       | 326/1271 [01:44<05:01,  3.13it/s, training_loss=0.346]\u001b[A\n",
            "Epoch 5:  26%|██▌       | 326/1271 [01:45<05:01,  3.13it/s, training_loss=0.156]\u001b[A\n",
            "Epoch 5:  26%|██▌       | 327/1271 [01:45<05:01,  3.13it/s, training_loss=0.156]\u001b[A\n",
            "Epoch 5:  26%|██▌       | 327/1271 [01:45<05:01,  3.13it/s, training_loss=0.039]\u001b[A\n",
            "Epoch 5:  26%|██▌       | 328/1271 [01:45<05:01,  3.12it/s, training_loss=0.039]\u001b[A\n",
            "Epoch 5:  26%|██▌       | 328/1271 [01:45<05:01,  3.12it/s, training_loss=0.081]\u001b[A\n",
            "Epoch 5:  26%|██▌       | 329/1271 [01:45<05:01,  3.12it/s, training_loss=0.081]\u001b[A\n",
            "Epoch 5:  26%|██▌       | 329/1271 [01:46<05:01,  3.12it/s, training_loss=0.251]\u001b[A\n",
            "Epoch 5:  26%|██▌       | 330/1271 [01:46<05:02,  3.11it/s, training_loss=0.251]\u001b[A\n",
            "Epoch 5:  26%|██▌       | 330/1271 [01:46<05:02,  3.11it/s, training_loss=0.460]\u001b[A\n",
            "Epoch 5:  26%|██▌       | 331/1271 [01:46<05:02,  3.11it/s, training_loss=0.460]\u001b[A\n",
            "Epoch 5:  26%|██▌       | 331/1271 [01:46<05:02,  3.11it/s, training_loss=0.190]\u001b[A\n",
            "Epoch 5:  26%|██▌       | 332/1271 [01:46<05:04,  3.09it/s, training_loss=0.190]\u001b[A\n",
            "Epoch 5:  26%|██▌       | 332/1271 [01:47<05:04,  3.09it/s, training_loss=0.113]\u001b[A\n",
            "Epoch 5:  26%|██▌       | 333/1271 [01:47<05:03,  3.09it/s, training_loss=0.113]\u001b[A\n",
            "Epoch 5:  26%|██▌       | 333/1271 [01:47<05:03,  3.09it/s, training_loss=0.714]\u001b[A\n",
            "Epoch 5:  26%|██▋       | 334/1271 [01:47<05:02,  3.10it/s, training_loss=0.714]\u001b[A\n",
            "Epoch 5:  26%|██▋       | 334/1271 [01:47<05:02,  3.10it/s, training_loss=0.142]\u001b[A\n",
            "Epoch 5:  26%|██▋       | 335/1271 [01:47<05:02,  3.10it/s, training_loss=0.142]\u001b[A\n",
            "Epoch 5:  26%|██▋       | 335/1271 [01:48<05:02,  3.10it/s, training_loss=0.293]\u001b[A\n",
            "Epoch 5:  26%|██▋       | 336/1271 [01:48<05:01,  3.10it/s, training_loss=0.293]\u001b[A\n",
            "Epoch 5:  26%|██▋       | 336/1271 [01:48<05:01,  3.10it/s, training_loss=0.174]\u001b[A\n",
            "Epoch 5:  27%|██▋       | 337/1271 [01:48<05:01,  3.10it/s, training_loss=0.174]\u001b[A\n",
            "Epoch 5:  27%|██▋       | 337/1271 [01:48<05:01,  3.10it/s, training_loss=0.170]\u001b[A\n",
            "Epoch 5:  27%|██▋       | 338/1271 [01:48<05:01,  3.09it/s, training_loss=0.170]\u001b[A\n",
            "Epoch 5:  27%|██▋       | 338/1271 [01:49<05:01,  3.09it/s, training_loss=0.041]\u001b[A\n",
            "Epoch 5:  27%|██▋       | 339/1271 [01:49<04:59,  3.11it/s, training_loss=0.041]\u001b[A\n",
            "Epoch 5:  27%|██▋       | 339/1271 [01:49<04:59,  3.11it/s, training_loss=0.102]\u001b[A\n",
            "Epoch 5:  27%|██▋       | 340/1271 [01:49<04:58,  3.12it/s, training_loss=0.102]\u001b[A\n",
            "Epoch 5:  27%|██▋       | 340/1271 [01:49<04:58,  3.12it/s, training_loss=0.847]\u001b[A\n",
            "Epoch 5:  27%|██▋       | 341/1271 [01:49<04:57,  3.12it/s, training_loss=0.847]\u001b[A\n",
            "Epoch 5:  27%|██▋       | 341/1271 [01:50<04:57,  3.12it/s, training_loss=0.705]\u001b[A\n",
            "Epoch 5:  27%|██▋       | 342/1271 [01:50<04:57,  3.12it/s, training_loss=0.705]\u001b[A\n",
            "Epoch 5:  27%|██▋       | 342/1271 [01:50<04:57,  3.12it/s, training_loss=0.273]\u001b[A\n",
            "Epoch 5:  27%|██▋       | 343/1271 [01:50<04:57,  3.12it/s, training_loss=0.273]\u001b[A\n",
            "Epoch 5:  27%|██▋       | 343/1271 [01:50<04:57,  3.12it/s, training_loss=0.610]\u001b[A\n",
            "Epoch 5:  27%|██▋       | 344/1271 [01:50<04:57,  3.12it/s, training_loss=0.610]\u001b[A\n",
            "Epoch 5:  27%|██▋       | 344/1271 [01:51<04:57,  3.12it/s, training_loss=0.201]\u001b[A\n",
            "Epoch 5:  27%|██▋       | 345/1271 [01:51<04:56,  3.12it/s, training_loss=0.201]\u001b[A\n",
            "Epoch 5:  27%|██▋       | 345/1271 [01:51<04:56,  3.12it/s, training_loss=0.411]\u001b[A\n",
            "Epoch 5:  27%|██▋       | 346/1271 [01:51<04:59,  3.09it/s, training_loss=0.411]\u001b[A\n",
            "Epoch 5:  27%|██▋       | 346/1271 [01:51<04:59,  3.09it/s, training_loss=0.348]\u001b[A\n",
            "Epoch 5:  27%|██▋       | 347/1271 [01:51<04:56,  3.11it/s, training_loss=0.348]\u001b[A\n",
            "Epoch 5:  27%|██▋       | 347/1271 [01:52<04:56,  3.11it/s, training_loss=0.137]\u001b[A\n",
            "Epoch 5:  27%|██▋       | 348/1271 [01:52<04:56,  3.11it/s, training_loss=0.137]\u001b[A\n",
            "Epoch 5:  27%|██▋       | 348/1271 [01:52<04:56,  3.11it/s, training_loss=0.693]\u001b[A\n",
            "Epoch 5:  27%|██▋       | 349/1271 [01:52<04:57,  3.10it/s, training_loss=0.693]\u001b[A\n",
            "Epoch 5:  27%|██▋       | 349/1271 [01:52<04:57,  3.10it/s, training_loss=0.075]\u001b[A\n",
            "Epoch 5:  28%|██▊       | 350/1271 [01:52<04:57,  3.10it/s, training_loss=0.075]\u001b[A\n",
            "Epoch 5:  28%|██▊       | 350/1271 [01:53<04:57,  3.10it/s, training_loss=0.202]\u001b[A\n",
            "Epoch 5:  28%|██▊       | 351/1271 [01:53<04:57,  3.10it/s, training_loss=0.202]\u001b[A\n",
            "Epoch 5:  28%|██▊       | 351/1271 [01:53<04:57,  3.10it/s, training_loss=0.093]\u001b[A\n",
            "Epoch 5:  28%|██▊       | 352/1271 [01:53<04:55,  3.11it/s, training_loss=0.093]\u001b[A\n",
            "Epoch 5:  28%|██▊       | 352/1271 [01:53<04:55,  3.11it/s, training_loss=0.213]\u001b[A\n",
            "Epoch 5:  28%|██▊       | 353/1271 [01:53<04:55,  3.10it/s, training_loss=0.213]\u001b[A\n",
            "Epoch 5:  28%|██▊       | 353/1271 [01:54<04:55,  3.10it/s, training_loss=0.115]\u001b[A\n",
            "Epoch 5:  28%|██▊       | 354/1271 [01:54<04:55,  3.10it/s, training_loss=0.115]\u001b[A\n",
            "Epoch 5:  28%|██▊       | 354/1271 [01:54<04:55,  3.10it/s, training_loss=0.179]\u001b[A\n",
            "Epoch 5:  28%|██▊       | 355/1271 [01:54<04:54,  3.11it/s, training_loss=0.179]\u001b[A\n",
            "Epoch 5:  28%|██▊       | 355/1271 [01:54<04:54,  3.11it/s, training_loss=0.404]\u001b[A\n",
            "Epoch 5:  28%|██▊       | 356/1271 [01:54<04:53,  3.12it/s, training_loss=0.404]\u001b[A\n",
            "Epoch 5:  28%|██▊       | 356/1271 [01:54<04:53,  3.12it/s, training_loss=0.184]\u001b[A\n",
            "Epoch 5:  28%|██▊       | 357/1271 [01:54<04:54,  3.11it/s, training_loss=0.184]\u001b[A\n",
            "Epoch 5:  28%|██▊       | 357/1271 [01:55<04:54,  3.11it/s, training_loss=0.172]\u001b[A\n",
            "Epoch 5:  28%|██▊       | 358/1271 [01:55<04:54,  3.10it/s, training_loss=0.172]\u001b[A\n",
            "Epoch 5:  28%|██▊       | 358/1271 [01:55<04:54,  3.10it/s, training_loss=0.093]\u001b[A\n",
            "Epoch 5:  28%|██▊       | 359/1271 [01:55<04:54,  3.10it/s, training_loss=0.093]\u001b[A\n",
            "Epoch 5:  28%|██▊       | 359/1271 [01:55<04:54,  3.10it/s, training_loss=0.291]\u001b[A\n",
            "Epoch 5:  28%|██▊       | 360/1271 [01:55<04:54,  3.10it/s, training_loss=0.291]\u001b[A\n",
            "Epoch 5:  28%|██▊       | 360/1271 [01:56<04:54,  3.10it/s, training_loss=0.086]\u001b[A\n",
            "Epoch 5:  28%|██▊       | 361/1271 [01:56<04:54,  3.09it/s, training_loss=0.086]\u001b[A\n",
            "Epoch 5:  28%|██▊       | 361/1271 [01:56<04:54,  3.09it/s, training_loss=0.355]\u001b[A\n",
            "Epoch 5:  28%|██▊       | 362/1271 [01:56<04:55,  3.07it/s, training_loss=0.355]\u001b[A\n",
            "Epoch 5:  28%|██▊       | 362/1271 [01:56<04:55,  3.07it/s, training_loss=0.385]\u001b[A\n",
            "Epoch 5:  29%|██▊       | 363/1271 [01:56<04:53,  3.09it/s, training_loss=0.385]\u001b[A\n",
            "Epoch 5:  29%|██▊       | 363/1271 [01:57<04:53,  3.09it/s, training_loss=0.054]\u001b[A\n",
            "Epoch 5:  29%|██▊       | 364/1271 [01:57<04:52,  3.10it/s, training_loss=0.054]\u001b[A\n",
            "Epoch 5:  29%|██▊       | 364/1271 [01:57<04:52,  3.10it/s, training_loss=0.336]\u001b[A\n",
            "Epoch 5:  29%|██▊       | 365/1271 [01:57<04:52,  3.10it/s, training_loss=0.336]\u001b[A\n",
            "Epoch 5:  29%|██▊       | 365/1271 [01:57<04:52,  3.10it/s, training_loss=0.350]\u001b[A\n",
            "Epoch 5:  29%|██▉       | 366/1271 [01:57<04:52,  3.09it/s, training_loss=0.350]\u001b[A\n",
            "Epoch 5:  29%|██▉       | 366/1271 [01:58<04:52,  3.09it/s, training_loss=0.048]\u001b[A\n",
            "Epoch 5:  29%|██▉       | 367/1271 [01:58<04:51,  3.10it/s, training_loss=0.048]\u001b[A\n",
            "Epoch 5:  29%|██▉       | 367/1271 [01:58<04:51,  3.10it/s, training_loss=0.046]\u001b[A\n",
            "Epoch 5:  29%|██▉       | 368/1271 [01:58<04:51,  3.10it/s, training_loss=0.046]\u001b[A\n",
            "Epoch 5:  29%|██▉       | 368/1271 [01:58<04:51,  3.10it/s, training_loss=0.263]\u001b[A\n",
            "Epoch 5:  29%|██▉       | 369/1271 [01:58<04:52,  3.09it/s, training_loss=0.263]\u001b[A\n",
            "Epoch 5:  29%|██▉       | 369/1271 [01:59<04:52,  3.09it/s, training_loss=0.154]\u001b[A\n",
            "Epoch 5:  29%|██▉       | 370/1271 [01:59<04:51,  3.09it/s, training_loss=0.154]\u001b[A\n",
            "Epoch 5:  29%|██▉       | 370/1271 [01:59<04:51,  3.09it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 5:  29%|██▉       | 371/1271 [01:59<04:50,  3.10it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 5:  29%|██▉       | 371/1271 [01:59<04:50,  3.10it/s, training_loss=0.357]\u001b[A\n",
            "Epoch 5:  29%|██▉       | 372/1271 [01:59<04:51,  3.08it/s, training_loss=0.357]\u001b[A\n",
            "Epoch 5:  29%|██▉       | 372/1271 [02:00<04:51,  3.08it/s, training_loss=0.358]\u001b[A\n",
            "Epoch 5:  29%|██▉       | 373/1271 [02:00<04:50,  3.09it/s, training_loss=0.358]\u001b[A\n",
            "Epoch 5:  29%|██▉       | 373/1271 [02:00<04:50,  3.09it/s, training_loss=0.028]\u001b[A\n",
            "Epoch 5:  29%|██▉       | 374/1271 [02:00<04:50,  3.08it/s, training_loss=0.028]\u001b[A\n",
            "Epoch 5:  29%|██▉       | 374/1271 [02:00<04:50,  3.08it/s, training_loss=0.150]\u001b[A\n",
            "Epoch 5:  30%|██▉       | 375/1271 [02:00<04:50,  3.09it/s, training_loss=0.150]\u001b[A\n",
            "Epoch 5:  30%|██▉       | 375/1271 [02:01<04:50,  3.09it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 5:  30%|██▉       | 376/1271 [02:01<04:48,  3.10it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 5:  30%|██▉       | 376/1271 [02:01<04:48,  3.10it/s, training_loss=0.090]\u001b[A\n",
            "Epoch 5:  30%|██▉       | 377/1271 [02:01<04:47,  3.11it/s, training_loss=0.090]\u001b[A\n",
            "Epoch 5:  30%|██▉       | 377/1271 [02:01<04:47,  3.11it/s, training_loss=0.464]\u001b[A\n",
            "Epoch 5:  30%|██▉       | 378/1271 [02:01<04:46,  3.12it/s, training_loss=0.464]\u001b[A\n",
            "Epoch 5:  30%|██▉       | 378/1271 [02:02<04:46,  3.12it/s, training_loss=0.348]\u001b[A\n",
            "Epoch 5:  30%|██▉       | 379/1271 [02:02<04:45,  3.12it/s, training_loss=0.348]\u001b[A\n",
            "Epoch 5:  30%|██▉       | 379/1271 [02:02<04:45,  3.12it/s, training_loss=0.056]\u001b[A\n",
            "Epoch 5:  30%|██▉       | 380/1271 [02:02<04:44,  3.13it/s, training_loss=0.056]\u001b[A\n",
            "Epoch 5:  30%|██▉       | 380/1271 [02:02<04:44,  3.13it/s, training_loss=0.137]\u001b[A\n",
            "Epoch 5:  30%|██▉       | 381/1271 [02:02<04:44,  3.13it/s, training_loss=0.137]\u001b[A\n",
            "Epoch 5:  30%|██▉       | 381/1271 [02:03<04:44,  3.13it/s, training_loss=0.058]\u001b[A\n",
            "Epoch 5:  30%|███       | 382/1271 [02:03<04:45,  3.12it/s, training_loss=0.058]\u001b[A\n",
            "Epoch 5:  30%|███       | 382/1271 [02:03<04:45,  3.12it/s, training_loss=0.270]\u001b[A\n",
            "Epoch 5:  30%|███       | 383/1271 [02:03<04:45,  3.11it/s, training_loss=0.270]\u001b[A\n",
            "Epoch 5:  30%|███       | 383/1271 [02:03<04:45,  3.11it/s, training_loss=0.468]\u001b[A\n",
            "Epoch 5:  30%|███       | 384/1271 [02:03<04:43,  3.13it/s, training_loss=0.468]\u001b[A\n",
            "Epoch 5:  30%|███       | 384/1271 [02:03<04:43,  3.13it/s, training_loss=0.417]\u001b[A\n",
            "Epoch 5:  30%|███       | 385/1271 [02:04<04:44,  3.12it/s, training_loss=0.417]\u001b[A\n",
            "Epoch 5:  30%|███       | 385/1271 [02:04<04:44,  3.12it/s, training_loss=0.395]\u001b[A\n",
            "Epoch 5:  30%|███       | 386/1271 [02:04<04:44,  3.11it/s, training_loss=0.395]\u001b[A\n",
            "Epoch 5:  30%|███       | 386/1271 [02:04<04:44,  3.11it/s, training_loss=0.178]\u001b[A\n",
            "Epoch 5:  30%|███       | 387/1271 [02:04<04:43,  3.12it/s, training_loss=0.178]\u001b[A\n",
            "Epoch 5:  30%|███       | 387/1271 [02:04<04:43,  3.12it/s, training_loss=0.060]\u001b[A\n",
            "Epoch 5:  31%|███       | 388/1271 [02:04<04:41,  3.14it/s, training_loss=0.060]\u001b[A\n",
            "Epoch 5:  31%|███       | 388/1271 [02:05<04:41,  3.14it/s, training_loss=0.110]\u001b[A\n",
            "Epoch 5:  31%|███       | 389/1271 [02:05<04:42,  3.12it/s, training_loss=0.110]\u001b[A\n",
            "Epoch 5:  31%|███       | 389/1271 [02:05<04:42,  3.12it/s, training_loss=0.343]\u001b[A\n",
            "Epoch 5:  31%|███       | 390/1271 [02:05<04:42,  3.12it/s, training_loss=0.343]\u001b[A\n",
            "Epoch 5:  31%|███       | 390/1271 [02:05<04:42,  3.12it/s, training_loss=0.169]\u001b[A\n",
            "Epoch 5:  31%|███       | 391/1271 [02:05<04:41,  3.12it/s, training_loss=0.169]\u001b[A\n",
            "Epoch 5:  31%|███       | 391/1271 [02:06<04:41,  3.12it/s, training_loss=0.660]\u001b[A\n",
            "Epoch 5:  31%|███       | 392/1271 [02:06<04:40,  3.14it/s, training_loss=0.660]\u001b[A\n",
            "Epoch 5:  31%|███       | 392/1271 [02:06<04:40,  3.14it/s, training_loss=0.060]\u001b[A\n",
            "Epoch 5:  31%|███       | 393/1271 [02:06<04:39,  3.14it/s, training_loss=0.060]\u001b[A\n",
            "Epoch 5:  31%|███       | 393/1271 [02:06<04:39,  3.14it/s, training_loss=0.107]\u001b[A\n",
            "Epoch 5:  31%|███       | 394/1271 [02:06<04:39,  3.14it/s, training_loss=0.107]\u001b[A\n",
            "Epoch 5:  31%|███       | 394/1271 [02:07<04:39,  3.14it/s, training_loss=0.194]\u001b[A\n",
            "Epoch 5:  31%|███       | 395/1271 [02:07<04:40,  3.12it/s, training_loss=0.194]\u001b[A\n",
            "Epoch 5:  31%|███       | 395/1271 [02:07<04:40,  3.12it/s, training_loss=0.014]\u001b[A\n",
            "Epoch 5:  31%|███       | 396/1271 [02:07<04:41,  3.11it/s, training_loss=0.014]\u001b[A\n",
            "Epoch 5:  31%|███       | 396/1271 [02:07<04:41,  3.11it/s, training_loss=0.050]\u001b[A\n",
            "Epoch 5:  31%|███       | 397/1271 [02:07<04:41,  3.11it/s, training_loss=0.050]\u001b[A\n",
            "Epoch 5:  31%|███       | 397/1271 [02:08<04:41,  3.11it/s, training_loss=0.267]\u001b[A\n",
            "Epoch 5:  31%|███▏      | 398/1271 [02:08<04:40,  3.12it/s, training_loss=0.267]\u001b[A\n",
            "Epoch 5:  31%|███▏      | 398/1271 [02:08<04:40,  3.12it/s, training_loss=0.064]\u001b[A\n",
            "Epoch 5:  31%|███▏      | 399/1271 [02:08<04:40,  3.11it/s, training_loss=0.064]\u001b[A\n",
            "Epoch 5:  31%|███▏      | 399/1271 [02:08<04:40,  3.11it/s, training_loss=0.054]\u001b[A\n",
            "Epoch 5:  31%|███▏      | 400/1271 [02:08<04:39,  3.12it/s, training_loss=0.054]\u001b[A\n",
            "Epoch 5:  31%|███▏      | 400/1271 [02:09<04:39,  3.12it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 5:  32%|███▏      | 401/1271 [02:09<04:39,  3.11it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 5:  32%|███▏      | 401/1271 [02:09<04:39,  3.11it/s, training_loss=0.052]\u001b[A\n",
            "Epoch 5:  32%|███▏      | 402/1271 [02:09<04:39,  3.11it/s, training_loss=0.052]\u001b[A\n",
            "Epoch 5:  32%|███▏      | 402/1271 [02:09<04:39,  3.11it/s, training_loss=0.602]\u001b[A\n",
            "Epoch 5:  32%|███▏      | 403/1271 [02:09<04:38,  3.11it/s, training_loss=0.602]\u001b[A\n",
            "Epoch 5:  32%|███▏      | 403/1271 [02:10<04:38,  3.11it/s, training_loss=0.506]\u001b[A\n",
            "Epoch 5:  32%|███▏      | 404/1271 [02:10<04:38,  3.11it/s, training_loss=0.506]\u001b[A\n",
            "Epoch 5:  32%|███▏      | 404/1271 [02:10<04:38,  3.11it/s, training_loss=0.146]\u001b[A\n",
            "Epoch 5:  32%|███▏      | 405/1271 [02:10<04:38,  3.11it/s, training_loss=0.146]\u001b[A\n",
            "Epoch 5:  32%|███▏      | 405/1271 [02:10<04:38,  3.11it/s, training_loss=0.570]\u001b[A\n",
            "Epoch 5:  32%|███▏      | 406/1271 [02:10<04:39,  3.10it/s, training_loss=0.570]\u001b[A\n",
            "Epoch 5:  32%|███▏      | 406/1271 [02:11<04:39,  3.10it/s, training_loss=0.041]\u001b[A\n",
            "Epoch 5:  32%|███▏      | 407/1271 [02:11<04:39,  3.09it/s, training_loss=0.041]\u001b[A\n",
            "Epoch 5:  32%|███▏      | 407/1271 [02:11<04:39,  3.09it/s, training_loss=0.306]\u001b[A\n",
            "Epoch 5:  32%|███▏      | 408/1271 [02:11<04:39,  3.09it/s, training_loss=0.306]\u001b[A\n",
            "Epoch 5:  32%|███▏      | 408/1271 [02:11<04:39,  3.09it/s, training_loss=0.424]\u001b[A\n",
            "Epoch 5:  32%|███▏      | 409/1271 [02:11<04:38,  3.09it/s, training_loss=0.424]\u001b[A\n",
            "Epoch 5:  32%|███▏      | 409/1271 [02:12<04:38,  3.09it/s, training_loss=0.287]\u001b[A\n",
            "Epoch 5:  32%|███▏      | 410/1271 [02:12<04:37,  3.11it/s, training_loss=0.287]\u001b[A\n",
            "Epoch 5:  32%|███▏      | 410/1271 [02:12<04:37,  3.11it/s, training_loss=0.179]\u001b[A\n",
            "Epoch 5:  32%|███▏      | 411/1271 [02:12<04:36,  3.11it/s, training_loss=0.179]\u001b[A\n",
            "Epoch 5:  32%|███▏      | 411/1271 [02:12<04:36,  3.11it/s, training_loss=0.653]\u001b[A\n",
            "Epoch 5:  32%|███▏      | 412/1271 [02:12<04:34,  3.13it/s, training_loss=0.653]\u001b[A\n",
            "Epoch 5:  32%|███▏      | 412/1271 [02:12<04:34,  3.13it/s, training_loss=0.232]\u001b[A\n",
            "Epoch 5:  32%|███▏      | 413/1271 [02:12<04:34,  3.12it/s, training_loss=0.232]\u001b[A\n",
            "Epoch 5:  32%|███▏      | 413/1271 [02:13<04:34,  3.12it/s, training_loss=0.284]\u001b[A\n",
            "Epoch 5:  33%|███▎      | 414/1271 [02:13<04:36,  3.09it/s, training_loss=0.284]\u001b[A\n",
            "Epoch 5:  33%|███▎      | 414/1271 [02:13<04:36,  3.09it/s, training_loss=0.257]\u001b[A\n",
            "Epoch 5:  33%|███▎      | 415/1271 [02:13<04:35,  3.11it/s, training_loss=0.257]\u001b[A\n",
            "Epoch 5:  33%|███▎      | 415/1271 [02:13<04:35,  3.11it/s, training_loss=0.150]\u001b[A\n",
            "Epoch 5:  33%|███▎      | 416/1271 [02:13<04:34,  3.11it/s, training_loss=0.150]\u001b[A\n",
            "Epoch 5:  33%|███▎      | 416/1271 [02:14<04:34,  3.11it/s, training_loss=0.038]\u001b[A\n",
            "Epoch 5:  33%|███▎      | 417/1271 [02:14<04:34,  3.11it/s, training_loss=0.038]\u001b[A\n",
            "Epoch 5:  33%|███▎      | 417/1271 [02:14<04:34,  3.11it/s, training_loss=0.102]\u001b[A\n",
            "Epoch 5:  33%|███▎      | 418/1271 [02:14<04:32,  3.13it/s, training_loss=0.102]\u001b[A\n",
            "Epoch 5:  33%|███▎      | 418/1271 [02:14<04:32,  3.13it/s, training_loss=0.102]\u001b[A\n",
            "Epoch 5:  33%|███▎      | 419/1271 [02:14<04:31,  3.14it/s, training_loss=0.102]\u001b[A\n",
            "Epoch 5:  33%|███▎      | 419/1271 [02:15<04:31,  3.14it/s, training_loss=0.164]\u001b[A\n",
            "Epoch 5:  33%|███▎      | 420/1271 [02:15<04:31,  3.14it/s, training_loss=0.164]\u001b[A\n",
            "Epoch 5:  33%|███▎      | 420/1271 [02:15<04:31,  3.14it/s, training_loss=0.254]\u001b[A\n",
            "Epoch 5:  33%|███▎      | 421/1271 [02:15<04:31,  3.13it/s, training_loss=0.254]\u001b[A\n",
            "Epoch 5:  33%|███▎      | 421/1271 [02:15<04:31,  3.13it/s, training_loss=0.090]\u001b[A\n",
            "Epoch 5:  33%|███▎      | 422/1271 [02:15<04:31,  3.13it/s, training_loss=0.090]\u001b[A\n",
            "Epoch 5:  33%|███▎      | 422/1271 [02:16<04:31,  3.13it/s, training_loss=0.180]\u001b[A\n",
            "Epoch 5:  33%|███▎      | 423/1271 [02:16<04:31,  3.12it/s, training_loss=0.180]\u001b[A\n",
            "Epoch 5:  33%|███▎      | 423/1271 [02:16<04:31,  3.12it/s, training_loss=0.268]\u001b[A\n",
            "Epoch 5:  33%|███▎      | 424/1271 [02:16<04:31,  3.13it/s, training_loss=0.268]\u001b[A\n",
            "Epoch 5:  33%|███▎      | 424/1271 [02:16<04:31,  3.13it/s, training_loss=0.241]\u001b[A\n",
            "Epoch 5:  33%|███▎      | 425/1271 [02:16<04:29,  3.14it/s, training_loss=0.241]\u001b[A\n",
            "Epoch 5:  33%|███▎      | 425/1271 [02:17<04:29,  3.14it/s, training_loss=0.323]\u001b[A\n",
            "Epoch 5:  34%|███▎      | 426/1271 [02:17<04:28,  3.14it/s, training_loss=0.323]\u001b[A\n",
            "Epoch 5:  34%|███▎      | 426/1271 [02:17<04:28,  3.14it/s, training_loss=0.866]\u001b[A\n",
            "Epoch 5:  34%|███▎      | 427/1271 [02:17<04:30,  3.12it/s, training_loss=0.866]\u001b[A\n",
            "Epoch 5:  34%|███▎      | 427/1271 [02:17<04:30,  3.12it/s, training_loss=0.432]\u001b[A\n",
            "Epoch 5:  34%|███▎      | 428/1271 [02:17<04:30,  3.12it/s, training_loss=0.432]\u001b[A\n",
            "Epoch 5:  34%|███▎      | 428/1271 [02:18<04:30,  3.12it/s, training_loss=0.700]\u001b[A\n",
            "Epoch 5:  34%|███▍      | 429/1271 [02:18<04:30,  3.11it/s, training_loss=0.700]\u001b[A\n",
            "Epoch 5:  34%|███▍      | 429/1271 [02:18<04:30,  3.11it/s, training_loss=0.212]\u001b[A\n",
            "Epoch 5:  34%|███▍      | 430/1271 [02:18<04:29,  3.12it/s, training_loss=0.212]\u001b[A\n",
            "Epoch 5:  34%|███▍      | 430/1271 [02:18<04:29,  3.12it/s, training_loss=0.142]\u001b[A\n",
            "Epoch 5:  34%|███▍      | 431/1271 [02:18<04:27,  3.14it/s, training_loss=0.142]\u001b[A\n",
            "Epoch 5:  34%|███▍      | 431/1271 [02:19<04:27,  3.14it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 5:  34%|███▍      | 432/1271 [02:19<04:27,  3.14it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 5:  34%|███▍      | 432/1271 [02:19<04:27,  3.14it/s, training_loss=0.156]\u001b[A\n",
            "Epoch 5:  34%|███▍      | 433/1271 [02:19<04:27,  3.13it/s, training_loss=0.156]\u001b[A\n",
            "Epoch 5:  34%|███▍      | 433/1271 [02:19<04:27,  3.13it/s, training_loss=0.183]\u001b[A\n",
            "Epoch 5:  34%|███▍      | 434/1271 [02:19<04:27,  3.12it/s, training_loss=0.183]\u001b[A\n",
            "Epoch 5:  34%|███▍      | 434/1271 [02:20<04:27,  3.12it/s, training_loss=0.037]\u001b[A\n",
            "Epoch 5:  34%|███▍      | 435/1271 [02:20<04:26,  3.13it/s, training_loss=0.037]\u001b[A\n",
            "Epoch 5:  34%|███▍      | 435/1271 [02:20<04:26,  3.13it/s, training_loss=0.156]\u001b[A\n",
            "Epoch 5:  34%|███▍      | 436/1271 [02:20<04:27,  3.12it/s, training_loss=0.156]\u001b[A\n",
            "Epoch 5:  34%|███▍      | 436/1271 [02:20<04:27,  3.12it/s, training_loss=0.042]\u001b[A\n",
            "Epoch 5:  34%|███▍      | 437/1271 [02:20<04:26,  3.13it/s, training_loss=0.042]\u001b[A\n",
            "Epoch 5:  34%|███▍      | 437/1271 [02:20<04:26,  3.13it/s, training_loss=0.287]\u001b[A\n",
            "Epoch 5:  34%|███▍      | 438/1271 [02:20<04:27,  3.12it/s, training_loss=0.287]\u001b[A\n",
            "Epoch 5:  34%|███▍      | 438/1271 [02:21<04:27,  3.12it/s, training_loss=0.325]\u001b[A\n",
            "Epoch 5:  35%|███▍      | 439/1271 [02:21<04:27,  3.11it/s, training_loss=0.325]\u001b[A\n",
            "Epoch 5:  35%|███▍      | 439/1271 [02:21<04:27,  3.11it/s, training_loss=0.592]\u001b[A\n",
            "Epoch 5:  35%|███▍      | 440/1271 [02:21<04:27,  3.10it/s, training_loss=0.592]\u001b[A\n",
            "Epoch 5:  35%|███▍      | 440/1271 [02:21<04:27,  3.10it/s, training_loss=0.219]\u001b[A\n",
            "Epoch 5:  35%|███▍      | 441/1271 [02:21<04:26,  3.12it/s, training_loss=0.219]\u001b[A\n",
            "Epoch 5:  35%|███▍      | 441/1271 [02:22<04:26,  3.12it/s, training_loss=0.111]\u001b[A\n",
            "Epoch 5:  35%|███▍      | 442/1271 [02:22<04:25,  3.13it/s, training_loss=0.111]\u001b[A\n",
            "Epoch 5:  35%|███▍      | 442/1271 [02:22<04:25,  3.13it/s, training_loss=0.228]\u001b[A\n",
            "Epoch 5:  35%|███▍      | 443/1271 [02:22<04:24,  3.13it/s, training_loss=0.228]\u001b[A\n",
            "Epoch 5:  35%|███▍      | 443/1271 [02:22<04:24,  3.13it/s, training_loss=0.087]\u001b[A\n",
            "Epoch 5:  35%|███▍      | 444/1271 [02:22<04:24,  3.13it/s, training_loss=0.087]\u001b[A\n",
            "Epoch 5:  35%|███▍      | 444/1271 [02:23<04:24,  3.13it/s, training_loss=0.067]\u001b[A\n",
            "Epoch 5:  35%|███▌      | 445/1271 [02:23<04:24,  3.12it/s, training_loss=0.067]\u001b[A\n",
            "Epoch 5:  35%|███▌      | 445/1271 [02:23<04:24,  3.12it/s, training_loss=0.179]\u001b[A\n",
            "Epoch 5:  35%|███▌      | 446/1271 [02:23<04:24,  3.12it/s, training_loss=0.179]\u001b[A\n",
            "Epoch 5:  35%|███▌      | 446/1271 [02:23<04:24,  3.12it/s, training_loss=0.284]\u001b[A\n",
            "Epoch 5:  35%|███▌      | 447/1271 [02:23<04:22,  3.14it/s, training_loss=0.284]\u001b[A\n",
            "Epoch 5:  35%|███▌      | 447/1271 [02:24<04:22,  3.14it/s, training_loss=0.369]\u001b[A\n",
            "Epoch 5:  35%|███▌      | 448/1271 [02:24<04:23,  3.12it/s, training_loss=0.369]\u001b[A\n",
            "Epoch 5:  35%|███▌      | 448/1271 [02:24<04:23,  3.12it/s, training_loss=0.177]\u001b[A\n",
            "Epoch 5:  35%|███▌      | 449/1271 [02:24<04:23,  3.12it/s, training_loss=0.177]\u001b[A\n",
            "Epoch 5:  35%|███▌      | 449/1271 [02:24<04:23,  3.12it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 5:  35%|███▌      | 450/1271 [02:24<04:25,  3.09it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 5:  35%|███▌      | 450/1271 [02:25<04:25,  3.09it/s, training_loss=0.388]\u001b[A\n",
            "Epoch 5:  35%|███▌      | 451/1271 [02:25<04:24,  3.10it/s, training_loss=0.388]\u001b[A\n",
            "Epoch 5:  35%|███▌      | 451/1271 [02:25<04:24,  3.10it/s, training_loss=0.456]\u001b[A\n",
            "Epoch 5:  36%|███▌      | 452/1271 [02:25<04:23,  3.11it/s, training_loss=0.456]\u001b[A\n",
            "Epoch 5:  36%|███▌      | 452/1271 [02:25<04:23,  3.11it/s, training_loss=0.155]\u001b[A\n",
            "Epoch 5:  36%|███▌      | 453/1271 [02:25<04:21,  3.13it/s, training_loss=0.155]\u001b[A\n",
            "Epoch 5:  36%|███▌      | 453/1271 [02:26<04:21,  3.13it/s, training_loss=0.067]\u001b[A\n",
            "Epoch 5:  36%|███▌      | 454/1271 [02:26<04:20,  3.13it/s, training_loss=0.067]\u001b[A\n",
            "Epoch 5:  36%|███▌      | 454/1271 [02:26<04:20,  3.13it/s, training_loss=0.233]\u001b[A\n",
            "Epoch 5:  36%|███▌      | 455/1271 [02:26<04:20,  3.13it/s, training_loss=0.233]\u001b[A\n",
            "Epoch 5:  36%|███▌      | 455/1271 [02:26<04:20,  3.13it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 5:  36%|███▌      | 456/1271 [02:26<04:20,  3.13it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 5:  36%|███▌      | 456/1271 [02:27<04:20,  3.13it/s, training_loss=0.396]\u001b[A\n",
            "Epoch 5:  36%|███▌      | 457/1271 [02:27<04:19,  3.13it/s, training_loss=0.396]\u001b[A\n",
            "Epoch 5:  36%|███▌      | 457/1271 [02:27<04:19,  3.13it/s, training_loss=0.603]\u001b[A\n",
            "Epoch 5:  36%|███▌      | 458/1271 [02:27<04:19,  3.13it/s, training_loss=0.603]\u001b[A\n",
            "Epoch 5:  36%|███▌      | 458/1271 [02:27<04:19,  3.13it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 5:  36%|███▌      | 459/1271 [02:27<04:18,  3.14it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 5:  36%|███▌      | 459/1271 [02:28<04:18,  3.14it/s, training_loss=0.078]\u001b[A\n",
            "Epoch 5:  36%|███▌      | 460/1271 [02:28<04:20,  3.12it/s, training_loss=0.078]\u001b[A\n",
            "Epoch 5:  36%|███▌      | 460/1271 [02:28<04:20,  3.12it/s, training_loss=0.122]\u001b[A\n",
            "Epoch 5:  36%|███▋      | 461/1271 [02:28<04:19,  3.12it/s, training_loss=0.122]\u001b[A\n",
            "Epoch 5:  36%|███▋      | 461/1271 [02:28<04:19,  3.12it/s, training_loss=0.051]\u001b[A\n",
            "Epoch 5:  36%|███▋      | 462/1271 [02:28<04:19,  3.12it/s, training_loss=0.051]\u001b[A\n",
            "Epoch 5:  36%|███▋      | 462/1271 [02:28<04:19,  3.12it/s, training_loss=0.086]\u001b[A\n",
            "Epoch 5:  36%|███▋      | 463/1271 [02:28<04:18,  3.12it/s, training_loss=0.086]\u001b[A\n",
            "Epoch 5:  36%|███▋      | 463/1271 [02:29<04:18,  3.12it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 5:  37%|███▋      | 464/1271 [02:29<04:17,  3.14it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 5:  37%|███▋      | 464/1271 [02:29<04:17,  3.14it/s, training_loss=0.112]\u001b[A\n",
            "Epoch 5:  37%|███▋      | 465/1271 [02:29<04:17,  3.13it/s, training_loss=0.112]\u001b[A\n",
            "Epoch 5:  37%|███▋      | 465/1271 [02:29<04:17,  3.13it/s, training_loss=0.014]\u001b[A\n",
            "Epoch 5:  37%|███▋      | 466/1271 [02:29<04:18,  3.11it/s, training_loss=0.014]\u001b[A\n",
            "Epoch 5:  37%|███▋      | 466/1271 [02:30<04:18,  3.11it/s, training_loss=0.316]\u001b[A\n",
            "Epoch 5:  37%|███▋      | 467/1271 [02:30<04:18,  3.11it/s, training_loss=0.316]\u001b[A\n",
            "Epoch 5:  37%|███▋      | 467/1271 [02:30<04:18,  3.11it/s, training_loss=0.433]\u001b[A\n",
            "Epoch 5:  37%|███▋      | 468/1271 [02:30<04:19,  3.09it/s, training_loss=0.433]\u001b[A\n",
            "Epoch 5:  37%|███▋      | 468/1271 [02:30<04:19,  3.09it/s, training_loss=0.458]\u001b[A\n",
            "Epoch 5:  37%|███▋      | 469/1271 [02:30<04:19,  3.09it/s, training_loss=0.458]\u001b[A\n",
            "Epoch 5:  37%|███▋      | 469/1271 [02:31<04:19,  3.09it/s, training_loss=0.060]\u001b[A\n",
            "Epoch 5:  37%|███▋      | 470/1271 [02:31<04:19,  3.08it/s, training_loss=0.060]\u001b[A\n",
            "Epoch 5:  37%|███▋      | 470/1271 [02:31<04:19,  3.08it/s, training_loss=0.050]\u001b[A\n",
            "Epoch 5:  37%|███▋      | 471/1271 [02:31<04:18,  3.10it/s, training_loss=0.050]\u001b[A\n",
            "Epoch 5:  37%|███▋      | 471/1271 [02:31<04:18,  3.10it/s, training_loss=0.001]\u001b[A\n",
            "Epoch 5:  37%|███▋      | 472/1271 [02:31<04:18,  3.09it/s, training_loss=0.001]\u001b[A\n",
            "Epoch 5:  37%|███▋      | 472/1271 [02:32<04:18,  3.09it/s, training_loss=0.388]\u001b[A\n",
            "Epoch 5:  37%|███▋      | 473/1271 [02:32<04:17,  3.10it/s, training_loss=0.388]\u001b[A\n",
            "Epoch 5:  37%|███▋      | 473/1271 [02:32<04:17,  3.10it/s, training_loss=0.125]\u001b[A\n",
            "Epoch 5:  37%|███▋      | 474/1271 [02:32<04:17,  3.10it/s, training_loss=0.125]\u001b[A\n",
            "Epoch 5:  37%|███▋      | 474/1271 [02:32<04:17,  3.10it/s, training_loss=0.263]\u001b[A\n",
            "Epoch 5:  37%|███▋      | 475/1271 [02:32<04:15,  3.11it/s, training_loss=0.263]\u001b[A\n",
            "Epoch 5:  37%|███▋      | 475/1271 [02:33<04:15,  3.11it/s, training_loss=0.261]\u001b[A\n",
            "Epoch 5:  37%|███▋      | 476/1271 [02:33<04:15,  3.11it/s, training_loss=0.261]\u001b[A\n",
            "Epoch 5:  37%|███▋      | 476/1271 [02:33<04:15,  3.11it/s, training_loss=0.132]\u001b[A\n",
            "Epoch 5:  38%|███▊      | 477/1271 [02:33<04:14,  3.12it/s, training_loss=0.132]\u001b[A\n",
            "Epoch 5:  38%|███▊      | 477/1271 [02:33<04:14,  3.12it/s, training_loss=0.395]\u001b[A\n",
            "Epoch 5:  38%|███▊      | 478/1271 [02:33<04:13,  3.13it/s, training_loss=0.395]\u001b[A\n",
            "Epoch 5:  38%|███▊      | 478/1271 [02:34<04:13,  3.13it/s, training_loss=0.344]\u001b[A\n",
            "Epoch 5:  38%|███▊      | 479/1271 [02:34<04:15,  3.10it/s, training_loss=0.344]\u001b[A\n",
            "Epoch 5:  38%|███▊      | 479/1271 [02:34<04:15,  3.10it/s, training_loss=0.325]\u001b[A\n",
            "Epoch 5:  38%|███▊      | 480/1271 [02:34<04:15,  3.09it/s, training_loss=0.325]\u001b[A\n",
            "Epoch 5:  38%|███▊      | 480/1271 [02:34<04:15,  3.09it/s, training_loss=0.437]\u001b[A\n",
            "Epoch 5:  38%|███▊      | 481/1271 [02:34<04:15,  3.09it/s, training_loss=0.437]\u001b[A\n",
            "Epoch 5:  38%|███▊      | 481/1271 [02:35<04:15,  3.09it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 5:  38%|███▊      | 482/1271 [02:35<04:14,  3.11it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 5:  38%|███▊      | 482/1271 [02:35<04:14,  3.11it/s, training_loss=0.130]\u001b[A\n",
            "Epoch 5:  38%|███▊      | 483/1271 [02:35<04:14,  3.09it/s, training_loss=0.130]\u001b[A\n",
            "Epoch 5:  38%|███▊      | 483/1271 [02:35<04:14,  3.09it/s, training_loss=0.326]\u001b[A\n",
            "Epoch 5:  38%|███▊      | 484/1271 [02:35<04:12,  3.12it/s, training_loss=0.326]\u001b[A\n",
            "Epoch 5:  38%|███▊      | 484/1271 [02:36<04:12,  3.12it/s, training_loss=0.530]\u001b[A\n",
            "Epoch 5:  38%|███▊      | 485/1271 [02:36<04:11,  3.12it/s, training_loss=0.530]\u001b[A\n",
            "Epoch 5:  38%|███▊      | 485/1271 [02:36<04:11,  3.12it/s, training_loss=0.105]\u001b[A\n",
            "Epoch 5:  38%|███▊      | 486/1271 [02:36<04:11,  3.12it/s, training_loss=0.105]\u001b[A\n",
            "Epoch 5:  38%|███▊      | 486/1271 [02:36<04:11,  3.12it/s, training_loss=0.363]\u001b[A\n",
            "Epoch 5:  38%|███▊      | 487/1271 [02:36<04:11,  3.12it/s, training_loss=0.363]\u001b[A\n",
            "Epoch 5:  38%|███▊      | 487/1271 [02:37<04:11,  3.12it/s, training_loss=0.157]\u001b[A\n",
            "Epoch 5:  38%|███▊      | 488/1271 [02:37<04:10,  3.12it/s, training_loss=0.157]\u001b[A\n",
            "Epoch 5:  38%|███▊      | 488/1271 [02:37<04:10,  3.12it/s, training_loss=0.526]\u001b[A\n",
            "Epoch 5:  38%|███▊      | 489/1271 [02:37<04:09,  3.13it/s, training_loss=0.526]\u001b[A\n",
            "Epoch 5:  38%|███▊      | 489/1271 [02:37<04:09,  3.13it/s, training_loss=0.172]\u001b[A\n",
            "Epoch 5:  39%|███▊      | 490/1271 [02:37<04:09,  3.13it/s, training_loss=0.172]\u001b[A\n",
            "Epoch 5:  39%|███▊      | 490/1271 [02:37<04:09,  3.13it/s, training_loss=0.166]\u001b[A\n",
            "Epoch 5:  39%|███▊      | 491/1271 [02:38<04:10,  3.11it/s, training_loss=0.166]\u001b[A\n",
            "Epoch 5:  39%|███▊      | 491/1271 [02:38<04:10,  3.11it/s, training_loss=0.113]\u001b[A\n",
            "Epoch 5:  39%|███▊      | 492/1271 [02:38<04:12,  3.08it/s, training_loss=0.113]\u001b[A\n",
            "Epoch 5:  39%|███▊      | 492/1271 [02:38<04:12,  3.08it/s, training_loss=0.335]\u001b[A\n",
            "Epoch 5:  39%|███▉      | 493/1271 [02:38<04:10,  3.11it/s, training_loss=0.335]\u001b[A\n",
            "Epoch 5:  39%|███▉      | 493/1271 [02:38<04:10,  3.11it/s, training_loss=0.260]\u001b[A\n",
            "Epoch 5:  39%|███▉      | 494/1271 [02:38<04:09,  3.11it/s, training_loss=0.260]\u001b[A\n",
            "Epoch 5:  39%|███▉      | 494/1271 [02:39<04:09,  3.11it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 5:  39%|███▉      | 495/1271 [02:39<04:09,  3.12it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 5:  39%|███▉      | 495/1271 [02:39<04:09,  3.12it/s, training_loss=0.112]\u001b[A\n",
            "Epoch 5:  39%|███▉      | 496/1271 [02:39<04:08,  3.12it/s, training_loss=0.112]\u001b[A\n",
            "Epoch 5:  39%|███▉      | 496/1271 [02:39<04:08,  3.12it/s, training_loss=0.138]\u001b[A\n",
            "Epoch 5:  39%|███▉      | 497/1271 [02:39<04:07,  3.13it/s, training_loss=0.138]\u001b[A\n",
            "Epoch 5:  39%|███▉      | 497/1271 [02:40<04:07,  3.13it/s, training_loss=0.051]\u001b[A\n",
            "Epoch 5:  39%|███▉      | 498/1271 [02:40<04:06,  3.13it/s, training_loss=0.051]\u001b[A\n",
            "Epoch 5:  39%|███▉      | 498/1271 [02:40<04:06,  3.13it/s, training_loss=0.256]\u001b[A\n",
            "Epoch 5:  39%|███▉      | 499/1271 [02:40<04:07,  3.12it/s, training_loss=0.256]\u001b[A\n",
            "Epoch 5:  39%|███▉      | 499/1271 [02:40<04:07,  3.12it/s, training_loss=0.399]\u001b[A\n",
            "Epoch 5:  39%|███▉      | 500/1271 [02:40<04:07,  3.12it/s, training_loss=0.399]\u001b[A\n",
            "Epoch 5:  39%|███▉      | 500/1271 [02:41<04:07,  3.12it/s, training_loss=0.345]\u001b[A\n",
            "Epoch 5:  39%|███▉      | 501/1271 [02:41<04:06,  3.12it/s, training_loss=0.345]\u001b[A\n",
            "Epoch 5:  39%|███▉      | 501/1271 [02:41<04:06,  3.12it/s, training_loss=0.166]\u001b[A\n",
            "Epoch 5:  39%|███▉      | 502/1271 [02:41<04:05,  3.13it/s, training_loss=0.166]\u001b[A\n",
            "Epoch 5:  39%|███▉      | 502/1271 [02:41<04:05,  3.13it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 5:  40%|███▉      | 503/1271 [02:41<04:04,  3.14it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 5:  40%|███▉      | 503/1271 [02:42<04:04,  3.14it/s, training_loss=0.055]\u001b[A\n",
            "Epoch 5:  40%|███▉      | 504/1271 [02:42<04:04,  3.13it/s, training_loss=0.055]\u001b[A\n",
            "Epoch 5:  40%|███▉      | 504/1271 [02:42<04:04,  3.13it/s, training_loss=0.152]\u001b[A\n",
            "Epoch 5:  40%|███▉      | 505/1271 [02:42<04:04,  3.13it/s, training_loss=0.152]\u001b[A\n",
            "Epoch 5:  40%|███▉      | 505/1271 [02:42<04:04,  3.13it/s, training_loss=0.285]\u001b[A\n",
            "Epoch 5:  40%|███▉      | 506/1271 [02:42<04:04,  3.14it/s, training_loss=0.285]\u001b[A\n",
            "Epoch 5:  40%|███▉      | 506/1271 [02:43<04:04,  3.14it/s, training_loss=0.134]\u001b[A\n",
            "Epoch 5:  40%|███▉      | 507/1271 [02:43<04:04,  3.13it/s, training_loss=0.134]\u001b[A\n",
            "Epoch 5:  40%|███▉      | 507/1271 [02:43<04:04,  3.13it/s, training_loss=0.034]\u001b[A\n",
            "Epoch 5:  40%|███▉      | 508/1271 [02:43<04:04,  3.12it/s, training_loss=0.034]\u001b[A\n",
            "Epoch 5:  40%|███▉      | 508/1271 [02:43<04:04,  3.12it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 5:  40%|████      | 509/1271 [02:43<04:06,  3.10it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 5:  40%|████      | 509/1271 [02:44<04:06,  3.10it/s, training_loss=0.055]\u001b[A\n",
            "Epoch 5:  40%|████      | 510/1271 [02:44<04:06,  3.09it/s, training_loss=0.055]\u001b[A\n",
            "Epoch 5:  40%|████      | 510/1271 [02:44<04:06,  3.09it/s, training_loss=0.454]\u001b[A\n",
            "Epoch 5:  40%|████      | 511/1271 [02:44<04:05,  3.09it/s, training_loss=0.454]\u001b[A\n",
            "Epoch 5:  40%|████      | 511/1271 [02:44<04:05,  3.09it/s, training_loss=0.120]\u001b[A\n",
            "Epoch 5:  40%|████      | 512/1271 [02:44<04:06,  3.08it/s, training_loss=0.120]\u001b[A\n",
            "Epoch 5:  40%|████      | 512/1271 [02:45<04:06,  3.08it/s, training_loss=0.238]\u001b[A\n",
            "Epoch 5:  40%|████      | 513/1271 [02:45<04:05,  3.09it/s, training_loss=0.238]\u001b[A\n",
            "Epoch 5:  40%|████      | 513/1271 [02:45<04:05,  3.09it/s, training_loss=0.027]\u001b[A\n",
            "Epoch 5:  40%|████      | 514/1271 [02:45<04:05,  3.09it/s, training_loss=0.027]\u001b[A\n",
            "Epoch 5:  40%|████      | 514/1271 [02:45<04:05,  3.09it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 5:  41%|████      | 515/1271 [02:45<04:04,  3.09it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 5:  41%|████      | 515/1271 [02:46<04:04,  3.09it/s, training_loss=0.336]\u001b[A\n",
            "Epoch 5:  41%|████      | 516/1271 [02:46<04:06,  3.07it/s, training_loss=0.336]\u001b[A\n",
            "Epoch 5:  41%|████      | 516/1271 [02:46<04:06,  3.07it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 5:  41%|████      | 517/1271 [02:46<04:05,  3.07it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 5:  41%|████      | 517/1271 [02:46<04:05,  3.07it/s, training_loss=0.023]\u001b[A\n",
            "Epoch 5:  41%|████      | 518/1271 [02:46<04:03,  3.09it/s, training_loss=0.023]\u001b[A\n",
            "Epoch 5:  41%|████      | 518/1271 [02:47<04:03,  3.09it/s, training_loss=0.351]\u001b[A\n",
            "Epoch 5:  41%|████      | 519/1271 [02:47<04:03,  3.09it/s, training_loss=0.351]\u001b[A\n",
            "Epoch 5:  41%|████      | 519/1271 [02:47<04:03,  3.09it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 5:  41%|████      | 520/1271 [02:47<04:01,  3.11it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 5:  41%|████      | 520/1271 [02:47<04:01,  3.11it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 5:  41%|████      | 521/1271 [02:47<04:00,  3.12it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 5:  41%|████      | 521/1271 [02:47<04:00,  3.12it/s, training_loss=0.101]\u001b[A\n",
            "Epoch 5:  41%|████      | 522/1271 [02:47<04:01,  3.11it/s, training_loss=0.101]\u001b[A\n",
            "Epoch 5:  41%|████      | 522/1271 [02:48<04:01,  3.11it/s, training_loss=0.242]\u001b[A\n",
            "Epoch 5:  41%|████      | 523/1271 [02:48<04:00,  3.11it/s, training_loss=0.242]\u001b[A\n",
            "Epoch 5:  41%|████      | 523/1271 [02:48<04:00,  3.11it/s, training_loss=0.025]\u001b[A\n",
            "Epoch 5:  41%|████      | 524/1271 [02:48<03:59,  3.12it/s, training_loss=0.025]\u001b[A\n",
            "Epoch 5:  41%|████      | 524/1271 [02:48<03:59,  3.12it/s, training_loss=0.072]\u001b[A\n",
            "Epoch 5:  41%|████▏     | 525/1271 [02:48<03:58,  3.13it/s, training_loss=0.072]\u001b[A\n",
            "Epoch 5:  41%|████▏     | 525/1271 [02:49<03:58,  3.13it/s, training_loss=0.080]\u001b[A\n",
            "Epoch 5:  41%|████▏     | 526/1271 [02:49<03:58,  3.12it/s, training_loss=0.080]\u001b[A\n",
            "Epoch 5:  41%|████▏     | 526/1271 [02:49<03:58,  3.12it/s, training_loss=0.240]\u001b[A\n",
            "Epoch 5:  41%|████▏     | 527/1271 [02:49<03:58,  3.12it/s, training_loss=0.240]\u001b[A\n",
            "Epoch 5:  41%|████▏     | 527/1271 [02:49<03:58,  3.12it/s, training_loss=0.157]\u001b[A\n",
            "Epoch 5:  42%|████▏     | 528/1271 [02:49<03:59,  3.10it/s, training_loss=0.157]\u001b[A\n",
            "Epoch 5:  42%|████▏     | 528/1271 [02:50<03:59,  3.10it/s, training_loss=0.363]\u001b[A\n",
            "Epoch 5:  42%|████▏     | 529/1271 [02:50<03:59,  3.10it/s, training_loss=0.363]\u001b[A\n",
            "Epoch 5:  42%|████▏     | 529/1271 [02:50<03:59,  3.10it/s, training_loss=0.065]\u001b[A\n",
            "Epoch 5:  42%|████▏     | 530/1271 [02:50<03:57,  3.11it/s, training_loss=0.065]\u001b[A\n",
            "Epoch 5:  42%|████▏     | 530/1271 [02:50<03:57,  3.11it/s, training_loss=0.243]\u001b[A\n",
            "Epoch 5:  42%|████▏     | 531/1271 [02:50<03:57,  3.11it/s, training_loss=0.243]\u001b[A\n",
            "Epoch 5:  42%|████▏     | 531/1271 [02:51<03:57,  3.11it/s, training_loss=0.229]\u001b[A\n",
            "Epoch 5:  42%|████▏     | 532/1271 [02:51<03:56,  3.12it/s, training_loss=0.229]\u001b[A\n",
            "Epoch 5:  42%|████▏     | 532/1271 [02:51<03:56,  3.12it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 5:  42%|████▏     | 533/1271 [02:51<03:57,  3.11it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 5:  42%|████▏     | 533/1271 [02:51<03:57,  3.11it/s, training_loss=0.142]\u001b[A\n",
            "Epoch 5:  42%|████▏     | 534/1271 [02:51<03:56,  3.12it/s, training_loss=0.142]\u001b[A\n",
            "Epoch 5:  42%|████▏     | 534/1271 [02:52<03:56,  3.12it/s, training_loss=0.443]\u001b[A\n",
            "Epoch 5:  42%|████▏     | 535/1271 [02:52<03:57,  3.10it/s, training_loss=0.443]\u001b[A\n",
            "Epoch 5:  42%|████▏     | 535/1271 [02:52<03:57,  3.10it/s, training_loss=0.178]\u001b[A\n",
            "Epoch 5:  42%|████▏     | 536/1271 [02:52<03:56,  3.10it/s, training_loss=0.178]\u001b[A\n",
            "Epoch 5:  42%|████▏     | 536/1271 [02:52<03:56,  3.10it/s, training_loss=0.070]\u001b[A\n",
            "Epoch 5:  42%|████▏     | 537/1271 [02:52<03:57,  3.09it/s, training_loss=0.070]\u001b[A\n",
            "Epoch 5:  42%|████▏     | 537/1271 [02:53<03:57,  3.09it/s, training_loss=0.078]\u001b[A\n",
            "Epoch 5:  42%|████▏     | 538/1271 [02:53<03:58,  3.08it/s, training_loss=0.078]\u001b[A\n",
            "Epoch 5:  42%|████▏     | 538/1271 [02:53<03:58,  3.08it/s, training_loss=0.049]\u001b[A\n",
            "Epoch 5:  42%|████▏     | 539/1271 [02:53<03:57,  3.08it/s, training_loss=0.049]\u001b[A\n",
            "Epoch 5:  42%|████▏     | 539/1271 [02:53<03:57,  3.08it/s, training_loss=0.292]\u001b[A\n",
            "Epoch 5:  42%|████▏     | 540/1271 [02:53<03:56,  3.09it/s, training_loss=0.292]\u001b[A\n",
            "Epoch 5:  42%|████▏     | 540/1271 [02:54<03:56,  3.09it/s, training_loss=0.115]\u001b[A\n",
            "Epoch 5:  43%|████▎     | 541/1271 [02:54<03:56,  3.09it/s, training_loss=0.115]\u001b[A\n",
            "Epoch 5:  43%|████▎     | 541/1271 [02:54<03:56,  3.09it/s, training_loss=0.120]\u001b[A\n",
            "Epoch 5:  43%|████▎     | 542/1271 [02:54<03:55,  3.09it/s, training_loss=0.120]\u001b[A\n",
            "Epoch 5:  43%|████▎     | 542/1271 [02:54<03:55,  3.09it/s, training_loss=0.475]\u001b[A\n",
            "Epoch 5:  43%|████▎     | 543/1271 [02:54<03:53,  3.12it/s, training_loss=0.475]\u001b[A\n",
            "Epoch 5:  43%|████▎     | 543/1271 [02:55<03:53,  3.12it/s, training_loss=0.451]\u001b[A\n",
            "Epoch 5:  43%|████▎     | 544/1271 [02:55<03:53,  3.12it/s, training_loss=0.451]\u001b[A\n",
            "Epoch 5:  43%|████▎     | 544/1271 [02:55<03:53,  3.12it/s, training_loss=0.244]\u001b[A\n",
            "Epoch 5:  43%|████▎     | 545/1271 [02:55<03:52,  3.12it/s, training_loss=0.244]\u001b[A\n",
            "Epoch 5:  43%|████▎     | 545/1271 [02:55<03:52,  3.12it/s, training_loss=0.755]\u001b[A\n",
            "Epoch 5:  43%|████▎     | 546/1271 [02:55<03:52,  3.12it/s, training_loss=0.755]\u001b[A\n",
            "Epoch 5:  43%|████▎     | 546/1271 [02:56<03:52,  3.12it/s, training_loss=0.397]\u001b[A\n",
            "Epoch 5:  43%|████▎     | 547/1271 [02:56<03:52,  3.11it/s, training_loss=0.397]\u001b[A\n",
            "Epoch 5:  43%|████▎     | 547/1271 [02:56<03:52,  3.11it/s, training_loss=0.345]\u001b[A\n",
            "Epoch 5:  43%|████▎     | 548/1271 [02:56<03:51,  3.12it/s, training_loss=0.345]\u001b[A\n",
            "Epoch 5:  43%|████▎     | 548/1271 [02:56<03:51,  3.12it/s, training_loss=0.345]\u001b[A\n",
            "Epoch 5:  43%|████▎     | 549/1271 [02:56<03:51,  3.12it/s, training_loss=0.345]\u001b[A\n",
            "Epoch 5:  43%|████▎     | 549/1271 [02:56<03:51,  3.12it/s, training_loss=0.311]\u001b[A\n",
            "Epoch 5:  43%|████▎     | 550/1271 [02:56<03:50,  3.12it/s, training_loss=0.311]\u001b[A\n",
            "Epoch 5:  43%|████▎     | 550/1271 [02:57<03:50,  3.12it/s, training_loss=0.115]\u001b[A\n",
            "Epoch 5:  43%|████▎     | 551/1271 [02:57<03:51,  3.11it/s, training_loss=0.115]\u001b[A\n",
            "Epoch 5:  43%|████▎     | 551/1271 [02:57<03:51,  3.11it/s, training_loss=0.036]\u001b[A\n",
            "Epoch 5:  43%|████▎     | 552/1271 [02:57<03:51,  3.11it/s, training_loss=0.036]\u001b[A\n",
            "Epoch 5:  43%|████▎     | 552/1271 [02:57<03:51,  3.11it/s, training_loss=0.040]\u001b[A\n",
            "Epoch 5:  44%|████▎     | 553/1271 [02:57<03:50,  3.11it/s, training_loss=0.040]\u001b[A\n",
            "Epoch 5:  44%|████▎     | 553/1271 [02:58<03:50,  3.11it/s, training_loss=0.172]\u001b[A\n",
            "Epoch 5:  44%|████▎     | 554/1271 [02:58<03:50,  3.11it/s, training_loss=0.172]\u001b[A\n",
            "Epoch 5:  44%|████▎     | 554/1271 [02:58<03:50,  3.11it/s, training_loss=0.165]\u001b[A\n",
            "Epoch 5:  44%|████▎     | 555/1271 [02:58<03:50,  3.11it/s, training_loss=0.165]\u001b[A\n",
            "Epoch 5:  44%|████▎     | 555/1271 [02:58<03:50,  3.11it/s, training_loss=0.176]\u001b[A\n",
            "Epoch 5:  44%|████▎     | 556/1271 [02:58<03:49,  3.12it/s, training_loss=0.176]\u001b[A\n",
            "Epoch 5:  44%|████▎     | 556/1271 [02:59<03:49,  3.12it/s, training_loss=0.258]\u001b[A\n",
            "Epoch 5:  44%|████▍     | 557/1271 [02:59<03:48,  3.13it/s, training_loss=0.258]\u001b[A\n",
            "Epoch 5:  44%|████▍     | 557/1271 [02:59<03:48,  3.13it/s, training_loss=0.271]\u001b[A\n",
            "Epoch 5:  44%|████▍     | 558/1271 [02:59<03:49,  3.10it/s, training_loss=0.271]\u001b[A\n",
            "Epoch 5:  44%|████▍     | 558/1271 [02:59<03:49,  3.10it/s, training_loss=0.043]\u001b[A\n",
            "Epoch 5:  44%|████▍     | 559/1271 [02:59<03:49,  3.11it/s, training_loss=0.043]\u001b[A\n",
            "Epoch 5:  44%|████▍     | 559/1271 [03:00<03:49,  3.11it/s, training_loss=0.060]\u001b[A\n",
            "Epoch 5:  44%|████▍     | 560/1271 [03:00<03:48,  3.11it/s, training_loss=0.060]\u001b[A\n",
            "Epoch 5:  44%|████▍     | 560/1271 [03:00<03:48,  3.11it/s, training_loss=0.572]\u001b[A\n",
            "Epoch 5:  44%|████▍     | 561/1271 [03:00<03:49,  3.10it/s, training_loss=0.572]\u001b[A\n",
            "Epoch 5:  44%|████▍     | 561/1271 [03:00<03:49,  3.10it/s, training_loss=0.624]\u001b[A\n",
            "Epoch 5:  44%|████▍     | 562/1271 [03:00<03:49,  3.10it/s, training_loss=0.624]\u001b[A\n",
            "Epoch 5:  44%|████▍     | 562/1271 [03:01<03:49,  3.10it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 5:  44%|████▍     | 563/1271 [03:01<03:47,  3.11it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 5:  44%|████▍     | 563/1271 [03:01<03:47,  3.11it/s, training_loss=0.344]\u001b[A\n",
            "Epoch 5:  44%|████▍     | 564/1271 [03:01<03:46,  3.12it/s, training_loss=0.344]\u001b[A\n",
            "Epoch 5:  44%|████▍     | 564/1271 [03:01<03:46,  3.12it/s, training_loss=0.660]\u001b[A\n",
            "Epoch 5:  44%|████▍     | 565/1271 [03:01<03:46,  3.11it/s, training_loss=0.660]\u001b[A\n",
            "Epoch 5:  44%|████▍     | 565/1271 [03:02<03:46,  3.11it/s, training_loss=0.240]\u001b[A\n",
            "Epoch 5:  45%|████▍     | 566/1271 [03:02<03:45,  3.12it/s, training_loss=0.240]\u001b[A\n",
            "Epoch 5:  45%|████▍     | 566/1271 [03:02<03:45,  3.12it/s, training_loss=0.455]\u001b[A\n",
            "Epoch 5:  45%|████▍     | 567/1271 [03:02<03:45,  3.13it/s, training_loss=0.455]\u001b[A\n",
            "Epoch 5:  45%|████▍     | 567/1271 [03:02<03:45,  3.13it/s, training_loss=0.058]\u001b[A\n",
            "Epoch 5:  45%|████▍     | 568/1271 [03:02<03:45,  3.12it/s, training_loss=0.058]\u001b[A\n",
            "Epoch 5:  45%|████▍     | 568/1271 [03:03<03:45,  3.12it/s, training_loss=0.192]\u001b[A\n",
            "Epoch 5:  45%|████▍     | 569/1271 [03:03<03:45,  3.12it/s, training_loss=0.192]\u001b[A\n",
            "Epoch 5:  45%|████▍     | 569/1271 [03:03<03:45,  3.12it/s, training_loss=0.403]\u001b[A\n",
            "Epoch 5:  45%|████▍     | 570/1271 [03:03<03:44,  3.12it/s, training_loss=0.403]\u001b[A\n",
            "Epoch 5:  45%|████▍     | 570/1271 [03:03<03:44,  3.12it/s, training_loss=0.456]\u001b[A\n",
            "Epoch 5:  45%|████▍     | 571/1271 [03:03<03:43,  3.13it/s, training_loss=0.456]\u001b[A\n",
            "Epoch 5:  45%|████▍     | 571/1271 [03:04<03:43,  3.13it/s, training_loss=0.408]\u001b[A\n",
            "Epoch 5:  45%|████▌     | 572/1271 [03:04<03:43,  3.12it/s, training_loss=0.408]\u001b[A\n",
            "Epoch 5:  45%|████▌     | 572/1271 [03:04<03:43,  3.12it/s, training_loss=0.095]\u001b[A\n",
            "Epoch 5:  45%|████▌     | 573/1271 [03:04<03:43,  3.12it/s, training_loss=0.095]\u001b[A\n",
            "Epoch 5:  45%|████▌     | 573/1271 [03:04<03:43,  3.12it/s, training_loss=0.467]\u001b[A\n",
            "Epoch 5:  45%|████▌     | 574/1271 [03:04<03:43,  3.11it/s, training_loss=0.467]\u001b[A\n",
            "Epoch 5:  45%|████▌     | 574/1271 [03:05<03:43,  3.11it/s, training_loss=0.077]\u001b[A\n",
            "Epoch 5:  45%|████▌     | 575/1271 [03:05<03:44,  3.10it/s, training_loss=0.077]\u001b[A\n",
            "Epoch 5:  45%|████▌     | 575/1271 [03:05<03:44,  3.10it/s, training_loss=0.162]\u001b[A\n",
            "Epoch 5:  45%|████▌     | 576/1271 [03:05<03:45,  3.09it/s, training_loss=0.162]\u001b[A\n",
            "Epoch 5:  45%|████▌     | 576/1271 [03:05<03:45,  3.09it/s, training_loss=0.033]\u001b[A\n",
            "Epoch 5:  45%|████▌     | 577/1271 [03:05<03:44,  3.09it/s, training_loss=0.033]\u001b[A\n",
            "Epoch 5:  45%|████▌     | 577/1271 [03:05<03:44,  3.09it/s, training_loss=0.061]\u001b[A\n",
            "Epoch 5:  45%|████▌     | 578/1271 [03:05<03:43,  3.10it/s, training_loss=0.061]\u001b[A\n",
            "Epoch 5:  45%|████▌     | 578/1271 [03:06<03:43,  3.10it/s, training_loss=0.007]\u001b[A\n",
            "Epoch 5:  46%|████▌     | 579/1271 [03:06<03:43,  3.10it/s, training_loss=0.007]\u001b[A\n",
            "Epoch 5:  46%|████▌     | 579/1271 [03:06<03:43,  3.10it/s, training_loss=0.562]\u001b[A\n",
            "Epoch 5:  46%|████▌     | 580/1271 [03:06<03:43,  3.09it/s, training_loss=0.562]\u001b[A\n",
            "Epoch 5:  46%|████▌     | 580/1271 [03:06<03:43,  3.09it/s, training_loss=0.087]\u001b[A\n",
            "Epoch 5:  46%|████▌     | 581/1271 [03:06<03:42,  3.10it/s, training_loss=0.087]\u001b[A\n",
            "Epoch 5:  46%|████▌     | 581/1271 [03:07<03:42,  3.10it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 5:  46%|████▌     | 582/1271 [03:07<03:41,  3.11it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 5:  46%|████▌     | 582/1271 [03:07<03:41,  3.11it/s, training_loss=0.091]\u001b[A\n",
            "Epoch 5:  46%|████▌     | 583/1271 [03:07<03:40,  3.12it/s, training_loss=0.091]\u001b[A\n",
            "Epoch 5:  46%|████▌     | 583/1271 [03:07<03:40,  3.12it/s, training_loss=0.252]\u001b[A\n",
            "Epoch 5:  46%|████▌     | 584/1271 [03:07<03:40,  3.11it/s, training_loss=0.252]\u001b[A\n",
            "Epoch 5:  46%|████▌     | 584/1271 [03:08<03:40,  3.11it/s, training_loss=0.044]\u001b[A\n",
            "Epoch 5:  46%|████▌     | 585/1271 [03:08<03:40,  3.11it/s, training_loss=0.044]\u001b[A\n",
            "Epoch 5:  46%|████▌     | 585/1271 [03:08<03:40,  3.11it/s, training_loss=0.298]\u001b[A\n",
            "Epoch 5:  46%|████▌     | 586/1271 [03:08<03:40,  3.11it/s, training_loss=0.298]\u001b[A\n",
            "Epoch 5:  46%|████▌     | 586/1271 [03:08<03:40,  3.11it/s, training_loss=0.497]\u001b[A\n",
            "Epoch 5:  46%|████▌     | 587/1271 [03:08<03:40,  3.11it/s, training_loss=0.497]\u001b[A\n",
            "Epoch 5:  46%|████▌     | 587/1271 [03:09<03:40,  3.11it/s, training_loss=0.226]\u001b[A\n",
            "Epoch 5:  46%|████▋     | 588/1271 [03:09<03:40,  3.09it/s, training_loss=0.226]\u001b[A\n",
            "Epoch 5:  46%|████▋     | 588/1271 [03:09<03:40,  3.09it/s, training_loss=0.625]\u001b[A\n",
            "Epoch 5:  46%|████▋     | 589/1271 [03:09<03:42,  3.06it/s, training_loss=0.625]\u001b[A\n",
            "Epoch 5:  46%|████▋     | 589/1271 [03:09<03:42,  3.06it/s, training_loss=0.287]\u001b[A\n",
            "Epoch 5:  46%|████▋     | 590/1271 [03:09<03:41,  3.08it/s, training_loss=0.287]\u001b[A\n",
            "Epoch 5:  46%|████▋     | 590/1271 [03:10<03:41,  3.08it/s, training_loss=0.426]\u001b[A\n",
            "Epoch 5:  46%|████▋     | 591/1271 [03:10<03:40,  3.09it/s, training_loss=0.426]\u001b[A\n",
            "Epoch 5:  46%|████▋     | 591/1271 [03:10<03:40,  3.09it/s, training_loss=0.242]\u001b[A\n",
            "Epoch 5:  47%|████▋     | 592/1271 [03:10<03:39,  3.10it/s, training_loss=0.242]\u001b[A\n",
            "Epoch 5:  47%|████▋     | 592/1271 [03:10<03:39,  3.10it/s, training_loss=0.305]\u001b[A\n",
            "Epoch 5:  47%|████▋     | 593/1271 [03:10<03:38,  3.10it/s, training_loss=0.305]\u001b[A\n",
            "Epoch 5:  47%|████▋     | 593/1271 [03:11<03:38,  3.10it/s, training_loss=0.266]\u001b[A\n",
            "Epoch 5:  47%|████▋     | 594/1271 [03:11<03:37,  3.12it/s, training_loss=0.266]\u001b[A\n",
            "Epoch 5:  47%|████▋     | 594/1271 [03:11<03:37,  3.12it/s, training_loss=0.168]\u001b[A\n",
            "Epoch 5:  47%|████▋     | 595/1271 [03:11<03:36,  3.12it/s, training_loss=0.168]\u001b[A\n",
            "Epoch 5:  47%|████▋     | 595/1271 [03:11<03:36,  3.12it/s, training_loss=0.143]\u001b[A\n",
            "Epoch 5:  47%|████▋     | 596/1271 [03:11<03:36,  3.12it/s, training_loss=0.143]\u001b[A\n",
            "Epoch 5:  47%|████▋     | 596/1271 [03:12<03:36,  3.12it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 5:  47%|████▋     | 597/1271 [03:12<03:36,  3.12it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 5:  47%|████▋     | 597/1271 [03:12<03:36,  3.12it/s, training_loss=0.183]\u001b[A\n",
            "Epoch 5:  47%|████▋     | 598/1271 [03:12<03:37,  3.10it/s, training_loss=0.183]\u001b[A\n",
            "Epoch 5:  47%|████▋     | 598/1271 [03:12<03:37,  3.10it/s, training_loss=0.382]\u001b[A\n",
            "Epoch 5:  47%|████▋     | 599/1271 [03:12<03:36,  3.10it/s, training_loss=0.382]\u001b[A\n",
            "Epoch 5:  47%|████▋     | 599/1271 [03:13<03:36,  3.10it/s, training_loss=0.454]\u001b[A\n",
            "Epoch 5:  47%|████▋     | 600/1271 [03:13<03:37,  3.09it/s, training_loss=0.454]\u001b[A\n",
            "Epoch 5:  47%|████▋     | 600/1271 [03:13<03:37,  3.09it/s, training_loss=0.042]\u001b[A\n",
            "Epoch 5:  47%|████▋     | 601/1271 [03:13<03:36,  3.09it/s, training_loss=0.042]\u001b[A\n",
            "Epoch 5:  47%|████▋     | 601/1271 [03:13<03:36,  3.09it/s, training_loss=0.019]\u001b[A\n",
            "Epoch 5:  47%|████▋     | 602/1271 [03:13<03:36,  3.09it/s, training_loss=0.019]\u001b[A\n",
            "Epoch 5:  47%|████▋     | 602/1271 [03:14<03:36,  3.09it/s, training_loss=0.027]\u001b[A\n",
            "Epoch 5:  47%|████▋     | 603/1271 [03:14<03:35,  3.10it/s, training_loss=0.027]\u001b[A\n",
            "Epoch 5:  47%|████▋     | 603/1271 [03:14<03:35,  3.10it/s, training_loss=0.552]\u001b[A\n",
            "Epoch 5:  48%|████▊     | 604/1271 [03:14<03:35,  3.10it/s, training_loss=0.552]\u001b[A\n",
            "Epoch 5:  48%|████▊     | 604/1271 [03:14<03:35,  3.10it/s, training_loss=0.284]\u001b[A\n",
            "Epoch 5:  48%|████▊     | 605/1271 [03:14<03:35,  3.09it/s, training_loss=0.284]\u001b[A\n",
            "Epoch 5:  48%|████▊     | 605/1271 [03:15<03:35,  3.09it/s, training_loss=0.274]\u001b[A\n",
            "Epoch 5:  48%|████▊     | 606/1271 [03:15<03:34,  3.09it/s, training_loss=0.274]\u001b[A\n",
            "Epoch 5:  48%|████▊     | 606/1271 [03:15<03:34,  3.09it/s, training_loss=0.088]\u001b[A\n",
            "Epoch 5:  48%|████▊     | 607/1271 [03:15<03:33,  3.10it/s, training_loss=0.088]\u001b[A\n",
            "Epoch 5:  48%|████▊     | 607/1271 [03:15<03:33,  3.10it/s, training_loss=0.462]\u001b[A\n",
            "Epoch 5:  48%|████▊     | 608/1271 [03:15<03:32,  3.11it/s, training_loss=0.462]\u001b[A\n",
            "Epoch 5:  48%|████▊     | 608/1271 [03:15<03:32,  3.11it/s, training_loss=0.315]\u001b[A\n",
            "Epoch 5:  48%|████▊     | 609/1271 [03:15<03:31,  3.13it/s, training_loss=0.315]\u001b[A\n",
            "Epoch 5:  48%|████▊     | 609/1271 [03:16<03:31,  3.13it/s, training_loss=0.421]\u001b[A\n",
            "Epoch 5:  48%|████▊     | 610/1271 [03:16<03:32,  3.12it/s, training_loss=0.421]\u001b[A\n",
            "Epoch 5:  48%|████▊     | 610/1271 [03:16<03:32,  3.12it/s, training_loss=0.237]\u001b[A\n",
            "Epoch 5:  48%|████▊     | 611/1271 [03:16<03:31,  3.12it/s, training_loss=0.237]\u001b[A\n",
            "Epoch 5:  48%|████▊     | 611/1271 [03:16<03:31,  3.12it/s, training_loss=0.678]\u001b[A\n",
            "Epoch 5:  48%|████▊     | 612/1271 [03:16<03:30,  3.12it/s, training_loss=0.678]\u001b[A\n",
            "Epoch 5:  48%|████▊     | 612/1271 [03:17<03:30,  3.12it/s, training_loss=0.195]\u001b[A\n",
            "Epoch 5:  48%|████▊     | 613/1271 [03:17<03:32,  3.10it/s, training_loss=0.195]\u001b[A\n",
            "Epoch 5:  48%|████▊     | 613/1271 [03:17<03:32,  3.10it/s, training_loss=0.181]\u001b[A\n",
            "Epoch 5:  48%|████▊     | 614/1271 [03:17<03:30,  3.12it/s, training_loss=0.181]\u001b[A\n",
            "Epoch 5:  48%|████▊     | 614/1271 [03:17<03:30,  3.12it/s, training_loss=0.344]\u001b[A\n",
            "Epoch 5:  48%|████▊     | 615/1271 [03:17<03:29,  3.13it/s, training_loss=0.344]\u001b[A\n",
            "Epoch 5:  48%|████▊     | 615/1271 [03:18<03:29,  3.13it/s, training_loss=0.293]\u001b[A\n",
            "Epoch 5:  48%|████▊     | 616/1271 [03:18<03:29,  3.13it/s, training_loss=0.293]\u001b[A\n",
            "Epoch 5:  48%|████▊     | 616/1271 [03:18<03:29,  3.13it/s, training_loss=0.590]\u001b[A\n",
            "Epoch 5:  49%|████▊     | 617/1271 [03:18<03:29,  3.11it/s, training_loss=0.590]\u001b[A\n",
            "Epoch 5:  49%|████▊     | 617/1271 [03:18<03:29,  3.11it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 5:  49%|████▊     | 618/1271 [03:18<03:31,  3.09it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 5:  49%|████▊     | 618/1271 [03:19<03:31,  3.09it/s, training_loss=0.379]\u001b[A\n",
            "Epoch 5:  49%|████▊     | 619/1271 [03:19<03:31,  3.09it/s, training_loss=0.379]\u001b[A\n",
            "Epoch 5:  49%|████▊     | 619/1271 [03:19<03:31,  3.09it/s, training_loss=0.054]\u001b[A\n",
            "Epoch 5:  49%|████▉     | 620/1271 [03:19<03:30,  3.09it/s, training_loss=0.054]\u001b[A\n",
            "Epoch 5:  49%|████▉     | 620/1271 [03:19<03:30,  3.09it/s, training_loss=0.515]\u001b[A\n",
            "Epoch 5:  49%|████▉     | 621/1271 [03:19<03:30,  3.09it/s, training_loss=0.515]\u001b[A\n",
            "Epoch 5:  49%|████▉     | 621/1271 [03:20<03:30,  3.09it/s, training_loss=0.210]\u001b[A\n",
            "Epoch 5:  49%|████▉     | 622/1271 [03:20<03:29,  3.10it/s, training_loss=0.210]\u001b[A\n",
            "Epoch 5:  49%|████▉     | 622/1271 [03:20<03:29,  3.10it/s, training_loss=0.544]\u001b[A\n",
            "Epoch 5:  49%|████▉     | 623/1271 [03:20<03:28,  3.10it/s, training_loss=0.544]\u001b[A\n",
            "Epoch 5:  49%|████▉     | 623/1271 [03:20<03:28,  3.10it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 5:  49%|████▉     | 624/1271 [03:20<03:28,  3.10it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 5:  49%|████▉     | 624/1271 [03:21<03:28,  3.10it/s, training_loss=0.118]\u001b[A\n",
            "Epoch 5:  49%|████▉     | 625/1271 [03:21<03:28,  3.10it/s, training_loss=0.118]\u001b[A\n",
            "Epoch 5:  49%|████▉     | 625/1271 [03:21<03:28,  3.10it/s, training_loss=0.201]\u001b[A\n",
            "Epoch 5:  49%|████▉     | 626/1271 [03:21<03:27,  3.10it/s, training_loss=0.201]\u001b[A\n",
            "Epoch 5:  49%|████▉     | 626/1271 [03:21<03:27,  3.10it/s, training_loss=0.264]\u001b[A\n",
            "Epoch 5:  49%|████▉     | 627/1271 [03:21<03:27,  3.11it/s, training_loss=0.264]\u001b[A\n",
            "Epoch 5:  49%|████▉     | 627/1271 [03:22<03:27,  3.11it/s, training_loss=0.121]\u001b[A\n",
            "Epoch 5:  49%|████▉     | 628/1271 [03:22<03:26,  3.12it/s, training_loss=0.121]\u001b[A\n",
            "Epoch 5:  49%|████▉     | 628/1271 [03:22<03:26,  3.12it/s, training_loss=1.028]\u001b[A\n",
            "Epoch 5:  49%|████▉     | 629/1271 [03:22<03:25,  3.12it/s, training_loss=1.028]\u001b[A\n",
            "Epoch 5:  49%|████▉     | 629/1271 [03:22<03:25,  3.12it/s, training_loss=0.070]\u001b[A\n",
            "Epoch 5:  50%|████▉     | 630/1271 [03:22<03:25,  3.12it/s, training_loss=0.070]\u001b[A\n",
            "Epoch 5:  50%|████▉     | 630/1271 [03:23<03:25,  3.12it/s, training_loss=0.080]\u001b[A\n",
            "Epoch 5:  50%|████▉     | 631/1271 [03:23<03:23,  3.14it/s, training_loss=0.080]\u001b[A\n",
            "Epoch 5:  50%|████▉     | 631/1271 [03:23<03:23,  3.14it/s, training_loss=0.278]\u001b[A\n",
            "Epoch 5:  50%|████▉     | 632/1271 [03:23<03:24,  3.12it/s, training_loss=0.278]\u001b[A\n",
            "Epoch 5:  50%|████▉     | 632/1271 [03:23<03:24,  3.12it/s, training_loss=0.046]\u001b[A\n",
            "Epoch 5:  50%|████▉     | 633/1271 [03:23<03:25,  3.10it/s, training_loss=0.046]\u001b[A\n",
            "Epoch 5:  50%|████▉     | 633/1271 [03:24<03:25,  3.10it/s, training_loss=0.439]\u001b[A\n",
            "Epoch 5:  50%|████▉     | 634/1271 [03:24<03:25,  3.10it/s, training_loss=0.439]\u001b[A\n",
            "Epoch 5:  50%|████▉     | 634/1271 [03:24<03:25,  3.10it/s, training_loss=0.056]\u001b[A\n",
            "Epoch 5:  50%|████▉     | 635/1271 [03:24<03:24,  3.11it/s, training_loss=0.056]\u001b[A\n",
            "Epoch 5:  50%|████▉     | 635/1271 [03:24<03:24,  3.11it/s, training_loss=0.112]\u001b[A\n",
            "Epoch 5:  50%|█████     | 636/1271 [03:24<03:24,  3.10it/s, training_loss=0.112]\u001b[A\n",
            "Epoch 5:  50%|█████     | 636/1271 [03:24<03:24,  3.10it/s, training_loss=0.272]\u001b[A\n",
            "Epoch 5:  50%|█████     | 637/1271 [03:24<03:23,  3.12it/s, training_loss=0.272]\u001b[A\n",
            "Epoch 5:  50%|█████     | 637/1271 [03:25<03:23,  3.12it/s, training_loss=0.478]\u001b[A\n",
            "Epoch 5:  50%|█████     | 638/1271 [03:25<03:21,  3.13it/s, training_loss=0.478]\u001b[A\n",
            "Epoch 5:  50%|█████     | 638/1271 [03:25<03:21,  3.13it/s, training_loss=0.169]\u001b[A\n",
            "Epoch 5:  50%|█████     | 639/1271 [03:25<03:22,  3.12it/s, training_loss=0.169]\u001b[A\n",
            "Epoch 5:  50%|█████     | 639/1271 [03:25<03:22,  3.12it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 5:  50%|█████     | 640/1271 [03:25<03:22,  3.11it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 5:  50%|█████     | 640/1271 [03:26<03:22,  3.11it/s, training_loss=0.027]\u001b[A\n",
            "Epoch 5:  50%|█████     | 641/1271 [03:26<03:22,  3.11it/s, training_loss=0.027]\u001b[A\n",
            "Epoch 5:  50%|█████     | 641/1271 [03:26<03:22,  3.11it/s, training_loss=0.067]\u001b[A\n",
            "Epoch 5:  51%|█████     | 642/1271 [03:26<03:21,  3.13it/s, training_loss=0.067]\u001b[A\n",
            "Epoch 5:  51%|█████     | 642/1271 [03:26<03:21,  3.13it/s, training_loss=0.116]\u001b[A\n",
            "Epoch 5:  51%|█████     | 643/1271 [03:26<03:19,  3.14it/s, training_loss=0.116]\u001b[A\n",
            "Epoch 5:  51%|█████     | 643/1271 [03:27<03:19,  3.14it/s, training_loss=0.116]\u001b[A\n",
            "Epoch 5:  51%|█████     | 644/1271 [03:27<03:19,  3.15it/s, training_loss=0.116]\u001b[A\n",
            "Epoch 5:  51%|█████     | 644/1271 [03:27<03:19,  3.15it/s, training_loss=0.221]\u001b[A\n",
            "Epoch 5:  51%|█████     | 645/1271 [03:27<03:19,  3.14it/s, training_loss=0.221]\u001b[A\n",
            "Epoch 5:  51%|█████     | 645/1271 [03:27<03:19,  3.14it/s, training_loss=0.129]\u001b[A\n",
            "Epoch 5:  51%|█████     | 646/1271 [03:27<03:19,  3.14it/s, training_loss=0.129]\u001b[A\n",
            "Epoch 5:  51%|█████     | 646/1271 [03:28<03:19,  3.14it/s, training_loss=0.405]\u001b[A\n",
            "Epoch 5:  51%|█████     | 647/1271 [03:28<03:20,  3.11it/s, training_loss=0.405]\u001b[A\n",
            "Epoch 5:  51%|█████     | 647/1271 [03:28<03:20,  3.11it/s, training_loss=0.467]\u001b[A\n",
            "Epoch 5:  51%|█████     | 648/1271 [03:28<03:20,  3.11it/s, training_loss=0.467]\u001b[A\n",
            "Epoch 5:  51%|█████     | 648/1271 [03:28<03:20,  3.11it/s, training_loss=0.032]\u001b[A\n",
            "Epoch 5:  51%|█████     | 649/1271 [03:28<03:20,  3.10it/s, training_loss=0.032]\u001b[A\n",
            "Epoch 5:  51%|█████     | 649/1271 [03:29<03:20,  3.10it/s, training_loss=0.618]\u001b[A\n",
            "Epoch 5:  51%|█████     | 650/1271 [03:29<03:19,  3.11it/s, training_loss=0.618]\u001b[A\n",
            "Epoch 5:  51%|█████     | 650/1271 [03:29<03:19,  3.11it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 5:  51%|█████     | 651/1271 [03:29<03:18,  3.13it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 5:  51%|█████     | 651/1271 [03:29<03:18,  3.13it/s, training_loss=0.155]\u001b[A\n",
            "Epoch 5:  51%|█████▏    | 652/1271 [03:29<03:18,  3.11it/s, training_loss=0.155]\u001b[A\n",
            "Epoch 5:  51%|█████▏    | 652/1271 [03:30<03:18,  3.11it/s, training_loss=0.326]\u001b[A\n",
            "Epoch 5:  51%|█████▏    | 653/1271 [03:30<03:18,  3.12it/s, training_loss=0.326]\u001b[A\n",
            "Epoch 5:  51%|█████▏    | 653/1271 [03:30<03:18,  3.12it/s, training_loss=0.205]\u001b[A\n",
            "Epoch 5:  51%|█████▏    | 654/1271 [03:30<03:17,  3.12it/s, training_loss=0.205]\u001b[A\n",
            "Epoch 5:  51%|█████▏    | 654/1271 [03:30<03:17,  3.12it/s, training_loss=0.402]\u001b[A\n",
            "Epoch 5:  52%|█████▏    | 655/1271 [03:30<03:17,  3.12it/s, training_loss=0.402]\u001b[A\n",
            "Epoch 5:  52%|█████▏    | 655/1271 [03:31<03:17,  3.12it/s, training_loss=0.139]\u001b[A\n",
            "Epoch 5:  52%|█████▏    | 656/1271 [03:31<03:17,  3.12it/s, training_loss=0.139]\u001b[A\n",
            "Epoch 5:  52%|█████▏    | 656/1271 [03:31<03:17,  3.12it/s, training_loss=0.068]\u001b[A\n",
            "Epoch 5:  52%|█████▏    | 657/1271 [03:31<03:17,  3.11it/s, training_loss=0.068]\u001b[A\n",
            "Epoch 5:  52%|█████▏    | 657/1271 [03:31<03:17,  3.11it/s, training_loss=0.471]\u001b[A\n",
            "Epoch 5:  52%|█████▏    | 658/1271 [03:31<03:16,  3.12it/s, training_loss=0.471]\u001b[A\n",
            "Epoch 5:  52%|█████▏    | 658/1271 [03:32<03:16,  3.12it/s, training_loss=0.086]\u001b[A\n",
            "Epoch 5:  52%|█████▏    | 659/1271 [03:32<03:15,  3.13it/s, training_loss=0.086]\u001b[A\n",
            "Epoch 5:  52%|█████▏    | 659/1271 [03:32<03:15,  3.13it/s, training_loss=0.065]\u001b[A\n",
            "Epoch 5:  52%|█████▏    | 660/1271 [03:32<03:14,  3.14it/s, training_loss=0.065]\u001b[A\n",
            "Epoch 5:  52%|█████▏    | 660/1271 [03:32<03:14,  3.14it/s, training_loss=0.216]\u001b[A\n",
            "Epoch 5:  52%|█████▏    | 661/1271 [03:32<03:14,  3.14it/s, training_loss=0.216]\u001b[A\n",
            "Epoch 5:  52%|█████▏    | 661/1271 [03:32<03:14,  3.14it/s, training_loss=0.081]\u001b[A\n",
            "Epoch 5:  52%|█████▏    | 662/1271 [03:32<03:14,  3.13it/s, training_loss=0.081]\u001b[A\n",
            "Epoch 5:  52%|█████▏    | 662/1271 [03:33<03:14,  3.13it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 5:  52%|█████▏    | 663/1271 [03:33<03:14,  3.12it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 5:  52%|█████▏    | 663/1271 [03:33<03:14,  3.12it/s, training_loss=0.120]\u001b[A\n",
            "Epoch 5:  52%|█████▏    | 664/1271 [03:33<03:14,  3.12it/s, training_loss=0.120]\u001b[A\n",
            "Epoch 5:  52%|█████▏    | 664/1271 [03:33<03:14,  3.12it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 5:  52%|█████▏    | 665/1271 [03:33<03:13,  3.13it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 5:  52%|█████▏    | 665/1271 [03:34<03:13,  3.13it/s, training_loss=0.337]\u001b[A\n",
            "Epoch 5:  52%|█████▏    | 666/1271 [03:34<03:13,  3.13it/s, training_loss=0.337]\u001b[A\n",
            "Epoch 5:  52%|█████▏    | 666/1271 [03:34<03:13,  3.13it/s, training_loss=0.388]\u001b[A\n",
            "Epoch 5:  52%|█████▏    | 667/1271 [03:34<03:13,  3.12it/s, training_loss=0.388]\u001b[A\n",
            "Epoch 5:  52%|█████▏    | 667/1271 [03:34<03:13,  3.12it/s, training_loss=0.458]\u001b[A\n",
            "Epoch 5:  53%|█████▎    | 668/1271 [03:34<03:13,  3.12it/s, training_loss=0.458]\u001b[A\n",
            "Epoch 5:  53%|█████▎    | 668/1271 [03:35<03:13,  3.12it/s, training_loss=0.033]\u001b[A\n",
            "Epoch 5:  53%|█████▎    | 669/1271 [03:35<03:12,  3.13it/s, training_loss=0.033]\u001b[A\n",
            "Epoch 5:  53%|█████▎    | 669/1271 [03:35<03:12,  3.13it/s, training_loss=0.250]\u001b[A\n",
            "Epoch 5:  53%|█████▎    | 670/1271 [03:35<03:12,  3.11it/s, training_loss=0.250]\u001b[A\n",
            "Epoch 5:  53%|█████▎    | 670/1271 [03:35<03:12,  3.11it/s, training_loss=0.055]\u001b[A\n",
            "Epoch 5:  53%|█████▎    | 671/1271 [03:35<03:12,  3.11it/s, training_loss=0.055]\u001b[A\n",
            "Epoch 5:  53%|█████▎    | 671/1271 [03:36<03:12,  3.11it/s, training_loss=0.261]\u001b[A\n",
            "Epoch 5:  53%|█████▎    | 672/1271 [03:36<03:11,  3.12it/s, training_loss=0.261]\u001b[A\n",
            "Epoch 5:  53%|█████▎    | 672/1271 [03:36<03:11,  3.12it/s, training_loss=0.021]\u001b[A\n",
            "Epoch 5:  53%|█████▎    | 673/1271 [03:36<03:11,  3.11it/s, training_loss=0.021]\u001b[A\n",
            "Epoch 5:  53%|█████▎    | 673/1271 [03:36<03:11,  3.11it/s, training_loss=0.169]\u001b[A\n",
            "Epoch 5:  53%|█████▎    | 674/1271 [03:36<03:11,  3.12it/s, training_loss=0.169]\u001b[A\n",
            "Epoch 5:  53%|█████▎    | 674/1271 [03:37<03:11,  3.12it/s, training_loss=0.046]\u001b[A\n",
            "Epoch 5:  53%|█████▎    | 675/1271 [03:37<03:11,  3.11it/s, training_loss=0.046]\u001b[A\n",
            "Epoch 5:  53%|█████▎    | 675/1271 [03:37<03:11,  3.11it/s, training_loss=0.760]\u001b[A\n",
            "Epoch 5:  53%|█████▎    | 676/1271 [03:37<03:11,  3.10it/s, training_loss=0.760]\u001b[A\n",
            "Epoch 5:  53%|█████▎    | 676/1271 [03:37<03:11,  3.10it/s, training_loss=0.052]\u001b[A\n",
            "Epoch 5:  53%|█████▎    | 677/1271 [03:37<03:11,  3.10it/s, training_loss=0.052]\u001b[A\n",
            "Epoch 5:  53%|█████▎    | 677/1271 [03:38<03:11,  3.10it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 5:  53%|█████▎    | 678/1271 [03:38<03:10,  3.11it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 5:  53%|█████▎    | 678/1271 [03:38<03:10,  3.11it/s, training_loss=0.020]\u001b[A\n",
            "Epoch 5:  53%|█████▎    | 679/1271 [03:38<03:10,  3.12it/s, training_loss=0.020]\u001b[A\n",
            "Epoch 5:  53%|█████▎    | 679/1271 [03:38<03:10,  3.12it/s, training_loss=0.043]\u001b[A\n",
            "Epoch 5:  54%|█████▎    | 680/1271 [03:38<03:09,  3.12it/s, training_loss=0.043]\u001b[A\n",
            "Epoch 5:  54%|█████▎    | 680/1271 [03:39<03:09,  3.12it/s, training_loss=0.039]\u001b[A\n",
            "Epoch 5:  54%|█████▎    | 681/1271 [03:39<03:08,  3.13it/s, training_loss=0.039]\u001b[A\n",
            "Epoch 5:  54%|█████▎    | 681/1271 [03:39<03:08,  3.13it/s, training_loss=0.209]\u001b[A\n",
            "Epoch 5:  54%|█████▎    | 682/1271 [03:39<03:07,  3.14it/s, training_loss=0.209]\u001b[A\n",
            "Epoch 5:  54%|█████▎    | 682/1271 [03:39<03:07,  3.14it/s, training_loss=0.320]\u001b[A\n",
            "Epoch 5:  54%|█████▎    | 683/1271 [03:39<03:07,  3.13it/s, training_loss=0.320]\u001b[A\n",
            "Epoch 5:  54%|█████▎    | 683/1271 [03:40<03:07,  3.13it/s, training_loss=0.253]\u001b[A\n",
            "Epoch 5:  54%|█████▍    | 684/1271 [03:40<03:06,  3.14it/s, training_loss=0.253]\u001b[A\n",
            "Epoch 5:  54%|█████▍    | 684/1271 [03:40<03:06,  3.14it/s, training_loss=0.305]\u001b[A\n",
            "Epoch 5:  54%|█████▍    | 685/1271 [03:40<03:06,  3.13it/s, training_loss=0.305]\u001b[A\n",
            "Epoch 5:  54%|█████▍    | 685/1271 [03:40<03:06,  3.13it/s, training_loss=0.074]\u001b[A\n",
            "Epoch 5:  54%|█████▍    | 686/1271 [03:40<03:07,  3.12it/s, training_loss=0.074]\u001b[A\n",
            "Epoch 5:  54%|█████▍    | 686/1271 [03:40<03:07,  3.12it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 5:  54%|█████▍    | 687/1271 [03:40<03:06,  3.13it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 5:  54%|█████▍    | 687/1271 [03:41<03:06,  3.13it/s, training_loss=0.061]\u001b[A\n",
            "Epoch 5:  54%|█████▍    | 688/1271 [03:41<03:06,  3.13it/s, training_loss=0.061]\u001b[A\n",
            "Epoch 5:  54%|█████▍    | 688/1271 [03:41<03:06,  3.13it/s, training_loss=0.317]\u001b[A\n",
            "Epoch 5:  54%|█████▍    | 689/1271 [03:41<03:06,  3.13it/s, training_loss=0.317]\u001b[A\n",
            "Epoch 5:  54%|█████▍    | 689/1271 [03:41<03:06,  3.13it/s, training_loss=0.086]\u001b[A\n",
            "Epoch 5:  54%|█████▍    | 690/1271 [03:41<03:05,  3.13it/s, training_loss=0.086]\u001b[A\n",
            "Epoch 5:  54%|█████▍    | 690/1271 [03:42<03:05,  3.13it/s, training_loss=0.511]\u001b[A\n",
            "Epoch 5:  54%|█████▍    | 691/1271 [03:42<03:06,  3.11it/s, training_loss=0.511]\u001b[A\n",
            "Epoch 5:  54%|█████▍    | 691/1271 [03:42<03:06,  3.11it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 5:  54%|█████▍    | 692/1271 [03:42<03:06,  3.10it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 5:  54%|█████▍    | 692/1271 [03:42<03:06,  3.10it/s, training_loss=0.136]\u001b[A\n",
            "Epoch 5:  55%|█████▍    | 693/1271 [03:42<03:07,  3.09it/s, training_loss=0.136]\u001b[A\n",
            "Epoch 5:  55%|█████▍    | 693/1271 [03:43<03:07,  3.09it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 5:  55%|█████▍    | 694/1271 [03:43<03:06,  3.10it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 5:  55%|█████▍    | 694/1271 [03:43<03:06,  3.10it/s, training_loss=0.048]\u001b[A\n",
            "Epoch 5:  55%|█████▍    | 695/1271 [03:43<03:05,  3.10it/s, training_loss=0.048]\u001b[A\n",
            "Epoch 5:  55%|█████▍    | 695/1271 [03:43<03:05,  3.10it/s, training_loss=0.781]\u001b[A\n",
            "Epoch 5:  55%|█████▍    | 696/1271 [03:43<03:04,  3.11it/s, training_loss=0.781]\u001b[A\n",
            "Epoch 5:  55%|█████▍    | 696/1271 [03:44<03:04,  3.11it/s, training_loss=0.232]\u001b[A\n",
            "Epoch 5:  55%|█████▍    | 697/1271 [03:44<03:02,  3.14it/s, training_loss=0.232]\u001b[A\n",
            "Epoch 5:  55%|█████▍    | 697/1271 [03:44<03:02,  3.14it/s, training_loss=0.278]\u001b[A\n",
            "Epoch 5:  55%|█████▍    | 698/1271 [03:44<03:03,  3.13it/s, training_loss=0.278]\u001b[A\n",
            "Epoch 5:  55%|█████▍    | 698/1271 [03:44<03:03,  3.13it/s, training_loss=0.038]\u001b[A\n",
            "Epoch 5:  55%|█████▍    | 699/1271 [03:44<03:02,  3.13it/s, training_loss=0.038]\u001b[A\n",
            "Epoch 5:  55%|█████▍    | 699/1271 [03:45<03:02,  3.13it/s, training_loss=0.388]\u001b[A\n",
            "Epoch 5:  55%|█████▌    | 700/1271 [03:45<03:02,  3.12it/s, training_loss=0.388]\u001b[A\n",
            "Epoch 5:  55%|█████▌    | 700/1271 [03:45<03:02,  3.12it/s, training_loss=0.350]\u001b[A\n",
            "Epoch 5:  55%|█████▌    | 701/1271 [03:45<03:03,  3.11it/s, training_loss=0.350]\u001b[A\n",
            "Epoch 5:  55%|█████▌    | 701/1271 [03:45<03:03,  3.11it/s, training_loss=0.087]\u001b[A\n",
            "Epoch 5:  55%|█████▌    | 702/1271 [03:45<03:04,  3.09it/s, training_loss=0.087]\u001b[A\n",
            "Epoch 5:  55%|█████▌    | 702/1271 [03:46<03:04,  3.09it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 5:  55%|█████▌    | 703/1271 [03:46<03:03,  3.09it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 5:  55%|█████▌    | 703/1271 [03:46<03:03,  3.09it/s, training_loss=0.450]\u001b[A\n",
            "Epoch 5:  55%|█████▌    | 704/1271 [03:46<03:02,  3.10it/s, training_loss=0.450]\u001b[A\n",
            "Epoch 5:  55%|█████▌    | 704/1271 [03:46<03:02,  3.10it/s, training_loss=0.247]\u001b[A\n",
            "Epoch 5:  55%|█████▌    | 705/1271 [03:46<03:02,  3.11it/s, training_loss=0.247]\u001b[A\n",
            "Epoch 5:  55%|█████▌    | 705/1271 [03:47<03:02,  3.11it/s, training_loss=0.736]\u001b[A\n",
            "Epoch 5:  56%|█████▌    | 706/1271 [03:47<03:01,  3.12it/s, training_loss=0.736]\u001b[A\n",
            "Epoch 5:  56%|█████▌    | 706/1271 [03:47<03:01,  3.12it/s, training_loss=0.085]\u001b[A\n",
            "Epoch 5:  56%|█████▌    | 707/1271 [03:47<03:00,  3.13it/s, training_loss=0.085]\u001b[A\n",
            "Epoch 5:  56%|█████▌    | 707/1271 [03:47<03:00,  3.13it/s, training_loss=0.114]\u001b[A\n",
            "Epoch 5:  56%|█████▌    | 708/1271 [03:47<03:00,  3.12it/s, training_loss=0.114]\u001b[A\n",
            "Epoch 5:  56%|█████▌    | 708/1271 [03:48<03:00,  3.12it/s, training_loss=0.315]\u001b[A\n",
            "Epoch 5:  56%|█████▌    | 709/1271 [03:48<03:00,  3.11it/s, training_loss=0.315]\u001b[A\n",
            "Epoch 5:  56%|█████▌    | 709/1271 [03:48<03:00,  3.11it/s, training_loss=0.020]\u001b[A\n",
            "Epoch 5:  56%|█████▌    | 710/1271 [03:48<02:59,  3.13it/s, training_loss=0.020]\u001b[A\n",
            "Epoch 5:  56%|█████▌    | 710/1271 [03:48<02:59,  3.13it/s, training_loss=0.708]\u001b[A\n",
            "Epoch 5:  56%|█████▌    | 711/1271 [03:48<02:58,  3.13it/s, training_loss=0.708]\u001b[A\n",
            "Epoch 5:  56%|█████▌    | 711/1271 [03:49<02:58,  3.13it/s, training_loss=0.092]\u001b[A\n",
            "Epoch 5:  56%|█████▌    | 712/1271 [03:49<02:58,  3.13it/s, training_loss=0.092]\u001b[A\n",
            "Epoch 5:  56%|█████▌    | 712/1271 [03:49<02:58,  3.13it/s, training_loss=0.576]\u001b[A\n",
            "Epoch 5:  56%|█████▌    | 713/1271 [03:49<02:58,  3.13it/s, training_loss=0.576]\u001b[A\n",
            "Epoch 5:  56%|█████▌    | 713/1271 [03:49<02:58,  3.13it/s, training_loss=0.332]\u001b[A\n",
            "Epoch 5:  56%|█████▌    | 714/1271 [03:49<02:59,  3.11it/s, training_loss=0.332]\u001b[A\n",
            "Epoch 5:  56%|█████▌    | 714/1271 [03:49<02:59,  3.11it/s, training_loss=0.011]\u001b[A\n",
            "Epoch 5:  56%|█████▋    | 715/1271 [03:49<02:58,  3.11it/s, training_loss=0.011]\u001b[A\n",
            "Epoch 5:  56%|█████▋    | 715/1271 [03:50<02:58,  3.11it/s, training_loss=0.007]\u001b[A\n",
            "Epoch 5:  56%|█████▋    | 716/1271 [03:50<02:58,  3.11it/s, training_loss=0.007]\u001b[A\n",
            "Epoch 5:  56%|█████▋    | 716/1271 [03:50<02:58,  3.11it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 5:  56%|█████▋    | 717/1271 [03:50<02:58,  3.11it/s, training_loss=0.009]\u001b[A\n",
            "Epoch 5:  56%|█████▋    | 717/1271 [03:50<02:58,  3.11it/s, training_loss=0.406]\u001b[A\n",
            "Epoch 5:  56%|█████▋    | 718/1271 [03:50<02:56,  3.12it/s, training_loss=0.406]\u001b[A\n",
            "Epoch 5:  56%|█████▋    | 718/1271 [03:51<02:56,  3.12it/s, training_loss=0.215]\u001b[A\n",
            "Epoch 5:  57%|█████▋    | 719/1271 [03:51<02:57,  3.11it/s, training_loss=0.215]\u001b[A\n",
            "Epoch 5:  57%|█████▋    | 719/1271 [03:51<02:57,  3.11it/s, training_loss=0.271]\u001b[A\n",
            "Epoch 5:  57%|█████▋    | 720/1271 [03:51<02:56,  3.12it/s, training_loss=0.271]\u001b[A\n",
            "Epoch 5:  57%|█████▋    | 720/1271 [03:51<02:56,  3.12it/s, training_loss=0.104]\u001b[A\n",
            "Epoch 5:  57%|█████▋    | 721/1271 [03:51<02:56,  3.12it/s, training_loss=0.104]\u001b[A\n",
            "Epoch 5:  57%|█████▋    | 721/1271 [03:52<02:56,  3.12it/s, training_loss=0.275]\u001b[A\n",
            "Epoch 5:  57%|█████▋    | 722/1271 [03:52<02:55,  3.13it/s, training_loss=0.275]\u001b[A\n",
            "Epoch 5:  57%|█████▋    | 722/1271 [03:52<02:55,  3.13it/s, training_loss=0.062]\u001b[A\n",
            "Epoch 5:  57%|█████▋    | 723/1271 [03:52<02:55,  3.12it/s, training_loss=0.062]\u001b[A\n",
            "Epoch 5:  57%|█████▋    | 723/1271 [03:52<02:55,  3.12it/s, training_loss=0.141]\u001b[A\n",
            "Epoch 5:  57%|█████▋    | 724/1271 [03:52<02:55,  3.12it/s, training_loss=0.141]\u001b[A\n",
            "Epoch 5:  57%|█████▋    | 724/1271 [03:53<02:55,  3.12it/s, training_loss=0.069]\u001b[A\n",
            "Epoch 5:  57%|█████▋    | 725/1271 [03:53<02:55,  3.12it/s, training_loss=0.069]\u001b[A\n",
            "Epoch 5:  57%|█████▋    | 725/1271 [03:53<02:55,  3.12it/s, training_loss=0.355]\u001b[A\n",
            "Epoch 5:  57%|█████▋    | 726/1271 [03:53<02:54,  3.12it/s, training_loss=0.355]\u001b[A\n",
            "Epoch 5:  57%|█████▋    | 726/1271 [03:53<02:54,  3.12it/s, training_loss=0.042]\u001b[A\n",
            "Epoch 5:  57%|█████▋    | 727/1271 [03:53<02:53,  3.13it/s, training_loss=0.042]\u001b[A\n",
            "Epoch 5:  57%|█████▋    | 727/1271 [03:54<02:53,  3.13it/s, training_loss=0.611]\u001b[A\n",
            "Epoch 5:  57%|█████▋    | 728/1271 [03:54<02:53,  3.12it/s, training_loss=0.611]\u001b[A\n",
            "Epoch 5:  57%|█████▋    | 728/1271 [03:54<02:53,  3.12it/s, training_loss=0.088]\u001b[A\n",
            "Epoch 5:  57%|█████▋    | 729/1271 [03:54<02:54,  3.11it/s, training_loss=0.088]\u001b[A\n",
            "Epoch 5:  57%|█████▋    | 729/1271 [03:54<02:54,  3.11it/s, training_loss=0.295]\u001b[A\n",
            "Epoch 5:  57%|█████▋    | 730/1271 [03:54<02:53,  3.11it/s, training_loss=0.295]\u001b[A\n",
            "Epoch 5:  57%|█████▋    | 730/1271 [03:55<02:53,  3.11it/s, training_loss=0.120]\u001b[A\n",
            "Epoch 5:  58%|█████▊    | 731/1271 [03:55<02:53,  3.11it/s, training_loss=0.120]\u001b[A\n",
            "Epoch 5:  58%|█████▊    | 731/1271 [03:55<02:53,  3.11it/s, training_loss=0.169]\u001b[A\n",
            "Epoch 5:  58%|█████▊    | 732/1271 [03:55<02:53,  3.11it/s, training_loss=0.169]\u001b[A\n",
            "Epoch 5:  58%|█████▊    | 732/1271 [03:55<02:53,  3.11it/s, training_loss=0.019]\u001b[A\n",
            "Epoch 5:  58%|█████▊    | 733/1271 [03:55<02:53,  3.09it/s, training_loss=0.019]\u001b[A\n",
            "Epoch 5:  58%|█████▊    | 733/1271 [03:56<02:53,  3.09it/s, training_loss=0.289]\u001b[A\n",
            "Epoch 5:  58%|█████▊    | 734/1271 [03:56<02:53,  3.09it/s, training_loss=0.289]\u001b[A\n",
            "Epoch 5:  58%|█████▊    | 734/1271 [03:56<02:53,  3.09it/s, training_loss=0.131]\u001b[A\n",
            "Epoch 5:  58%|█████▊    | 735/1271 [03:56<02:54,  3.08it/s, training_loss=0.131]\u001b[A\n",
            "Epoch 5:  58%|█████▊    | 735/1271 [03:56<02:54,  3.08it/s, training_loss=0.535]\u001b[A\n",
            "Epoch 5:  58%|█████▊    | 736/1271 [03:56<02:53,  3.09it/s, training_loss=0.535]\u001b[A\n",
            "Epoch 5:  58%|█████▊    | 736/1271 [03:57<02:53,  3.09it/s, training_loss=0.010]\u001b[A\n",
            "Epoch 5:  58%|█████▊    | 737/1271 [03:57<02:52,  3.09it/s, training_loss=0.010]\u001b[A\n",
            "Epoch 5:  58%|█████▊    | 737/1271 [03:57<02:52,  3.09it/s, training_loss=0.052]\u001b[A\n",
            "Epoch 5:  58%|█████▊    | 738/1271 [03:57<02:52,  3.09it/s, training_loss=0.052]\u001b[A\n",
            "Epoch 5:  58%|█████▊    | 738/1271 [03:57<02:52,  3.09it/s, training_loss=0.279]\u001b[A\n",
            "Epoch 5:  58%|█████▊    | 739/1271 [03:57<02:52,  3.09it/s, training_loss=0.279]\u001b[A\n",
            "Epoch 5:  58%|█████▊    | 739/1271 [03:58<02:52,  3.09it/s, training_loss=0.175]\u001b[A\n",
            "Epoch 5:  58%|█████▊    | 740/1271 [03:58<02:51,  3.09it/s, training_loss=0.175]\u001b[A\n",
            "Epoch 5:  58%|█████▊    | 740/1271 [03:58<02:51,  3.09it/s, training_loss=0.292]\u001b[A\n",
            "Epoch 5:  58%|█████▊    | 741/1271 [03:58<02:51,  3.09it/s, training_loss=0.292]\u001b[A\n",
            "Epoch 5:  58%|█████▊    | 741/1271 [03:58<02:51,  3.09it/s, training_loss=0.145]\u001b[A\n",
            "Epoch 5:  58%|█████▊    | 742/1271 [03:58<02:50,  3.10it/s, training_loss=0.145]\u001b[A\n",
            "Epoch 5:  58%|█████▊    | 742/1271 [03:58<02:50,  3.10it/s, training_loss=0.397]\u001b[A\n",
            "Epoch 5:  58%|█████▊    | 743/1271 [03:58<02:49,  3.11it/s, training_loss=0.397]\u001b[A\n",
            "Epoch 5:  58%|█████▊    | 743/1271 [03:59<02:49,  3.11it/s, training_loss=0.151]\u001b[A\n",
            "Epoch 5:  59%|█████▊    | 744/1271 [03:59<02:49,  3.11it/s, training_loss=0.151]\u001b[A\n",
            "Epoch 5:  59%|█████▊    | 744/1271 [03:59<02:49,  3.11it/s, training_loss=0.109]\u001b[A\n",
            "Epoch 5:  59%|█████▊    | 745/1271 [03:59<02:48,  3.12it/s, training_loss=0.109]\u001b[A\n",
            "Epoch 5:  59%|█████▊    | 745/1271 [03:59<02:48,  3.12it/s, training_loss=0.021]\u001b[A\n",
            "Epoch 5:  59%|█████▊    | 746/1271 [03:59<02:48,  3.12it/s, training_loss=0.021]\u001b[A\n",
            "Epoch 5:  59%|█████▊    | 746/1271 [04:00<02:48,  3.12it/s, training_loss=0.212]\u001b[A\n",
            "Epoch 5:  59%|█████▉    | 747/1271 [04:00<02:49,  3.09it/s, training_loss=0.212]\u001b[A\n",
            "Epoch 5:  59%|█████▉    | 747/1271 [04:00<02:49,  3.09it/s, training_loss=0.040]\u001b[A\n",
            "Epoch 5:  59%|█████▉    | 748/1271 [04:00<02:49,  3.09it/s, training_loss=0.040]\u001b[A\n",
            "Epoch 5:  59%|█████▉    | 748/1271 [04:00<02:49,  3.09it/s, training_loss=0.128]\u001b[A\n",
            "Epoch 5:  59%|█████▉    | 749/1271 [04:00<02:48,  3.10it/s, training_loss=0.128]\u001b[A\n",
            "Epoch 5:  59%|█████▉    | 749/1271 [04:01<02:48,  3.10it/s, training_loss=0.452]\u001b[A\n",
            "Epoch 5:  59%|█████▉    | 750/1271 [04:01<02:48,  3.09it/s, training_loss=0.452]\u001b[A\n",
            "Epoch 5:  59%|█████▉    | 750/1271 [04:01<02:48,  3.09it/s, training_loss=0.084]\u001b[A\n",
            "Epoch 5:  59%|█████▉    | 751/1271 [04:01<02:47,  3.10it/s, training_loss=0.084]\u001b[A\n",
            "Epoch 5:  59%|█████▉    | 751/1271 [04:01<02:47,  3.10it/s, training_loss=0.144]\u001b[A\n",
            "Epoch 5:  59%|█████▉    | 752/1271 [04:01<02:46,  3.11it/s, training_loss=0.144]\u001b[A\n",
            "Epoch 5:  59%|█████▉    | 752/1271 [04:02<02:46,  3.11it/s, training_loss=0.380]\u001b[A\n",
            "Epoch 5:  59%|█████▉    | 753/1271 [04:02<02:46,  3.12it/s, training_loss=0.380]\u001b[A\n",
            "Epoch 5:  59%|█████▉    | 753/1271 [04:02<02:46,  3.12it/s, training_loss=0.404]\u001b[A\n",
            "Epoch 5:  59%|█████▉    | 754/1271 [04:02<02:46,  3.11it/s, training_loss=0.404]\u001b[A\n",
            "Epoch 5:  59%|█████▉    | 754/1271 [04:02<02:46,  3.11it/s, training_loss=0.123]\u001b[A\n",
            "Epoch 5:  59%|█████▉    | 755/1271 [04:02<02:45,  3.11it/s, training_loss=0.123]\u001b[A\n",
            "Epoch 5:  59%|█████▉    | 755/1271 [04:03<02:45,  3.11it/s, training_loss=0.249]\u001b[A\n",
            "Epoch 5:  59%|█████▉    | 756/1271 [04:03<02:45,  3.12it/s, training_loss=0.249]\u001b[A\n",
            "Epoch 5:  59%|█████▉    | 756/1271 [04:03<02:45,  3.12it/s, training_loss=0.010]\u001b[A\n",
            "Epoch 5:  60%|█████▉    | 757/1271 [04:03<02:44,  3.13it/s, training_loss=0.010]\u001b[A\n",
            "Epoch 5:  60%|█████▉    | 757/1271 [04:03<02:44,  3.13it/s, training_loss=0.140]\u001b[A\n",
            "Epoch 5:  60%|█████▉    | 758/1271 [04:03<02:43,  3.13it/s, training_loss=0.140]\u001b[A\n",
            "Epoch 5:  60%|█████▉    | 758/1271 [04:04<02:43,  3.13it/s, training_loss=0.109]\u001b[A\n",
            "Epoch 5:  60%|█████▉    | 759/1271 [04:04<02:42,  3.14it/s, training_loss=0.109]\u001b[A\n",
            "Epoch 5:  60%|█████▉    | 759/1271 [04:04<02:42,  3.14it/s, training_loss=0.498]\u001b[A\n",
            "Epoch 5:  60%|█████▉    | 760/1271 [04:04<02:43,  3.13it/s, training_loss=0.498]\u001b[A\n",
            "Epoch 5:  60%|█████▉    | 760/1271 [04:04<02:43,  3.13it/s, training_loss=0.030]\u001b[A\n",
            "Epoch 5:  60%|█████▉    | 761/1271 [04:04<02:43,  3.13it/s, training_loss=0.030]\u001b[A\n",
            "Epoch 5:  60%|█████▉    | 761/1271 [04:05<02:43,  3.13it/s, training_loss=0.223]\u001b[A\n",
            "Epoch 5:  60%|█████▉    | 762/1271 [04:05<02:42,  3.14it/s, training_loss=0.223]\u001b[A\n",
            "Epoch 5:  60%|█████▉    | 762/1271 [04:05<02:42,  3.14it/s, training_loss=0.044]\u001b[A\n",
            "Epoch 5:  60%|██████    | 763/1271 [04:05<02:42,  3.13it/s, training_loss=0.044]\u001b[A\n",
            "Epoch 5:  60%|██████    | 763/1271 [04:05<02:42,  3.13it/s, training_loss=0.121]\u001b[A\n",
            "Epoch 5:  60%|██████    | 764/1271 [04:05<02:42,  3.13it/s, training_loss=0.121]\u001b[A\n",
            "Epoch 5:  60%|██████    | 764/1271 [04:06<02:42,  3.13it/s, training_loss=0.423]\u001b[A\n",
            "Epoch 5:  60%|██████    | 765/1271 [04:06<02:43,  3.10it/s, training_loss=0.423]\u001b[A\n",
            "Epoch 5:  60%|██████    | 765/1271 [04:06<02:43,  3.10it/s, training_loss=0.145]\u001b[A\n",
            "Epoch 5:  60%|██████    | 766/1271 [04:06<02:43,  3.09it/s, training_loss=0.145]\u001b[A\n",
            "Epoch 5:  60%|██████    | 766/1271 [04:06<02:43,  3.09it/s, training_loss=0.765]\u001b[A\n",
            "Epoch 5:  60%|██████    | 767/1271 [04:06<02:43,  3.09it/s, training_loss=0.765]\u001b[A\n",
            "Epoch 5:  60%|██████    | 767/1271 [04:07<02:43,  3.09it/s, training_loss=0.133]\u001b[A\n",
            "Epoch 5:  60%|██████    | 768/1271 [04:07<02:41,  3.11it/s, training_loss=0.133]\u001b[A\n",
            "Epoch 5:  60%|██████    | 768/1271 [04:07<02:41,  3.11it/s, training_loss=0.108]\u001b[A\n",
            "Epoch 5:  61%|██████    | 769/1271 [04:07<02:41,  3.12it/s, training_loss=0.108]\u001b[A\n",
            "Epoch 5:  61%|██████    | 769/1271 [04:07<02:41,  3.12it/s, training_loss=0.134]\u001b[A\n",
            "Epoch 5:  61%|██████    | 770/1271 [04:07<02:41,  3.10it/s, training_loss=0.134]\u001b[A\n",
            "Epoch 5:  61%|██████    | 770/1271 [04:07<02:41,  3.10it/s, training_loss=0.311]\u001b[A\n",
            "Epoch 5:  61%|██████    | 771/1271 [04:07<02:41,  3.10it/s, training_loss=0.311]\u001b[A\n",
            "Epoch 5:  61%|██████    | 771/1271 [04:08<02:41,  3.10it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 5:  61%|██████    | 772/1271 [04:08<02:40,  3.10it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 5:  61%|██████    | 772/1271 [04:08<02:40,  3.10it/s, training_loss=0.423]\u001b[A\n",
            "Epoch 5:  61%|██████    | 773/1271 [04:08<02:40,  3.10it/s, training_loss=0.423]\u001b[A\n",
            "Epoch 5:  61%|██████    | 773/1271 [04:08<02:40,  3.10it/s, training_loss=0.029]\u001b[A\n",
            "Epoch 5:  61%|██████    | 774/1271 [04:08<02:41,  3.09it/s, training_loss=0.029]\u001b[A\n",
            "Epoch 5:  61%|██████    | 774/1271 [04:09<02:41,  3.09it/s, training_loss=0.089]\u001b[A\n",
            "Epoch 5:  61%|██████    | 775/1271 [04:09<02:40,  3.09it/s, training_loss=0.089]\u001b[A\n",
            "Epoch 5:  61%|██████    | 775/1271 [04:09<02:40,  3.09it/s, training_loss=0.313]\u001b[A\n",
            "Epoch 5:  61%|██████    | 776/1271 [04:09<02:39,  3.10it/s, training_loss=0.313]\u001b[A\n",
            "Epoch 5:  61%|██████    | 776/1271 [04:09<02:39,  3.10it/s, training_loss=0.366]\u001b[A\n",
            "Epoch 5:  61%|██████    | 777/1271 [04:09<02:40,  3.07it/s, training_loss=0.366]\u001b[A\n",
            "Epoch 5:  61%|██████    | 777/1271 [04:10<02:40,  3.07it/s, training_loss=0.259]\u001b[A\n",
            "Epoch 5:  61%|██████    | 778/1271 [04:10<02:40,  3.07it/s, training_loss=0.259]\u001b[A\n",
            "Epoch 5:  61%|██████    | 778/1271 [04:10<02:40,  3.07it/s, training_loss=0.292]\u001b[A\n",
            "Epoch 5:  61%|██████▏   | 779/1271 [04:10<02:39,  3.09it/s, training_loss=0.292]\u001b[A\n",
            "Epoch 5:  61%|██████▏   | 779/1271 [04:10<02:39,  3.09it/s, training_loss=0.204]\u001b[A\n",
            "Epoch 5:  61%|██████▏   | 780/1271 [04:10<02:39,  3.09it/s, training_loss=0.204]\u001b[A\n",
            "Epoch 5:  61%|██████▏   | 780/1271 [04:11<02:39,  3.09it/s, training_loss=0.331]\u001b[A\n",
            "Epoch 5:  61%|██████▏   | 781/1271 [04:11<02:38,  3.09it/s, training_loss=0.331]\u001b[A\n",
            "Epoch 5:  61%|██████▏   | 781/1271 [04:11<02:38,  3.09it/s, training_loss=0.001]\u001b[A\n",
            "Epoch 5:  62%|██████▏   | 782/1271 [04:11<02:37,  3.11it/s, training_loss=0.001]\u001b[A\n",
            "Epoch 5:  62%|██████▏   | 782/1271 [04:11<02:37,  3.11it/s, training_loss=0.123]\u001b[A\n",
            "Epoch 5:  62%|██████▏   | 783/1271 [04:11<02:37,  3.11it/s, training_loss=0.123]\u001b[A\n",
            "Epoch 5:  62%|██████▏   | 783/1271 [04:12<02:37,  3.11it/s, training_loss=0.040]\u001b[A\n",
            "Epoch 5:  62%|██████▏   | 784/1271 [04:12<02:36,  3.11it/s, training_loss=0.040]\u001b[A\n",
            "Epoch 5:  62%|██████▏   | 784/1271 [04:12<02:36,  3.11it/s, training_loss=0.104]\u001b[A\n",
            "Epoch 5:  62%|██████▏   | 785/1271 [04:12<02:36,  3.10it/s, training_loss=0.104]\u001b[A\n",
            "Epoch 5:  62%|██████▏   | 785/1271 [04:12<02:36,  3.10it/s, training_loss=0.182]\u001b[A\n",
            "Epoch 5:  62%|██████▏   | 786/1271 [04:12<02:35,  3.11it/s, training_loss=0.182]\u001b[A\n",
            "Epoch 5:  62%|██████▏   | 786/1271 [04:13<02:35,  3.11it/s, training_loss=0.450]\u001b[A\n",
            "Epoch 5:  62%|██████▏   | 787/1271 [04:13<02:36,  3.10it/s, training_loss=0.450]\u001b[A\n",
            "Epoch 5:  62%|██████▏   | 787/1271 [04:13<02:36,  3.10it/s, training_loss=0.209]\u001b[A\n",
            "Epoch 5:  62%|██████▏   | 788/1271 [04:13<02:34,  3.13it/s, training_loss=0.209]\u001b[A\n",
            "Epoch 5:  62%|██████▏   | 788/1271 [04:13<02:34,  3.13it/s, training_loss=0.163]\u001b[A\n",
            "Epoch 5:  62%|██████▏   | 789/1271 [04:13<02:34,  3.12it/s, training_loss=0.163]\u001b[A\n",
            "Epoch 5:  62%|██████▏   | 789/1271 [04:14<02:34,  3.12it/s, training_loss=0.293]\u001b[A\n",
            "Epoch 5:  62%|██████▏   | 790/1271 [04:14<02:34,  3.12it/s, training_loss=0.293]\u001b[A\n",
            "Epoch 5:  62%|██████▏   | 790/1271 [04:14<02:34,  3.12it/s, training_loss=0.136]\u001b[A\n",
            "Epoch 5:  62%|██████▏   | 791/1271 [04:14<02:33,  3.12it/s, training_loss=0.136]\u001b[A\n",
            "Epoch 5:  62%|██████▏   | 791/1271 [04:14<02:33,  3.12it/s, training_loss=0.234]\u001b[A\n",
            "Epoch 5:  62%|██████▏   | 792/1271 [04:14<02:33,  3.13it/s, training_loss=0.234]\u001b[A\n",
            "Epoch 5:  62%|██████▏   | 792/1271 [04:15<02:33,  3.13it/s, training_loss=0.054]\u001b[A\n",
            "Epoch 5:  62%|██████▏   | 793/1271 [04:15<02:33,  3.12it/s, training_loss=0.054]\u001b[A\n",
            "Epoch 5:  62%|██████▏   | 793/1271 [04:15<02:33,  3.12it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 5:  62%|██████▏   | 794/1271 [04:15<02:34,  3.10it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 5:  62%|██████▏   | 794/1271 [04:15<02:34,  3.10it/s, training_loss=0.010]\u001b[A\n",
            "Epoch 5:  63%|██████▎   | 795/1271 [04:15<02:33,  3.10it/s, training_loss=0.010]\u001b[A\n",
            "Epoch 5:  63%|██████▎   | 795/1271 [04:16<02:33,  3.10it/s, training_loss=0.188]\u001b[A\n",
            "Epoch 5:  63%|██████▎   | 796/1271 [04:16<02:33,  3.09it/s, training_loss=0.188]\u001b[A\n",
            "Epoch 5:  63%|██████▎   | 796/1271 [04:16<02:33,  3.09it/s, training_loss=0.385]\u001b[A\n",
            "Epoch 5:  63%|██████▎   | 797/1271 [04:16<02:33,  3.09it/s, training_loss=0.385]\u001b[A\n",
            "Epoch 5:  63%|██████▎   | 797/1271 [04:16<02:33,  3.09it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 5:  63%|██████▎   | 798/1271 [04:16<02:32,  3.10it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 5:  63%|██████▎   | 798/1271 [04:17<02:32,  3.10it/s, training_loss=0.144]\u001b[A\n",
            "Epoch 5:  63%|██████▎   | 799/1271 [04:17<02:32,  3.10it/s, training_loss=0.144]\u001b[A\n",
            "Epoch 5:  63%|██████▎   | 799/1271 [04:17<02:32,  3.10it/s, training_loss=0.208]\u001b[A\n",
            "Epoch 5:  63%|██████▎   | 800/1271 [04:17<02:32,  3.09it/s, training_loss=0.208]\u001b[A\n",
            "Epoch 5:  63%|██████▎   | 800/1271 [04:17<02:32,  3.09it/s, training_loss=0.145]\u001b[A\n",
            "Epoch 5:  63%|██████▎   | 801/1271 [04:17<02:31,  3.10it/s, training_loss=0.145]\u001b[A\n",
            "Epoch 5:  63%|██████▎   | 801/1271 [04:17<02:31,  3.10it/s, training_loss=0.529]\u001b[A\n",
            "Epoch 5:  63%|██████▎   | 802/1271 [04:17<02:30,  3.11it/s, training_loss=0.529]\u001b[A\n",
            "Epoch 5:  63%|██████▎   | 802/1271 [04:18<02:30,  3.11it/s, training_loss=0.076]\u001b[A\n",
            "Epoch 5:  63%|██████▎   | 803/1271 [04:18<02:30,  3.12it/s, training_loss=0.076]\u001b[A\n",
            "Epoch 5:  63%|██████▎   | 803/1271 [04:18<02:30,  3.12it/s, training_loss=0.269]\u001b[A\n",
            "Epoch 5:  63%|██████▎   | 804/1271 [04:18<02:30,  3.11it/s, training_loss=0.269]\u001b[A\n",
            "Epoch 5:  63%|██████▎   | 804/1271 [04:18<02:30,  3.11it/s, training_loss=0.231]\u001b[A\n",
            "Epoch 5:  63%|██████▎   | 805/1271 [04:18<02:29,  3.11it/s, training_loss=0.231]\u001b[A\n",
            "Epoch 5:  63%|██████▎   | 805/1271 [04:19<02:29,  3.11it/s, training_loss=0.158]\u001b[A\n",
            "Epoch 5:  63%|██████▎   | 806/1271 [04:19<02:29,  3.11it/s, training_loss=0.158]\u001b[A\n",
            "Epoch 5:  63%|██████▎   | 806/1271 [04:19<02:29,  3.11it/s, training_loss=0.275]\u001b[A\n",
            "Epoch 5:  63%|██████▎   | 807/1271 [04:19<02:29,  3.11it/s, training_loss=0.275]\u001b[A\n",
            "Epoch 5:  63%|██████▎   | 807/1271 [04:19<02:29,  3.11it/s, training_loss=0.248]\u001b[A\n",
            "Epoch 5:  64%|██████▎   | 808/1271 [04:19<02:28,  3.12it/s, training_loss=0.248]\u001b[A\n",
            "Epoch 5:  64%|██████▎   | 808/1271 [04:20<02:28,  3.12it/s, training_loss=0.078]\u001b[A\n",
            "Epoch 5:  64%|██████▎   | 809/1271 [04:20<02:27,  3.13it/s, training_loss=0.078]\u001b[A\n",
            "Epoch 5:  64%|██████▎   | 809/1271 [04:20<02:27,  3.13it/s, training_loss=0.286]\u001b[A\n",
            "Epoch 5:  64%|██████▎   | 810/1271 [04:20<02:27,  3.13it/s, training_loss=0.286]\u001b[A\n",
            "Epoch 5:  64%|██████▎   | 810/1271 [04:20<02:27,  3.13it/s, training_loss=0.146]\u001b[A\n",
            "Epoch 5:  64%|██████▍   | 811/1271 [04:20<02:26,  3.13it/s, training_loss=0.146]\u001b[A\n",
            "Epoch 5:  64%|██████▍   | 811/1271 [04:21<02:26,  3.13it/s, training_loss=0.245]\u001b[A\n",
            "Epoch 5:  64%|██████▍   | 812/1271 [04:21<02:26,  3.12it/s, training_loss=0.245]\u001b[A\n",
            "Epoch 5:  64%|██████▍   | 812/1271 [04:21<02:26,  3.12it/s, training_loss=0.728]\u001b[A\n",
            "Epoch 5:  64%|██████▍   | 813/1271 [04:21<02:26,  3.13it/s, training_loss=0.728]\u001b[A\n",
            "Epoch 5:  64%|██████▍   | 813/1271 [04:21<02:26,  3.13it/s, training_loss=0.310]\u001b[A\n",
            "Epoch 5:  64%|██████▍   | 814/1271 [04:21<02:26,  3.13it/s, training_loss=0.310]\u001b[A\n",
            "Epoch 5:  64%|██████▍   | 814/1271 [04:22<02:26,  3.13it/s, training_loss=0.087]\u001b[A\n",
            "Epoch 5:  64%|██████▍   | 815/1271 [04:22<02:25,  3.12it/s, training_loss=0.087]\u001b[A\n",
            "Epoch 5:  64%|██████▍   | 815/1271 [04:22<02:25,  3.12it/s, training_loss=0.300]\u001b[A\n",
            "Epoch 5:  64%|██████▍   | 816/1271 [04:22<02:26,  3.11it/s, training_loss=0.300]\u001b[A\n",
            "Epoch 5:  64%|██████▍   | 816/1271 [04:22<02:26,  3.11it/s, training_loss=0.181]\u001b[A\n",
            "Epoch 5:  64%|██████▍   | 817/1271 [04:22<02:26,  3.10it/s, training_loss=0.181]\u001b[A\n",
            "Epoch 5:  64%|██████▍   | 817/1271 [04:23<02:26,  3.10it/s, training_loss=0.275]\u001b[A\n",
            "Epoch 5:  64%|██████▍   | 818/1271 [04:23<02:25,  3.11it/s, training_loss=0.275]\u001b[A\n",
            "Epoch 5:  64%|██████▍   | 818/1271 [04:23<02:25,  3.11it/s, training_loss=0.106]\u001b[A\n",
            "Epoch 5:  64%|██████▍   | 819/1271 [04:23<02:24,  3.12it/s, training_loss=0.106]\u001b[A\n",
            "Epoch 5:  64%|██████▍   | 819/1271 [04:23<02:24,  3.12it/s, training_loss=0.395]\u001b[A\n",
            "Epoch 5:  65%|██████▍   | 820/1271 [04:23<02:24,  3.13it/s, training_loss=0.395]\u001b[A\n",
            "Epoch 5:  65%|██████▍   | 820/1271 [04:24<02:24,  3.13it/s, training_loss=0.047]\u001b[A\n",
            "Epoch 5:  65%|██████▍   | 821/1271 [04:24<02:24,  3.12it/s, training_loss=0.047]\u001b[A\n",
            "Epoch 5:  65%|██████▍   | 821/1271 [04:24<02:24,  3.12it/s, training_loss=0.094]\u001b[A\n",
            "Epoch 5:  65%|██████▍   | 822/1271 [04:24<02:23,  3.12it/s, training_loss=0.094]\u001b[A\n",
            "Epoch 5:  65%|██████▍   | 822/1271 [04:24<02:23,  3.12it/s, training_loss=0.027]\u001b[A\n",
            "Epoch 5:  65%|██████▍   | 823/1271 [04:24<02:24,  3.10it/s, training_loss=0.027]\u001b[A\n",
            "Epoch 5:  65%|██████▍   | 823/1271 [04:25<02:24,  3.10it/s, training_loss=0.200]\u001b[A\n",
            "Epoch 5:  65%|██████▍   | 824/1271 [04:25<02:23,  3.11it/s, training_loss=0.200]\u001b[A\n",
            "Epoch 5:  65%|██████▍   | 824/1271 [04:25<02:23,  3.11it/s, training_loss=0.794]\u001b[A\n",
            "Epoch 5:  65%|██████▍   | 825/1271 [04:25<02:24,  3.10it/s, training_loss=0.794]\u001b[A\n",
            "Epoch 5:  65%|██████▍   | 825/1271 [04:25<02:24,  3.10it/s, training_loss=0.493]\u001b[A\n",
            "Epoch 5:  65%|██████▍   | 826/1271 [04:25<02:24,  3.09it/s, training_loss=0.493]\u001b[A\n",
            "Epoch 5:  65%|██████▍   | 826/1271 [04:26<02:24,  3.09it/s, training_loss=0.014]\u001b[A\n",
            "Epoch 5:  65%|██████▌   | 827/1271 [04:26<02:23,  3.10it/s, training_loss=0.014]\u001b[A\n",
            "Epoch 5:  65%|██████▌   | 827/1271 [04:26<02:23,  3.10it/s, training_loss=0.406]\u001b[A\n",
            "Epoch 5:  65%|██████▌   | 828/1271 [04:26<02:22,  3.10it/s, training_loss=0.406]\u001b[A\n",
            "Epoch 5:  65%|██████▌   | 828/1271 [04:26<02:22,  3.10it/s, training_loss=0.560]\u001b[A\n",
            "Epoch 5:  65%|██████▌   | 829/1271 [04:26<02:22,  3.10it/s, training_loss=0.560]\u001b[A\n",
            "Epoch 5:  65%|██████▌   | 829/1271 [04:26<02:22,  3.10it/s, training_loss=0.129]\u001b[A\n",
            "Epoch 5:  65%|██████▌   | 830/1271 [04:26<02:21,  3.11it/s, training_loss=0.129]\u001b[A\n",
            "Epoch 5:  65%|██████▌   | 830/1271 [04:27<02:21,  3.11it/s, training_loss=0.220]\u001b[A\n",
            "Epoch 5:  65%|██████▌   | 831/1271 [04:27<02:22,  3.09it/s, training_loss=0.220]\u001b[A\n",
            "Epoch 5:  65%|██████▌   | 831/1271 [04:27<02:22,  3.09it/s, training_loss=0.109]\u001b[A\n",
            "Epoch 5:  65%|██████▌   | 832/1271 [04:27<02:21,  3.10it/s, training_loss=0.109]\u001b[A\n",
            "Epoch 5:  65%|██████▌   | 832/1271 [04:27<02:21,  3.10it/s, training_loss=0.288]\u001b[A\n",
            "Epoch 5:  66%|██████▌   | 833/1271 [04:27<02:20,  3.11it/s, training_loss=0.288]\u001b[A\n",
            "Epoch 5:  66%|██████▌   | 833/1271 [04:28<02:20,  3.11it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 5:  66%|██████▌   | 834/1271 [04:28<02:20,  3.11it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 5:  66%|██████▌   | 834/1271 [04:28<02:20,  3.11it/s, training_loss=0.039]\u001b[A\n",
            "Epoch 5:  66%|██████▌   | 835/1271 [04:28<02:20,  3.11it/s, training_loss=0.039]\u001b[A\n",
            "Epoch 5:  66%|██████▌   | 835/1271 [04:28<02:20,  3.11it/s, training_loss=0.483]\u001b[A\n",
            "Epoch 5:  66%|██████▌   | 836/1271 [04:28<02:19,  3.12it/s, training_loss=0.483]\u001b[A\n",
            "Epoch 5:  66%|██████▌   | 836/1271 [04:29<02:19,  3.12it/s, training_loss=0.426]\u001b[A\n",
            "Epoch 5:  66%|██████▌   | 837/1271 [04:29<02:19,  3.10it/s, training_loss=0.426]\u001b[A\n",
            "Epoch 5:  66%|██████▌   | 837/1271 [04:29<02:19,  3.10it/s, training_loss=0.011]\u001b[A\n",
            "Epoch 5:  66%|██████▌   | 838/1271 [04:29<02:19,  3.10it/s, training_loss=0.011]\u001b[A\n",
            "Epoch 5:  66%|██████▌   | 838/1271 [04:29<02:19,  3.10it/s, training_loss=0.447]\u001b[A\n",
            "Epoch 5:  66%|██████▌   | 839/1271 [04:29<02:19,  3.10it/s, training_loss=0.447]\u001b[A\n",
            "Epoch 5:  66%|██████▌   | 839/1271 [04:30<02:19,  3.10it/s, training_loss=0.285]\u001b[A\n",
            "Epoch 5:  66%|██████▌   | 840/1271 [04:30<02:19,  3.10it/s, training_loss=0.285]\u001b[A\n",
            "Epoch 5:  66%|██████▌   | 840/1271 [04:30<02:19,  3.10it/s, training_loss=0.119]\u001b[A\n",
            "Epoch 5:  66%|██████▌   | 841/1271 [04:30<02:19,  3.09it/s, training_loss=0.119]\u001b[A\n",
            "Epoch 5:  66%|██████▌   | 841/1271 [04:30<02:19,  3.09it/s, training_loss=0.273]\u001b[A\n",
            "Epoch 5:  66%|██████▌   | 842/1271 [04:30<02:18,  3.10it/s, training_loss=0.273]\u001b[A\n",
            "Epoch 5:  66%|██████▌   | 842/1271 [04:31<02:18,  3.10it/s, training_loss=0.124]\u001b[A\n",
            "Epoch 5:  66%|██████▋   | 843/1271 [04:31<02:17,  3.11it/s, training_loss=0.124]\u001b[A\n",
            "Epoch 5:  66%|██████▋   | 843/1271 [04:31<02:17,  3.11it/s, training_loss=0.407]\u001b[A\n",
            "Epoch 5:  66%|██████▋   | 844/1271 [04:31<02:17,  3.10it/s, training_loss=0.407]\u001b[A\n",
            "Epoch 5:  66%|██████▋   | 844/1271 [04:31<02:17,  3.10it/s, training_loss=0.426]\u001b[A\n",
            "Epoch 5:  66%|██████▋   | 845/1271 [04:31<02:17,  3.11it/s, training_loss=0.426]\u001b[A\n",
            "Epoch 5:  66%|██████▋   | 845/1271 [04:32<02:17,  3.11it/s, training_loss=0.608]\u001b[A\n",
            "Epoch 5:  67%|██████▋   | 846/1271 [04:32<02:16,  3.11it/s, training_loss=0.608]\u001b[A\n",
            "Epoch 5:  67%|██████▋   | 846/1271 [04:32<02:16,  3.11it/s, training_loss=0.001]\u001b[A\n",
            "Epoch 5:  67%|██████▋   | 847/1271 [04:32<02:15,  3.12it/s, training_loss=0.001]\u001b[A\n",
            "Epoch 5:  67%|██████▋   | 847/1271 [04:32<02:15,  3.12it/s, training_loss=0.278]\u001b[A\n",
            "Epoch 5:  67%|██████▋   | 848/1271 [04:32<02:15,  3.13it/s, training_loss=0.278]\u001b[A\n",
            "Epoch 5:  67%|██████▋   | 848/1271 [04:33<02:15,  3.13it/s, training_loss=0.084]\u001b[A\n",
            "Epoch 5:  67%|██████▋   | 849/1271 [04:33<02:15,  3.12it/s, training_loss=0.084]\u001b[A\n",
            "Epoch 5:  67%|██████▋   | 849/1271 [04:33<02:15,  3.12it/s, training_loss=0.593]\u001b[A\n",
            "Epoch 5:  67%|██████▋   | 850/1271 [04:33<02:15,  3.11it/s, training_loss=0.593]\u001b[A\n",
            "Epoch 5:  67%|██████▋   | 850/1271 [04:33<02:15,  3.11it/s, training_loss=0.367]\u001b[A\n",
            "Epoch 5:  67%|██████▋   | 851/1271 [04:33<02:15,  3.11it/s, training_loss=0.367]\u001b[A\n",
            "Epoch 5:  67%|██████▋   | 851/1271 [04:34<02:15,  3.11it/s, training_loss=0.138]\u001b[A\n",
            "Epoch 5:  67%|██████▋   | 852/1271 [04:34<02:14,  3.12it/s, training_loss=0.138]\u001b[A\n",
            "Epoch 5:  67%|██████▋   | 852/1271 [04:34<02:14,  3.12it/s, training_loss=0.911]\u001b[A\n",
            "Epoch 5:  67%|██████▋   | 853/1271 [04:34<02:14,  3.11it/s, training_loss=0.911]\u001b[A\n",
            "Epoch 5:  67%|██████▋   | 853/1271 [04:34<02:14,  3.11it/s, training_loss=0.263]\u001b[A\n",
            "Epoch 5:  67%|██████▋   | 854/1271 [04:34<02:13,  3.12it/s, training_loss=0.263]\u001b[A\n",
            "Epoch 5:  67%|██████▋   | 854/1271 [04:35<02:13,  3.12it/s, training_loss=0.064]\u001b[A\n",
            "Epoch 5:  67%|██████▋   | 855/1271 [04:35<02:13,  3.13it/s, training_loss=0.064]\u001b[A\n",
            "Epoch 5:  67%|██████▋   | 855/1271 [04:35<02:13,  3.13it/s, training_loss=0.666]\u001b[A\n",
            "Epoch 5:  67%|██████▋   | 856/1271 [04:35<02:12,  3.12it/s, training_loss=0.666]\u001b[A\n",
            "Epoch 5:  67%|██████▋   | 856/1271 [04:35<02:12,  3.12it/s, training_loss=0.315]\u001b[A\n",
            "Epoch 5:  67%|██████▋   | 857/1271 [04:35<02:13,  3.10it/s, training_loss=0.315]\u001b[A\n",
            "Epoch 5:  67%|██████▋   | 857/1271 [04:35<02:13,  3.10it/s, training_loss=0.203]\u001b[A\n",
            "Epoch 5:  68%|██████▊   | 858/1271 [04:35<02:12,  3.12it/s, training_loss=0.203]\u001b[A\n",
            "Epoch 5:  68%|██████▊   | 858/1271 [04:36<02:12,  3.12it/s, training_loss=0.290]\u001b[A\n",
            "Epoch 5:  68%|██████▊   | 859/1271 [04:36<02:12,  3.11it/s, training_loss=0.290]\u001b[A\n",
            "Epoch 5:  68%|██████▊   | 859/1271 [04:36<02:12,  3.11it/s, training_loss=0.187]\u001b[A\n",
            "Epoch 5:  68%|██████▊   | 860/1271 [04:36<02:12,  3.11it/s, training_loss=0.187]\u001b[A\n",
            "Epoch 5:  68%|██████▊   | 860/1271 [04:36<02:12,  3.11it/s, training_loss=0.330]\u001b[A\n",
            "Epoch 5:  68%|██████▊   | 861/1271 [04:36<02:12,  3.10it/s, training_loss=0.330]\u001b[A\n",
            "Epoch 5:  68%|██████▊   | 861/1271 [04:37<02:12,  3.10it/s, training_loss=0.178]\u001b[A\n",
            "Epoch 5:  68%|██████▊   | 862/1271 [04:37<02:11,  3.10it/s, training_loss=0.178]\u001b[A\n",
            "Epoch 5:  68%|██████▊   | 862/1271 [04:37<02:11,  3.10it/s, training_loss=0.083]\u001b[A\n",
            "Epoch 5:  68%|██████▊   | 863/1271 [04:37<02:11,  3.11it/s, training_loss=0.083]\u001b[A\n",
            "Epoch 5:  68%|██████▊   | 863/1271 [04:37<02:11,  3.11it/s, training_loss=1.012]\u001b[A\n",
            "Epoch 5:  68%|██████▊   | 864/1271 [04:37<02:10,  3.11it/s, training_loss=1.012]\u001b[A\n",
            "Epoch 5:  68%|██████▊   | 864/1271 [04:38<02:10,  3.11it/s, training_loss=0.087]\u001b[A\n",
            "Epoch 5:  68%|██████▊   | 865/1271 [04:38<02:10,  3.11it/s, training_loss=0.087]\u001b[A\n",
            "Epoch 5:  68%|██████▊   | 865/1271 [04:38<02:10,  3.11it/s, training_loss=0.610]\u001b[A\n",
            "Epoch 5:  68%|██████▊   | 866/1271 [04:38<02:10,  3.09it/s, training_loss=0.610]\u001b[A\n",
            "Epoch 5:  68%|██████▊   | 866/1271 [04:38<02:10,  3.09it/s, training_loss=0.722]\u001b[A\n",
            "Epoch 5:  68%|██████▊   | 867/1271 [04:38<02:10,  3.10it/s, training_loss=0.722]\u001b[A\n",
            "Epoch 5:  68%|██████▊   | 867/1271 [04:39<02:10,  3.10it/s, training_loss=0.145]\u001b[A\n",
            "Epoch 5:  68%|██████▊   | 868/1271 [04:39<02:10,  3.09it/s, training_loss=0.145]\u001b[A\n",
            "Epoch 5:  68%|██████▊   | 868/1271 [04:39<02:10,  3.09it/s, training_loss=0.149]\u001b[A\n",
            "Epoch 5:  68%|██████▊   | 869/1271 [04:39<02:09,  3.10it/s, training_loss=0.149]\u001b[A\n",
            "Epoch 5:  68%|██████▊   | 869/1271 [04:39<02:09,  3.10it/s, training_loss=0.117]\u001b[A\n",
            "Epoch 5:  68%|██████▊   | 870/1271 [04:39<02:09,  3.11it/s, training_loss=0.117]\u001b[A\n",
            "Epoch 5:  68%|██████▊   | 870/1271 [04:40<02:09,  3.11it/s, training_loss=0.357]\u001b[A\n",
            "Epoch 5:  69%|██████▊   | 871/1271 [04:40<02:08,  3.12it/s, training_loss=0.357]\u001b[A\n",
            "Epoch 5:  69%|██████▊   | 871/1271 [04:40<02:08,  3.12it/s, training_loss=0.085]\u001b[A\n",
            "Epoch 5:  69%|██████▊   | 872/1271 [04:40<02:07,  3.12it/s, training_loss=0.085]\u001b[A\n",
            "Epoch 5:  69%|██████▊   | 872/1271 [04:40<02:07,  3.12it/s, training_loss=0.207]\u001b[A\n",
            "Epoch 5:  69%|██████▊   | 873/1271 [04:40<02:07,  3.12it/s, training_loss=0.207]\u001b[A\n",
            "Epoch 5:  69%|██████▊   | 873/1271 [04:41<02:07,  3.12it/s, training_loss=0.884]\u001b[A\n",
            "Epoch 5:  69%|██████▉   | 874/1271 [04:41<02:07,  3.11it/s, training_loss=0.884]\u001b[A\n",
            "Epoch 5:  69%|██████▉   | 874/1271 [04:41<02:07,  3.11it/s, training_loss=0.475]\u001b[A\n",
            "Epoch 5:  69%|██████▉   | 875/1271 [04:41<02:07,  3.10it/s, training_loss=0.475]\u001b[A\n",
            "Epoch 5:  69%|██████▉   | 875/1271 [04:41<02:07,  3.10it/s, training_loss=0.258]\u001b[A\n",
            "Epoch 5:  69%|██████▉   | 876/1271 [04:41<02:07,  3.10it/s, training_loss=0.258]\u001b[A\n",
            "Epoch 5:  69%|██████▉   | 876/1271 [04:42<02:07,  3.10it/s, training_loss=0.442]\u001b[A\n",
            "Epoch 5:  69%|██████▉   | 877/1271 [04:42<02:06,  3.10it/s, training_loss=0.442]\u001b[A\n",
            "Epoch 5:  69%|██████▉   | 877/1271 [04:42<02:06,  3.10it/s, training_loss=0.333]\u001b[A\n",
            "Epoch 5:  69%|██████▉   | 878/1271 [04:42<02:06,  3.10it/s, training_loss=0.333]\u001b[A\n",
            "Epoch 5:  69%|██████▉   | 878/1271 [04:42<02:06,  3.10it/s, training_loss=0.529]\u001b[A\n",
            "Epoch 5:  69%|██████▉   | 879/1271 [04:42<02:06,  3.09it/s, training_loss=0.529]\u001b[A\n",
            "Epoch 5:  69%|██████▉   | 879/1271 [04:43<02:06,  3.09it/s, training_loss=0.013]\u001b[A\n",
            "Epoch 5:  69%|██████▉   | 880/1271 [04:43<02:05,  3.11it/s, training_loss=0.013]\u001b[A\n",
            "Epoch 5:  69%|██████▉   | 880/1271 [04:43<02:05,  3.11it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 5:  69%|██████▉   | 881/1271 [04:43<02:04,  3.12it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 5:  69%|██████▉   | 881/1271 [04:43<02:04,  3.12it/s, training_loss=0.190]\u001b[A\n",
            "Epoch 5:  69%|██████▉   | 882/1271 [04:43<02:05,  3.11it/s, training_loss=0.190]\u001b[A\n",
            "Epoch 5:  69%|██████▉   | 882/1271 [04:44<02:05,  3.11it/s, training_loss=0.396]\u001b[A\n",
            "Epoch 5:  69%|██████▉   | 883/1271 [04:44<02:04,  3.12it/s, training_loss=0.396]\u001b[A\n",
            "Epoch 5:  69%|██████▉   | 883/1271 [04:44<02:04,  3.12it/s, training_loss=0.417]\u001b[A\n",
            "Epoch 5:  70%|██████▉   | 884/1271 [04:44<02:04,  3.12it/s, training_loss=0.417]\u001b[A\n",
            "Epoch 5:  70%|██████▉   | 884/1271 [04:44<02:04,  3.12it/s, training_loss=0.015]\u001b[A\n",
            "Epoch 5:  70%|██████▉   | 885/1271 [04:44<02:04,  3.09it/s, training_loss=0.015]\u001b[A\n",
            "Epoch 5:  70%|██████▉   | 885/1271 [04:44<02:04,  3.09it/s, training_loss=0.165]\u001b[A\n",
            "Epoch 5:  70%|██████▉   | 886/1271 [04:44<02:04,  3.10it/s, training_loss=0.165]\u001b[A\n",
            "Epoch 5:  70%|██████▉   | 886/1271 [04:45<02:04,  3.10it/s, training_loss=0.231]\u001b[A\n",
            "Epoch 5:  70%|██████▉   | 887/1271 [04:45<02:03,  3.10it/s, training_loss=0.231]\u001b[A\n",
            "Epoch 5:  70%|██████▉   | 887/1271 [04:45<02:03,  3.10it/s, training_loss=0.428]\u001b[A\n",
            "Epoch 5:  70%|██████▉   | 888/1271 [04:45<02:03,  3.11it/s, training_loss=0.428]\u001b[A\n",
            "Epoch 5:  70%|██████▉   | 888/1271 [04:45<02:03,  3.11it/s, training_loss=0.061]\u001b[A\n",
            "Epoch 5:  70%|██████▉   | 889/1271 [04:45<02:02,  3.11it/s, training_loss=0.061]\u001b[A\n",
            "Epoch 5:  70%|██████▉   | 889/1271 [04:46<02:02,  3.11it/s, training_loss=0.483]\u001b[A\n",
            "Epoch 5:  70%|███████   | 890/1271 [04:46<02:02,  3.11it/s, training_loss=0.483]\u001b[A\n",
            "Epoch 5:  70%|███████   | 890/1271 [04:46<02:02,  3.11it/s, training_loss=0.416]\u001b[A\n",
            "Epoch 5:  70%|███████   | 891/1271 [04:46<02:01,  3.12it/s, training_loss=0.416]\u001b[A\n",
            "Epoch 5:  70%|███████   | 891/1271 [04:46<02:01,  3.12it/s, training_loss=0.171]\u001b[A\n",
            "Epoch 5:  70%|███████   | 892/1271 [04:46<02:01,  3.12it/s, training_loss=0.171]\u001b[A\n",
            "Epoch 5:  70%|███████   | 892/1271 [04:47<02:01,  3.12it/s, training_loss=0.315]\u001b[A\n",
            "Epoch 5:  70%|███████   | 893/1271 [04:47<02:00,  3.12it/s, training_loss=0.315]\u001b[A\n",
            "Epoch 5:  70%|███████   | 893/1271 [04:47<02:00,  3.12it/s, training_loss=0.424]\u001b[A\n",
            "Epoch 5:  70%|███████   | 894/1271 [04:47<02:00,  3.12it/s, training_loss=0.424]\u001b[A\n",
            "Epoch 5:  70%|███████   | 894/1271 [04:47<02:00,  3.12it/s, training_loss=0.283]\u001b[A\n",
            "Epoch 5:  70%|███████   | 895/1271 [04:47<02:00,  3.12it/s, training_loss=0.283]\u001b[A\n",
            "Epoch 5:  70%|███████   | 895/1271 [04:48<02:00,  3.12it/s, training_loss=0.166]\u001b[A\n",
            "Epoch 5:  70%|███████   | 896/1271 [04:48<02:00,  3.11it/s, training_loss=0.166]\u001b[A\n",
            "Epoch 5:  70%|███████   | 896/1271 [04:48<02:00,  3.11it/s, training_loss=0.593]\u001b[A\n",
            "Epoch 5:  71%|███████   | 897/1271 [04:48<02:00,  3.11it/s, training_loss=0.593]\u001b[A\n",
            "Epoch 5:  71%|███████   | 897/1271 [04:48<02:00,  3.11it/s, training_loss=0.177]\u001b[A\n",
            "Epoch 5:  71%|███████   | 898/1271 [04:48<01:59,  3.11it/s, training_loss=0.177]\u001b[A\n",
            "Epoch 5:  71%|███████   | 898/1271 [04:49<01:59,  3.11it/s, training_loss=0.157]\u001b[A\n",
            "Epoch 5:  71%|███████   | 899/1271 [04:49<01:59,  3.12it/s, training_loss=0.157]\u001b[A\n",
            "Epoch 5:  71%|███████   | 899/1271 [04:49<01:59,  3.12it/s, training_loss=0.299]\u001b[A\n",
            "Epoch 5:  71%|███████   | 900/1271 [04:49<01:58,  3.12it/s, training_loss=0.299]\u001b[A\n",
            "Epoch 5:  71%|███████   | 900/1271 [04:49<01:58,  3.12it/s, training_loss=0.267]\u001b[A\n",
            "Epoch 5:  71%|███████   | 901/1271 [04:49<01:58,  3.12it/s, training_loss=0.267]\u001b[A\n",
            "Epoch 5:  71%|███████   | 901/1271 [04:50<01:58,  3.12it/s, training_loss=0.194]\u001b[A\n",
            "Epoch 5:  71%|███████   | 902/1271 [04:50<01:57,  3.13it/s, training_loss=0.194]\u001b[A\n",
            "Epoch 5:  71%|███████   | 902/1271 [04:50<01:57,  3.13it/s, training_loss=0.323]\u001b[A\n",
            "Epoch 5:  71%|███████   | 903/1271 [04:50<01:57,  3.13it/s, training_loss=0.323]\u001b[A\n",
            "Epoch 5:  71%|███████   | 903/1271 [04:50<01:57,  3.13it/s, training_loss=0.227]\u001b[A\n",
            "Epoch 5:  71%|███████   | 904/1271 [04:50<01:57,  3.12it/s, training_loss=0.227]\u001b[A\n",
            "Epoch 5:  71%|███████   | 904/1271 [04:51<01:57,  3.12it/s, training_loss=0.609]\u001b[A\n",
            "Epoch 5:  71%|███████   | 905/1271 [04:51<01:57,  3.12it/s, training_loss=0.609]\u001b[A\n",
            "Epoch 5:  71%|███████   | 905/1271 [04:51<01:57,  3.12it/s, training_loss=0.450]\u001b[A\n",
            "Epoch 5:  71%|███████▏  | 906/1271 [04:51<01:57,  3.12it/s, training_loss=0.450]\u001b[A\n",
            "Epoch 5:  71%|███████▏  | 906/1271 [04:51<01:57,  3.12it/s, training_loss=0.534]\u001b[A\n",
            "Epoch 5:  71%|███████▏  | 907/1271 [04:51<01:56,  3.12it/s, training_loss=0.534]\u001b[A\n",
            "Epoch 5:  71%|███████▏  | 907/1271 [04:52<01:56,  3.12it/s, training_loss=0.311]\u001b[A\n",
            "Epoch 5:  71%|███████▏  | 908/1271 [04:52<01:56,  3.12it/s, training_loss=0.311]\u001b[A\n",
            "Epoch 5:  71%|███████▏  | 908/1271 [04:52<01:56,  3.12it/s, training_loss=0.193]\u001b[A\n",
            "Epoch 5:  72%|███████▏  | 909/1271 [04:52<01:55,  3.13it/s, training_loss=0.193]\u001b[A\n",
            "Epoch 5:  72%|███████▏  | 909/1271 [04:52<01:55,  3.13it/s, training_loss=0.597]\u001b[A\n",
            "Epoch 5:  72%|███████▏  | 910/1271 [04:52<01:55,  3.14it/s, training_loss=0.597]\u001b[A\n",
            "Epoch 5:  72%|███████▏  | 910/1271 [04:52<01:55,  3.14it/s, training_loss=0.137]\u001b[A\n",
            "Epoch 5:  72%|███████▏  | 911/1271 [04:53<01:54,  3.14it/s, training_loss=0.137]\u001b[A\n",
            "Epoch 5:  72%|███████▏  | 911/1271 [04:53<01:54,  3.14it/s, training_loss=0.455]\u001b[A\n",
            "Epoch 5:  72%|███████▏  | 912/1271 [04:53<01:54,  3.13it/s, training_loss=0.455]\u001b[A\n",
            "Epoch 5:  72%|███████▏  | 912/1271 [04:53<01:54,  3.13it/s, training_loss=0.202]\u001b[A\n",
            "Epoch 5:  72%|███████▏  | 913/1271 [04:53<01:54,  3.13it/s, training_loss=0.202]\u001b[A\n",
            "Epoch 5:  72%|███████▏  | 913/1271 [04:53<01:54,  3.13it/s, training_loss=0.130]\u001b[A\n",
            "Epoch 5:  72%|███████▏  | 914/1271 [04:53<01:54,  3.12it/s, training_loss=0.130]\u001b[A\n",
            "Epoch 5:  72%|███████▏  | 914/1271 [04:54<01:54,  3.12it/s, training_loss=0.526]\u001b[A\n",
            "Epoch 5:  72%|███████▏  | 915/1271 [04:54<01:54,  3.12it/s, training_loss=0.526]\u001b[A\n",
            "Epoch 5:  72%|███████▏  | 915/1271 [04:54<01:54,  3.12it/s, training_loss=0.171]\u001b[A\n",
            "Epoch 5:  72%|███████▏  | 916/1271 [04:54<01:53,  3.12it/s, training_loss=0.171]\u001b[A\n",
            "Epoch 5:  72%|███████▏  | 916/1271 [04:54<01:53,  3.12it/s, training_loss=0.272]\u001b[A\n",
            "Epoch 5:  72%|███████▏  | 917/1271 [04:54<01:53,  3.12it/s, training_loss=0.272]\u001b[A\n",
            "Epoch 5:  72%|███████▏  | 917/1271 [04:55<01:53,  3.12it/s, training_loss=0.516]\u001b[A\n",
            "Epoch 5:  72%|███████▏  | 918/1271 [04:55<01:53,  3.11it/s, training_loss=0.516]\u001b[A\n",
            "Epoch 5:  72%|███████▏  | 918/1271 [04:55<01:53,  3.11it/s, training_loss=0.046]\u001b[A\n",
            "Epoch 5:  72%|███████▏  | 919/1271 [04:55<01:52,  3.12it/s, training_loss=0.046]\u001b[A\n",
            "Epoch 5:  72%|███████▏  | 919/1271 [04:55<01:52,  3.12it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 5:  72%|███████▏  | 920/1271 [04:55<01:52,  3.12it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 5:  72%|███████▏  | 920/1271 [04:56<01:52,  3.12it/s, training_loss=0.060]\u001b[A\n",
            "Epoch 5:  72%|███████▏  | 921/1271 [04:56<01:52,  3.12it/s, training_loss=0.060]\u001b[A\n",
            "Epoch 5:  72%|███████▏  | 921/1271 [04:56<01:52,  3.12it/s, training_loss=0.197]\u001b[A\n",
            "Epoch 5:  73%|███████▎  | 922/1271 [04:56<01:51,  3.13it/s, training_loss=0.197]\u001b[A\n",
            "Epoch 5:  73%|███████▎  | 922/1271 [04:56<01:51,  3.13it/s, training_loss=0.440]\u001b[A\n",
            "Epoch 5:  73%|███████▎  | 923/1271 [04:56<01:51,  3.12it/s, training_loss=0.440]\u001b[A\n",
            "Epoch 5:  73%|███████▎  | 923/1271 [04:57<01:51,  3.12it/s, training_loss=0.165]\u001b[A\n",
            "Epoch 5:  73%|███████▎  | 924/1271 [04:57<01:51,  3.12it/s, training_loss=0.165]\u001b[A\n",
            "Epoch 5:  73%|███████▎  | 924/1271 [04:57<01:51,  3.12it/s, training_loss=0.150]\u001b[A\n",
            "Epoch 5:  73%|███████▎  | 925/1271 [04:57<01:50,  3.12it/s, training_loss=0.150]\u001b[A\n",
            "Epoch 5:  73%|███████▎  | 925/1271 [04:57<01:50,  3.12it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 5:  73%|███████▎  | 926/1271 [04:57<01:50,  3.12it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 5:  73%|███████▎  | 926/1271 [04:58<01:50,  3.12it/s, training_loss=0.096]\u001b[A\n",
            "Epoch 5:  73%|███████▎  | 927/1271 [04:58<01:50,  3.10it/s, training_loss=0.096]\u001b[A\n",
            "Epoch 5:  73%|███████▎  | 927/1271 [04:58<01:50,  3.10it/s, training_loss=0.109]\u001b[A\n",
            "Epoch 5:  73%|███████▎  | 928/1271 [04:58<01:50,  3.11it/s, training_loss=0.109]\u001b[A\n",
            "Epoch 5:  73%|███████▎  | 928/1271 [04:58<01:50,  3.11it/s, training_loss=0.282]\u001b[A\n",
            "Epoch 5:  73%|███████▎  | 929/1271 [04:58<01:49,  3.12it/s, training_loss=0.282]\u001b[A\n",
            "Epoch 5:  73%|███████▎  | 929/1271 [04:59<01:49,  3.12it/s, training_loss=0.225]\u001b[A\n",
            "Epoch 5:  73%|███████▎  | 930/1271 [04:59<01:49,  3.11it/s, training_loss=0.225]\u001b[A\n",
            "Epoch 5:  73%|███████▎  | 930/1271 [04:59<01:49,  3.11it/s, training_loss=0.056]\u001b[A\n",
            "Epoch 5:  73%|███████▎  | 931/1271 [04:59<01:49,  3.10it/s, training_loss=0.056]\u001b[A\n",
            "Epoch 5:  73%|███████▎  | 931/1271 [04:59<01:49,  3.10it/s, training_loss=0.729]\u001b[A\n",
            "Epoch 5:  73%|███████▎  | 932/1271 [04:59<01:48,  3.11it/s, training_loss=0.729]\u001b[A\n",
            "Epoch 5:  73%|███████▎  | 932/1271 [05:00<01:48,  3.11it/s, training_loss=0.440]\u001b[A\n",
            "Epoch 5:  73%|███████▎  | 933/1271 [05:00<01:48,  3.12it/s, training_loss=0.440]\u001b[A\n",
            "Epoch 5:  73%|███████▎  | 933/1271 [05:00<01:48,  3.12it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 5:  73%|███████▎  | 934/1271 [05:00<01:48,  3.12it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 5:  73%|███████▎  | 934/1271 [05:00<01:48,  3.12it/s, training_loss=0.052]\u001b[A\n",
            "Epoch 5:  74%|███████▎  | 935/1271 [05:00<01:48,  3.10it/s, training_loss=0.052]\u001b[A\n",
            "Epoch 5:  74%|███████▎  | 935/1271 [05:01<01:48,  3.10it/s, training_loss=0.014]\u001b[A\n",
            "Epoch 5:  74%|███████▎  | 936/1271 [05:01<01:47,  3.10it/s, training_loss=0.014]\u001b[A\n",
            "Epoch 5:  74%|███████▎  | 936/1271 [05:01<01:47,  3.10it/s, training_loss=0.272]\u001b[A\n",
            "Epoch 5:  74%|███████▎  | 937/1271 [05:01<01:47,  3.11it/s, training_loss=0.272]\u001b[A\n",
            "Epoch 5:  74%|███████▎  | 937/1271 [05:01<01:47,  3.11it/s, training_loss=0.730]\u001b[A\n",
            "Epoch 5:  74%|███████▍  | 938/1271 [05:01<01:47,  3.10it/s, training_loss=0.730]\u001b[A\n",
            "Epoch 5:  74%|███████▍  | 938/1271 [05:01<01:47,  3.10it/s, training_loss=0.243]\u001b[A\n",
            "Epoch 5:  74%|███████▍  | 939/1271 [05:01<01:46,  3.11it/s, training_loss=0.243]\u001b[A\n",
            "Epoch 5:  74%|███████▍  | 939/1271 [05:02<01:46,  3.11it/s, training_loss=0.195]\u001b[A\n",
            "Epoch 5:  74%|███████▍  | 940/1271 [05:02<01:46,  3.11it/s, training_loss=0.195]\u001b[A\n",
            "Epoch 5:  74%|███████▍  | 940/1271 [05:02<01:46,  3.11it/s, training_loss=0.087]\u001b[A\n",
            "Epoch 5:  74%|███████▍  | 941/1271 [05:02<01:45,  3.13it/s, training_loss=0.087]\u001b[A\n",
            "Epoch 5:  74%|███████▍  | 941/1271 [05:02<01:45,  3.13it/s, training_loss=0.329]\u001b[A\n",
            "Epoch 5:  74%|███████▍  | 942/1271 [05:02<01:45,  3.12it/s, training_loss=0.329]\u001b[A\n",
            "Epoch 5:  74%|███████▍  | 942/1271 [05:03<01:45,  3.12it/s, training_loss=0.132]\u001b[A\n",
            "Epoch 5:  74%|███████▍  | 943/1271 [05:03<01:45,  3.12it/s, training_loss=0.132]\u001b[A\n",
            "Epoch 5:  74%|███████▍  | 943/1271 [05:03<01:45,  3.12it/s, training_loss=0.010]\u001b[A\n",
            "Epoch 5:  74%|███████▍  | 944/1271 [05:03<01:44,  3.12it/s, training_loss=0.010]\u001b[A\n",
            "Epoch 5:  74%|███████▍  | 944/1271 [05:03<01:44,  3.12it/s, training_loss=0.711]\u001b[A\n",
            "Epoch 5:  74%|███████▍  | 945/1271 [05:03<01:44,  3.13it/s, training_loss=0.711]\u001b[A\n",
            "Epoch 5:  74%|███████▍  | 945/1271 [05:04<01:44,  3.13it/s, training_loss=0.013]\u001b[A\n",
            "Epoch 5:  74%|███████▍  | 946/1271 [05:04<01:43,  3.13it/s, training_loss=0.013]\u001b[A\n",
            "Epoch 5:  74%|███████▍  | 946/1271 [05:04<01:43,  3.13it/s, training_loss=0.271]\u001b[A\n",
            "Epoch 5:  75%|███████▍  | 947/1271 [05:04<01:43,  3.12it/s, training_loss=0.271]\u001b[A\n",
            "Epoch 5:  75%|███████▍  | 947/1271 [05:04<01:43,  3.12it/s, training_loss=0.015]\u001b[A\n",
            "Epoch 5:  75%|███████▍  | 948/1271 [05:04<01:43,  3.12it/s, training_loss=0.015]\u001b[A\n",
            "Epoch 5:  75%|███████▍  | 948/1271 [05:05<01:43,  3.12it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 5:  75%|███████▍  | 949/1271 [05:05<01:42,  3.13it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 5:  75%|███████▍  | 949/1271 [05:05<01:42,  3.13it/s, training_loss=0.229]\u001b[A\n",
            "Epoch 5:  75%|███████▍  | 950/1271 [05:05<01:42,  3.13it/s, training_loss=0.229]\u001b[A\n",
            "Epoch 5:  75%|███████▍  | 950/1271 [05:05<01:42,  3.13it/s, training_loss=0.055]\u001b[A\n",
            "Epoch 5:  75%|███████▍  | 951/1271 [05:05<01:42,  3.11it/s, training_loss=0.055]\u001b[A\n",
            "Epoch 5:  75%|███████▍  | 951/1271 [05:06<01:42,  3.11it/s, training_loss=0.801]\u001b[A\n",
            "Epoch 5:  75%|███████▍  | 952/1271 [05:06<01:43,  3.09it/s, training_loss=0.801]\u001b[A\n",
            "Epoch 5:  75%|███████▍  | 952/1271 [05:06<01:43,  3.09it/s, training_loss=0.529]\u001b[A\n",
            "Epoch 5:  75%|███████▍  | 953/1271 [05:06<01:42,  3.09it/s, training_loss=0.529]\u001b[A\n",
            "Epoch 5:  75%|███████▍  | 953/1271 [05:06<01:42,  3.09it/s, training_loss=0.136]\u001b[A\n",
            "Epoch 5:  75%|███████▌  | 954/1271 [05:06<01:42,  3.10it/s, training_loss=0.136]\u001b[A\n",
            "Epoch 5:  75%|███████▌  | 954/1271 [05:07<01:42,  3.10it/s, training_loss=0.197]\u001b[A\n",
            "Epoch 5:  75%|███████▌  | 955/1271 [05:07<01:42,  3.09it/s, training_loss=0.197]\u001b[A\n",
            "Epoch 5:  75%|███████▌  | 955/1271 [05:07<01:42,  3.09it/s, training_loss=0.127]\u001b[A\n",
            "Epoch 5:  75%|███████▌  | 956/1271 [05:07<01:41,  3.09it/s, training_loss=0.127]\u001b[A\n",
            "Epoch 5:  75%|███████▌  | 956/1271 [05:07<01:41,  3.09it/s, training_loss=0.122]\u001b[A\n",
            "Epoch 5:  75%|███████▌  | 957/1271 [05:07<01:41,  3.09it/s, training_loss=0.122]\u001b[A\n",
            "Epoch 5:  75%|███████▌  | 957/1271 [05:08<01:41,  3.09it/s, training_loss=0.038]\u001b[A\n",
            "Epoch 5:  75%|███████▌  | 958/1271 [05:08<01:41,  3.09it/s, training_loss=0.038]\u001b[A\n",
            "Epoch 5:  75%|███████▌  | 958/1271 [05:08<01:41,  3.09it/s, training_loss=0.114]\u001b[A\n",
            "Epoch 5:  75%|███████▌  | 959/1271 [05:08<01:40,  3.09it/s, training_loss=0.114]\u001b[A\n",
            "Epoch 5:  75%|███████▌  | 959/1271 [05:08<01:40,  3.09it/s, training_loss=0.149]\u001b[A\n",
            "Epoch 5:  76%|███████▌  | 960/1271 [05:08<01:40,  3.10it/s, training_loss=0.149]\u001b[A\n",
            "Epoch 5:  76%|███████▌  | 960/1271 [05:09<01:40,  3.10it/s, training_loss=0.011]\u001b[A\n",
            "Epoch 5:  76%|███████▌  | 961/1271 [05:09<01:40,  3.08it/s, training_loss=0.011]\u001b[A\n",
            "Epoch 5:  76%|███████▌  | 961/1271 [05:09<01:40,  3.08it/s, training_loss=0.307]\u001b[A\n",
            "Epoch 5:  76%|███████▌  | 962/1271 [05:09<01:40,  3.09it/s, training_loss=0.307]\u001b[A\n",
            "Epoch 5:  76%|███████▌  | 962/1271 [05:09<01:40,  3.09it/s, training_loss=0.423]\u001b[A\n",
            "Epoch 5:  76%|███████▌  | 963/1271 [05:09<01:40,  3.07it/s, training_loss=0.423]\u001b[A\n",
            "Epoch 5:  76%|███████▌  | 963/1271 [05:10<01:40,  3.07it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 5:  76%|███████▌  | 964/1271 [05:10<01:39,  3.07it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 5:  76%|███████▌  | 964/1271 [05:10<01:39,  3.07it/s, training_loss=0.142]\u001b[A\n",
            "Epoch 5:  76%|███████▌  | 965/1271 [05:10<01:39,  3.09it/s, training_loss=0.142]\u001b[A\n",
            "Epoch 5:  76%|███████▌  | 965/1271 [05:10<01:39,  3.09it/s, training_loss=0.352]\u001b[A\n",
            "Epoch 5:  76%|███████▌  | 966/1271 [05:10<01:38,  3.10it/s, training_loss=0.352]\u001b[A\n",
            "Epoch 5:  76%|███████▌  | 966/1271 [05:11<01:38,  3.10it/s, training_loss=0.459]\u001b[A\n",
            "Epoch 5:  76%|███████▌  | 967/1271 [05:11<01:38,  3.10it/s, training_loss=0.459]\u001b[A\n",
            "Epoch 5:  76%|███████▌  | 967/1271 [05:11<01:38,  3.10it/s, training_loss=0.210]\u001b[A\n",
            "Epoch 5:  76%|███████▌  | 968/1271 [05:11<01:38,  3.09it/s, training_loss=0.210]\u001b[A\n",
            "Epoch 5:  76%|███████▌  | 968/1271 [05:11<01:38,  3.09it/s, training_loss=0.214]\u001b[A\n",
            "Epoch 5:  76%|███████▌  | 969/1271 [05:11<01:37,  3.10it/s, training_loss=0.214]\u001b[A\n",
            "Epoch 5:  76%|███████▌  | 969/1271 [05:11<01:37,  3.10it/s, training_loss=0.031]\u001b[A\n",
            "Epoch 5:  76%|███████▋  | 970/1271 [05:11<01:37,  3.10it/s, training_loss=0.031]\u001b[A\n",
            "Epoch 5:  76%|███████▋  | 970/1271 [05:12<01:37,  3.10it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 5:  76%|███████▋  | 971/1271 [05:12<01:36,  3.11it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 5:  76%|███████▋  | 971/1271 [05:12<01:36,  3.11it/s, training_loss=0.378]\u001b[A\n",
            "Epoch 5:  76%|███████▋  | 972/1271 [05:12<01:36,  3.10it/s, training_loss=0.378]\u001b[A\n",
            "Epoch 5:  76%|███████▋  | 972/1271 [05:12<01:36,  3.10it/s, training_loss=0.210]\u001b[A\n",
            "Epoch 5:  77%|███████▋  | 973/1271 [05:12<01:35,  3.11it/s, training_loss=0.210]\u001b[A\n",
            "Epoch 5:  77%|███████▋  | 973/1271 [05:13<01:35,  3.11it/s, training_loss=0.044]\u001b[A\n",
            "Epoch 5:  77%|███████▋  | 974/1271 [05:13<01:35,  3.11it/s, training_loss=0.044]\u001b[A\n",
            "Epoch 5:  77%|███████▋  | 974/1271 [05:13<01:35,  3.11it/s, training_loss=0.158]\u001b[A\n",
            "Epoch 5:  77%|███████▋  | 975/1271 [05:13<01:35,  3.11it/s, training_loss=0.158]\u001b[A\n",
            "Epoch 5:  77%|███████▋  | 975/1271 [05:13<01:35,  3.11it/s, training_loss=0.116]\u001b[A\n",
            "Epoch 5:  77%|███████▋  | 976/1271 [05:13<01:34,  3.11it/s, training_loss=0.116]\u001b[A\n",
            "Epoch 5:  77%|███████▋  | 976/1271 [05:14<01:34,  3.11it/s, training_loss=0.387]\u001b[A\n",
            "Epoch 5:  77%|███████▋  | 977/1271 [05:14<01:34,  3.11it/s, training_loss=0.387]\u001b[A\n",
            "Epoch 5:  77%|███████▋  | 977/1271 [05:14<01:34,  3.11it/s, training_loss=0.084]\u001b[A\n",
            "Epoch 5:  77%|███████▋  | 978/1271 [05:14<01:34,  3.12it/s, training_loss=0.084]\u001b[A\n",
            "Epoch 5:  77%|███████▋  | 978/1271 [05:14<01:34,  3.12it/s, training_loss=0.101]\u001b[A\n",
            "Epoch 5:  77%|███████▋  | 979/1271 [05:14<01:33,  3.12it/s, training_loss=0.101]\u001b[A\n",
            "Epoch 5:  77%|███████▋  | 979/1271 [05:15<01:33,  3.12it/s, training_loss=0.324]\u001b[A\n",
            "Epoch 5:  77%|███████▋  | 980/1271 [05:15<01:33,  3.12it/s, training_loss=0.324]\u001b[A\n",
            "Epoch 5:  77%|███████▋  | 980/1271 [05:15<01:33,  3.12it/s, training_loss=0.364]\u001b[A\n",
            "Epoch 5:  77%|███████▋  | 981/1271 [05:15<01:33,  3.11it/s, training_loss=0.364]\u001b[A\n",
            "Epoch 5:  77%|███████▋  | 981/1271 [05:15<01:33,  3.11it/s, training_loss=0.116]\u001b[A\n",
            "Epoch 5:  77%|███████▋  | 982/1271 [05:15<01:32,  3.12it/s, training_loss=0.116]\u001b[A\n",
            "Epoch 5:  77%|███████▋  | 982/1271 [05:16<01:32,  3.12it/s, training_loss=0.232]\u001b[A\n",
            "Epoch 5:  77%|███████▋  | 983/1271 [05:16<01:31,  3.13it/s, training_loss=0.232]\u001b[A\n",
            "Epoch 5:  77%|███████▋  | 983/1271 [05:16<01:31,  3.13it/s, training_loss=0.223]\u001b[A\n",
            "Epoch 5:  77%|███████▋  | 984/1271 [05:16<01:31,  3.14it/s, training_loss=0.223]\u001b[A\n",
            "Epoch 5:  77%|███████▋  | 984/1271 [05:16<01:31,  3.14it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 5:  77%|███████▋  | 985/1271 [05:16<01:30,  3.15it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 5:  77%|███████▋  | 985/1271 [05:17<01:30,  3.15it/s, training_loss=0.106]\u001b[A\n",
            "Epoch 5:  78%|███████▊  | 986/1271 [05:17<01:31,  3.11it/s, training_loss=0.106]\u001b[A\n",
            "Epoch 5:  78%|███████▊  | 986/1271 [05:17<01:31,  3.11it/s, training_loss=0.054]\u001b[A\n",
            "Epoch 5:  78%|███████▊  | 987/1271 [05:17<01:31,  3.11it/s, training_loss=0.054]\u001b[A\n",
            "Epoch 5:  78%|███████▊  | 987/1271 [05:17<01:31,  3.11it/s, training_loss=0.032]\u001b[A\n",
            "Epoch 5:  78%|███████▊  | 988/1271 [05:17<01:31,  3.10it/s, training_loss=0.032]\u001b[A\n",
            "Epoch 5:  78%|███████▊  | 988/1271 [05:18<01:31,  3.10it/s, training_loss=0.379]\u001b[A\n",
            "Epoch 5:  78%|███████▊  | 989/1271 [05:18<01:31,  3.09it/s, training_loss=0.379]\u001b[A\n",
            "Epoch 5:  78%|███████▊  | 989/1271 [05:18<01:31,  3.09it/s, training_loss=0.318]\u001b[A\n",
            "Epoch 5:  78%|███████▊  | 990/1271 [05:18<01:31,  3.08it/s, training_loss=0.318]\u001b[A\n",
            "Epoch 5:  78%|███████▊  | 990/1271 [05:18<01:31,  3.08it/s, training_loss=0.353]\u001b[A\n",
            "Epoch 5:  78%|███████▊  | 991/1271 [05:18<01:31,  3.07it/s, training_loss=0.353]\u001b[A\n",
            "Epoch 5:  78%|███████▊  | 991/1271 [05:19<01:31,  3.07it/s, training_loss=0.455]\u001b[A\n",
            "Epoch 5:  78%|███████▊  | 992/1271 [05:19<01:30,  3.08it/s, training_loss=0.455]\u001b[A\n",
            "Epoch 5:  78%|███████▊  | 992/1271 [05:19<01:30,  3.08it/s, training_loss=0.267]\u001b[A\n",
            "Epoch 5:  78%|███████▊  | 993/1271 [05:19<01:29,  3.10it/s, training_loss=0.267]\u001b[A\n",
            "Epoch 5:  78%|███████▊  | 993/1271 [05:19<01:29,  3.10it/s, training_loss=0.454]\u001b[A\n",
            "Epoch 5:  78%|███████▊  | 994/1271 [05:19<01:29,  3.11it/s, training_loss=0.454]\u001b[A\n",
            "Epoch 5:  78%|███████▊  | 994/1271 [05:20<01:29,  3.11it/s, training_loss=0.007]\u001b[A\n",
            "Epoch 5:  78%|███████▊  | 995/1271 [05:20<01:28,  3.12it/s, training_loss=0.007]\u001b[A\n",
            "Epoch 5:  78%|███████▊  | 995/1271 [05:20<01:28,  3.12it/s, training_loss=0.053]\u001b[A\n",
            "Epoch 5:  78%|███████▊  | 996/1271 [05:20<01:27,  3.13it/s, training_loss=0.053]\u001b[A\n",
            "Epoch 5:  78%|███████▊  | 996/1271 [05:20<01:27,  3.13it/s, training_loss=0.047]\u001b[A\n",
            "Epoch 5:  78%|███████▊  | 997/1271 [05:20<01:27,  3.13it/s, training_loss=0.047]\u001b[A\n",
            "Epoch 5:  78%|███████▊  | 997/1271 [05:20<01:27,  3.13it/s, training_loss=0.048]\u001b[A\n",
            "Epoch 5:  79%|███████▊  | 998/1271 [05:20<01:27,  3.12it/s, training_loss=0.048]\u001b[A\n",
            "Epoch 5:  79%|███████▊  | 998/1271 [05:21<01:27,  3.12it/s, training_loss=0.374]\u001b[A\n",
            "Epoch 5:  79%|███████▊  | 999/1271 [05:21<01:27,  3.12it/s, training_loss=0.374]\u001b[A\n",
            "Epoch 5:  79%|███████▊  | 999/1271 [05:21<01:27,  3.12it/s, training_loss=0.063]\u001b[A\n",
            "Epoch 5:  79%|███████▊  | 1000/1271 [05:21<01:26,  3.13it/s, training_loss=0.063]\u001b[A\n",
            "Epoch 5:  79%|███████▊  | 1000/1271 [05:21<01:26,  3.13it/s, training_loss=0.164]\u001b[A\n",
            "Epoch 5:  79%|███████▉  | 1001/1271 [05:21<01:26,  3.13it/s, training_loss=0.164]\u001b[A\n",
            "Epoch 5:  79%|███████▉  | 1001/1271 [05:22<01:26,  3.13it/s, training_loss=0.219]\u001b[A\n",
            "Epoch 5:  79%|███████▉  | 1002/1271 [05:22<01:26,  3.11it/s, training_loss=0.219]\u001b[A\n",
            "Epoch 5:  79%|███████▉  | 1002/1271 [05:22<01:26,  3.11it/s, training_loss=0.014]\u001b[A\n",
            "Epoch 5:  79%|███████▉  | 1003/1271 [05:22<01:25,  3.12it/s, training_loss=0.014]\u001b[A\n",
            "Epoch 5:  79%|███████▉  | 1003/1271 [05:22<01:25,  3.12it/s, training_loss=0.251]\u001b[A\n",
            "Epoch 5:  79%|███████▉  | 1004/1271 [05:22<01:26,  3.09it/s, training_loss=0.251]\u001b[A\n",
            "Epoch 5:  79%|███████▉  | 1004/1271 [05:23<01:26,  3.09it/s, training_loss=0.084]\u001b[A\n",
            "Epoch 5:  79%|███████▉  | 1005/1271 [05:23<01:26,  3.09it/s, training_loss=0.084]\u001b[A\n",
            "Epoch 5:  79%|███████▉  | 1005/1271 [05:23<01:26,  3.09it/s, training_loss=0.001]\u001b[A\n",
            "Epoch 5:  79%|███████▉  | 1006/1271 [05:23<01:25,  3.10it/s, training_loss=0.001]\u001b[A\n",
            "Epoch 5:  79%|███████▉  | 1006/1271 [05:23<01:25,  3.10it/s, training_loss=0.253]\u001b[A\n",
            "Epoch 5:  79%|███████▉  | 1007/1271 [05:23<01:25,  3.09it/s, training_loss=0.253]\u001b[A\n",
            "Epoch 5:  79%|███████▉  | 1007/1271 [05:24<01:25,  3.09it/s, training_loss=0.289]\u001b[A\n",
            "Epoch 5:  79%|███████▉  | 1008/1271 [05:24<01:24,  3.10it/s, training_loss=0.289]\u001b[A\n",
            "Epoch 5:  79%|███████▉  | 1008/1271 [05:24<01:24,  3.10it/s, training_loss=0.016]\u001b[A\n",
            "Epoch 5:  79%|███████▉  | 1009/1271 [05:24<01:25,  3.07it/s, training_loss=0.016]\u001b[A\n",
            "Epoch 5:  79%|███████▉  | 1009/1271 [05:24<01:25,  3.07it/s, training_loss=0.055]\u001b[A\n",
            "Epoch 5:  79%|███████▉  | 1010/1271 [05:24<01:24,  3.09it/s, training_loss=0.055]\u001b[A\n",
            "Epoch 5:  79%|███████▉  | 1010/1271 [05:25<01:24,  3.09it/s, training_loss=0.681]\u001b[A\n",
            "Epoch 5:  80%|███████▉  | 1011/1271 [05:25<01:23,  3.10it/s, training_loss=0.681]\u001b[A\n",
            "Epoch 5:  80%|███████▉  | 1011/1271 [05:25<01:23,  3.10it/s, training_loss=0.118]\u001b[A\n",
            "Epoch 5:  80%|███████▉  | 1012/1271 [05:25<01:23,  3.09it/s, training_loss=0.118]\u001b[A\n",
            "Epoch 5:  80%|███████▉  | 1012/1271 [05:25<01:23,  3.09it/s, training_loss=0.406]\u001b[A\n",
            "Epoch 5:  80%|███████▉  | 1013/1271 [05:25<01:23,  3.10it/s, training_loss=0.406]\u001b[A\n",
            "Epoch 5:  80%|███████▉  | 1013/1271 [05:26<01:23,  3.10it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 5:  80%|███████▉  | 1014/1271 [05:26<01:22,  3.11it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 5:  80%|███████▉  | 1014/1271 [05:26<01:22,  3.11it/s, training_loss=0.038]\u001b[A\n",
            "Epoch 5:  80%|███████▉  | 1015/1271 [05:26<01:22,  3.12it/s, training_loss=0.038]\u001b[A\n",
            "Epoch 5:  80%|███████▉  | 1015/1271 [05:26<01:22,  3.12it/s, training_loss=0.220]\u001b[A\n",
            "Epoch 5:  80%|███████▉  | 1016/1271 [05:26<01:21,  3.12it/s, training_loss=0.220]\u001b[A\n",
            "Epoch 5:  80%|███████▉  | 1016/1271 [05:27<01:21,  3.12it/s, training_loss=0.153]\u001b[A\n",
            "Epoch 5:  80%|████████  | 1017/1271 [05:27<01:21,  3.11it/s, training_loss=0.153]\u001b[A\n",
            "Epoch 5:  80%|████████  | 1017/1271 [05:27<01:21,  3.11it/s, training_loss=0.222]\u001b[A\n",
            "Epoch 5:  80%|████████  | 1018/1271 [05:27<01:21,  3.12it/s, training_loss=0.222]\u001b[A\n",
            "Epoch 5:  80%|████████  | 1018/1271 [05:27<01:21,  3.12it/s, training_loss=0.424]\u001b[A\n",
            "Epoch 5:  80%|████████  | 1019/1271 [05:27<01:20,  3.12it/s, training_loss=0.424]\u001b[A\n",
            "Epoch 5:  80%|████████  | 1019/1271 [05:28<01:20,  3.12it/s, training_loss=0.394]\u001b[A\n",
            "Epoch 5:  80%|████████  | 1020/1271 [05:28<01:20,  3.10it/s, training_loss=0.394]\u001b[A\n",
            "Epoch 5:  80%|████████  | 1020/1271 [05:28<01:20,  3.10it/s, training_loss=0.072]\u001b[A\n",
            "Epoch 5:  80%|████████  | 1021/1271 [05:28<01:20,  3.11it/s, training_loss=0.072]\u001b[A\n",
            "Epoch 5:  80%|████████  | 1021/1271 [05:28<01:20,  3.11it/s, training_loss=0.200]\u001b[A\n",
            "Epoch 5:  80%|████████  | 1022/1271 [05:28<01:20,  3.10it/s, training_loss=0.200]\u001b[A\n",
            "Epoch 5:  80%|████████  | 1022/1271 [05:29<01:20,  3.10it/s, training_loss=0.806]\u001b[A\n",
            "Epoch 5:  80%|████████  | 1023/1271 [05:29<01:19,  3.10it/s, training_loss=0.806]\u001b[A\n",
            "Epoch 5:  80%|████████  | 1023/1271 [05:29<01:19,  3.10it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 5:  81%|████████  | 1024/1271 [05:29<01:19,  3.11it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 5:  81%|████████  | 1024/1271 [05:29<01:19,  3.11it/s, training_loss=0.137]\u001b[A\n",
            "Epoch 5:  81%|████████  | 1025/1271 [05:29<01:18,  3.12it/s, training_loss=0.137]\u001b[A\n",
            "Epoch 5:  81%|████████  | 1025/1271 [05:29<01:18,  3.12it/s, training_loss=0.174]\u001b[A\n",
            "Epoch 5:  81%|████████  | 1026/1271 [05:30<01:18,  3.13it/s, training_loss=0.174]\u001b[A\n",
            "Epoch 5:  81%|████████  | 1026/1271 [05:30<01:18,  3.13it/s, training_loss=0.229]\u001b[A\n",
            "Epoch 5:  81%|████████  | 1027/1271 [05:30<01:18,  3.12it/s, training_loss=0.229]\u001b[A\n",
            "Epoch 5:  81%|████████  | 1027/1271 [05:30<01:18,  3.12it/s, training_loss=0.404]\u001b[A\n",
            "Epoch 5:  81%|████████  | 1028/1271 [05:30<01:18,  3.11it/s, training_loss=0.404]\u001b[A\n",
            "Epoch 5:  81%|████████  | 1028/1271 [05:30<01:18,  3.11it/s, training_loss=0.171]\u001b[A\n",
            "Epoch 5:  81%|████████  | 1029/1271 [05:30<01:17,  3.11it/s, training_loss=0.171]\u001b[A\n",
            "Epoch 5:  81%|████████  | 1029/1271 [05:31<01:17,  3.11it/s, training_loss=0.349]\u001b[A\n",
            "Epoch 5:  81%|████████  | 1030/1271 [05:31<01:17,  3.11it/s, training_loss=0.349]\u001b[A\n",
            "Epoch 5:  81%|████████  | 1030/1271 [05:31<01:17,  3.11it/s, training_loss=0.312]\u001b[A\n",
            "Epoch 5:  81%|████████  | 1031/1271 [05:31<01:17,  3.11it/s, training_loss=0.312]\u001b[A\n",
            "Epoch 5:  81%|████████  | 1031/1271 [05:31<01:17,  3.11it/s, training_loss=0.152]\u001b[A\n",
            "Epoch 5:  81%|████████  | 1032/1271 [05:31<01:16,  3.12it/s, training_loss=0.152]\u001b[A\n",
            "Epoch 5:  81%|████████  | 1032/1271 [05:32<01:16,  3.12it/s, training_loss=0.429]\u001b[A\n",
            "Epoch 5:  81%|████████▏ | 1033/1271 [05:32<01:16,  3.12it/s, training_loss=0.429]\u001b[A\n",
            "Epoch 5:  81%|████████▏ | 1033/1271 [05:32<01:16,  3.12it/s, training_loss=0.179]\u001b[A\n",
            "Epoch 5:  81%|████████▏ | 1034/1271 [05:32<01:15,  3.12it/s, training_loss=0.179]\u001b[A\n",
            "Epoch 5:  81%|████████▏ | 1034/1271 [05:32<01:15,  3.12it/s, training_loss=0.030]\u001b[A\n",
            "Epoch 5:  81%|████████▏ | 1035/1271 [05:32<01:15,  3.13it/s, training_loss=0.030]\u001b[A\n",
            "Epoch 5:  81%|████████▏ | 1035/1271 [05:33<01:15,  3.13it/s, training_loss=0.268]\u001b[A\n",
            "Epoch 5:  82%|████████▏ | 1036/1271 [05:33<01:14,  3.13it/s, training_loss=0.268]\u001b[A\n",
            "Epoch 5:  82%|████████▏ | 1036/1271 [05:33<01:14,  3.13it/s, training_loss=0.197]\u001b[A\n",
            "Epoch 5:  82%|████████▏ | 1037/1271 [05:33<01:15,  3.11it/s, training_loss=0.197]\u001b[A\n",
            "Epoch 5:  82%|████████▏ | 1037/1271 [05:33<01:15,  3.11it/s, training_loss=0.266]\u001b[A\n",
            "Epoch 5:  82%|████████▏ | 1038/1271 [05:33<01:14,  3.11it/s, training_loss=0.266]\u001b[A\n",
            "Epoch 5:  82%|████████▏ | 1038/1271 [05:34<01:14,  3.11it/s, training_loss=0.492]\u001b[A\n",
            "Epoch 5:  82%|████████▏ | 1039/1271 [05:34<01:14,  3.11it/s, training_loss=0.492]\u001b[A\n",
            "Epoch 5:  82%|████████▏ | 1039/1271 [05:34<01:14,  3.11it/s, training_loss=0.475]\u001b[A\n",
            "Epoch 5:  82%|████████▏ | 1040/1271 [05:34<01:14,  3.10it/s, training_loss=0.475]\u001b[A\n",
            "Epoch 5:  82%|████████▏ | 1040/1271 [05:34<01:14,  3.10it/s, training_loss=0.181]\u001b[A\n",
            "Epoch 5:  82%|████████▏ | 1041/1271 [05:34<01:13,  3.11it/s, training_loss=0.181]\u001b[A\n",
            "Epoch 5:  82%|████████▏ | 1041/1271 [05:35<01:13,  3.11it/s, training_loss=0.264]\u001b[A\n",
            "Epoch 5:  82%|████████▏ | 1042/1271 [05:35<01:13,  3.11it/s, training_loss=0.264]\u001b[A\n",
            "Epoch 5:  82%|████████▏ | 1042/1271 [05:35<01:13,  3.11it/s, training_loss=0.039]\u001b[A\n",
            "Epoch 5:  82%|████████▏ | 1043/1271 [05:35<01:13,  3.12it/s, training_loss=0.039]\u001b[A\n",
            "Epoch 5:  82%|████████▏ | 1043/1271 [05:35<01:13,  3.12it/s, training_loss=0.367]\u001b[A\n",
            "Epoch 5:  82%|████████▏ | 1044/1271 [05:35<01:12,  3.12it/s, training_loss=0.367]\u001b[A\n",
            "Epoch 5:  82%|████████▏ | 1044/1271 [05:36<01:12,  3.12it/s, training_loss=0.345]\u001b[A\n",
            "Epoch 5:  82%|████████▏ | 1045/1271 [05:36<01:12,  3.12it/s, training_loss=0.345]\u001b[A\n",
            "Epoch 5:  82%|████████▏ | 1045/1271 [05:36<01:12,  3.12it/s, training_loss=0.310]\u001b[A\n",
            "Epoch 5:  82%|████████▏ | 1046/1271 [05:36<01:12,  3.12it/s, training_loss=0.310]\u001b[A\n",
            "Epoch 5:  82%|████████▏ | 1046/1271 [05:36<01:12,  3.12it/s, training_loss=0.260]\u001b[A\n",
            "Epoch 5:  82%|████████▏ | 1047/1271 [05:36<01:12,  3.10it/s, training_loss=0.260]\u001b[A\n",
            "Epoch 5:  82%|████████▏ | 1047/1271 [05:37<01:12,  3.10it/s, training_loss=0.139]\u001b[A\n",
            "Epoch 5:  82%|████████▏ | 1048/1271 [05:37<01:11,  3.10it/s, training_loss=0.139]\u001b[A\n",
            "Epoch 5:  82%|████████▏ | 1048/1271 [05:37<01:11,  3.10it/s, training_loss=0.069]\u001b[A\n",
            "Epoch 5:  83%|████████▎ | 1049/1271 [05:37<01:12,  3.08it/s, training_loss=0.069]\u001b[A\n",
            "Epoch 5:  83%|████████▎ | 1049/1271 [05:37<01:12,  3.08it/s, training_loss=0.295]\u001b[A\n",
            "Epoch 5:  83%|████████▎ | 1050/1271 [05:37<01:11,  3.09it/s, training_loss=0.295]\u001b[A\n",
            "Epoch 5:  83%|████████▎ | 1050/1271 [05:38<01:11,  3.09it/s, training_loss=0.492]\u001b[A\n",
            "Epoch 5:  83%|████████▎ | 1051/1271 [05:38<01:11,  3.08it/s, training_loss=0.492]\u001b[A\n",
            "Epoch 5:  83%|████████▎ | 1051/1271 [05:38<01:11,  3.08it/s, training_loss=0.149]\u001b[A\n",
            "Epoch 5:  83%|████████▎ | 1052/1271 [05:38<01:10,  3.09it/s, training_loss=0.149]\u001b[A\n",
            "Epoch 5:  83%|████████▎ | 1052/1271 [05:38<01:10,  3.09it/s, training_loss=0.216]\u001b[A\n",
            "Epoch 5:  83%|████████▎ | 1053/1271 [05:38<01:10,  3.08it/s, training_loss=0.216]\u001b[A\n",
            "Epoch 5:  83%|████████▎ | 1053/1271 [05:39<01:10,  3.08it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 5:  83%|████████▎ | 1054/1271 [05:39<01:10,  3.09it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 5:  83%|████████▎ | 1054/1271 [05:39<01:10,  3.09it/s, training_loss=0.035]\u001b[A\n",
            "Epoch 5:  83%|████████▎ | 1055/1271 [05:39<01:09,  3.09it/s, training_loss=0.035]\u001b[A\n",
            "Epoch 5:  83%|████████▎ | 1055/1271 [05:39<01:09,  3.09it/s, training_loss=0.225]\u001b[A\n",
            "Epoch 5:  83%|████████▎ | 1056/1271 [05:39<01:09,  3.10it/s, training_loss=0.225]\u001b[A\n",
            "Epoch 5:  83%|████████▎ | 1056/1271 [05:39<01:09,  3.10it/s, training_loss=0.303]\u001b[A\n",
            "Epoch 5:  83%|████████▎ | 1057/1271 [05:39<01:08,  3.11it/s, training_loss=0.303]\u001b[A\n",
            "Epoch 5:  83%|████████▎ | 1057/1271 [05:40<01:08,  3.11it/s, training_loss=0.202]\u001b[A\n",
            "Epoch 5:  83%|████████▎ | 1058/1271 [05:40<01:08,  3.10it/s, training_loss=0.202]\u001b[A\n",
            "Epoch 5:  83%|████████▎ | 1058/1271 [05:40<01:08,  3.10it/s, training_loss=0.149]\u001b[A\n",
            "Epoch 5:  83%|████████▎ | 1059/1271 [05:40<01:08,  3.10it/s, training_loss=0.149]\u001b[A\n",
            "Epoch 5:  83%|████████▎ | 1059/1271 [05:40<01:08,  3.10it/s, training_loss=0.083]\u001b[A\n",
            "Epoch 5:  83%|████████▎ | 1060/1271 [05:40<01:08,  3.10it/s, training_loss=0.083]\u001b[A\n",
            "Epoch 5:  83%|████████▎ | 1060/1271 [05:41<01:08,  3.10it/s, training_loss=0.133]\u001b[A\n",
            "Epoch 5:  83%|████████▎ | 1061/1271 [05:41<01:07,  3.12it/s, training_loss=0.133]\u001b[A\n",
            "Epoch 5:  83%|████████▎ | 1061/1271 [05:41<01:07,  3.12it/s, training_loss=0.223]\u001b[A\n",
            "Epoch 5:  84%|████████▎ | 1062/1271 [05:41<01:06,  3.13it/s, training_loss=0.223]\u001b[A\n",
            "Epoch 5:  84%|████████▎ | 1062/1271 [05:41<01:06,  3.13it/s, training_loss=0.318]\u001b[A\n",
            "Epoch 5:  84%|████████▎ | 1063/1271 [05:41<01:06,  3.13it/s, training_loss=0.318]\u001b[A\n",
            "Epoch 5:  84%|████████▎ | 1063/1271 [05:42<01:06,  3.13it/s, training_loss=0.595]\u001b[A\n",
            "Epoch 5:  84%|████████▎ | 1064/1271 [05:42<01:06,  3.13it/s, training_loss=0.595]\u001b[A\n",
            "Epoch 5:  84%|████████▎ | 1064/1271 [05:42<01:06,  3.13it/s, training_loss=0.178]\u001b[A\n",
            "Epoch 5:  84%|████████▍ | 1065/1271 [05:42<01:05,  3.13it/s, training_loss=0.178]\u001b[A\n",
            "Epoch 5:  84%|████████▍ | 1065/1271 [05:42<01:05,  3.13it/s, training_loss=0.145]\u001b[A\n",
            "Epoch 5:  84%|████████▍ | 1066/1271 [05:42<01:05,  3.13it/s, training_loss=0.145]\u001b[A\n",
            "Epoch 5:  84%|████████▍ | 1066/1271 [05:43<01:05,  3.13it/s, training_loss=0.610]\u001b[A\n",
            "Epoch 5:  84%|████████▍ | 1067/1271 [05:43<01:05,  3.12it/s, training_loss=0.610]\u001b[A\n",
            "Epoch 5:  84%|████████▍ | 1067/1271 [05:43<01:05,  3.12it/s, training_loss=0.271]\u001b[A\n",
            "Epoch 5:  84%|████████▍ | 1068/1271 [05:43<01:05,  3.10it/s, training_loss=0.271]\u001b[A\n",
            "Epoch 5:  84%|████████▍ | 1068/1271 [05:43<01:05,  3.10it/s, training_loss=0.085]\u001b[A\n",
            "Epoch 5:  84%|████████▍ | 1069/1271 [05:43<01:04,  3.11it/s, training_loss=0.085]\u001b[A\n",
            "Epoch 5:  84%|████████▍ | 1069/1271 [05:44<01:04,  3.11it/s, training_loss=0.257]\u001b[A\n",
            "Epoch 5:  84%|████████▍ | 1070/1271 [05:44<01:04,  3.11it/s, training_loss=0.257]\u001b[A\n",
            "Epoch 5:  84%|████████▍ | 1070/1271 [05:44<01:04,  3.11it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 5:  84%|████████▍ | 1071/1271 [05:44<01:04,  3.12it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 5:  84%|████████▍ | 1071/1271 [05:44<01:04,  3.12it/s, training_loss=0.326]\u001b[A\n",
            "Epoch 5:  84%|████████▍ | 1072/1271 [05:44<01:03,  3.12it/s, training_loss=0.326]\u001b[A\n",
            "Epoch 5:  84%|████████▍ | 1072/1271 [05:45<01:03,  3.12it/s, training_loss=0.169]\u001b[A\n",
            "Epoch 5:  84%|████████▍ | 1073/1271 [05:45<01:03,  3.12it/s, training_loss=0.169]\u001b[A\n",
            "Epoch 5:  84%|████████▍ | 1073/1271 [05:45<01:03,  3.12it/s, training_loss=0.050]\u001b[A\n",
            "Epoch 5:  85%|████████▍ | 1074/1271 [05:45<01:03,  3.12it/s, training_loss=0.050]\u001b[A\n",
            "Epoch 5:  85%|████████▍ | 1074/1271 [05:45<01:03,  3.12it/s, training_loss=0.047]\u001b[A\n",
            "Epoch 5:  85%|████████▍ | 1075/1271 [05:45<01:02,  3.12it/s, training_loss=0.047]\u001b[A\n",
            "Epoch 5:  85%|████████▍ | 1075/1271 [05:46<01:02,  3.12it/s, training_loss=0.843]\u001b[A\n",
            "Epoch 5:  85%|████████▍ | 1076/1271 [05:46<01:02,  3.12it/s, training_loss=0.843]\u001b[A\n",
            "Epoch 5:  85%|████████▍ | 1076/1271 [05:46<01:02,  3.12it/s, training_loss=0.057]\u001b[A\n",
            "Epoch 5:  85%|████████▍ | 1077/1271 [05:46<01:01,  3.13it/s, training_loss=0.057]\u001b[A\n",
            "Epoch 5:  85%|████████▍ | 1077/1271 [05:46<01:01,  3.13it/s, training_loss=0.161]\u001b[A\n",
            "Epoch 5:  85%|████████▍ | 1078/1271 [05:46<01:01,  3.14it/s, training_loss=0.161]\u001b[A\n",
            "Epoch 5:  85%|████████▍ | 1078/1271 [05:47<01:01,  3.14it/s, training_loss=0.352]\u001b[A\n",
            "Epoch 5:  85%|████████▍ | 1079/1271 [05:47<01:01,  3.11it/s, training_loss=0.352]\u001b[A\n",
            "Epoch 5:  85%|████████▍ | 1079/1271 [05:47<01:01,  3.11it/s, training_loss=0.251]\u001b[A\n",
            "Epoch 5:  85%|████████▍ | 1080/1271 [05:47<01:01,  3.11it/s, training_loss=0.251]\u001b[A\n",
            "Epoch 5:  85%|████████▍ | 1080/1271 [05:47<01:01,  3.11it/s, training_loss=0.245]\u001b[A\n",
            "Epoch 5:  85%|████████▌ | 1081/1271 [05:47<01:01,  3.11it/s, training_loss=0.245]\u001b[A\n",
            "Epoch 5:  85%|████████▌ | 1081/1271 [05:47<01:01,  3.11it/s, training_loss=0.035]\u001b[A\n",
            "Epoch 5:  85%|████████▌ | 1082/1271 [05:48<01:00,  3.11it/s, training_loss=0.035]\u001b[A\n",
            "Epoch 5:  85%|████████▌ | 1082/1271 [05:48<01:00,  3.11it/s, training_loss=0.297]\u001b[A\n",
            "Epoch 5:  85%|████████▌ | 1083/1271 [05:48<01:00,  3.11it/s, training_loss=0.297]\u001b[A\n",
            "Epoch 5:  85%|████████▌ | 1083/1271 [05:48<01:00,  3.11it/s, training_loss=0.078]\u001b[A\n",
            "Epoch 5:  85%|████████▌ | 1084/1271 [05:48<00:59,  3.12it/s, training_loss=0.078]\u001b[A\n",
            "Epoch 5:  85%|████████▌ | 1084/1271 [05:48<00:59,  3.12it/s, training_loss=0.273]\u001b[A\n",
            "Epoch 5:  85%|████████▌ | 1085/1271 [05:48<00:59,  3.11it/s, training_loss=0.273]\u001b[A\n",
            "Epoch 5:  85%|████████▌ | 1085/1271 [05:49<00:59,  3.11it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 5:  85%|████████▌ | 1086/1271 [05:49<00:59,  3.13it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 5:  85%|████████▌ | 1086/1271 [05:49<00:59,  3.13it/s, training_loss=0.040]\u001b[A\n",
            "Epoch 5:  86%|████████▌ | 1087/1271 [05:49<00:58,  3.13it/s, training_loss=0.040]\u001b[A\n",
            "Epoch 5:  86%|████████▌ | 1087/1271 [05:49<00:58,  3.13it/s, training_loss=0.052]\u001b[A\n",
            "Epoch 5:  86%|████████▌ | 1088/1271 [05:49<00:58,  3.13it/s, training_loss=0.052]\u001b[A\n",
            "Epoch 5:  86%|████████▌ | 1088/1271 [05:50<00:58,  3.13it/s, training_loss=0.201]\u001b[A\n",
            "Epoch 5:  86%|████████▌ | 1089/1271 [05:50<00:58,  3.13it/s, training_loss=0.201]\u001b[A\n",
            "Epoch 5:  86%|████████▌ | 1089/1271 [05:50<00:58,  3.13it/s, training_loss=0.088]\u001b[A\n",
            "Epoch 5:  86%|████████▌ | 1090/1271 [05:50<00:57,  3.14it/s, training_loss=0.088]\u001b[A\n",
            "Epoch 5:  86%|████████▌ | 1090/1271 [05:50<00:57,  3.14it/s, training_loss=0.036]\u001b[A\n",
            "Epoch 5:  86%|████████▌ | 1091/1271 [05:50<00:57,  3.13it/s, training_loss=0.036]\u001b[A\n",
            "Epoch 5:  86%|████████▌ | 1091/1271 [05:51<00:57,  3.13it/s, training_loss=0.593]\u001b[A\n",
            "Epoch 5:  86%|████████▌ | 1092/1271 [05:51<00:57,  3.14it/s, training_loss=0.593]\u001b[A\n",
            "Epoch 5:  86%|████████▌ | 1092/1271 [05:51<00:57,  3.14it/s, training_loss=0.216]\u001b[A\n",
            "Epoch 5:  86%|████████▌ | 1093/1271 [05:51<00:56,  3.14it/s, training_loss=0.216]\u001b[A\n",
            "Epoch 5:  86%|████████▌ | 1093/1271 [05:51<00:56,  3.14it/s, training_loss=0.238]\u001b[A\n",
            "Epoch 5:  86%|████████▌ | 1094/1271 [05:51<00:56,  3.14it/s, training_loss=0.238]\u001b[A\n",
            "Epoch 5:  86%|████████▌ | 1094/1271 [05:52<00:56,  3.14it/s, training_loss=0.155]\u001b[A\n",
            "Epoch 5:  86%|████████▌ | 1095/1271 [05:52<00:56,  3.12it/s, training_loss=0.155]\u001b[A\n",
            "Epoch 5:  86%|████████▌ | 1095/1271 [05:52<00:56,  3.12it/s, training_loss=0.025]\u001b[A\n",
            "Epoch 5:  86%|████████▌ | 1096/1271 [05:52<00:56,  3.11it/s, training_loss=0.025]\u001b[A\n",
            "Epoch 5:  86%|████████▌ | 1096/1271 [05:52<00:56,  3.11it/s, training_loss=0.060]\u001b[A\n",
            "Epoch 5:  86%|████████▋ | 1097/1271 [05:52<00:55,  3.11it/s, training_loss=0.060]\u001b[A\n",
            "Epoch 5:  86%|████████▋ | 1097/1271 [05:53<00:55,  3.11it/s, training_loss=0.169]\u001b[A\n",
            "Epoch 5:  86%|████████▋ | 1098/1271 [05:53<00:55,  3.10it/s, training_loss=0.169]\u001b[A\n",
            "Epoch 5:  86%|████████▋ | 1098/1271 [05:53<00:55,  3.10it/s, training_loss=0.340]\u001b[A\n",
            "Epoch 5:  86%|████████▋ | 1099/1271 [05:53<00:55,  3.09it/s, training_loss=0.340]\u001b[A\n",
            "Epoch 5:  86%|████████▋ | 1099/1271 [05:53<00:55,  3.09it/s, training_loss=0.117]\u001b[A\n",
            "Epoch 5:  87%|████████▋ | 1100/1271 [05:53<00:55,  3.10it/s, training_loss=0.117]\u001b[A\n",
            "Epoch 5:  87%|████████▋ | 1100/1271 [05:54<00:55,  3.10it/s, training_loss=0.218]\u001b[A\n",
            "Epoch 5:  87%|████████▋ | 1101/1271 [05:54<00:54,  3.10it/s, training_loss=0.218]\u001b[A\n",
            "Epoch 5:  87%|████████▋ | 1101/1271 [05:54<00:54,  3.10it/s, training_loss=0.278]\u001b[A\n",
            "Epoch 5:  87%|████████▋ | 1102/1271 [05:54<00:54,  3.11it/s, training_loss=0.278]\u001b[A\n",
            "Epoch 5:  87%|████████▋ | 1102/1271 [05:54<00:54,  3.11it/s, training_loss=0.206]\u001b[A\n",
            "Epoch 5:  87%|████████▋ | 1103/1271 [05:54<00:53,  3.12it/s, training_loss=0.206]\u001b[A\n",
            "Epoch 5:  87%|████████▋ | 1103/1271 [05:55<00:53,  3.12it/s, training_loss=0.059]\u001b[A\n",
            "Epoch 5:  87%|████████▋ | 1104/1271 [05:55<00:53,  3.12it/s, training_loss=0.059]\u001b[A\n",
            "Epoch 5:  87%|████████▋ | 1104/1271 [05:55<00:53,  3.12it/s, training_loss=0.295]\u001b[A\n",
            "Epoch 5:  87%|████████▋ | 1105/1271 [05:55<00:53,  3.10it/s, training_loss=0.295]\u001b[A\n",
            "Epoch 5:  87%|████████▋ | 1105/1271 [05:55<00:53,  3.10it/s, training_loss=0.354]\u001b[A\n",
            "Epoch 5:  87%|████████▋ | 1106/1271 [05:55<00:53,  3.11it/s, training_loss=0.354]\u001b[A\n",
            "Epoch 5:  87%|████████▋ | 1106/1271 [05:56<00:53,  3.11it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 5:  87%|████████▋ | 1107/1271 [05:56<00:52,  3.11it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 5:  87%|████████▋ | 1107/1271 [05:56<00:52,  3.11it/s, training_loss=0.460]\u001b[A\n",
            "Epoch 5:  87%|████████▋ | 1108/1271 [05:56<00:52,  3.10it/s, training_loss=0.460]\u001b[A\n",
            "Epoch 5:  87%|████████▋ | 1108/1271 [05:56<00:52,  3.10it/s, training_loss=0.209]\u001b[A\n",
            "Epoch 5:  87%|████████▋ | 1109/1271 [05:56<00:52,  3.08it/s, training_loss=0.209]\u001b[A\n",
            "Epoch 5:  87%|████████▋ | 1109/1271 [05:56<00:52,  3.08it/s, training_loss=0.445]\u001b[A\n",
            "Epoch 5:  87%|████████▋ | 1110/1271 [05:57<00:52,  3.07it/s, training_loss=0.445]\u001b[A\n",
            "Epoch 5:  87%|████████▋ | 1110/1271 [05:57<00:52,  3.07it/s, training_loss=0.235]\u001b[A\n",
            "Epoch 5:  87%|████████▋ | 1111/1271 [05:57<00:51,  3.08it/s, training_loss=0.235]\u001b[A\n",
            "Epoch 5:  87%|████████▋ | 1111/1271 [05:57<00:51,  3.08it/s, training_loss=0.212]\u001b[A\n",
            "Epoch 5:  87%|████████▋ | 1112/1271 [05:57<00:51,  3.10it/s, training_loss=0.212]\u001b[A\n",
            "Epoch 5:  87%|████████▋ | 1112/1271 [05:57<00:51,  3.10it/s, training_loss=0.345]\u001b[A\n",
            "Epoch 5:  88%|████████▊ | 1113/1271 [05:57<00:51,  3.10it/s, training_loss=0.345]\u001b[A\n",
            "Epoch 5:  88%|████████▊ | 1113/1271 [05:58<00:51,  3.10it/s, training_loss=0.132]\u001b[A\n",
            "Epoch 5:  88%|████████▊ | 1114/1271 [05:58<00:50,  3.10it/s, training_loss=0.132]\u001b[A\n",
            "Epoch 5:  88%|████████▊ | 1114/1271 [05:58<00:50,  3.10it/s, training_loss=0.199]\u001b[A\n",
            "Epoch 5:  88%|████████▊ | 1115/1271 [05:58<00:50,  3.11it/s, training_loss=0.199]\u001b[A\n",
            "Epoch 5:  88%|████████▊ | 1115/1271 [05:58<00:50,  3.11it/s, training_loss=0.125]\u001b[A\n",
            "Epoch 5:  88%|████████▊ | 1116/1271 [05:58<00:49,  3.10it/s, training_loss=0.125]\u001b[A\n",
            "Epoch 5:  88%|████████▊ | 1116/1271 [05:59<00:49,  3.10it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 5:  88%|████████▊ | 1117/1271 [05:59<00:49,  3.12it/s, training_loss=0.004]\u001b[A\n",
            "Epoch 5:  88%|████████▊ | 1117/1271 [05:59<00:49,  3.12it/s, training_loss=0.074]\u001b[A\n",
            "Epoch 5:  88%|████████▊ | 1118/1271 [05:59<00:49,  3.12it/s, training_loss=0.074]\u001b[A\n",
            "Epoch 5:  88%|████████▊ | 1118/1271 [05:59<00:49,  3.12it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 5:  88%|████████▊ | 1119/1271 [05:59<00:48,  3.11it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 5:  88%|████████▊ | 1119/1271 [06:00<00:48,  3.11it/s, training_loss=0.112]\u001b[A\n",
            "Epoch 5:  88%|████████▊ | 1120/1271 [06:00<00:48,  3.10it/s, training_loss=0.112]\u001b[A\n",
            "Epoch 5:  88%|████████▊ | 1120/1271 [06:00<00:48,  3.10it/s, training_loss=0.287]\u001b[A\n",
            "Epoch 5:  88%|████████▊ | 1121/1271 [06:00<00:48,  3.10it/s, training_loss=0.287]\u001b[A\n",
            "Epoch 5:  88%|████████▊ | 1121/1271 [06:00<00:48,  3.10it/s, training_loss=0.199]\u001b[A\n",
            "Epoch 5:  88%|████████▊ | 1122/1271 [06:00<00:47,  3.11it/s, training_loss=0.199]\u001b[A\n",
            "Epoch 5:  88%|████████▊ | 1122/1271 [06:01<00:47,  3.11it/s, training_loss=0.144]\u001b[A\n",
            "Epoch 5:  88%|████████▊ | 1123/1271 [06:01<00:47,  3.10it/s, training_loss=0.144]\u001b[A\n",
            "Epoch 5:  88%|████████▊ | 1123/1271 [06:01<00:47,  3.10it/s, training_loss=0.058]\u001b[A\n",
            "Epoch 5:  88%|████████▊ | 1124/1271 [06:01<00:47,  3.10it/s, training_loss=0.058]\u001b[A\n",
            "Epoch 5:  88%|████████▊ | 1124/1271 [06:01<00:47,  3.10it/s, training_loss=0.172]\u001b[A\n",
            "Epoch 5:  89%|████████▊ | 1125/1271 [06:01<00:47,  3.10it/s, training_loss=0.172]\u001b[A\n",
            "Epoch 5:  89%|████████▊ | 1125/1271 [06:02<00:47,  3.10it/s, training_loss=0.049]\u001b[A\n",
            "Epoch 5:  89%|████████▊ | 1126/1271 [06:02<00:46,  3.11it/s, training_loss=0.049]\u001b[A\n",
            "Epoch 5:  89%|████████▊ | 1126/1271 [06:02<00:46,  3.11it/s, training_loss=0.131]\u001b[A\n",
            "Epoch 5:  89%|████████▊ | 1127/1271 [06:02<00:46,  3.12it/s, training_loss=0.131]\u001b[A\n",
            "Epoch 5:  89%|████████▊ | 1127/1271 [06:02<00:46,  3.12it/s, training_loss=0.225]\u001b[A\n",
            "Epoch 5:  89%|████████▊ | 1128/1271 [06:02<00:45,  3.12it/s, training_loss=0.225]\u001b[A\n",
            "Epoch 5:  89%|████████▊ | 1128/1271 [06:03<00:45,  3.12it/s, training_loss=0.335]\u001b[A\n",
            "Epoch 5:  89%|████████▉ | 1129/1271 [06:03<00:45,  3.12it/s, training_loss=0.335]\u001b[A\n",
            "Epoch 5:  89%|████████▉ | 1129/1271 [06:03<00:45,  3.12it/s, training_loss=0.222]\u001b[A\n",
            "Epoch 5:  89%|████████▉ | 1130/1271 [06:03<00:45,  3.10it/s, training_loss=0.222]\u001b[A\n",
            "Epoch 5:  89%|████████▉ | 1130/1271 [06:03<00:45,  3.10it/s, training_loss=0.054]\u001b[A\n",
            "Epoch 5:  89%|████████▉ | 1131/1271 [06:03<00:45,  3.11it/s, training_loss=0.054]\u001b[A\n",
            "Epoch 5:  89%|████████▉ | 1131/1271 [06:04<00:45,  3.11it/s, training_loss=0.209]\u001b[A\n",
            "Epoch 5:  89%|████████▉ | 1132/1271 [06:04<00:44,  3.11it/s, training_loss=0.209]\u001b[A\n",
            "Epoch 5:  89%|████████▉ | 1132/1271 [06:04<00:44,  3.11it/s, training_loss=0.631]\u001b[A\n",
            "Epoch 5:  89%|████████▉ | 1133/1271 [06:04<00:44,  3.12it/s, training_loss=0.631]\u001b[A\n",
            "Epoch 5:  89%|████████▉ | 1133/1271 [06:04<00:44,  3.12it/s, training_loss=0.565]\u001b[A\n",
            "Epoch 5:  89%|████████▉ | 1134/1271 [06:04<00:43,  3.13it/s, training_loss=0.565]\u001b[A\n",
            "Epoch 5:  89%|████████▉ | 1134/1271 [06:05<00:43,  3.13it/s, training_loss=0.149]\u001b[A\n",
            "Epoch 5:  89%|████████▉ | 1135/1271 [06:05<00:43,  3.11it/s, training_loss=0.149]\u001b[A\n",
            "Epoch 5:  89%|████████▉ | 1135/1271 [06:05<00:43,  3.11it/s, training_loss=0.132]\u001b[A\n",
            "Epoch 5:  89%|████████▉ | 1136/1271 [06:05<00:43,  3.10it/s, training_loss=0.132]\u001b[A\n",
            "Epoch 5:  89%|████████▉ | 1136/1271 [06:05<00:43,  3.10it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 5:  89%|████████▉ | 1137/1271 [06:05<00:43,  3.08it/s, training_loss=0.008]\u001b[A\n",
            "Epoch 5:  89%|████████▉ | 1137/1271 [06:06<00:43,  3.08it/s, training_loss=0.333]\u001b[A\n",
            "Epoch 5:  90%|████████▉ | 1138/1271 [06:06<00:43,  3.08it/s, training_loss=0.333]\u001b[A\n",
            "Epoch 5:  90%|████████▉ | 1138/1271 [06:06<00:43,  3.08it/s, training_loss=0.201]\u001b[A\n",
            "Epoch 5:  90%|████████▉ | 1139/1271 [06:06<00:42,  3.08it/s, training_loss=0.201]\u001b[A\n",
            "Epoch 5:  90%|████████▉ | 1139/1271 [06:06<00:42,  3.08it/s, training_loss=0.186]\u001b[A\n",
            "Epoch 5:  90%|████████▉ | 1140/1271 [06:06<00:42,  3.08it/s, training_loss=0.186]\u001b[A\n",
            "Epoch 5:  90%|████████▉ | 1140/1271 [06:06<00:42,  3.08it/s, training_loss=0.341]\u001b[A\n",
            "Epoch 5:  90%|████████▉ | 1141/1271 [06:06<00:42,  3.08it/s, training_loss=0.341]\u001b[A\n",
            "Epoch 5:  90%|████████▉ | 1141/1271 [06:07<00:42,  3.08it/s, training_loss=0.163]\u001b[A\n",
            "Epoch 5:  90%|████████▉ | 1142/1271 [06:07<00:41,  3.08it/s, training_loss=0.163]\u001b[A\n",
            "Epoch 5:  90%|████████▉ | 1142/1271 [06:07<00:41,  3.08it/s, training_loss=0.429]\u001b[A\n",
            "Epoch 5:  90%|████████▉ | 1143/1271 [06:07<00:41,  3.08it/s, training_loss=0.429]\u001b[A\n",
            "Epoch 5:  90%|████████▉ | 1143/1271 [06:07<00:41,  3.08it/s, training_loss=0.038]\u001b[A\n",
            "Epoch 5:  90%|█████████ | 1144/1271 [06:07<00:41,  3.08it/s, training_loss=0.038]\u001b[A\n",
            "Epoch 5:  90%|█████████ | 1144/1271 [06:08<00:41,  3.08it/s, training_loss=0.043]\u001b[A\n",
            "Epoch 5:  90%|█████████ | 1145/1271 [06:08<00:40,  3.09it/s, training_loss=0.043]\u001b[A\n",
            "Epoch 5:  90%|█████████ | 1145/1271 [06:08<00:40,  3.09it/s, training_loss=0.342]\u001b[A\n",
            "Epoch 5:  90%|█████████ | 1146/1271 [06:08<00:40,  3.11it/s, training_loss=0.342]\u001b[A\n",
            "Epoch 5:  90%|█████████ | 1146/1271 [06:08<00:40,  3.11it/s, training_loss=0.624]\u001b[A\n",
            "Epoch 5:  90%|█████████ | 1147/1271 [06:08<00:39,  3.12it/s, training_loss=0.624]\u001b[A\n",
            "Epoch 5:  90%|█████████ | 1147/1271 [06:09<00:39,  3.12it/s, training_loss=0.289]\u001b[A\n",
            "Epoch 5:  90%|█████████ | 1148/1271 [06:09<00:39,  3.12it/s, training_loss=0.289]\u001b[A\n",
            "Epoch 5:  90%|█████████ | 1148/1271 [06:09<00:39,  3.12it/s, training_loss=0.621]\u001b[A\n",
            "Epoch 5:  90%|█████████ | 1149/1271 [06:09<00:39,  3.10it/s, training_loss=0.621]\u001b[A\n",
            "Epoch 5:  90%|█████████ | 1149/1271 [06:09<00:39,  3.10it/s, training_loss=0.200]\u001b[A\n",
            "Epoch 5:  90%|█████████ | 1150/1271 [06:09<00:38,  3.11it/s, training_loss=0.200]\u001b[A\n",
            "Epoch 5:  90%|█████████ | 1150/1271 [06:10<00:38,  3.11it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 5:  91%|█████████ | 1151/1271 [06:10<00:38,  3.11it/s, training_loss=0.006]\u001b[A\n",
            "Epoch 5:  91%|█████████ | 1151/1271 [06:10<00:38,  3.11it/s, training_loss=0.117]\u001b[A\n",
            "Epoch 5:  91%|█████████ | 1152/1271 [06:10<00:38,  3.09it/s, training_loss=0.117]\u001b[A\n",
            "Epoch 5:  91%|█████████ | 1152/1271 [06:10<00:38,  3.09it/s, training_loss=0.553]\u001b[A\n",
            "Epoch 5:  91%|█████████ | 1153/1271 [06:10<00:37,  3.11it/s, training_loss=0.553]\u001b[A\n",
            "Epoch 5:  91%|█████████ | 1153/1271 [06:11<00:37,  3.11it/s, training_loss=0.032]\u001b[A\n",
            "Epoch 5:  91%|█████████ | 1154/1271 [06:11<00:37,  3.13it/s, training_loss=0.032]\u001b[A\n",
            "Epoch 5:  91%|█████████ | 1154/1271 [06:11<00:37,  3.13it/s, training_loss=0.079]\u001b[A\n",
            "Epoch 5:  91%|█████████ | 1155/1271 [06:11<00:37,  3.11it/s, training_loss=0.079]\u001b[A\n",
            "Epoch 5:  91%|█████████ | 1155/1271 [06:11<00:37,  3.11it/s, training_loss=0.268]\u001b[A\n",
            "Epoch 5:  91%|█████████ | 1156/1271 [06:11<00:36,  3.11it/s, training_loss=0.268]\u001b[A\n",
            "Epoch 5:  91%|█████████ | 1156/1271 [06:12<00:36,  3.11it/s, training_loss=0.168]\u001b[A\n",
            "Epoch 5:  91%|█████████ | 1157/1271 [06:12<00:36,  3.11it/s, training_loss=0.168]\u001b[A\n",
            "Epoch 5:  91%|█████████ | 1157/1271 [06:12<00:36,  3.11it/s, training_loss=0.021]\u001b[A\n",
            "Epoch 5:  91%|█████████ | 1158/1271 [06:12<00:36,  3.11it/s, training_loss=0.021]\u001b[A\n",
            "Epoch 5:  91%|█████████ | 1158/1271 [06:12<00:36,  3.11it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 5:  91%|█████████ | 1159/1271 [06:12<00:35,  3.12it/s, training_loss=0.003]\u001b[A\n",
            "Epoch 5:  91%|█████████ | 1159/1271 [06:13<00:35,  3.12it/s, training_loss=0.107]\u001b[A\n",
            "Epoch 5:  91%|█████████▏| 1160/1271 [06:13<00:35,  3.11it/s, training_loss=0.107]\u001b[A\n",
            "Epoch 5:  91%|█████████▏| 1160/1271 [06:13<00:35,  3.11it/s, training_loss=0.416]\u001b[A\n",
            "Epoch 5:  91%|█████████▏| 1161/1271 [06:13<00:35,  3.12it/s, training_loss=0.416]\u001b[A\n",
            "Epoch 5:  91%|█████████▏| 1161/1271 [06:13<00:35,  3.12it/s, training_loss=0.163]\u001b[A\n",
            "Epoch 5:  91%|█████████▏| 1162/1271 [06:13<00:34,  3.12it/s, training_loss=0.163]\u001b[A\n",
            "Epoch 5:  91%|█████████▏| 1162/1271 [06:14<00:34,  3.12it/s, training_loss=0.513]\u001b[A\n",
            "Epoch 5:  92%|█████████▏| 1163/1271 [06:14<00:34,  3.12it/s, training_loss=0.513]\u001b[A\n",
            "Epoch 5:  92%|█████████▏| 1163/1271 [06:14<00:34,  3.12it/s, training_loss=0.353]\u001b[A\n",
            "Epoch 5:  92%|█████████▏| 1164/1271 [06:14<00:34,  3.13it/s, training_loss=0.353]\u001b[A\n",
            "Epoch 5:  92%|█████████▏| 1164/1271 [06:14<00:34,  3.13it/s, training_loss=0.039]\u001b[A\n",
            "Epoch 5:  92%|█████████▏| 1165/1271 [06:14<00:33,  3.12it/s, training_loss=0.039]\u001b[A\n",
            "Epoch 5:  92%|█████████▏| 1165/1271 [06:15<00:33,  3.12it/s, training_loss=0.164]\u001b[A\n",
            "Epoch 5:  92%|█████████▏| 1166/1271 [06:15<00:33,  3.12it/s, training_loss=0.164]\u001b[A\n",
            "Epoch 5:  92%|█████████▏| 1166/1271 [06:15<00:33,  3.12it/s, training_loss=0.109]\u001b[A\n",
            "Epoch 5:  92%|█████████▏| 1167/1271 [06:15<00:33,  3.10it/s, training_loss=0.109]\u001b[A\n",
            "Epoch 5:  92%|█████████▏| 1167/1271 [06:15<00:33,  3.10it/s, training_loss=0.086]\u001b[A\n",
            "Epoch 5:  92%|█████████▏| 1168/1271 [06:15<00:33,  3.11it/s, training_loss=0.086]\u001b[A\n",
            "Epoch 5:  92%|█████████▏| 1168/1271 [06:15<00:33,  3.11it/s, training_loss=0.377]\u001b[A\n",
            "Epoch 5:  92%|█████████▏| 1169/1271 [06:15<00:32,  3.12it/s, training_loss=0.377]\u001b[A\n",
            "Epoch 5:  92%|█████████▏| 1169/1271 [06:16<00:32,  3.12it/s, training_loss=0.131]\u001b[A\n",
            "Epoch 5:  92%|█████████▏| 1170/1271 [06:16<00:32,  3.13it/s, training_loss=0.131]\u001b[A\n",
            "Epoch 5:  92%|█████████▏| 1170/1271 [06:16<00:32,  3.13it/s, training_loss=0.079]\u001b[A\n",
            "Epoch 5:  92%|█████████▏| 1171/1271 [06:16<00:31,  3.13it/s, training_loss=0.079]\u001b[A\n",
            "Epoch 5:  92%|█████████▏| 1171/1271 [06:16<00:31,  3.13it/s, training_loss=0.099]\u001b[A\n",
            "Epoch 5:  92%|█████████▏| 1172/1271 [06:16<00:31,  3.12it/s, training_loss=0.099]\u001b[A\n",
            "Epoch 5:  92%|█████████▏| 1172/1271 [06:17<00:31,  3.12it/s, training_loss=0.082]\u001b[A\n",
            "Epoch 5:  92%|█████████▏| 1173/1271 [06:17<00:31,  3.12it/s, training_loss=0.082]\u001b[A\n",
            "Epoch 5:  92%|█████████▏| 1173/1271 [06:17<00:31,  3.12it/s, training_loss=0.087]\u001b[A\n",
            "Epoch 5:  92%|█████████▏| 1174/1271 [06:17<00:31,  3.10it/s, training_loss=0.087]\u001b[A\n",
            "Epoch 5:  92%|█████████▏| 1174/1271 [06:17<00:31,  3.10it/s, training_loss=0.287]\u001b[A\n",
            "Epoch 5:  92%|█████████▏| 1175/1271 [06:17<00:30,  3.11it/s, training_loss=0.287]\u001b[A\n",
            "Epoch 5:  92%|█████████▏| 1175/1271 [06:18<00:30,  3.11it/s, training_loss=0.312]\u001b[A\n",
            "Epoch 5:  93%|█████████▎| 1176/1271 [06:18<00:30,  3.10it/s, training_loss=0.312]\u001b[A\n",
            "Epoch 5:  93%|█████████▎| 1176/1271 [06:18<00:30,  3.10it/s, training_loss=0.052]\u001b[A\n",
            "Epoch 5:  93%|█████████▎| 1177/1271 [06:18<00:30,  3.10it/s, training_loss=0.052]\u001b[A\n",
            "Epoch 5:  93%|█████████▎| 1177/1271 [06:18<00:30,  3.10it/s, training_loss=0.148]\u001b[A\n",
            "Epoch 5:  93%|█████████▎| 1178/1271 [06:18<00:29,  3.11it/s, training_loss=0.148]\u001b[A\n",
            "Epoch 5:  93%|█████████▎| 1178/1271 [06:19<00:29,  3.11it/s, training_loss=0.390]\u001b[A\n",
            "Epoch 5:  93%|█████████▎| 1179/1271 [06:19<00:29,  3.13it/s, training_loss=0.390]\u001b[A\n",
            "Epoch 5:  93%|█████████▎| 1179/1271 [06:19<00:29,  3.13it/s, training_loss=0.131]\u001b[A\n",
            "Epoch 5:  93%|█████████▎| 1180/1271 [06:19<00:28,  3.14it/s, training_loss=0.131]\u001b[A\n",
            "Epoch 5:  93%|█████████▎| 1180/1271 [06:19<00:28,  3.14it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 5:  93%|█████████▎| 1181/1271 [06:19<00:28,  3.13it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 5:  93%|█████████▎| 1181/1271 [06:20<00:28,  3.13it/s, training_loss=0.341]\u001b[A\n",
            "Epoch 5:  93%|█████████▎| 1182/1271 [06:20<00:28,  3.12it/s, training_loss=0.341]\u001b[A\n",
            "Epoch 5:  93%|█████████▎| 1182/1271 [06:20<00:28,  3.12it/s, training_loss=0.594]\u001b[A\n",
            "Epoch 5:  93%|█████████▎| 1183/1271 [06:20<00:28,  3.12it/s, training_loss=0.594]\u001b[A\n",
            "Epoch 5:  93%|█████████▎| 1183/1271 [06:20<00:28,  3.12it/s, training_loss=0.044]\u001b[A\n",
            "Epoch 5:  93%|█████████▎| 1184/1271 [06:20<00:27,  3.12it/s, training_loss=0.044]\u001b[A\n",
            "Epoch 5:  93%|█████████▎| 1184/1271 [06:21<00:27,  3.12it/s, training_loss=0.111]\u001b[A\n",
            "Epoch 5:  93%|█████████▎| 1185/1271 [06:21<00:27,  3.12it/s, training_loss=0.111]\u001b[A\n",
            "Epoch 5:  93%|█████████▎| 1185/1271 [06:21<00:27,  3.12it/s, training_loss=0.207]\u001b[A\n",
            "Epoch 5:  93%|█████████▎| 1186/1271 [06:21<00:27,  3.12it/s, training_loss=0.207]\u001b[A\n",
            "Epoch 5:  93%|█████████▎| 1186/1271 [06:21<00:27,  3.12it/s, training_loss=0.720]\u001b[A\n",
            "Epoch 5:  93%|█████████▎| 1187/1271 [06:21<00:26,  3.12it/s, training_loss=0.720]\u001b[A\n",
            "Epoch 5:  93%|█████████▎| 1187/1271 [06:22<00:26,  3.12it/s, training_loss=0.190]\u001b[A\n",
            "Epoch 5:  93%|█████████▎| 1188/1271 [06:22<00:26,  3.12it/s, training_loss=0.190]\u001b[A\n",
            "Epoch 5:  93%|█████████▎| 1188/1271 [06:22<00:26,  3.12it/s, training_loss=0.487]\u001b[A\n",
            "Epoch 5:  94%|█████████▎| 1189/1271 [06:22<00:26,  3.10it/s, training_loss=0.487]\u001b[A\n",
            "Epoch 5:  94%|█████████▎| 1189/1271 [06:22<00:26,  3.10it/s, training_loss=0.313]\u001b[A\n",
            "Epoch 5:  94%|█████████▎| 1190/1271 [06:22<00:26,  3.09it/s, training_loss=0.313]\u001b[A\n",
            "Epoch 5:  94%|█████████▎| 1190/1271 [06:23<00:26,  3.09it/s, training_loss=0.022]\u001b[A\n",
            "Epoch 5:  94%|█████████▎| 1191/1271 [06:23<00:25,  3.09it/s, training_loss=0.022]\u001b[A\n",
            "Epoch 5:  94%|█████████▎| 1191/1271 [06:23<00:25,  3.09it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 5:  94%|█████████▍| 1192/1271 [06:23<00:25,  3.10it/s, training_loss=0.097]\u001b[A\n",
            "Epoch 5:  94%|█████████▍| 1192/1271 [06:23<00:25,  3.10it/s, training_loss=0.338]\u001b[A\n",
            "Epoch 5:  94%|█████████▍| 1193/1271 [06:23<00:25,  3.09it/s, training_loss=0.338]\u001b[A\n",
            "Epoch 5:  94%|█████████▍| 1193/1271 [06:24<00:25,  3.09it/s, training_loss=0.142]\u001b[A\n",
            "Epoch 5:  94%|█████████▍| 1194/1271 [06:24<00:24,  3.10it/s, training_loss=0.142]\u001b[A\n",
            "Epoch 5:  94%|█████████▍| 1194/1271 [06:24<00:24,  3.10it/s, training_loss=0.127]\u001b[A\n",
            "Epoch 5:  94%|█████████▍| 1195/1271 [06:24<00:24,  3.10it/s, training_loss=0.127]\u001b[A\n",
            "Epoch 5:  94%|█████████▍| 1195/1271 [06:24<00:24,  3.10it/s, training_loss=0.491]\u001b[A\n",
            "Epoch 5:  94%|█████████▍| 1196/1271 [06:24<00:24,  3.11it/s, training_loss=0.491]\u001b[A\n",
            "Epoch 5:  94%|█████████▍| 1196/1271 [06:24<00:24,  3.11it/s, training_loss=0.435]\u001b[A\n",
            "Epoch 5:  94%|█████████▍| 1197/1271 [06:24<00:23,  3.09it/s, training_loss=0.435]\u001b[A\n",
            "Epoch 5:  94%|█████████▍| 1197/1271 [06:25<00:23,  3.09it/s, training_loss=0.142]\u001b[A\n",
            "Epoch 5:  94%|█████████▍| 1198/1271 [06:25<00:23,  3.11it/s, training_loss=0.142]\u001b[A\n",
            "Epoch 5:  94%|█████████▍| 1198/1271 [06:25<00:23,  3.11it/s, training_loss=0.227]\u001b[A\n",
            "Epoch 5:  94%|█████████▍| 1199/1271 [06:25<00:23,  3.12it/s, training_loss=0.227]\u001b[A\n",
            "Epoch 5:  94%|█████████▍| 1199/1271 [06:25<00:23,  3.12it/s, training_loss=0.329]\u001b[A\n",
            "Epoch 5:  94%|█████████▍| 1200/1271 [06:25<00:22,  3.13it/s, training_loss=0.329]\u001b[A\n",
            "Epoch 5:  94%|█████████▍| 1200/1271 [06:26<00:22,  3.13it/s, training_loss=0.216]\u001b[A\n",
            "Epoch 5:  94%|█████████▍| 1201/1271 [06:26<00:22,  3.13it/s, training_loss=0.216]\u001b[A\n",
            "Epoch 5:  94%|█████████▍| 1201/1271 [06:26<00:22,  3.13it/s, training_loss=0.117]\u001b[A\n",
            "Epoch 5:  95%|█████████▍| 1202/1271 [06:26<00:22,  3.12it/s, training_loss=0.117]\u001b[A\n",
            "Epoch 5:  95%|█████████▍| 1202/1271 [06:26<00:22,  3.12it/s, training_loss=0.416]\u001b[A\n",
            "Epoch 5:  95%|█████████▍| 1203/1271 [06:26<00:21,  3.11it/s, training_loss=0.416]\u001b[A\n",
            "Epoch 5:  95%|█████████▍| 1203/1271 [06:27<00:21,  3.11it/s, training_loss=0.160]\u001b[A\n",
            "Epoch 5:  95%|█████████▍| 1204/1271 [06:27<00:21,  3.11it/s, training_loss=0.160]\u001b[A\n",
            "Epoch 5:  95%|█████████▍| 1204/1271 [06:27<00:21,  3.11it/s, training_loss=0.255]\u001b[A\n",
            "Epoch 5:  95%|█████████▍| 1205/1271 [06:27<00:21,  3.11it/s, training_loss=0.255]\u001b[A\n",
            "Epoch 5:  95%|█████████▍| 1205/1271 [06:27<00:21,  3.11it/s, training_loss=0.181]\u001b[A\n",
            "Epoch 5:  95%|█████████▍| 1206/1271 [06:27<00:21,  3.08it/s, training_loss=0.181]\u001b[A\n",
            "Epoch 5:  95%|█████████▍| 1206/1271 [06:28<00:21,  3.08it/s, training_loss=0.045]\u001b[A\n",
            "Epoch 5:  95%|█████████▍| 1207/1271 [06:28<00:20,  3.09it/s, training_loss=0.045]\u001b[A\n",
            "Epoch 5:  95%|█████████▍| 1207/1271 [06:28<00:20,  3.09it/s, training_loss=0.401]\u001b[A\n",
            "Epoch 5:  95%|█████████▌| 1208/1271 [06:28<00:20,  3.09it/s, training_loss=0.401]\u001b[A\n",
            "Epoch 5:  95%|█████████▌| 1208/1271 [06:28<00:20,  3.09it/s, training_loss=0.084]\u001b[A\n",
            "Epoch 5:  95%|█████████▌| 1209/1271 [06:28<00:20,  3.09it/s, training_loss=0.084]\u001b[A\n",
            "Epoch 5:  95%|█████████▌| 1209/1271 [06:29<00:20,  3.09it/s, training_loss=0.186]\u001b[A\n",
            "Epoch 5:  95%|█████████▌| 1210/1271 [06:29<00:19,  3.11it/s, training_loss=0.186]\u001b[A\n",
            "Epoch 5:  95%|█████████▌| 1210/1271 [06:29<00:19,  3.11it/s, training_loss=0.296]\u001b[A\n",
            "Epoch 5:  95%|█████████▌| 1211/1271 [06:29<00:19,  3.11it/s, training_loss=0.296]\u001b[A\n",
            "Epoch 5:  95%|█████████▌| 1211/1271 [06:29<00:19,  3.11it/s, training_loss=0.169]\u001b[A\n",
            "Epoch 5:  95%|█████████▌| 1212/1271 [06:29<00:19,  3.10it/s, training_loss=0.169]\u001b[A\n",
            "Epoch 5:  95%|█████████▌| 1212/1271 [06:30<00:19,  3.10it/s, training_loss=0.143]\u001b[A\n",
            "Epoch 5:  95%|█████████▌| 1213/1271 [06:30<00:18,  3.10it/s, training_loss=0.143]\u001b[A\n",
            "Epoch 5:  95%|█████████▌| 1213/1271 [06:30<00:18,  3.10it/s, training_loss=0.145]\u001b[A\n",
            "Epoch 5:  96%|█████████▌| 1214/1271 [06:30<00:18,  3.12it/s, training_loss=0.145]\u001b[A\n",
            "Epoch 5:  96%|█████████▌| 1214/1271 [06:30<00:18,  3.12it/s, training_loss=0.021]\u001b[A\n",
            "Epoch 5:  96%|█████████▌| 1215/1271 [06:30<00:17,  3.12it/s, training_loss=0.021]\u001b[A\n",
            "Epoch 5:  96%|█████████▌| 1215/1271 [06:31<00:17,  3.12it/s, training_loss=0.139]\u001b[A\n",
            "Epoch 5:  96%|█████████▌| 1216/1271 [06:31<00:17,  3.11it/s, training_loss=0.139]\u001b[A\n",
            "Epoch 5:  96%|█████████▌| 1216/1271 [06:31<00:17,  3.11it/s, training_loss=0.458]\u001b[A\n",
            "Epoch 5:  96%|█████████▌| 1217/1271 [06:31<00:17,  3.12it/s, training_loss=0.458]\u001b[A\n",
            "Epoch 5:  96%|█████████▌| 1217/1271 [06:31<00:17,  3.12it/s, training_loss=0.010]\u001b[A\n",
            "Epoch 5:  96%|█████████▌| 1218/1271 [06:31<00:16,  3.12it/s, training_loss=0.010]\u001b[A\n",
            "Epoch 5:  96%|█████████▌| 1218/1271 [06:32<00:16,  3.12it/s, training_loss=0.173]\u001b[A\n",
            "Epoch 5:  96%|█████████▌| 1219/1271 [06:32<00:16,  3.11it/s, training_loss=0.173]\u001b[A\n",
            "Epoch 5:  96%|█████████▌| 1219/1271 [06:32<00:16,  3.11it/s, training_loss=0.049]\u001b[A\n",
            "Epoch 5:  96%|█████████▌| 1220/1271 [06:32<00:16,  3.11it/s, training_loss=0.049]\u001b[A\n",
            "Epoch 5:  96%|█████████▌| 1220/1271 [06:32<00:16,  3.11it/s, training_loss=0.276]\u001b[A\n",
            "Epoch 5:  96%|█████████▌| 1221/1271 [06:32<00:16,  3.09it/s, training_loss=0.276]\u001b[A\n",
            "Epoch 5:  96%|█████████▌| 1221/1271 [06:33<00:16,  3.09it/s, training_loss=0.293]\u001b[A\n",
            "Epoch 5:  96%|█████████▌| 1222/1271 [06:33<00:15,  3.09it/s, training_loss=0.293]\u001b[A\n",
            "Epoch 5:  96%|█████████▌| 1222/1271 [06:33<00:15,  3.09it/s, training_loss=0.023]\u001b[A\n",
            "Epoch 5:  96%|█████████▌| 1223/1271 [06:33<00:15,  3.11it/s, training_loss=0.023]\u001b[A\n",
            "Epoch 5:  96%|█████████▌| 1223/1271 [06:33<00:15,  3.11it/s, training_loss=0.018]\u001b[A\n",
            "Epoch 5:  96%|█████████▋| 1224/1271 [06:33<00:15,  3.11it/s, training_loss=0.018]\u001b[A\n",
            "Epoch 5:  96%|█████████▋| 1224/1271 [06:33<00:15,  3.11it/s, training_loss=0.173]\u001b[A\n",
            "Epoch 5:  96%|█████████▋| 1225/1271 [06:33<00:14,  3.12it/s, training_loss=0.173]\u001b[A\n",
            "Epoch 5:  96%|█████████▋| 1225/1271 [06:34<00:14,  3.12it/s, training_loss=0.025]\u001b[A\n",
            "Epoch 5:  96%|█████████▋| 1226/1271 [06:34<00:14,  3.14it/s, training_loss=0.025]\u001b[A\n",
            "Epoch 5:  96%|█████████▋| 1226/1271 [06:34<00:14,  3.14it/s, training_loss=0.054]\u001b[A\n",
            "Epoch 5:  97%|█████████▋| 1227/1271 [06:34<00:13,  3.14it/s, training_loss=0.054]\u001b[A\n",
            "Epoch 5:  97%|█████████▋| 1227/1271 [06:34<00:13,  3.14it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 5:  97%|█████████▋| 1228/1271 [06:34<00:13,  3.16it/s, training_loss=0.012]\u001b[A\n",
            "Epoch 5:  97%|█████████▋| 1228/1271 [06:35<00:13,  3.16it/s, training_loss=0.359]\u001b[A\n",
            "Epoch 5:  97%|█████████▋| 1229/1271 [06:35<00:13,  3.15it/s, training_loss=0.359]\u001b[A\n",
            "Epoch 5:  97%|█████████▋| 1229/1271 [06:35<00:13,  3.15it/s, training_loss=0.302]\u001b[A\n",
            "Epoch 5:  97%|█████████▋| 1230/1271 [06:35<00:13,  3.14it/s, training_loss=0.302]\u001b[A\n",
            "Epoch 5:  97%|█████████▋| 1230/1271 [06:35<00:13,  3.14it/s, training_loss=0.375]\u001b[A\n",
            "Epoch 5:  97%|█████████▋| 1231/1271 [06:35<00:12,  3.13it/s, training_loss=0.375]\u001b[A\n",
            "Epoch 5:  97%|█████████▋| 1231/1271 [06:36<00:12,  3.13it/s, training_loss=0.836]\u001b[A\n",
            "Epoch 5:  97%|█████████▋| 1232/1271 [06:36<00:12,  3.12it/s, training_loss=0.836]\u001b[A\n",
            "Epoch 5:  97%|█████████▋| 1232/1271 [06:36<00:12,  3.12it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 5:  97%|█████████▋| 1233/1271 [06:36<00:12,  3.12it/s, training_loss=0.002]\u001b[A\n",
            "Epoch 5:  97%|█████████▋| 1233/1271 [06:36<00:12,  3.12it/s, training_loss=0.025]\u001b[A\n",
            "Epoch 5:  97%|█████████▋| 1234/1271 [06:36<00:11,  3.11it/s, training_loss=0.025]\u001b[A\n",
            "Epoch 5:  97%|█████████▋| 1234/1271 [06:37<00:11,  3.11it/s, training_loss=0.184]\u001b[A\n",
            "Epoch 5:  97%|█████████▋| 1235/1271 [06:37<00:11,  3.11it/s, training_loss=0.184]\u001b[A\n",
            "Epoch 5:  97%|█████████▋| 1235/1271 [06:37<00:11,  3.11it/s, training_loss=0.178]\u001b[A\n",
            "Epoch 5:  97%|█████████▋| 1236/1271 [06:37<00:11,  3.11it/s, training_loss=0.178]\u001b[A\n",
            "Epoch 5:  97%|█████████▋| 1236/1271 [06:37<00:11,  3.11it/s, training_loss=0.114]\u001b[A\n",
            "Epoch 5:  97%|█████████▋| 1237/1271 [06:37<00:10,  3.12it/s, training_loss=0.114]\u001b[A\n",
            "Epoch 5:  97%|█████████▋| 1237/1271 [06:38<00:10,  3.12it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 5:  97%|█████████▋| 1238/1271 [06:38<00:10,  3.12it/s, training_loss=0.005]\u001b[A\n",
            "Epoch 5:  97%|█████████▋| 1238/1271 [06:38<00:10,  3.12it/s, training_loss=0.337]\u001b[A\n",
            "Epoch 5:  97%|█████████▋| 1239/1271 [06:38<00:10,  3.13it/s, training_loss=0.337]\u001b[A\n",
            "Epoch 5:  97%|█████████▋| 1239/1271 [06:38<00:10,  3.13it/s, training_loss=0.592]\u001b[A\n",
            "Epoch 5:  98%|█████████▊| 1240/1271 [06:38<00:09,  3.12it/s, training_loss=0.592]\u001b[A\n",
            "Epoch 5:  98%|█████████▊| 1240/1271 [06:39<00:09,  3.12it/s, training_loss=0.266]\u001b[A\n",
            "Epoch 5:  98%|█████████▊| 1241/1271 [06:39<00:09,  3.12it/s, training_loss=0.266]\u001b[A\n",
            "Epoch 5:  98%|█████████▊| 1241/1271 [06:39<00:09,  3.12it/s, training_loss=0.125]\u001b[A\n",
            "Epoch 5:  98%|█████████▊| 1242/1271 [06:39<00:09,  3.12it/s, training_loss=0.125]\u001b[A\n",
            "Epoch 5:  98%|█████████▊| 1242/1271 [06:39<00:09,  3.12it/s, training_loss=0.031]\u001b[A\n",
            "Epoch 5:  98%|█████████▊| 1243/1271 [06:39<00:08,  3.13it/s, training_loss=0.031]\u001b[A\n",
            "Epoch 5:  98%|█████████▊| 1243/1271 [06:40<00:08,  3.13it/s, training_loss=0.230]\u001b[A\n",
            "Epoch 5:  98%|█████████▊| 1244/1271 [06:40<00:08,  3.13it/s, training_loss=0.230]\u001b[A\n",
            "Epoch 5:  98%|█████████▊| 1244/1271 [06:40<00:08,  3.13it/s, training_loss=0.449]\u001b[A\n",
            "Epoch 5:  98%|█████████▊| 1245/1271 [06:40<00:08,  3.12it/s, training_loss=0.449]\u001b[A\n",
            "Epoch 5:  98%|█████████▊| 1245/1271 [06:40<00:08,  3.12it/s, training_loss=0.434]\u001b[A\n",
            "Epoch 5:  98%|█████████▊| 1246/1271 [06:40<00:08,  3.11it/s, training_loss=0.434]\u001b[A\n",
            "Epoch 5:  98%|█████████▊| 1246/1271 [06:41<00:08,  3.11it/s, training_loss=0.065]\u001b[A\n",
            "Epoch 5:  98%|█████████▊| 1247/1271 [06:41<00:07,  3.12it/s, training_loss=0.065]\u001b[A\n",
            "Epoch 5:  98%|█████████▊| 1247/1271 [06:41<00:07,  3.12it/s, training_loss=0.139]\u001b[A\n",
            "Epoch 5:  98%|█████████▊| 1248/1271 [06:41<00:07,  3.13it/s, training_loss=0.139]\u001b[A\n",
            "Epoch 5:  98%|█████████▊| 1248/1271 [06:41<00:07,  3.13it/s, training_loss=0.151]\u001b[A\n",
            "Epoch 5:  98%|█████████▊| 1249/1271 [06:41<00:07,  3.13it/s, training_loss=0.151]\u001b[A\n",
            "Epoch 5:  98%|█████████▊| 1249/1271 [06:41<00:07,  3.13it/s, training_loss=0.555]\u001b[A\n",
            "Epoch 5:  98%|█████████▊| 1250/1271 [06:41<00:06,  3.12it/s, training_loss=0.555]\u001b[A\n",
            "Epoch 5:  98%|█████████▊| 1250/1271 [06:42<00:06,  3.12it/s, training_loss=0.041]\u001b[A\n",
            "Epoch 5:  98%|█████████▊| 1251/1271 [06:42<00:06,  3.13it/s, training_loss=0.041]\u001b[A\n",
            "Epoch 5:  98%|█████████▊| 1251/1271 [06:42<00:06,  3.13it/s, training_loss=0.529]\u001b[A\n",
            "Epoch 5:  99%|█████████▊| 1252/1271 [06:42<00:06,  3.13it/s, training_loss=0.529]\u001b[A\n",
            "Epoch 5:  99%|█████████▊| 1252/1271 [06:42<00:06,  3.13it/s, training_loss=0.015]\u001b[A\n",
            "Epoch 5:  99%|█████████▊| 1253/1271 [06:42<00:05,  3.14it/s, training_loss=0.015]\u001b[A\n",
            "Epoch 5:  99%|█████████▊| 1253/1271 [06:43<00:05,  3.14it/s, training_loss=0.125]\u001b[A\n",
            "Epoch 5:  99%|█████████▊| 1254/1271 [06:43<00:05,  3.14it/s, training_loss=0.125]\u001b[A\n",
            "Epoch 5:  99%|█████████▊| 1254/1271 [06:43<00:05,  3.14it/s, training_loss=0.106]\u001b[A\n",
            "Epoch 5:  99%|█████████▊| 1255/1271 [06:43<00:05,  3.12it/s, training_loss=0.106]\u001b[A\n",
            "Epoch 5:  99%|█████████▊| 1255/1271 [06:43<00:05,  3.12it/s, training_loss=0.113]\u001b[A\n",
            "Epoch 5:  99%|█████████▉| 1256/1271 [06:43<00:04,  3.11it/s, training_loss=0.113]\u001b[A\n",
            "Epoch 5:  99%|█████████▉| 1256/1271 [06:44<00:04,  3.11it/s, training_loss=0.191]\u001b[A\n",
            "Epoch 5:  99%|█████████▉| 1257/1271 [06:44<00:04,  3.11it/s, training_loss=0.191]\u001b[A\n",
            "Epoch 5:  99%|█████████▉| 1257/1271 [06:44<00:04,  3.11it/s, training_loss=0.331]\u001b[A\n",
            "Epoch 5:  99%|█████████▉| 1258/1271 [06:44<00:04,  3.10it/s, training_loss=0.331]\u001b[A\n",
            "Epoch 5:  99%|█████████▉| 1258/1271 [06:44<00:04,  3.10it/s, training_loss=0.199]\u001b[A\n",
            "Epoch 5:  99%|█████████▉| 1259/1271 [06:44<00:03,  3.09it/s, training_loss=0.199]\u001b[A\n",
            "Epoch 5:  99%|█████████▉| 1259/1271 [06:45<00:03,  3.09it/s, training_loss=0.119]\u001b[A\n",
            "Epoch 5:  99%|█████████▉| 1260/1271 [06:45<00:03,  3.08it/s, training_loss=0.119]\u001b[A\n",
            "Epoch 5:  99%|█████████▉| 1260/1271 [06:45<00:03,  3.08it/s, training_loss=0.393]\u001b[A\n",
            "Epoch 5:  99%|█████████▉| 1261/1271 [06:45<00:03,  3.11it/s, training_loss=0.393]\u001b[A\n",
            "Epoch 5:  99%|█████████▉| 1261/1271 [06:45<00:03,  3.11it/s, training_loss=0.157]\u001b[A\n",
            "Epoch 5:  99%|█████████▉| 1262/1271 [06:45<00:02,  3.11it/s, training_loss=0.157]\u001b[A\n",
            "Epoch 5:  99%|█████████▉| 1262/1271 [06:46<00:02,  3.11it/s, training_loss=0.243]\u001b[A\n",
            "Epoch 5:  99%|█████████▉| 1263/1271 [06:46<00:02,  3.11it/s, training_loss=0.243]\u001b[A\n",
            "Epoch 5:  99%|█████████▉| 1263/1271 [06:46<00:02,  3.11it/s, training_loss=0.049]\u001b[A\n",
            "Epoch 5:  99%|█████████▉| 1264/1271 [06:46<00:02,  3.13it/s, training_loss=0.049]\u001b[A\n",
            "Epoch 5:  99%|█████████▉| 1264/1271 [06:46<00:02,  3.13it/s, training_loss=0.113]\u001b[A\n",
            "Epoch 5: 100%|█████████▉| 1265/1271 [06:46<00:01,  3.13it/s, training_loss=0.113]\u001b[A\n",
            "Epoch 5: 100%|█████████▉| 1265/1271 [06:47<00:01,  3.13it/s, training_loss=0.054]\u001b[A\n",
            "Epoch 5: 100%|█████████▉| 1266/1271 [06:47<00:01,  3.12it/s, training_loss=0.054]\u001b[A\n",
            "Epoch 5: 100%|█████████▉| 1266/1271 [06:47<00:01,  3.12it/s, training_loss=0.293]\u001b[A\n",
            "Epoch 5: 100%|█████████▉| 1267/1271 [06:47<00:01,  3.10it/s, training_loss=0.293]\u001b[A\n",
            "Epoch 5: 100%|█████████▉| 1267/1271 [06:47<00:01,  3.10it/s, training_loss=0.024]\u001b[A\n",
            "Epoch 5: 100%|█████████▉| 1268/1271 [06:47<00:00,  3.11it/s, training_loss=0.024]\u001b[A\n",
            "Epoch 5: 100%|█████████▉| 1268/1271 [06:48<00:00,  3.11it/s, training_loss=0.178]\u001b[A\n",
            "Epoch 5: 100%|█████████▉| 1269/1271 [06:48<00:00,  3.12it/s, training_loss=0.178]\u001b[A\n",
            "Epoch 5: 100%|█████████▉| 1269/1271 [06:48<00:00,  3.12it/s, training_loss=0.021]\u001b[A\n",
            "Epoch 5: 100%|█████████▉| 1270/1271 [06:48<00:00,  3.12it/s, training_loss=0.021]\u001b[A\n",
            "Epoch 5: 100%|█████████▉| 1270/1271 [06:48<00:00,  3.12it/s, training_loss=0.001]\u001b[A\n",
            "Epoch 5: 100%|██████████| 1271/1271 [06:48<00:00,  3.73it/s, training_loss=0.001]\u001b[A\n",
            " 80%|████████  | 4/5 [35:48<07:14, 434.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5\n",
            "Training loss: 0.6835919609982125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [36:08<00:00, 433.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 0.7440567450929019\n",
            "F1 Score (Weighted): 0.7496632218792628\n",
            "Recall@5: 0.9836552748885586\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set the random seeds for reproducibility\n",
        "seed_val = 17\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Ensure to use GPU if available\n",
        "model.to(device)\n",
        "\n",
        "# Training loop\n",
        "for epoch in tqdm(range(1, epochs+1)):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    loss_train_total = 0\n",
        "\n",
        "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
        "    for batch in progress_bar:\n",
        "        model.zero_grad()\n",
        "\n",
        "        batch = tuple(b.to(device) for b in batch)\n",
        "\n",
        "        inputs = {\n",
        "            'input_ids': batch[0],\n",
        "            'attention_mask': batch[1],\n",
        "            'labels': batch[2],\n",
        "        }\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        loss = outputs[0]\n",
        "        loss_train_total += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        progress_bar.set_postfix({'training_loss': f'{loss.item() / len(batch):.3f}'})\n",
        "\n",
        "    torch.save(model.state_dict(), f'finetuned_BERT_epoch_{epoch}.model')\n",
        "\n",
        "    tqdm.write(f'\\nEpoch {epoch}')\n",
        "    loss_train_avg = loss_train_total / len(dataloader_train)\n",
        "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
        "\n",
        "    loss_val_avg, predictions, true_vals, recall_at_5 = evaluate(dataloader_validation)\n",
        "    val_f1 = f1_score_func(predictions, true_vals)\n",
        "    tqdm.write(f'Validation loss: {loss_val_avg}')\n",
        "    tqdm.write(f'F1 Score (Weighted): {val_f1}')\n",
        "    tqdm.write(f'Recall@5: {recall_at_5}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73-u_sLOmeX5",
        "outputId": "edc29c4a-69e3-404c-bf77-f3f1bc427c7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class: FY  Notice of Funding Opportunity: Bipartisan Infrastructure Law (BIL) Airport Terminal Program (ATP) Grants: The Department of Transportation (DOT), Federal Aviation Administration (FAA) announces the opportunity to apply for approximately $ billion in FY  discretionary funds for the Airport Terminal Program (ATP), made available under the Infrastructure Investment and Jobs Act of  (IIJA), Pub. L. -, herein referred to as the Bipartisan Infrastructure Law (BIL). The purpose of the ATP is to make annual grants available to eligible airports for airport terminal and airport-owned Airport Traffic Control Towers development projects that address the aging infrastructure of our nation&#;s airports. In addition, ATP grants will align with DOT&#;s Strategic Framework FY- at https://www.transportation.gov/administrations/office-policy/fy--strategic-framework. The FY  ATP will be implemented consistent with law and in alignment with the priorities in Executive Order , Implementation of the Infrastructure Investments and Jobs Act ( FR ), which are to invest efficiently and equitably; promote the competitiveness of the U.S. economy; improve job opportunities by focusing on high labor standards; strengthen infrastructure resilience to all hazards including climate change; and to effectively coordinate with State, local, Tribal, and territorial government partners. \n",
            "Accuracy: 12/25 (0.4800)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Airport Improvement Program Discretionary Grants: The U.S. Department of Transportation&#;s Federal Aviation Administration (FAA) announces the opportunity to apply for an estimated $. billion in Fiscal Year (FY)  discretionary grants under the Airport Improvement Program (AIP), per  U.S.C. &#; . The FAA awards these annually appropriated discretionary funds through the FAA&#;s long-standing iterative, competitive grant process. Prior to the publication of this Notice of Funding Opportunity (NOFO), the FAA identified eligible applicants in its National Plan of Integrated Airport Systems (NPIAS) and compiled potentially eligible projects through the -year Airports Capital Improvement Plan (ACIP). Both of these processes are described in FAA Order ., &#;Formulation of NPIAS and ACIP,&#; adhering to  U.S.C. &#; , which authorizes discretionary funds. The AIP funds airport capital improvements and rehabilitation projects. All discretionary grant funding is subject to appropriations, statutory requirements, and related program funding availability. \n",
            "Accuracy: 163/163 (1.0000)\n",
            "\n",
            "Class: Grants for Buses and Bus Facilities Program: The Federal Transit Administration (FTA) announces the availability of approximately $. million of Fiscal Year  funds for buses, bus facilities, and bus equipment. The purpose of the Grants for Buses and Bus Facilities Program is to assist in the financing of buses and bus facilities capital projects, including replacing, rehabilitating, purchasing or leasing buses or related equipment, and rehabilitating, purchasing, constructing or leasing bus-related facilities. \n",
            "Accuracy: 14/14 (1.0000)\n",
            "\n",
            "Class: Solicitation of Project Proposals for the Low or No Emission (Low-No) Program:  Notice of Funding Opportunity (NOFO): Solicitation of Project Proposals for the Low or No Emission Program (Low-No) Program. The Federal Transit Administration (FTA) announces the availability of $ million of Fiscal Year  funds for the purchase or lease of low or no emission vehicles as well as related equipment or facilities. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time on //. The main purpose of the Low-No Program is to support the transition of the nation&#;s transit fleet to the lowest polluting and most energy efficient transit vehicles. The Low-No Program provides funding to State and local governmental authorities for the purchase or lease of zero-emission and low-emission transit buses, including acquisition, construction, and leasing of required supporting facilities. \n",
            "Accuracy: 0/6 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in Fiscal Year (FY)  Section  Urbanized Area Formula Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected passenger ferry projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to designated recipients or eligible direct recipients of Section  funds to assist in the financing of capital projects to support existing passenger ferry service, establish new ferry service, and to repair and modernize ferry boats, terminals, and related facilities and equipment. \n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: Pilot Program for Transit-Oriented Development (TOD) Planning: Notice of Funding Opportunity for the Pilot Program for Transit-Oriented Development (TOD) Planning. The Federal Transit Administration (FTA) announces the availability of approximately $. million in Pilot Program for TOD Planning funding to support comprehensive planning associated with new fixed guideway and core capacity improvement projects. FTA may award amounts ranging from $, to $,,. The Pilot Program for TOD Planning provides funding to local communities to integrate land use and transportation planning in new fixed guideway and core capacity transit project corridors. As required by statute, any comprehensive planning funded through the pilot program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations. The statute also requires that the planning work be associated with a new fixed guideway or core capacity transit project as defined in Federal transit statute ( USC (a); also see the NOFO for the definitions). Consistent with statutory direction, FTA is seeking comprehensive planning projects covering an entire transit capital project corridor, rather than proposals that involve planning for individual station areas or only a small section of the corridor. To ensure that planning work reflects the needs and aspirations of the local community and results in concrete, specific deliverables and outcomes, FTA is requiring that transit project sponsors partner with entities with land use planning authority in the transit project corridor. \n",
            "Accuracy: 0/4 (0.0000)\n",
            "\n",
            "Class: Public Transportation on Indian Reservations Program: The Federal Transit Administration (FTA) announces the availability of approximately $ million in funding provided by the Public Transportation on Indian Reservations Program (Tribal Transit Program (TTP)), as authorized by  U.S.C. (c)()(A), as amended by the Fixing America&apos;s Surface Transportation Act (FAST), Public Law - (December , ). The primary purpose of these competitively selected grants is to support planning, capital, and, in limited circumstances, operating assistance for tribal public transit services. Funds distributed to Indian tribes under the TTP should not replace or reduce funds that Indian tribes receive from states through FTA&#;s Section  program. \n",
            "Accuracy: 1/6 (0.1667)\n",
            "\n",
            "Class: INFRA Grants: The Nationally Significant Freight and Highway Projects (INFRA) program provides Federal financial assistance to highway and freight projects of national or regional significance. This notice solicits applications for awards under the program&#;s fiscal year (FY)  funding, subject to the availability of appropriated funds. The Department is focusing the competition on transportation infrastructure projects that support four key objectives, each of which is discussed in greater detail in the Notice of Funding Opportunity: () Supporting economic vitality at the national and regional level; () Leveraging Federal funding to attract non-Federal sources of infrastructure investment; () Deploying innovative technology, encouraging innovative approaches to project delivery, and incentivizing the use of innovative financing; and () Holding grant recipients accountable for their performance. \n",
            "Accuracy: 0/3 (0.0000)\n",
            "\n",
            "Class:  Tribal Transportation Program Safety Fund: A Notice of Funding Availability (NOFA) for Tribal Transportation Program Safety Funds (TTPSF) has been published. This notice announces the availability of $. million for projects in three categories: safety plans; data assessment, improvement, and analysis activities; and infrastructure improvement and other eligible activities as listed in  U.S.C.(a)() The deadline for submissions is //. For complete instructions on preparing and submitting an application for TTPSF, please download the application information and NOFO at https://flh.fhwa.dot.gov/programs/ttp/safety/ttpsf.htm Please submit your application directly to FHWA through the &quot;apply now&quot; button at: https://flh.fhwa.dot.gov/programs/ttp/safety/ttpsf.htm \n",
            "Accuracy: 14/14 (1.0000)\n",
            "\n",
            "Class: Grants for Buses and Bus Facilities Program: The purpose of the Grants for Buses and Bus Facilities Program is to assist in the financing of buses and bus facilities capital projects, including replacing, rehabilitating, purchasing or leasing buses or related equipment, and rehabilitating, purchasing, constructing or leasing bus-related facilities.\n",
            "Accuracy: 14/14 (1.0000)\n",
            "\n",
            "Class: Low or No Emission Program (Low-No Program): The main purpose of the Low-No Program is to support the transition of the nation&#;s transit fleet to the lowest polluting and most energy efficient transit vehicles. The Low-No Program provides funding to State and local governmental authorities for the purchase or lease of zero-emission and low-emission transit buses, including acquisition, construction, and leasing of required supporting facilities.\n",
            "Accuracy: 7/14 (0.5000)\n",
            "\n",
            "Class: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in Fiscal Year (FY)  Section  Urbanized Area Formula Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected passenger ferry projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to designated recipients or eligible direct recipients of Section  funds to assist in the financing of capital projects to support existing passenger ferry service, establish new ferry service, and to repair and modernize ferry boats, terminals, and related facilities and equipment. \n",
            "Accuracy: 0/2 (0.0000)\n",
            "\n",
            "Class: Pilot Program for Transit-Oriented Development (TOD) Planning: Notice of Funding Opportunity for PILOT PROGRAM FOR TRANSIT-ORIENTED DEVELOPMENT (TOD) PLANNING. The Federal Transit Administration (FTA) announces the opportunity to apply for approximately $. million of funding under the Pilot Program for TOD Planning (Catalog of Federal Domestic Assistance #.). As required by federal transit law and subject to funding availability, funds will be awarded competitively to support comprehensive planning associated with new fixed guideway and core capacity improvement projects. FTA anticipates minimum grant awards of $, and maximum grant awards of $,,. The Pilot Program for TOD Planning is intended to fund comprehensive planning that supports economic development, ridership, multimodal connectivity and accessibility, increased transit access for pedestrian and bicycle traffic, and mixed-use development near transit stations. The program also encourages identification of infrastructure needs and engagement with the private sector. Consistent with statutory direction, FTA is seeking comprehensive planning projects covering an entire transit capital project corridor, rather than proposals that involve planning for individual station areas or only a small section of the corridor. To ensure any proposed planning work reflects the needs and aspirations of the local community and results in concrete, specific deliverables and outcomes, transit project sponsors must partner with entities with land use planning authority in the transit project corridor to conduct the planning work.The Pilot Program for TOD Planning helps support FTA&#;s mission of improving public transportation for America&#;s communities by providing funding to local communities to integrate land use and transportation planning around a new fixed guideway or core capacity improvement project. Per statute, any comprehensive planning funded through the program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations.\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Public Transportation on Indian Reservations Program: The Federal Transit Administration (FTA) announces the availability of approximately $ million in funding provided by the Public Transportation on Indian Reservations Program (Tribal Transit Program (TTP)), as authorized by  U.S.C. (c)()(A), as amended by the Fixing America&apos;s Surface Transportation Act (FAST), Public Law - (December , ). This is a national solicitation for project proposals and includes the selection criteria and program eligibility information for Fiscal Year  projects. The primary purpose of these competitively selected grants is to support planning, capital, and, in limited circumstances, operating assistance for tribal public transit services. Funds distributed to Indian tribes under the TTP should not replace or reduce funds that Indian tribes receive from states through FTA&#;s Section  program.\n",
            "Accuracy: 0/4 (0.0000)\n",
            "\n",
            "Class: *INFRA Grants: The U.S. Department of Transportation (USDOT) is seeking applicants for the FY  round of the Infrastructure for Rebuilding America (INFRA) discretionary grant program to fund transportation projects of national and regional significance that result in good-paying jobs, improve safety, apply transformative technology, and explicitly address climate change and racial equity. The funding available for this year&#;s grants totals approximately $ million.The Department recognizes the role that infrastructure investment plays in economic development and job creation, and the added urgency of this funding at time when the COVID- pandemic has put stress on state and local budgets.For the first time, the USDOT seeks INFRA projects that address climate change and environmental justice. Projects will be evaluated on whether they were planned as part of a comprehensive strategy to address climate change, or whether they support strategies to reduce greenhouse gas emissions such as deploying zero-emission-vehicle infrastructure or encouraging modal shift and a reduction in vehicle-miles-traveled. Racial equity will also be considered as a selection criterion, to the extent that project sponsors have completed equity-focused community outreach, and projects are designed to benefit underserved communities. The Department will also consider whether the project is located in a federally designated community development zone, including qualified Opportunity Zones, Empowerment Zones, Promise Zones, or Choice Neighborhoods.USDOT seeks projects that apply innovative technology, delivery, or financing methods with proven outcomes to deliver projects in a cost effective manner. The Department will make awards under the INFRA program to both large and small projects. For a large project, the INFRA grant must be at least $ million. For a small project, the grant must be at least $ million. Under statutory requirements,  percent of available funds are reserved for small projects, and the Department must award at least  percent of funding for rural projects. INFRA grants may be used to fund a variety of components of an infrastructure project, however, the Department is specifically focused on projects in which the local sponsor is significantly invested and is positioned to proceed rapidly to construction. Eligible INFRA project costs may include: reconstruction, rehabilitation, acquisition of property (including land related to the project and improvements to the land), environmental mitigation, construction contingencies, equipment acquisition, and operational improvements directly related to system performance. The INFRA NOFO also announces the creation of the &#;INFRA Extra&#; Program, which will identify competitive INFRA applicants who do not receive an INFRA award and authorize them to seek a Transportation Infrastructure Finance and Innovation Act of  (TIFIA) loan up to  percent of their project cost.\n",
            "Accuracy: 0/3 (0.0000)\n",
            "\n",
            "Class: Fiscal Year  Competitive Funding Opportunity; Grants for Buses and Bus Facilities Program: The Federal Transit Administration (FTA) announces the availability of approximately $. million of Fiscal Year  funds for buses, bus facilities, and bus equipment. Synopses and full announcement will be posted on Grants.gov as opportunity FTA---BUS. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time November ,.\n",
            "Accuracy: 0/10 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  Passenger Ferry Grant Program (Ferry Program) (Federal Assistance Listing #.). Of that amount, $ million is available only for low or zero-emission ferries or ferries using electric battery or fuel cell components and the infrastructure to support such ferries. As required by Federal public transportation law, funds will be awarded competitively to designated recipients or eligible direct recipients of Urbanized Area Formula funds to support capital projects to improve existing passenger ferry service, establish new ferry service, and repair and modernize ferry boats, terminals, and related facilities and equipment.\n",
            "Accuracy: 0/2 (0.0000)\n",
            "\n",
            "Class:  Competitive Funding Opportunity: Pilot Program for Transit-Oriented Development (TOD) Planning: The Pilot Program for TOD Planning helps support FTA&#;s mission of improving public transportation for America&#;s communities by providing funding to local communities to integrate land use and transportation planning around a new fixed guideway or core capacity improvement project. Per statute, any comprehensive planning funded through the program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations. Additional information is available at: https://www.transit.dot.gov/TODPilot.Notice of Funding Opportunity (NOFO): Notice of Funding Opportunity for PILOT PROGRAM FOR TRANSIT-ORIENTED DEVELOPMENT (TOD) PLANNING. The Federal Transit Administration (FTA) announces the opportunity to apply for $,, of funding under the Pilot Program for TOD Planning (Catalog of Federal Domestic Assistance #.). As required by federal transit law and subject to funding availability, funds will be awarded competitively to support comprehensive planning associated with new fixed guideway and core capacity improvement projects. FTA anticipates minimum grant awards of $, and maximum grant awards of $,,. Synopses and full announcement are posted on Grants.gov as opportunity FTA---TPE-TODP. The Pilot Program for TOD Planning is intended to fund comprehensive planning that supports economic development, ridership, multimodal connectivity and accessibility, increased transit access for pedestrian and bicycle traffic, and mixed-use development near transit stations. The program also encourages identification of infrastructure needs and engagement with the private sector.Consistent with statutory direction, FTA is seeking comprehensive planning projects covering an entire transit capital project corridor, rather than proposals that involve planning for individual station areas or only a small section of the corridor. To ensure any proposed planning work reflects the needs and aspirations of the local community and results in concrete, specific deliverables and outcomes, transit project sponsors must partner with entities with land use planning authority in the transit project corridor to conduct the planning work.Link and Instructions for attaching the supplemental form to the SF-: All applicants must complete a supplemental form (PDF) specific to the Pilot Program for TOD Planning and attach it to their submission in Grants.gov. See section D of the NOFO for information about additional required application contents.Webinar: FTA will conduct a webinar for prospective applicants after the NOFO is published in the Federal Register. Further information will be posted on FTA&#;s website (https://www.transit.dot.gov/TODPilot) when available. Dates: An applicant must submit a proposal electronically by : p.m. Eastern Daylight Time on June , . Any agency intending to apply should initiate the process of registering on the Grants.gov site immediately to ensure completion of registration before the submission deadline. \n",
            "Accuracy: 0/3 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity:  Public Transportation on Indian Reservations Program; Tribal Transit Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants for the Fiscal Year (FY)  Public Transportation on Indian Reservations (Tribal Transit) Program (Federal Assistance Listing: .). As required by Federal public transportation law, funds will be awarded competitively for any purpose eligible under FTA&#;s Formula Grants for Rural Areas Program,  U.S.C. , including planning, capital, and operating assistance for tribal public transit services in rural areas. \n",
            "Accuracy: 5/6 (0.8333)\n",
            "\n",
            "Class: INFRA Grants: The U.S. Department of Transportation (USDOT) is seeking applicants for the FY  round of the Infrastructure for Rebuilding America (INFRA) discretionary grant program to fund transportation projects of national and regional significance that result in good-paying jobs, improve safety, apply transformative technology, and explicitly address climate change and racial equity. The funding available for this year&#;s grants totals approximately $ million.The Department recognizes the role that infrastructure investment plays in economic development and job creation, and the added urgency of this funding at time when the COVID- pandemic has put stress on state and local budgets.For the first time, the USDOT seeks INFRA projects that address climate change and environmental justice. Projects will be evaluated on whether they were planned as part of a comprehensive strategy to address climate change, or whether they support strategies to reduce greenhouse gas emissions such as deploying zero-emission-vehicle infrastructure or encouraging modal shift and a reduction in vehicle-miles-traveled. Racial equity will also be considered as a selection criterion, to the extent that project sponsors have completed equity-focused community outreach, and projects are designed to benefit underserved communities. The Department will also consider whether the project is located in a federally designated community development zone, including qualified Opportunity Zones, Empowerment Zones, Promise Zones, or Choice Neighborhoods.USDOT seeks projects that apply innovative technology, delivery, or financing methods with proven outcomes to deliver projects in a cost effective manner. The Department will make awards under the INFRA program to both large and small projects. For a large project, the INFRA grant must be at least $ million. For a small project, the grant must be at least $ million. Under statutory requirements,  percent of available funds are reserved for small projects, and the Department must award at least  percent of funding for rural projects. INFRA grants may be used to fund a variety of components of an infrastructure project, however, the Department is specifically focused on projects in which the local sponsor is significantly invested and is positioned to proceed rapidly to construction. Eligible INFRA project costs may include: reconstruction, rehabilitation, acquisition of property (including land related to the project and improvements to the land), environmental mitigation, construction contingencies, equipment acquisition, and operational improvements directly related to system performance. The INFRA NOFO also announces the creation of the &#;INFRA Extra&#; Program, which will identify competitive INFRA applicants who do not receive an INFRA award and authorize them to seek a Transportation Infrastructure Finance and Innovation Act of  (TIFIA) loan up to  percent of their project cost.\n",
            "Accuracy: 0/17 (0.0000)\n",
            "\n",
            "Class: FY Tribal Transportation Program Safety Fund (TTPSF): A Notice of Funding Opportunity (NOFO) for Tribal Transportation Program Safety Funds (TTPSF) has been published. This notice announces the availability of approximately $. million for projects in four categories: safety plans; data assessment, improvement, and analysis activities; systemic roadway departure countermeasures; and infrastructure improvement and other eligible activities as listed in  U.S.C.(a)(). The goal of these projects is to reduce fatalities and serious injuries from transportation incidents, such as motor vehicle crashes. For complete instructions on preparing and submitting an application for TTPSF, please download the application, application information, systemic roadway departure risk assessment form (if applicable) and NOFO at https://highways.dot.gov/federal-lands/programs-tribal/safety/funds. Applications will NOT be submitted on Grants.gov. Instead, please use the &quot;apply now&quot; button on the program web page.\n",
            "Accuracy: 0/9 (0.0000)\n",
            "\n",
            "Class: Advanced Transportation Technologies and  Innovative Mobility Deployment (ATTIMD) Program: AMENDMENT  (//): This amendment is for the purpose of correcting a typo contained in Table D. on page  of . This table, contained in Section D. for CONTENT AND FORM OF APPLICATION SUBMISSION, erroneously omitted the Project Outcome Criteria from that table&#;s submission requirements for Volume . This amendment also revises the verbiage included on page  of the NOFO to clarify that all resumes for key personnel should be included in item f) Appendix &#; R&#;sum&#;s for Key Personnel. Please note that Resumes do not count against the page limitations for Volume . Please refer to Amendment  in the &#;Related Documents&#; tab for more information. -- END OF AMENDMENT  -- AMENDMENT  (//): This amendment adds the weblink to the recording of the October , , informational webinar for this opportunity. The weblink is included in the attachments page for this opportunity as well as below: LINK: https://usdot.zoomgov.com/rec/share/eQNNRGISy-UxfqGpSDzINRbzvcdkAjDdoulZvwRCt-sARp.kdAJIUFsFZZEpf PASSCODE: c@X*&amp;@ -- END OF AMENDMENT  -- AMENDMENT  (//): This amendment adds the registration information for the October , , informational webinar for this opportunity. Please refer to the &quot;Related Documents&quot; tab for more information. -- END OF AMENDMENT  -- BASE NOFO (//): The Advanced Transportation Technologies and Innovative Mobility Deployment Program (ATTIMD), established in Section (c)() of title , United States Code (U.S.C.), also known as the Advanced Transportation Technology and Innovation (ATTAIN) Program, directs FHWA to award grants to eligible entities to deploy, install, and operate advanced transportation technologies to improve safety, mobility, efficiency, system performance, intermodal connectivity, and infrastructure return on investment. FHWA intends for these model technology deployments to help demonstrate how emerging transportation technologies, data, and their applications can be effectively deployed and integrated with existing systems to provide access to essential services and other destinations. This also includes efforts to increase connectivity to employment, education, services, and other opportunities; support workforce development; or contribute to increased mobility, particularly for persons with visible and hidden disabilities and elderly individuals. The Assistance Listing Number for this opportunity is . &#; Highway Research and Development. The Federal Highway Administration (FHWA) hereby requests applications to result in awards to eligible entities to deploy, install, and operate advanced transportation technologies to improve safety, mobility, efficiency, system performance, intermodal connectivity, and infrastructure return on investment. These model deployments are expected to provide benefits in the form of: reduced traffic-related fatalities and injuries; reduced traffic congestion and improved travel time reliability; reduced transportation-related emissions; optimized multimodal system performance; improved access to transportation alternatives, including for underserved populations; improved integration of payment systems; public access to real-time integrated traffic, transit, and multimodal transportation information to make informed travel decisions; cost savings to transportation agencies, businesses, and the traveling public; or other benefits to transportation users and the general public. This competitive ATTAIN Program will promote the use of innovative transportation solutions. The deployment of these technologies will provide Congress and FHWA with valuable real-life data and feedback to inform future decision-making. Cost Sharing: Cost-sharing or matching is required, with the maximum Federal share being  percent; hence, this NOFO requires a minimum non-Federal cost share of  percent. Cost sharing or matching means the portion of project costs not paid by Federal funds. NOTE: FHWA previously issued six NOFOs for the Advanced Transportation and Congestion Management Technologies Deployment (ATCMTD) Grant Program before the program was amended and renamed to the Advanced Transportation Technologies and Innovative Mobility Deployment Program by Section  of the Infrastructure Investment and Jobs Act (IIJA) (Public Law -, also known as the &#;Bipartisan Infrastructure Law&#; (BIL)).\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: FY  and FY  Competitive Funding Opportunity: Competitive Grants for Rail Vehicle Replacement Program (Rail Program): The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in Fiscal Year (FY)  and  Section  State of Good Repair Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected rail vehicle replacement projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to States and local governmental authorities to assist in the funding of capital projects to replace rail rolling stock.\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: Bridge Investment Program - Planning, Bridge Projects, and Large Bridge Projects: This Notice of Funding Opportunity (NOFO) is to solicit applications for three categories of Bridge Investment Program (BIP) funding opportunities: () Planning; () Bridge Projects (projects with eligible costs less than $ million); and () Large Bridge Projects (projects with eligible costs greater than $ million). Eligible applicants may submit applications for any of the three funding categories, but each category has distinct eligibility and selection criteria and application deadlines.These funds will be awarded on a competitive basis for planning, feasibility analysis, and revenue forecasting associated with the development of a project that would subsequently be eligible to apply for BIP funding under either the Bridge Projects or Large Bridge Projects funding categories.A total of $. billion in  BIP funds are available for the Bridge Projects and Large Bridge Projects funding opportunities. These funds will be awarded on a competitive basis for bridge replacement, rehabilitation, preservation, and protection projects that: () improve the safety, efficiency, and reliability of the movement of people and freight over bridges; and () improve the condition of bridges in the United States by reducing (a) the number of bridges, and total person miles traveled over bridges, that are in poor condition or that are in fair condition and at risk of falling into poor condition within the next three years, or (b) the number of bridges, and total person miles traveled over bridges, that do not meet current geometric design standards or cannot meet the load and traffic requirements typical of the regional transportation network. In addition, Large Bridge Projects that receive a BIP award of not less than $ million are eligible for multiyear grants, in which DOT can award available funds to a project over the course of several years in accordance with an agreement and in alignment with its schedule. In selecting Bridge Projects and Large Bridge Projects, FHWA will consider the extent to which BIP funds leverage non-Federal contributions from sponsors and stakeholders involved in the planning, design, and construction of eligible projects. \n",
            "Accuracy: 0/6 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity; Low or No Emission Grant Program: Low-No ProgramThe Federal Transit Administration (FTA) announces the opportunity to apply for approximately $. billion in competitive grants under the fiscal year (FY)  Low or No Emission Grant Program (Low-No Program) (Federal Assistance Listing: .) and approximately $ million in FY  funds under the Grants for Buses and Bus Facilities Program (Buses and Bus Facilities Program) (Federal Assistance Listing .), subject to availability of appropriated funding. Synopses and full announcement will be posted on Grants.gov as funding opportunity ID FTA--TPM for Low-No applications and FTA--TPM for Buses and Bus Facilities applications. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time May , .\n",
            "Accuracy: 15/15 (1.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: All Stations Accessibility Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  All Stations Accessibility Program (ASAP) (Federal Assistance Listing #.). As required by Federal public transportation law, funds will be awarded competitively to eligible designated recipients that operate or allocate funds to inaccessible pre-ADA&#;or &#;legacy&#; &#; rail fixed guideway public transportation systems, and States (including territories and Washington, D.C.) and local governmental entities that operate or financially support legacy rail fixed guideway public transportation systems and corresponding legacy stations/facilities for capital projects to repair, improve, modify, retrofit, or relocate infrastructure of stations or facilities for passenger use, including load-bearing members that are an essential part of the structural frame; or () for planning projects to develop or modify a plan for pursuing public transportation accessibility projects, assessments of accessibility, or assessments of planned modifications to stations or facilities for passenger use projects; or programs of projects in an eligible area.\n",
            "Accuracy: 2/2 (1.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Buses and Bus Facilities Competitive Program: Bus and Bus Facilities ProgramThe Federal Transit Administration (FTA) announces the opportunity to apply for approximately $. billion in competitive grants under the fiscal year (FY)  Low or No Emission Grant Program (Low-No Program) (Federal Assistance Listing: .) and approximately $ million in FY  funds under the Grants for Buses and Bus Facilities Program (Buses and Bus Facilities Program) (Federal Assistance Listing .), subject to availability of appropriated funding. Synopses and full announcement will be posted on Grants.gov as funding opportunity ID FTA--TPM for Low-No applications and FTA--TPM for Buses and Bus Facilities applications. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time May , \n",
            "Accuracy: 0/7 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Electric or Low-Emitting Ferry Pilot Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  Electric or Low-Emitting Ferry Pilot Program (ELEF) (Federal Assistance Listing #.). ELEF makes funding available competitively to assist in the financing of capital projects for the purchase of electric or low-emitting ferry vessels that reduce emissions by using alternative fuels or on-board energy storage systems and related charging infrastructure to reduce emissions or produce zero onboard emissions under normal operation.\n",
            "Accuracy: 0/2 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $. million in competitive grants under the Fiscal Year (FY)  Passenger Ferry Grant Program (PFG) (Federal Assistance Listing #.). Of that amount, $. million is available only for low or zero-emission ferries or ferries using electric battery or fuel cell components and the infrastructure to support such ferries. As required by Federal public transportation law, funds will be awarded competitively to designated recipients or eligible direct recipients of Urbanized Area Formula funds to support capital projects to improve existing passenger ferry service, establish new ferry service, and repair and modernize ferry boats, terminals, and related facilities and equipment. \n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: FY  Pilot Program for Transit-Oriented Development (TOD) Planning: The Pilot Program for TOD Planning helps support FTA&#;s mission of improving public transportation for America&#;s communities by providing funding to local communities to integrate land use and transportation planning around a new fixed guideway or core capacity improvement project. Per statute, any comprehensive or site specific planning funded through the program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations.\n",
            "Accuracy: 0/3 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Public Transportation on Indian Reservations Program; Tribal Transit Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $. million in competitive grants for the Fiscal Year (FY)  Public Transportation on Indian Reservations (Tribal Transit) Program. As required by Federal public transportation law, funds will be awarded competitively for any purpose eligible under FTA&#;s Formula Grants for Rural Areas Program, including planning, capital, and operating assistance for tribal public transit services in rural areas. FTA may award additional funding that is made available to the program prior to the announcement of project selections.\n",
            "Accuracy: 0/4 (0.0000)\n",
            "\n",
            "Class: FY Consolidated Rail Infrastructure and Safety Improvements Grant Program: This program funds projects that improve the safety, efficiency, and reliability of intercity passenger and freight rail.\n",
            "Accuracy: 8/10 (0.8000)\n",
            "\n",
            "Class: FY Corridor Identification and Development Grant Program: This program facilitates the development of intercity passenger rail corridors.\n",
            "Accuracy: 10/10 (1.0000)\n",
            "\n",
            "Class: FY Federal-State Partnership for Intercity Passenger Rail Grant Program for projects not located on the Northeast Corridor: This program funds capital projects that reduce the state of good repair backlog, improve performance, or expand or establish new intercity passenger rail service.\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: Fiscal Year  National Culvert Removal, Replacement, and Restoration Grant Program (Culvert AOP Program): -- ORIGINAL NOFO POSTING - // -- The primary goal of the Culvert AOP Program is to improve or restore anadromous fish passage through the replacement, removal, repair, or improvement of culverts or weirs. The grant program prioritizes projects that would improve fish passage for: (A) anadromous fish stocks listed as an endangered species or a threatened species under section  of the Endangered Species Act of  ( U.S.C. &#; ); (B) anadromous fish stocks identified by the National Marine Fisheries Service (NMFS) or the U.S. Fish and Wildlife Service (USFWS) that could reasonably become listed as an endangered species or a threatened species under that section; (C) anadromous fish stocks identified by the NMFS or the USFWS as prey for endangered species, threatened species, or protected species, including Southern resident orcas (Orcinus orca); or (D) anadromous fish stocks identified by the NMFS or the USFWS as climate resilient stocks ( U.S.C. &#; (e)()). The program also prioritizes projects that would open up more than  meters of upstream (anadromous) habitat before the end of the natural habitat ( U.S.C. &#; (e)()). \n",
            "Accuracy: 9/9 (1.0000)\n",
            "\n",
            "Class: FY  National Infrastructure Investments: Agency Name: Department of TransportationDescription: The Infrastructure Investment and Jobs Act (Pub. L. -, November , , &#;Bipartisan Infrastructure Law,&#; or &#;BIL&#;) authorized and appropriated $. billion to be awarded by the Department of Transportation (&#;DOT&#;) for FY  for Local and Regional Project Assistance Program Grants under National Infrastructure Investments. This Notice of Funding Opportunity (NOFO) solicits applications for projects to be funded under the Local and Regional Project Assistance Program, known as the RAISE Grants program, including any additional funding appropriated for the RAISE Grants program under the FY  Appropriations Act. On March , , the Consolidated Appropriations Act,  (Pub. L. -, &#;FY  Appropriations Act&#;) appropriated an additional $ million for the FY  RAISE Grant Program. Therefore, a total of $. billion in funding is now available for the FY  RAISE Grant Program. Applicants should note that the two funding streams (BIL funding and FY  Appropriations Act funding) have slightly different funding restrictions and requirements that may affect competitiveness. Please see the Notice of Funding Opportunity Amendment No. for details.As with previous rounds, funds for the FY  RAISE Transportation program are to be awarded on a competitive basis for projects that will have a significant local or regional impact.\n",
            "Accuracy: 0/25 (0.0000)\n",
            "\n",
            "Class: FY  Natural Gas Distribution Infrastructure Safety and Modernization Grant: FY  Natural Gas Distribution Infrastructure Safety and Modernization Grant\n",
            "Accuracy: 6/6 (1.0000)\n",
            "\n",
            "Class: The Infrastructure Investment and Jobs Act (IIJA) Notice of Funding Opportunity for America&#;s Marine Highway Program: This notice announces the availability of funding for grants and establishes selection criteria and application requirements for the America&#;s Marine Highway Program (&#;AMHP&#;). The purpose of this program is to make grants available to previously designated Marine Highway Projects that support the development and expansion of documented vessels or port and landside infrastructure. The Department also seeks eligible grant projects that will strengthen American supply chains. The U.S. Department of Transportation (&#;DOT&#; or &#;Department&#;) will award Marine Highway Grants to implement projects or components of projects previously designated by the Secretary of Transportation (&#;Secretary&#;) under the AMHP. Only Marine Highway Projects the Secretary designates before the Notice of Funding Opportunity (&#;NOFO&#;) closing date are eligible for funding as described in this notice. TIMING OF GRANT APPLICATIONS: Applications must be received by the Maritime Administration (&#;MARAD&#;) by : p.m. E.D.T. on April , . ADDRESSES: Grant applications must be submitted electronically using Grants.gov (https://www.grants.gov). Please be aware that you must complete the Grants.gov registration process before submitting your application and that the registration process usually takes  to  weeks to complete. Applicants are strongly encouraged to make submissions in advance of the deadline. FOR FURTHER INFORMATION CONTACT: Fred Jones, Office of Ports &amp; Waterways Planning, Room W&#;, Maritime Administration, U.S. Department of Transportation,  New Jersey Avenue S.E., Washington, D.C. , phone --, or email Fred.Jones@dot.gov. Persons who use a telecommunications device for the deaf (TDD) may call the Federal Information Relay Service (FIRS) at --- to contact the above individual during business hours. The FIRS is available twenty-four hours a day, seven days a week, to leave a message or question with the above individual. You will receive a reply during regular business hours.\n",
            "Accuracy: 0/2 (0.0000)\n",
            "\n",
            "Class: *FY  Notice of Funding Opportunity: Bipartisan Infrastructure Law (BIL) Airport Terminal Program (ATP) Grants: The Department of Transportation (DOT), Federal Aviation Administration (FAA) announces the opportunity to apply for approximately $ billion in FY  discretionary funds for the Airport Terminal Program (ATP), made available under the Infrastructure Investment and Jobs Act of  (IIJA), Pub. L. -, herein referred to as the Bipartisan Infrastructure Law (BIL). The purpose of the ATP is to make annual grants available to eligible airports for airport terminal and airport-owned Airport Traffic Control Towers development projects that address the aging infrastructure of our nation&#;s airports. In addition, ATP grants will align with DOT&#;s Strategic Framework FY- at https://www.transportation.gov/administrations/office-policy/fy--strategic-framework. The FY  ATP will be implemented consistent with law and in alignment with the priorities in Executive Order , Implementation of the Infrastructure Investments and Jobs Act ( FR ), which are to invest efficiently and equitably; promote the competitiveness of the U.S. economy; improve job opportunities by focusing on high labor standards; strengthen infrastructure resilience to all hazards including climate change; and to effectively coordinate with State, local, Tribal, and territorial government partners. \n",
            "Accuracy: 28/28 (1.0000)\n",
            "\n",
            "Class: INFRA Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families. The Infrastructure for Rebuilding America (INFRA) program is an existing competitive program that will see a more than  percent increase in this year&#;s funding due to the Bipartisan Infrastructure Law. These grants advance the Administration&#;s priorities of rebuilding America&#;s infrastructure and creating jobs by funding highway, multimodal freight and rail projects that position America to win the st century. Projects will improve safety, generate economic benefits, reduce congestion, enhance resiliency, and hold the greatest promise to eliminate supply chain bottlenecks and improve critical freight movements. Last year, DOT received over $ billion of project applications, but could only fund around $ billion of projects. To see last year&#;s INFRA grants, click here. The Bipartisan Infrastructure Law provides approximately $ billion for INFRA over  years, of which approximately $. billion will be made available through this NOFO. \n",
            "Accuracy: 0/4 (0.0000)\n",
            "\n",
            "Class: Mega Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The National Infrastructure Project Assistance (MEGA) program was created in the Bipartisan Infrastructure Law to fund major projects that are too large or complex for traditional funding programs. The program will provide grants on a competitive basis to support multijurisdictional or regional projects of significance that may also cut across multiple modes of transportation. Eligible projects could include highway, bridge, freight, port, passenger rail, and public transportation projects of national and regional significance. These could be bridges or tunnels connecting two states; new rail and transit lines that improve equity and reduce emissions; and freight hubs integrating ship, train and truck traffic while improving environmental justice. DOT will award  percent of funding to projects greater than $ million in cost, and  percent to projects greater than $ million but less than $ million in cost. The program will receive up to $ billion this year alone and be able to provide multi-year funding to projects.\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: Rural Surface Transportation Grant Program: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The Rural Surface Transportation Grant Program (RURAL) was created in the Bipartisan Infrastructure Law and will support projects to improve and expand the surface transportation infrastructure in rural areas to increase connectivity, improve the safety and reliability of the movement of people and freight, and generate regional economic growth and improve quality of life. Eligible projects for Rural grants include highway, bridge, and tunnel projects that help improve freight, safety, and provide or increase access to an agricultural, commercial, energy, or transportation facilities that support the economy of a rural area. This year alone, DOT will award up to $ million in grants through the rural program&#;part of the $ billion included in the Bipartisan Infrastructure Law over five years.\n",
            "Accuracy: 0/2 (0.0000)\n",
            "\n",
            "Class: *FY  Notice of Funding Opportunity: Bipartisan Infrastructure Law (BIL) FAA Contract Tower (FCT) Competitive Grant Program: The Department of Transportation (DOT), Federal Aviation Administration (FAA) announces the opportunity to apply for $ million in FY  Airport Infrastructure Grant funds for the FCT competitive grant program, made available under the Infrastructure Investment and Jobs Act of  (IIJA), Pub. L. -, herein referred to as the Bipartisan Infrastructure Law (BIL). The purpose of the FCT competitive grant program is to make annual grants available to eligible airports for airport-owned airport traffic control tower (ATCT) projects that address the aging infrastructure of the nation&#;s airports. The FAA will consider ATCT projects that sustain, construct, repair, improve, rehabilitate, modernize, replace, or relocate non-approach control towers; or acquire and install air traffic control, communications, and related equipment to be used in those towers; or construct a remote tower certified by the FAA including acquisition and installation of air traffic control, communications, or related equipment. In addition, FT grants will align with DOT&#;s Strategic Framework FY- at https://www.transportation.gov/administrations/office-policy/fy--strategic-framework. The FY  FCT will be implemented consistent with law and in alignment with the priorities in Executive Order , Implementation of the Infrastructure Investments and Jobs Act ( FR ), which are to invest efficiently and equitably; promote the competitiveness of the U.S. economy; improve job opportunities by focusing on high labor standards; strengthen infrastructure resilience to all hazards including climate change; and to effectively coordinate with State, local, Tribal, and territorial government partners. \n",
            "Accuracy: 1/8 (0.1250)\n",
            "\n",
            "Class: FY Railroad Crossing Elimination Grant Program: The purpose of the RCE Program is to provide funding for highway-rail and pathway-rail grade crossing improvement projects that focus on improving the safety and mobility of people and goods.\n",
            "Accuracy: 9/9 (1.0000)\n",
            "\n",
            "Class: Reconnecting Communities Pilot Discretionary Grant Program: The Reconnecting Communities Pilot Program notice of funding opportunity was amended on September ,  to include technical corrections to the Key Information Table. See updated materials in the &quot;Manage Related Documents&quot; section, as well as on our website at https://www.transportation.gov/grants/reconnecting-communitiesThe purpose of this notice is to solicit applications for Reconnecting Communities Pilot (RCP) Program grants. Funds for the Fiscal Year (FY)  RCP Program are to be awarded on a competitive basis for projects that reconnect communities by removing, retrofitting, or mitigating highways or other transportation facilities that create barriers to community connectivity, including to mobility, access, or economic development.The variety of transformative solutions to knit communities back together can include: high-quality public transportation, infrastructure removal, pedestrian walkways and overpasses, capping over roadways, linear parks and trail connectors, roadway redesigns and complete streets conversions, and main street revitalization. The program will award two types of grants: Planning Grants and Capital Construction Grants. Please consult the full Notice of Funding Opportunity for full details about the application process, requirements, and forms.\n",
            "Accuracy: 0/7 (0.0000)\n",
            "\n",
            "Class: Safe Streets and Roads for All Discretionary Grant Program: THIS NOTICE OF FUNDING OPPORTUNITY WAS AMENDED ON AUGUST , . PLEASE SEE THE GRANTS.GOV PACKAGE UPDATES, AND THE SAFE STREETS AND ROADS FOR ALL (SSA) WEBSITE IN THE LINK PROVIDED BELOW FOR MORE INFORMATION. DEADLINE REMAINS SEPTEMBER , .The purpose of this notice is to solicit applications for SSA grants. Funds for the fiscal year (FY)  SSA grant program are to be awarded on a competitive basis to support planning, infrastructure, behavioral, and operational initiatives to prevent death and serious injury on roads and streets involving all roadway users, including pedestrians; bicyclists; public transportation, personal conveyance, and micromobility users; motorists; and commercial vehicle operators. Those interested in applying for Implementation Grants should use the application materials in PKG; those interested in applying for Action Plan Grants should use the application materials in PKG. See application instructions to determine which grant to apply for in this funding round.\n",
            "Accuracy: 0/6 (0.0000)\n",
            "\n",
            "Class: Strengthening Mobility and Revolutionizing Transportation (SMART) Grants Program: The purpose of this notice is to solicit applications for Strengthening Mobility and Revolutionizing Transportation (SMART) grants. Funds for the fiscal year (FY)  SMART Grants Program are to be awarded on a competitive basis to conduct demonstration projects focused on advanced smart city or community technologies and systems to improve transportation efficiency and safety.\n",
            "Accuracy: 8/9 (0.8889)\n",
            "\n",
            "Class: FY- Federal-State Partnership for Intercity Passenger Rail Grant Program for projects located on the Northeast Corridor: This program funds capital projects on the Northeast Corridor that reduce the state of good repair backlog, improve performance, or expand or establish new intercity passenger rail service.\n",
            "Accuracy: 0/4 (0.0000)\n",
            "\n",
            "Class: Fiscal Years - Wildlife Crossings Pilot Program (WCPP): The primary goals of the WCPP are to save lives, prevent serious injuries, and protect motorists and wildlife by reducing WVCs, and improve habitat connectivity for terrestrial and aquatic species. Reduction of wildlife vehicle collisions and improvement of terrestrial and aquatic habitat connectivity are the primary merit criteria that will be used in reviewing applications, and each of the primary merit criteria are of equal importance.\n",
            "Accuracy: 0/3 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity; Buses and Bus Facilities Program: The Federal Transit Administration (FTA) announces the availability of approximately $ million in competitive grants under the Buses and Bus Facilities Program is to assist in the financing of buses and bus facilities capital projects, including replacing, rehabilitating, purchasing or leasing buses or related equipment, and rehabilitating, purchasing, constructing or leasing bus-related facilities. Synopses and full announcement will be posted on Grants.gov as opportunity FTA---TPM-BUS. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time April , .\n",
            "Accuracy: 7/7 (1.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity; Low or No Emission Vehicle Program: The Federal Transit Administration (FTA) announces the availability of approximately $. billion in competitive grants under the Low or No Emission Grant Program (Low-No Program) for the purchase or lease of zero-emission and low-emission transit buses, including acquisition, construction, and leasing of required supporting facilities. Synopses and full announcement will be posted on Grants.gov as opportunity FTA---TPM-LWNO. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time April , .\n",
            "Accuracy: 0/12 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Areas of Persistent Poverty Program: To create new opportunities for those experiencing poverty, the U.S. Department of Transportation&#;s Federal Transit Administration (FTA) today announced a new Notice of Funding Opportunity (NOFO) to help improve transit in areas experiencing long-term economic distress. The $ million in competitive grant funding through FTA&#;s Areas of Persistent Poverty (AoPP) Program provides more resources to underserved and disadvantaged communities seeking to expand or improve transit. \n",
            "Accuracy: 7/7 (1.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Ferry Service for Rural Communities Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  Ferry Service for Rural Communities Program (FSRC) (Federal Assistance Listing #.). FSRC makes funding available competitively to assist in the financing of capital, planning, and operating assistance for eligible ferry services.\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $. million in competitive grants under the Fiscal Year (FY)  Passenger Ferry Grant Program (PFG) (Federal Assistance Listing #.). Of that amount, $ million is available only for low or zero-emission ferries or ferries using electric battery or fuel cell components and the infrastructure to support such ferries. As required by Federal public transportation law, funds will be awarded competitively to designated recipients or eligible direct recipients of Urbanized Area Formula funds to support capital projects to improve existing passenger ferry service, establish new ferry service, and repair and modernize ferry boats, terminals, and related facilities and equipment. \n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: FY  National Infrastructure Investments: The Infrastructure Investment and Jobs Act of  (&#;Bipartisan Infrastructure Law,&#; or &#;BIL&#;) authorized and appropriated $. billion to be awarded by the Department of Transportation (&#;DOT&#;) for Local and Regional Project Assistance Program Grants under National Infrastructure Investments in FY . This Notice of Funding Opportunity (NOFO) solicits applications for projects funded under the Local and Regional Project Assistance Program, known as the RAISE Grants program. If the FY  Appropriations Act provides additional funding for the RAISE grants program and/or significantly different requirements for National Infrastructure Investment funds, the Department will amend this Notice with guidance on additional requirements.As with previous rounds, funds for the FY  RAISE Transportation program are to be awarded on a competitive basis for projects that will have a significant local or regional impact.\n",
            "Accuracy: 23/24 (0.9583)\n",
            "\n",
            "Class: Neighborhood Access and Equity (NAE) Program: The Department of Transportation is combining two major discretionary grant programs, the Reconnecting Communities Pilot (RCP) and Neighborhood Access and Equity (NAE) programs, into one Notice of Funding Opportunity (NOFO). Together, this combined program is known as the Reconnecting Communities and Neighborhoods (RCN) Program. The primary goal of the NAE Program is to assist economically disadvantaged or undeserved communities for planning and construction activities. Funds for the fiscal year (FY)  NAE grant program are to be awarded on a competitive basis to support planning, capital construction, and regional partnership activities that aim to restore community connectivity through the removal, retrofit, mitigation or replacement of highways, roadways, or other infrastructure facilities that create barriers to mobility, access or economic development. Applicants must submit their applications via Valid Eval at the links below: Community Planning Grants: https://usg.valideval.com/teams/rcn_planning/signup Capital Construction Grants: https://usg.valideval.com/teams/rcn_capitalconstruction/signup Do not submit applications through Grants.gov. Applications must be submitted by : PM Eastern Time on Thursday, September , . Late applications will not be accepted. \n",
            "Accuracy: 21/21 (1.0000)\n",
            "\n",
            "Class: FY  Notice of Funding Opportunity for Small Shipyard Grants: This notice announces the intention of the Maritime Administration to provide grants to small shipyards. Under the Small Shipyard Grant Program, there is currently $,, available for grants for capital and related improvements to qualified shipyard facilities that will be effective in fostering efficiency, competitive operations, and quality ship construction, repair, and reconfiguration. Potential applicants are advised that it is expected, based on experience, that the number of applications will far exceed the funds available and that only a small percentage of applicants will be funded. Applications (SF- and the Addendum) will be accepted online through Grants.gov by  pm on the closing date (February , ). See full text under Related Documents Tab (FY  SSG NOFO) or the MARAD Web Page for detailed submission instructions.\n",
            "Accuracy: 4/4 (1.0000)\n",
            "\n",
            "Class: Safe Streets and Roads for All Funding Opportunity: The purpose of this notice is to solicit applications for Safe Streets and Roads for All (SSA) grants. Funds for the fiscal year (FY)  SSA grant program are to be awarded on a competitive basis to support planning and demonstration activities, as well as projects and strategies to prevent death and serious injury on roads and streets involving all roadway users, including pedestrians; bicyclists; public transportation, personal conveyance, and micromobility users; motorists; and commercial vehicle operators. Applicants must submit their applications via Valid Eval at https://usg.valideval.com/teams/usdot_ssa__implementation/signup for Implementation Grant applicants, and https://usg.valideval.com/teams/usdot_ssa__planning_demo/signup for Planning and Demonstration Grants. Do not submit your applications through Grants.Gov. Applications must be submitted by : PM Eastern Time on Monday, July , . Late applications will not be accepted.\n",
            "Accuracy: 57/58 (0.9828)\n",
            "\n",
            "Class: SMART Grants Notice of Funding: The purpose of this notice is to solicit applications for Strengthening Mobility and Revolutionizing Transportation (SMART) Stage  Planning and Prototyping grants. Funds for the fiscal year (FY)  SMART Grants Program are to be awarded on a competitive basis to conduct demonstration projects focused on advanced smart city or community technologies and systems to improve transportation efficiency and safety. Applications must be submitted via Valid Eval, an online submission proposal system used by USDOT, at https://usg.valideval.com/teams/USDOT_SMART_/signup. USDOT will not accept or review application materials submitted via Grants.gov. \n",
            "Accuracy: 0/5 (0.0000)\n",
            "\n",
            "Class:  Tribal Transportation Program Safety Fund: Eligible projects described in section (a)() are strategies, activities, and projects on a public road that are consistent with a transportation safety plan and that (i) correct or improve a hazardous road location or feature, or (ii) address a highway safety problem. TTPSF emphasizes the development of strategic transportation safety plans using a data-driven process as a means for Tribes to identify transportation safety needs and determine how those needs will be addressed in Tribal communities. FHWA has identified four eligibility categories: transportation safety plans; data assessment, improvement, and analysis activities; systemic roadway departure countermeasures; and infrastructure improvements and other eligible activities as listed in  U.S.C. &#; (a)().\n",
            "Accuracy: 13/13 (1.0000)\n",
            "\n",
            "Class: United States Marine Highway Grants: This funding opportunity solicits applications for fiscal year (FY)  United States Marine Highway Program (USMHP) grants. Funds for FY  USMHP grants will be awarded on a competitive basis to assist in funding eligible projects for the purpose of developing, expanding, or promoting marine highway transportation. This opportunity announces the availability of up to $,, million in funding for grants under this program and establishes selection criteria and application requirements. All USMHP grant recipients must meet all applicable Federal requirements, including domestic content (&#;Buy America&#;) requirements. This program was formerly known as the America&#;s Marine Highway Program\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: INFRA Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families. The Infrastructure for Rebuilding America (INFRA) program is an existing competitive program that will see a more than  percent increase in this year&#;s funding due to the Bipartisan Infrastructure Law. These grants advance the Administration&#;s priorities of rebuilding America&#;s infrastructure and creating jobs by funding highway, multimodal freight and rail projects that position America to win the st century. Projects will improve safety, generate economic benefits, reduce congestion, enhance resiliency, and hold the greatest promise to eliminate supply chain bottlenecks and improve critical freight movements. Last year, DOT received over $ billion in application requests, but could only fund around $. billion of projects. To see last year&#;s INFRA grants, click here. The Bipartisan Infrastructure Law provides approximately $ billion for INFRA over  years, of which approximately $. billion will be made available through this NOFO. \n",
            "Accuracy: 0/4 (0.0000)\n",
            "\n",
            "Class: Mega Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The National Infrastructure Project Assistance (MEGA) program was created in the Bipartisan Infrastructure Law to fund major projects that are too large or complex for traditional funding programs. The program will provide grants on a competitive basis to support multijurisdictional or regional projects of significance that may also cut across multiple modes of transportation. Eligible projects could include highway, bridge, freight, port, passenger rail, and related public transportation projects of national and regional significance. DOT will award approximately  percent of funding to projects greater than $ million in cost, and approximately  percent to projects greater than $ million but less than $ million in cost.For more information about last year&apos;s Mega awards, visit here: https://www.transportation.gov/grants/mega-grant-program/FYawards\n",
            "Accuracy: 0/2 (0.0000)\n",
            "\n",
            "Class: Rural Surface Transportation Grant Progam: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The Rural Surface Transportation Grant Program (RURAL) was created in the Bipartisan Infrastructure Law and will support projects to improve and expand the surface transportation infrastructure in rural areas to increase connectivity, improve the safety and reliability of the movement of people and freight, and generate regional economic growth and improve quality of life. Eligible projects for Rural grants include highway, bridge, and tunnel projects that help improve freight, safety, and provide or increase access to an agricultural, commercial, energy, or transportation facilities that support the economy of a rural area. This year alone, DOT will award up to $ million in grants through the rural program&#;part of the $ billion included in the Bipartisan Infrastructure Law over five years. At least % of rural funding must be awarded in amounts of $ million or more. If you are seeking less than $ million, you are competing for only about $. million nationwide.For more information on last year&apos;s rural awards, see: https://www.transportation.gov/grants/rural--fact-sheets\n",
            "Accuracy: 0/5 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Rail Vehicle Replacement Grant Program (RAIL): The Federal Transit Administration (FTA) announces the opportunity to apply for approximately $ million in Fiscal Year (FY)  Section  State of Good Repair Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected rail vehicle replacement projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to States and local governmental authorities to assist in the funding of capital projects to replace rail rolling stock.\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'FY  Notice of Funding Opportunity: Bipartisan Infrastructure Law (BIL) Airport Terminal Program (ATP) Grants: The Department of Transportation (DOT), Federal Aviation Administration (FAA) announces the opportunity to apply for approximately $ billion in FY  discretionary funds for the Airport Terminal Program (ATP), made available under the Infrastructure Investment and Jobs Act of  (IIJA), Pub. L. -, herein referred to as the Bipartisan Infrastructure Law (BIL). The purpose of the ATP is to make annual grants available to eligible airports for airport terminal and airport-owned Airport Traffic Control Towers development projects that address the aging infrastructure of our nation&#;s airports. In addition, ATP grants will align with DOT&#;s Strategic Framework FY- at https://www.transportation.gov/administrations/office-policy/fy--strategic-framework. The FY  ATP will be implemented consistent with law and in alignment with the priorities in Executive Order , Implementation of the Infrastructure Investments and Jobs Act ( FR ), which are to invest efficiently and equitably; promote the competitiveness of the U.S. economy; improve job opportunities by focusing on high labor standards; strengthen infrastructure resilience to all hazards including climate change; and to effectively coordinate with State, local, Tribal, and territorial government partners. ': 0.48,\n",
              " 'FY  Competitive Funding Opportunity: Airport Improvement Program Discretionary Grants: The U.S. Department of Transportation&#;s Federal Aviation Administration (FAA) announces the opportunity to apply for an estimated $. billion in Fiscal Year (FY)  discretionary grants under the Airport Improvement Program (AIP), per  U.S.C. &#; . The FAA awards these annually appropriated discretionary funds through the FAA&#;s long-standing iterative, competitive grant process. Prior to the publication of this Notice of Funding Opportunity (NOFO), the FAA identified eligible applicants in its National Plan of Integrated Airport Systems (NPIAS) and compiled potentially eligible projects through the -year Airports Capital Improvement Plan (ACIP). Both of these processes are described in FAA Order ., &#;Formulation of NPIAS and ACIP,&#; adhering to  U.S.C. &#; , which authorizes discretionary funds. The AIP funds airport capital improvements and rehabilitation projects. All discretionary grant funding is subject to appropriations, statutory requirements, and related program funding availability. ': 1.0,\n",
              " 'Grants for Buses and Bus Facilities Program: The Federal Transit Administration (FTA) announces the availability of approximately $. million of Fiscal Year  funds for buses, bus facilities, and bus equipment. The purpose of the Grants for Buses and Bus Facilities Program is to assist in the financing of buses and bus facilities capital projects, including replacing, rehabilitating, purchasing or leasing buses or related equipment, and rehabilitating, purchasing, constructing or leasing bus-related facilities. ': 1.0,\n",
              " 'Solicitation of Project Proposals for the Low or No Emission (Low-No) Program:  Notice of Funding Opportunity (NOFO): Solicitation of Project Proposals for the Low or No Emission Program (Low-No) Program. The Federal Transit Administration (FTA) announces the availability of $ million of Fiscal Year  funds for the purchase or lease of low or no emission vehicles as well as related equipment or facilities. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time on //. The main purpose of the Low-No Program is to support the transition of the nation&#;s transit fleet to the lowest polluting and most energy efficient transit vehicles. The Low-No Program provides funding to State and local governmental authorities for the purchase or lease of zero-emission and low-emission transit buses, including acquisition, construction, and leasing of required supporting facilities. ': 0.0,\n",
              " 'FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in Fiscal Year (FY)  Section  Urbanized Area Formula Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected passenger ferry projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to designated recipients or eligible direct recipients of Section  funds to assist in the financing of capital projects to support existing passenger ferry service, establish new ferry service, and to repair and modernize ferry boats, terminals, and related facilities and equipment. ': 0.0,\n",
              " 'Pilot Program for Transit-Oriented Development (TOD) Planning: Notice of Funding Opportunity for the Pilot Program for Transit-Oriented Development (TOD) Planning. The Federal Transit Administration (FTA) announces the availability of approximately $. million in Pilot Program for TOD Planning funding to support comprehensive planning associated with new fixed guideway and core capacity improvement projects. FTA may award amounts ranging from $, to $,,. The Pilot Program for TOD Planning provides funding to local communities to integrate land use and transportation planning in new fixed guideway and core capacity transit project corridors. As required by statute, any comprehensive planning funded through the pilot program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations. The statute also requires that the planning work be associated with a new fixed guideway or core capacity transit project as defined in Federal transit statute ( USC (a); also see the NOFO for the definitions). Consistent with statutory direction, FTA is seeking comprehensive planning projects covering an entire transit capital project corridor, rather than proposals that involve planning for individual station areas or only a small section of the corridor. To ensure that planning work reflects the needs and aspirations of the local community and results in concrete, specific deliverables and outcomes, FTA is requiring that transit project sponsors partner with entities with land use planning authority in the transit project corridor. ': 0.0,\n",
              " 'Public Transportation on Indian Reservations Program: The Federal Transit Administration (FTA) announces the availability of approximately $ million in funding provided by the Public Transportation on Indian Reservations Program (Tribal Transit Program (TTP)), as authorized by  U.S.C. (c)()(A), as amended by the Fixing America&apos;s Surface Transportation Act (FAST), Public Law - (December , ). The primary purpose of these competitively selected grants is to support planning, capital, and, in limited circumstances, operating assistance for tribal public transit services. Funds distributed to Indian tribes under the TTP should not replace or reduce funds that Indian tribes receive from states through FTA&#;s Section  program. ': 0.16666666666666666,\n",
              " 'INFRA Grants: The Nationally Significant Freight and Highway Projects (INFRA) program provides Federal financial assistance to highway and freight projects of national or regional significance. This notice solicits applications for awards under the program&#;s fiscal year (FY)  funding, subject to the availability of appropriated funds. The Department is focusing the competition on transportation infrastructure projects that support four key objectives, each of which is discussed in greater detail in the Notice of Funding Opportunity: () Supporting economic vitality at the national and regional level; () Leveraging Federal funding to attract non-Federal sources of infrastructure investment; () Deploying innovative technology, encouraging innovative approaches to project delivery, and incentivizing the use of innovative financing; and () Holding grant recipients accountable for their performance. ': 0.0,\n",
              " ' Tribal Transportation Program Safety Fund: A Notice of Funding Availability (NOFA) for Tribal Transportation Program Safety Funds (TTPSF) has been published. This notice announces the availability of $. million for projects in three categories: safety plans; data assessment, improvement, and analysis activities; and infrastructure improvement and other eligible activities as listed in  U.S.C.(a)() The deadline for submissions is //. For complete instructions on preparing and submitting an application for TTPSF, please download the application information and NOFO at https://flh.fhwa.dot.gov/programs/ttp/safety/ttpsf.htm Please submit your application directly to FHWA through the &quot;apply now&quot; button at: https://flh.fhwa.dot.gov/programs/ttp/safety/ttpsf.htm ': 1.0,\n",
              " 'Grants for Buses and Bus Facilities Program: The purpose of the Grants for Buses and Bus Facilities Program is to assist in the financing of buses and bus facilities capital projects, including replacing, rehabilitating, purchasing or leasing buses or related equipment, and rehabilitating, purchasing, constructing or leasing bus-related facilities.': 1.0,\n",
              " 'Low or No Emission Program (Low-No Program): The main purpose of the Low-No Program is to support the transition of the nation&#;s transit fleet to the lowest polluting and most energy efficient transit vehicles. The Low-No Program provides funding to State and local governmental authorities for the purchase or lease of zero-emission and low-emission transit buses, including acquisition, construction, and leasing of required supporting facilities.': 0.5,\n",
              " 'Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in Fiscal Year (FY)  Section  Urbanized Area Formula Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected passenger ferry projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to designated recipients or eligible direct recipients of Section  funds to assist in the financing of capital projects to support existing passenger ferry service, establish new ferry service, and to repair and modernize ferry boats, terminals, and related facilities and equipment. ': 0.0,\n",
              " 'Pilot Program for Transit-Oriented Development (TOD) Planning: Notice of Funding Opportunity for PILOT PROGRAM FOR TRANSIT-ORIENTED DEVELOPMENT (TOD) PLANNING. The Federal Transit Administration (FTA) announces the opportunity to apply for approximately $. million of funding under the Pilot Program for TOD Planning (Catalog of Federal Domestic Assistance #.). As required by federal transit law and subject to funding availability, funds will be awarded competitively to support comprehensive planning associated with new fixed guideway and core capacity improvement projects. FTA anticipates minimum grant awards of $, and maximum grant awards of $,,. The Pilot Program for TOD Planning is intended to fund comprehensive planning that supports economic development, ridership, multimodal connectivity and accessibility, increased transit access for pedestrian and bicycle traffic, and mixed-use development near transit stations. The program also encourages identification of infrastructure needs and engagement with the private sector. Consistent with statutory direction, FTA is seeking comprehensive planning projects covering an entire transit capital project corridor, rather than proposals that involve planning for individual station areas or only a small section of the corridor. To ensure any proposed planning work reflects the needs and aspirations of the local community and results in concrete, specific deliverables and outcomes, transit project sponsors must partner with entities with land use planning authority in the transit project corridor to conduct the planning work.The Pilot Program for TOD Planning helps support FTA&#;s mission of improving public transportation for America&#;s communities by providing funding to local communities to integrate land use and transportation planning around a new fixed guideway or core capacity improvement project. Per statute, any comprehensive planning funded through the program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations.': 0.0,\n",
              " 'FY  Competitive Funding Opportunity: Public Transportation on Indian Reservations Program: The Federal Transit Administration (FTA) announces the availability of approximately $ million in funding provided by the Public Transportation on Indian Reservations Program (Tribal Transit Program (TTP)), as authorized by  U.S.C. (c)()(A), as amended by the Fixing America&apos;s Surface Transportation Act (FAST), Public Law - (December , ). This is a national solicitation for project proposals and includes the selection criteria and program eligibility information for Fiscal Year  projects. The primary purpose of these competitively selected grants is to support planning, capital, and, in limited circumstances, operating assistance for tribal public transit services. Funds distributed to Indian tribes under the TTP should not replace or reduce funds that Indian tribes receive from states through FTA&#;s Section  program.': 0.0,\n",
              " '*INFRA Grants: The U.S. Department of Transportation (USDOT) is seeking applicants for the FY  round of the Infrastructure for Rebuilding America (INFRA) discretionary grant program to fund transportation projects of national and regional significance that result in good-paying jobs, improve safety, apply transformative technology, and explicitly address climate change and racial equity. The funding available for this year&#;s grants totals approximately $ million.The Department recognizes the role that infrastructure investment plays in economic development and job creation, and the added urgency of this funding at time when the COVID- pandemic has put stress on state and local budgets.For the first time, the USDOT seeks INFRA projects that address climate change and environmental justice. Projects will be evaluated on whether they were planned as part of a comprehensive strategy to address climate change, or whether they support strategies to reduce greenhouse gas emissions such as deploying zero-emission-vehicle infrastructure or encouraging modal shift and a reduction in vehicle-miles-traveled. Racial equity will also be considered as a selection criterion, to the extent that project sponsors have completed equity-focused community outreach, and projects are designed to benefit underserved communities. The Department will also consider whether the project is located in a federally designated community development zone, including qualified Opportunity Zones, Empowerment Zones, Promise Zones, or Choice Neighborhoods.USDOT seeks projects that apply innovative technology, delivery, or financing methods with proven outcomes to deliver projects in a cost effective manner. The Department will make awards under the INFRA program to both large and small projects. For a large project, the INFRA grant must be at least $ million. For a small project, the grant must be at least $ million. Under statutory requirements,  percent of available funds are reserved for small projects, and the Department must award at least  percent of funding for rural projects. INFRA grants may be used to fund a variety of components of an infrastructure project, however, the Department is specifically focused on projects in which the local sponsor is significantly invested and is positioned to proceed rapidly to construction. Eligible INFRA project costs may include: reconstruction, rehabilitation, acquisition of property (including land related to the project and improvements to the land), environmental mitigation, construction contingencies, equipment acquisition, and operational improvements directly related to system performance. The INFRA NOFO also announces the creation of the &#;INFRA Extra&#; Program, which will identify competitive INFRA applicants who do not receive an INFRA award and authorize them to seek a Transportation Infrastructure Finance and Innovation Act of  (TIFIA) loan up to  percent of their project cost.': 0.0,\n",
              " 'Fiscal Year  Competitive Funding Opportunity; Grants for Buses and Bus Facilities Program: The Federal Transit Administration (FTA) announces the availability of approximately $. million of Fiscal Year  funds for buses, bus facilities, and bus equipment. Synopses and full announcement will be posted on Grants.gov as opportunity FTA---BUS. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time November ,.': 0.0,\n",
              " 'FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  Passenger Ferry Grant Program (Ferry Program) (Federal Assistance Listing #.). Of that amount, $ million is available only for low or zero-emission ferries or ferries using electric battery or fuel cell components and the infrastructure to support such ferries. As required by Federal public transportation law, funds will be awarded competitively to designated recipients or eligible direct recipients of Urbanized Area Formula funds to support capital projects to improve existing passenger ferry service, establish new ferry service, and repair and modernize ferry boats, terminals, and related facilities and equipment.': 0.0,\n",
              " ' Competitive Funding Opportunity: Pilot Program for Transit-Oriented Development (TOD) Planning: The Pilot Program for TOD Planning helps support FTA&#;s mission of improving public transportation for America&#;s communities by providing funding to local communities to integrate land use and transportation planning around a new fixed guideway or core capacity improvement project. Per statute, any comprehensive planning funded through the program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations. Additional information is available at: https://www.transit.dot.gov/TODPilot.Notice of Funding Opportunity (NOFO): Notice of Funding Opportunity for PILOT PROGRAM FOR TRANSIT-ORIENTED DEVELOPMENT (TOD) PLANNING. The Federal Transit Administration (FTA) announces the opportunity to apply for $,, of funding under the Pilot Program for TOD Planning (Catalog of Federal Domestic Assistance #.). As required by federal transit law and subject to funding availability, funds will be awarded competitively to support comprehensive planning associated with new fixed guideway and core capacity improvement projects. FTA anticipates minimum grant awards of $, and maximum grant awards of $,,. Synopses and full announcement are posted on Grants.gov as opportunity FTA---TPE-TODP. The Pilot Program for TOD Planning is intended to fund comprehensive planning that supports economic development, ridership, multimodal connectivity and accessibility, increased transit access for pedestrian and bicycle traffic, and mixed-use development near transit stations. The program also encourages identification of infrastructure needs and engagement with the private sector.Consistent with statutory direction, FTA is seeking comprehensive planning projects covering an entire transit capital project corridor, rather than proposals that involve planning for individual station areas or only a small section of the corridor. To ensure any proposed planning work reflects the needs and aspirations of the local community and results in concrete, specific deliverables and outcomes, transit project sponsors must partner with entities with land use planning authority in the transit project corridor to conduct the planning work.Link and Instructions for attaching the supplemental form to the SF-: All applicants must complete a supplemental form (PDF) specific to the Pilot Program for TOD Planning and attach it to their submission in Grants.gov. See section D of the NOFO for information about additional required application contents.Webinar: FTA will conduct a webinar for prospective applicants after the NOFO is published in the Federal Register. Further information will be posted on FTA&#;s website (https://www.transit.dot.gov/TODPilot) when available. Dates: An applicant must submit a proposal electronically by : p.m. Eastern Daylight Time on June , . Any agency intending to apply should initiate the process of registering on the Grants.gov site immediately to ensure completion of registration before the submission deadline. ': 0.0,\n",
              " 'FY  Competitive Funding Opportunity:  Public Transportation on Indian Reservations Program; Tribal Transit Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants for the Fiscal Year (FY)  Public Transportation on Indian Reservations (Tribal Transit) Program (Federal Assistance Listing: .). As required by Federal public transportation law, funds will be awarded competitively for any purpose eligible under FTA&#;s Formula Grants for Rural Areas Program,  U.S.C. , including planning, capital, and operating assistance for tribal public transit services in rural areas. ': 0.8333333333333334,\n",
              " 'INFRA Grants: The U.S. Department of Transportation (USDOT) is seeking applicants for the FY  round of the Infrastructure for Rebuilding America (INFRA) discretionary grant program to fund transportation projects of national and regional significance that result in good-paying jobs, improve safety, apply transformative technology, and explicitly address climate change and racial equity. The funding available for this year&#;s grants totals approximately $ million.The Department recognizes the role that infrastructure investment plays in economic development and job creation, and the added urgency of this funding at time when the COVID- pandemic has put stress on state and local budgets.For the first time, the USDOT seeks INFRA projects that address climate change and environmental justice. Projects will be evaluated on whether they were planned as part of a comprehensive strategy to address climate change, or whether they support strategies to reduce greenhouse gas emissions such as deploying zero-emission-vehicle infrastructure or encouraging modal shift and a reduction in vehicle-miles-traveled. Racial equity will also be considered as a selection criterion, to the extent that project sponsors have completed equity-focused community outreach, and projects are designed to benefit underserved communities. The Department will also consider whether the project is located in a federally designated community development zone, including qualified Opportunity Zones, Empowerment Zones, Promise Zones, or Choice Neighborhoods.USDOT seeks projects that apply innovative technology, delivery, or financing methods with proven outcomes to deliver projects in a cost effective manner. The Department will make awards under the INFRA program to both large and small projects. For a large project, the INFRA grant must be at least $ million. For a small project, the grant must be at least $ million. Under statutory requirements,  percent of available funds are reserved for small projects, and the Department must award at least  percent of funding for rural projects. INFRA grants may be used to fund a variety of components of an infrastructure project, however, the Department is specifically focused on projects in which the local sponsor is significantly invested and is positioned to proceed rapidly to construction. Eligible INFRA project costs may include: reconstruction, rehabilitation, acquisition of property (including land related to the project and improvements to the land), environmental mitigation, construction contingencies, equipment acquisition, and operational improvements directly related to system performance. The INFRA NOFO also announces the creation of the &#;INFRA Extra&#; Program, which will identify competitive INFRA applicants who do not receive an INFRA award and authorize them to seek a Transportation Infrastructure Finance and Innovation Act of  (TIFIA) loan up to  percent of their project cost.': 0.0,\n",
              " 'FY Tribal Transportation Program Safety Fund (TTPSF): A Notice of Funding Opportunity (NOFO) for Tribal Transportation Program Safety Funds (TTPSF) has been published. This notice announces the availability of approximately $. million for projects in four categories: safety plans; data assessment, improvement, and analysis activities; systemic roadway departure countermeasures; and infrastructure improvement and other eligible activities as listed in  U.S.C.(a)(). The goal of these projects is to reduce fatalities and serious injuries from transportation incidents, such as motor vehicle crashes. For complete instructions on preparing and submitting an application for TTPSF, please download the application, application information, systemic roadway departure risk assessment form (if applicable) and NOFO at https://highways.dot.gov/federal-lands/programs-tribal/safety/funds. Applications will NOT be submitted on Grants.gov. Instead, please use the &quot;apply now&quot; button on the program web page.': 0.0,\n",
              " 'Advanced Transportation Technologies and  Innovative Mobility Deployment (ATTIMD) Program: AMENDMENT  (//): This amendment is for the purpose of correcting a typo contained in Table D. on page  of . This table, contained in Section D. for CONTENT AND FORM OF APPLICATION SUBMISSION, erroneously omitted the Project Outcome Criteria from that table&#;s submission requirements for Volume . This amendment also revises the verbiage included on page  of the NOFO to clarify that all resumes for key personnel should be included in item f) Appendix &#; R&#;sum&#;s for Key Personnel. Please note that Resumes do not count against the page limitations for Volume . Please refer to Amendment  in the &#;Related Documents&#; tab for more information. -- END OF AMENDMENT  -- AMENDMENT  (//): This amendment adds the weblink to the recording of the October , , informational webinar for this opportunity. The weblink is included in the attachments page for this opportunity as well as below: LINK: https://usdot.zoomgov.com/rec/share/eQNNRGISy-UxfqGpSDzINRbzvcdkAjDdoulZvwRCt-sARp.kdAJIUFsFZZEpf PASSCODE: c@X*&amp;@ -- END OF AMENDMENT  -- AMENDMENT  (//): This amendment adds the registration information for the October , , informational webinar for this opportunity. Please refer to the &quot;Related Documents&quot; tab for more information. -- END OF AMENDMENT  -- BASE NOFO (//): The Advanced Transportation Technologies and Innovative Mobility Deployment Program (ATTIMD), established in Section (c)() of title , United States Code (U.S.C.), also known as the Advanced Transportation Technology and Innovation (ATTAIN) Program, directs FHWA to award grants to eligible entities to deploy, install, and operate advanced transportation technologies to improve safety, mobility, efficiency, system performance, intermodal connectivity, and infrastructure return on investment. FHWA intends for these model technology deployments to help demonstrate how emerging transportation technologies, data, and their applications can be effectively deployed and integrated with existing systems to provide access to essential services and other destinations. This also includes efforts to increase connectivity to employment, education, services, and other opportunities; support workforce development; or contribute to increased mobility, particularly for persons with visible and hidden disabilities and elderly individuals. The Assistance Listing Number for this opportunity is . &#; Highway Research and Development. The Federal Highway Administration (FHWA) hereby requests applications to result in awards to eligible entities to deploy, install, and operate advanced transportation technologies to improve safety, mobility, efficiency, system performance, intermodal connectivity, and infrastructure return on investment. These model deployments are expected to provide benefits in the form of: reduced traffic-related fatalities and injuries; reduced traffic congestion and improved travel time reliability; reduced transportation-related emissions; optimized multimodal system performance; improved access to transportation alternatives, including for underserved populations; improved integration of payment systems; public access to real-time integrated traffic, transit, and multimodal transportation information to make informed travel decisions; cost savings to transportation agencies, businesses, and the traveling public; or other benefits to transportation users and the general public. This competitive ATTAIN Program will promote the use of innovative transportation solutions. The deployment of these technologies will provide Congress and FHWA with valuable real-life data and feedback to inform future decision-making. Cost Sharing: Cost-sharing or matching is required, with the maximum Federal share being  percent; hence, this NOFO requires a minimum non-Federal cost share of  percent. Cost sharing or matching means the portion of project costs not paid by Federal funds. NOTE: FHWA previously issued six NOFOs for the Advanced Transportation and Congestion Management Technologies Deployment (ATCMTD) Grant Program before the program was amended and renamed to the Advanced Transportation Technologies and Innovative Mobility Deployment Program by Section  of the Infrastructure Investment and Jobs Act (IIJA) (Public Law -, also known as the &#;Bipartisan Infrastructure Law&#; (BIL)).': 0.0,\n",
              " 'FY  and FY  Competitive Funding Opportunity: Competitive Grants for Rail Vehicle Replacement Program (Rail Program): The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in Fiscal Year (FY)  and  Section  State of Good Repair Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected rail vehicle replacement projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to States and local governmental authorities to assist in the funding of capital projects to replace rail rolling stock.': 0.0,\n",
              " 'Bridge Investment Program - Planning, Bridge Projects, and Large Bridge Projects: This Notice of Funding Opportunity (NOFO) is to solicit applications for three categories of Bridge Investment Program (BIP) funding opportunities: () Planning; () Bridge Projects (projects with eligible costs less than $ million); and () Large Bridge Projects (projects with eligible costs greater than $ million). Eligible applicants may submit applications for any of the three funding categories, but each category has distinct eligibility and selection criteria and application deadlines.These funds will be awarded on a competitive basis for planning, feasibility analysis, and revenue forecasting associated with the development of a project that would subsequently be eligible to apply for BIP funding under either the Bridge Projects or Large Bridge Projects funding categories.A total of $. billion in  BIP funds are available for the Bridge Projects and Large Bridge Projects funding opportunities. These funds will be awarded on a competitive basis for bridge replacement, rehabilitation, preservation, and protection projects that: () improve the safety, efficiency, and reliability of the movement of people and freight over bridges; and () improve the condition of bridges in the United States by reducing (a) the number of bridges, and total person miles traveled over bridges, that are in poor condition or that are in fair condition and at risk of falling into poor condition within the next three years, or (b) the number of bridges, and total person miles traveled over bridges, that do not meet current geometric design standards or cannot meet the load and traffic requirements typical of the regional transportation network. In addition, Large Bridge Projects that receive a BIP award of not less than $ million are eligible for multiyear grants, in which DOT can award available funds to a project over the course of several years in accordance with an agreement and in alignment with its schedule. In selecting Bridge Projects and Large Bridge Projects, FHWA will consider the extent to which BIP funds leverage non-Federal contributions from sponsors and stakeholders involved in the planning, design, and construction of eligible projects. ': 0.0,\n",
              " 'FY  Competitive Funding Opportunity; Low or No Emission Grant Program: Low-No ProgramThe Federal Transit Administration (FTA) announces the opportunity to apply for approximately $. billion in competitive grants under the fiscal year (FY)  Low or No Emission Grant Program (Low-No Program) (Federal Assistance Listing: .) and approximately $ million in FY  funds under the Grants for Buses and Bus Facilities Program (Buses and Bus Facilities Program) (Federal Assistance Listing .), subject to availability of appropriated funding. Synopses and full announcement will be posted on Grants.gov as funding opportunity ID FTA--TPM for Low-No applications and FTA--TPM for Buses and Bus Facilities applications. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time May , .': 1.0,\n",
              " 'FY  Competitive Funding Opportunity: All Stations Accessibility Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  All Stations Accessibility Program (ASAP) (Federal Assistance Listing #.). As required by Federal public transportation law, funds will be awarded competitively to eligible designated recipients that operate or allocate funds to inaccessible pre-ADA&#;or &#;legacy&#; &#; rail fixed guideway public transportation systems, and States (including territories and Washington, D.C.) and local governmental entities that operate or financially support legacy rail fixed guideway public transportation systems and corresponding legacy stations/facilities for capital projects to repair, improve, modify, retrofit, or relocate infrastructure of stations or facilities for passenger use, including load-bearing members that are an essential part of the structural frame; or () for planning projects to develop or modify a plan for pursuing public transportation accessibility projects, assessments of accessibility, or assessments of planned modifications to stations or facilities for passenger use projects; or programs of projects in an eligible area.': 1.0,\n",
              " 'FY  Competitive Funding Opportunity: Buses and Bus Facilities Competitive Program: Bus and Bus Facilities ProgramThe Federal Transit Administration (FTA) announces the opportunity to apply for approximately $. billion in competitive grants under the fiscal year (FY)  Low or No Emission Grant Program (Low-No Program) (Federal Assistance Listing: .) and approximately $ million in FY  funds under the Grants for Buses and Bus Facilities Program (Buses and Bus Facilities Program) (Federal Assistance Listing .), subject to availability of appropriated funding. Synopses and full announcement will be posted on Grants.gov as funding opportunity ID FTA--TPM for Low-No applications and FTA--TPM for Buses and Bus Facilities applications. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time May , ': 0.0,\n",
              " 'FY  Competitive Funding Opportunity: Electric or Low-Emitting Ferry Pilot Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  Electric or Low-Emitting Ferry Pilot Program (ELEF) (Federal Assistance Listing #.). ELEF makes funding available competitively to assist in the financing of capital projects for the purchase of electric or low-emitting ferry vessels that reduce emissions by using alternative fuels or on-board energy storage systems and related charging infrastructure to reduce emissions or produce zero onboard emissions under normal operation.': 0.0,\n",
              " 'FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $. million in competitive grants under the Fiscal Year (FY)  Passenger Ferry Grant Program (PFG) (Federal Assistance Listing #.). Of that amount, $. million is available only for low or zero-emission ferries or ferries using electric battery or fuel cell components and the infrastructure to support such ferries. As required by Federal public transportation law, funds will be awarded competitively to designated recipients or eligible direct recipients of Urbanized Area Formula funds to support capital projects to improve existing passenger ferry service, establish new ferry service, and repair and modernize ferry boats, terminals, and related facilities and equipment. ': 0.0,\n",
              " 'FY  Pilot Program for Transit-Oriented Development (TOD) Planning: The Pilot Program for TOD Planning helps support FTA&#;s mission of improving public transportation for America&#;s communities by providing funding to local communities to integrate land use and transportation planning around a new fixed guideway or core capacity improvement project. Per statute, any comprehensive or site specific planning funded through the program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations.': 0.0,\n",
              " 'FY  Competitive Funding Opportunity: Public Transportation on Indian Reservations Program; Tribal Transit Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $. million in competitive grants for the Fiscal Year (FY)  Public Transportation on Indian Reservations (Tribal Transit) Program. As required by Federal public transportation law, funds will be awarded competitively for any purpose eligible under FTA&#;s Formula Grants for Rural Areas Program, including planning, capital, and operating assistance for tribal public transit services in rural areas. FTA may award additional funding that is made available to the program prior to the announcement of project selections.': 0.0,\n",
              " 'FY Consolidated Rail Infrastructure and Safety Improvements Grant Program: This program funds projects that improve the safety, efficiency, and reliability of intercity passenger and freight rail.': 0.8,\n",
              " 'FY Corridor Identification and Development Grant Program: This program facilitates the development of intercity passenger rail corridors.': 1.0,\n",
              " 'FY Federal-State Partnership for Intercity Passenger Rail Grant Program for projects not located on the Northeast Corridor: This program funds capital projects that reduce the state of good repair backlog, improve performance, or expand or establish new intercity passenger rail service.': 0.0,\n",
              " 'Fiscal Year  National Culvert Removal, Replacement, and Restoration Grant Program (Culvert AOP Program): -- ORIGINAL NOFO POSTING - // -- The primary goal of the Culvert AOP Program is to improve or restore anadromous fish passage through the replacement, removal, repair, or improvement of culverts or weirs. The grant program prioritizes projects that would improve fish passage for: (A) anadromous fish stocks listed as an endangered species or a threatened species under section  of the Endangered Species Act of  ( U.S.C. &#; ); (B) anadromous fish stocks identified by the National Marine Fisheries Service (NMFS) or the U.S. Fish and Wildlife Service (USFWS) that could reasonably become listed as an endangered species or a threatened species under that section; (C) anadromous fish stocks identified by the NMFS or the USFWS as prey for endangered species, threatened species, or protected species, including Southern resident orcas (Orcinus orca); or (D) anadromous fish stocks identified by the NMFS or the USFWS as climate resilient stocks ( U.S.C. &#; (e)()). The program also prioritizes projects that would open up more than  meters of upstream (anadromous) habitat before the end of the natural habitat ( U.S.C. &#; (e)()). ': 1.0,\n",
              " 'FY  National Infrastructure Investments: Agency Name: Department of TransportationDescription: The Infrastructure Investment and Jobs Act (Pub. L. -, November , , &#;Bipartisan Infrastructure Law,&#; or &#;BIL&#;) authorized and appropriated $. billion to be awarded by the Department of Transportation (&#;DOT&#;) for FY  for Local and Regional Project Assistance Program Grants under National Infrastructure Investments. This Notice of Funding Opportunity (NOFO) solicits applications for projects to be funded under the Local and Regional Project Assistance Program, known as the RAISE Grants program, including any additional funding appropriated for the RAISE Grants program under the FY  Appropriations Act. On March , , the Consolidated Appropriations Act,  (Pub. L. -, &#;FY  Appropriations Act&#;) appropriated an additional $ million for the FY  RAISE Grant Program. Therefore, a total of $. billion in funding is now available for the FY  RAISE Grant Program. Applicants should note that the two funding streams (BIL funding and FY  Appropriations Act funding) have slightly different funding restrictions and requirements that may affect competitiveness. Please see the Notice of Funding Opportunity Amendment No. for details.As with previous rounds, funds for the FY  RAISE Transportation program are to be awarded on a competitive basis for projects that will have a significant local or regional impact.': 0.0,\n",
              " 'FY  Natural Gas Distribution Infrastructure Safety and Modernization Grant: FY  Natural Gas Distribution Infrastructure Safety and Modernization Grant': 1.0,\n",
              " 'The Infrastructure Investment and Jobs Act (IIJA) Notice of Funding Opportunity for America&#;s Marine Highway Program: This notice announces the availability of funding for grants and establishes selection criteria and application requirements for the America&#;s Marine Highway Program (&#;AMHP&#;). The purpose of this program is to make grants available to previously designated Marine Highway Projects that support the development and expansion of documented vessels or port and landside infrastructure. The Department also seeks eligible grant projects that will strengthen American supply chains. The U.S. Department of Transportation (&#;DOT&#; or &#;Department&#;) will award Marine Highway Grants to implement projects or components of projects previously designated by the Secretary of Transportation (&#;Secretary&#;) under the AMHP. Only Marine Highway Projects the Secretary designates before the Notice of Funding Opportunity (&#;NOFO&#;) closing date are eligible for funding as described in this notice. TIMING OF GRANT APPLICATIONS: Applications must be received by the Maritime Administration (&#;MARAD&#;) by : p.m. E.D.T. on April , . ADDRESSES: Grant applications must be submitted electronically using Grants.gov (https://www.grants.gov). Please be aware that you must complete the Grants.gov registration process before submitting your application and that the registration process usually takes  to  weeks to complete. Applicants are strongly encouraged to make submissions in advance of the deadline. FOR FURTHER INFORMATION CONTACT: Fred Jones, Office of Ports &amp; Waterways Planning, Room W&#;, Maritime Administration, U.S. Department of Transportation,  New Jersey Avenue S.E., Washington, D.C. , phone --, or email Fred.Jones@dot.gov. Persons who use a telecommunications device for the deaf (TDD) may call the Federal Information Relay Service (FIRS) at --- to contact the above individual during business hours. The FIRS is available twenty-four hours a day, seven days a week, to leave a message or question with the above individual. You will receive a reply during regular business hours.': 0.0,\n",
              " '*FY  Notice of Funding Opportunity: Bipartisan Infrastructure Law (BIL) Airport Terminal Program (ATP) Grants: The Department of Transportation (DOT), Federal Aviation Administration (FAA) announces the opportunity to apply for approximately $ billion in FY  discretionary funds for the Airport Terminal Program (ATP), made available under the Infrastructure Investment and Jobs Act of  (IIJA), Pub. L. -, herein referred to as the Bipartisan Infrastructure Law (BIL). The purpose of the ATP is to make annual grants available to eligible airports for airport terminal and airport-owned Airport Traffic Control Towers development projects that address the aging infrastructure of our nation&#;s airports. In addition, ATP grants will align with DOT&#;s Strategic Framework FY- at https://www.transportation.gov/administrations/office-policy/fy--strategic-framework. The FY  ATP will be implemented consistent with law and in alignment with the priorities in Executive Order , Implementation of the Infrastructure Investments and Jobs Act ( FR ), which are to invest efficiently and equitably; promote the competitiveness of the U.S. economy; improve job opportunities by focusing on high labor standards; strengthen infrastructure resilience to all hazards including climate change; and to effectively coordinate with State, local, Tribal, and territorial government partners. ': 1.0,\n",
              " 'INFRA Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families. The Infrastructure for Rebuilding America (INFRA) program is an existing competitive program that will see a more than  percent increase in this year&#;s funding due to the Bipartisan Infrastructure Law. These grants advance the Administration&#;s priorities of rebuilding America&#;s infrastructure and creating jobs by funding highway, multimodal freight and rail projects that position America to win the st century. Projects will improve safety, generate economic benefits, reduce congestion, enhance resiliency, and hold the greatest promise to eliminate supply chain bottlenecks and improve critical freight movements. Last year, DOT received over $ billion of project applications, but could only fund around $ billion of projects. To see last year&#;s INFRA grants, click here. The Bipartisan Infrastructure Law provides approximately $ billion for INFRA over  years, of which approximately $. billion will be made available through this NOFO. ': 0.0,\n",
              " 'Mega Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The National Infrastructure Project Assistance (MEGA) program was created in the Bipartisan Infrastructure Law to fund major projects that are too large or complex for traditional funding programs. The program will provide grants on a competitive basis to support multijurisdictional or regional projects of significance that may also cut across multiple modes of transportation. Eligible projects could include highway, bridge, freight, port, passenger rail, and public transportation projects of national and regional significance. These could be bridges or tunnels connecting two states; new rail and transit lines that improve equity and reduce emissions; and freight hubs integrating ship, train and truck traffic while improving environmental justice. DOT will award  percent of funding to projects greater than $ million in cost, and  percent to projects greater than $ million but less than $ million in cost. The program will receive up to $ billion this year alone and be able to provide multi-year funding to projects.': 0.0,\n",
              " 'Rural Surface Transportation Grant Program: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The Rural Surface Transportation Grant Program (RURAL) was created in the Bipartisan Infrastructure Law and will support projects to improve and expand the surface transportation infrastructure in rural areas to increase connectivity, improve the safety and reliability of the movement of people and freight, and generate regional economic growth and improve quality of life. Eligible projects for Rural grants include highway, bridge, and tunnel projects that help improve freight, safety, and provide or increase access to an agricultural, commercial, energy, or transportation facilities that support the economy of a rural area. This year alone, DOT will award up to $ million in grants through the rural program&#;part of the $ billion included in the Bipartisan Infrastructure Law over five years.': 0.0,\n",
              " '*FY  Notice of Funding Opportunity: Bipartisan Infrastructure Law (BIL) FAA Contract Tower (FCT) Competitive Grant Program: The Department of Transportation (DOT), Federal Aviation Administration (FAA) announces the opportunity to apply for $ million in FY  Airport Infrastructure Grant funds for the FCT competitive grant program, made available under the Infrastructure Investment and Jobs Act of  (IIJA), Pub. L. -, herein referred to as the Bipartisan Infrastructure Law (BIL). The purpose of the FCT competitive grant program is to make annual grants available to eligible airports for airport-owned airport traffic control tower (ATCT) projects that address the aging infrastructure of the nation&#;s airports. The FAA will consider ATCT projects that sustain, construct, repair, improve, rehabilitate, modernize, replace, or relocate non-approach control towers; or acquire and install air traffic control, communications, and related equipment to be used in those towers; or construct a remote tower certified by the FAA including acquisition and installation of air traffic control, communications, or related equipment. In addition, FT grants will align with DOT&#;s Strategic Framework FY- at https://www.transportation.gov/administrations/office-policy/fy--strategic-framework. The FY  FCT will be implemented consistent with law and in alignment with the priorities in Executive Order , Implementation of the Infrastructure Investments and Jobs Act ( FR ), which are to invest efficiently and equitably; promote the competitiveness of the U.S. economy; improve job opportunities by focusing on high labor standards; strengthen infrastructure resilience to all hazards including climate change; and to effectively coordinate with State, local, Tribal, and territorial government partners. ': 0.125,\n",
              " 'FY Railroad Crossing Elimination Grant Program: The purpose of the RCE Program is to provide funding for highway-rail and pathway-rail grade crossing improvement projects that focus on improving the safety and mobility of people and goods.': 1.0,\n",
              " 'Reconnecting Communities Pilot Discretionary Grant Program: The Reconnecting Communities Pilot Program notice of funding opportunity was amended on September ,  to include technical corrections to the Key Information Table. See updated materials in the &quot;Manage Related Documents&quot; section, as well as on our website at https://www.transportation.gov/grants/reconnecting-communitiesThe purpose of this notice is to solicit applications for Reconnecting Communities Pilot (RCP) Program grants. Funds for the Fiscal Year (FY)  RCP Program are to be awarded on a competitive basis for projects that reconnect communities by removing, retrofitting, or mitigating highways or other transportation facilities that create barriers to community connectivity, including to mobility, access, or economic development.The variety of transformative solutions to knit communities back together can include: high-quality public transportation, infrastructure removal, pedestrian walkways and overpasses, capping over roadways, linear parks and trail connectors, roadway redesigns and complete streets conversions, and main street revitalization. The program will award two types of grants: Planning Grants and Capital Construction Grants. Please consult the full Notice of Funding Opportunity for full details about the application process, requirements, and forms.': 0.0,\n",
              " 'Safe Streets and Roads for All Discretionary Grant Program: THIS NOTICE OF FUNDING OPPORTUNITY WAS AMENDED ON AUGUST , . PLEASE SEE THE GRANTS.GOV PACKAGE UPDATES, AND THE SAFE STREETS AND ROADS FOR ALL (SSA) WEBSITE IN THE LINK PROVIDED BELOW FOR MORE INFORMATION. DEADLINE REMAINS SEPTEMBER , .The purpose of this notice is to solicit applications for SSA grants. Funds for the fiscal year (FY)  SSA grant program are to be awarded on a competitive basis to support planning, infrastructure, behavioral, and operational initiatives to prevent death and serious injury on roads and streets involving all roadway users, including pedestrians; bicyclists; public transportation, personal conveyance, and micromobility users; motorists; and commercial vehicle operators. Those interested in applying for Implementation Grants should use the application materials in PKG; those interested in applying for Action Plan Grants should use the application materials in PKG. See application instructions to determine which grant to apply for in this funding round.': 0.0,\n",
              " 'Strengthening Mobility and Revolutionizing Transportation (SMART) Grants Program: The purpose of this notice is to solicit applications for Strengthening Mobility and Revolutionizing Transportation (SMART) grants. Funds for the fiscal year (FY)  SMART Grants Program are to be awarded on a competitive basis to conduct demonstration projects focused on advanced smart city or community technologies and systems to improve transportation efficiency and safety.': 0.8888888888888888,\n",
              " 'FY- Federal-State Partnership for Intercity Passenger Rail Grant Program for projects located on the Northeast Corridor: This program funds capital projects on the Northeast Corridor that reduce the state of good repair backlog, improve performance, or expand or establish new intercity passenger rail service.': 0.0,\n",
              " 'Fiscal Years - Wildlife Crossings Pilot Program (WCPP): The primary goals of the WCPP are to save lives, prevent serious injuries, and protect motorists and wildlife by reducing WVCs, and improve habitat connectivity for terrestrial and aquatic species. Reduction of wildlife vehicle collisions and improvement of terrestrial and aquatic habitat connectivity are the primary merit criteria that will be used in reviewing applications, and each of the primary merit criteria are of equal importance.': 0.0,\n",
              " 'FY  Competitive Funding Opportunity; Buses and Bus Facilities Program: The Federal Transit Administration (FTA) announces the availability of approximately $ million in competitive grants under the Buses and Bus Facilities Program is to assist in the financing of buses and bus facilities capital projects, including replacing, rehabilitating, purchasing or leasing buses or related equipment, and rehabilitating, purchasing, constructing or leasing bus-related facilities. Synopses and full announcement will be posted on Grants.gov as opportunity FTA---TPM-BUS. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time April , .': 1.0,\n",
              " 'FY  Competitive Funding Opportunity; Low or No Emission Vehicle Program: The Federal Transit Administration (FTA) announces the availability of approximately $. billion in competitive grants under the Low or No Emission Grant Program (Low-No Program) for the purchase or lease of zero-emission and low-emission transit buses, including acquisition, construction, and leasing of required supporting facilities. Synopses and full announcement will be posted on Grants.gov as opportunity FTA---TPM-LWNO. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time April , .': 0.0,\n",
              " 'FY  Competitive Funding Opportunity: Areas of Persistent Poverty Program: To create new opportunities for those experiencing poverty, the U.S. Department of Transportation&#;s Federal Transit Administration (FTA) today announced a new Notice of Funding Opportunity (NOFO) to help improve transit in areas experiencing long-term economic distress. The $ million in competitive grant funding through FTA&#;s Areas of Persistent Poverty (AoPP) Program provides more resources to underserved and disadvantaged communities seeking to expand or improve transit. ': 1.0,\n",
              " 'FY  Competitive Funding Opportunity: Ferry Service for Rural Communities Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  Ferry Service for Rural Communities Program (FSRC) (Federal Assistance Listing #.). FSRC makes funding available competitively to assist in the financing of capital, planning, and operating assistance for eligible ferry services.': 0.0,\n",
              " 'FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $. million in competitive grants under the Fiscal Year (FY)  Passenger Ferry Grant Program (PFG) (Federal Assistance Listing #.). Of that amount, $ million is available only for low or zero-emission ferries or ferries using electric battery or fuel cell components and the infrastructure to support such ferries. As required by Federal public transportation law, funds will be awarded competitively to designated recipients or eligible direct recipients of Urbanized Area Formula funds to support capital projects to improve existing passenger ferry service, establish new ferry service, and repair and modernize ferry boats, terminals, and related facilities and equipment. ': 0.0,\n",
              " 'FY  National Infrastructure Investments: The Infrastructure Investment and Jobs Act of  (&#;Bipartisan Infrastructure Law,&#; or &#;BIL&#;) authorized and appropriated $. billion to be awarded by the Department of Transportation (&#;DOT&#;) for Local and Regional Project Assistance Program Grants under National Infrastructure Investments in FY . This Notice of Funding Opportunity (NOFO) solicits applications for projects funded under the Local and Regional Project Assistance Program, known as the RAISE Grants program. If the FY  Appropriations Act provides additional funding for the RAISE grants program and/or significantly different requirements for National Infrastructure Investment funds, the Department will amend this Notice with guidance on additional requirements.As with previous rounds, funds for the FY  RAISE Transportation program are to be awarded on a competitive basis for projects that will have a significant local or regional impact.': 0.9583333333333334,\n",
              " 'Neighborhood Access and Equity (NAE) Program: The Department of Transportation is combining two major discretionary grant programs, the Reconnecting Communities Pilot (RCP) and Neighborhood Access and Equity (NAE) programs, into one Notice of Funding Opportunity (NOFO). Together, this combined program is known as the Reconnecting Communities and Neighborhoods (RCN) Program. The primary goal of the NAE Program is to assist economically disadvantaged or undeserved communities for planning and construction activities. Funds for the fiscal year (FY)  NAE grant program are to be awarded on a competitive basis to support planning, capital construction, and regional partnership activities that aim to restore community connectivity through the removal, retrofit, mitigation or replacement of highways, roadways, or other infrastructure facilities that create barriers to mobility, access or economic development. Applicants must submit their applications via Valid Eval at the links below: Community Planning Grants: https://usg.valideval.com/teams/rcn_planning/signup Capital Construction Grants: https://usg.valideval.com/teams/rcn_capitalconstruction/signup Do not submit applications through Grants.gov. Applications must be submitted by : PM Eastern Time on Thursday, September , . Late applications will not be accepted. ': 1.0,\n",
              " 'FY  Notice of Funding Opportunity for Small Shipyard Grants: This notice announces the intention of the Maritime Administration to provide grants to small shipyards. Under the Small Shipyard Grant Program, there is currently $,, available for grants for capital and related improvements to qualified shipyard facilities that will be effective in fostering efficiency, competitive operations, and quality ship construction, repair, and reconfiguration. Potential applicants are advised that it is expected, based on experience, that the number of applications will far exceed the funds available and that only a small percentage of applicants will be funded. Applications (SF- and the Addendum) will be accepted online through Grants.gov by  pm on the closing date (February , ). See full text under Related Documents Tab (FY  SSG NOFO) or the MARAD Web Page for detailed submission instructions.': 1.0,\n",
              " 'Safe Streets and Roads for All Funding Opportunity: The purpose of this notice is to solicit applications for Safe Streets and Roads for All (SSA) grants. Funds for the fiscal year (FY)  SSA grant program are to be awarded on a competitive basis to support planning and demonstration activities, as well as projects and strategies to prevent death and serious injury on roads and streets involving all roadway users, including pedestrians; bicyclists; public transportation, personal conveyance, and micromobility users; motorists; and commercial vehicle operators. Applicants must submit their applications via Valid Eval at https://usg.valideval.com/teams/usdot_ssa__implementation/signup for Implementation Grant applicants, and https://usg.valideval.com/teams/usdot_ssa__planning_demo/signup for Planning and Demonstration Grants. Do not submit your applications through Grants.Gov. Applications must be submitted by : PM Eastern Time on Monday, July , . Late applications will not be accepted.': 0.9827586206896551,\n",
              " 'SMART Grants Notice of Funding: The purpose of this notice is to solicit applications for Strengthening Mobility and Revolutionizing Transportation (SMART) Stage  Planning and Prototyping grants. Funds for the fiscal year (FY)  SMART Grants Program are to be awarded on a competitive basis to conduct demonstration projects focused on advanced smart city or community technologies and systems to improve transportation efficiency and safety. Applications must be submitted via Valid Eval, an online submission proposal system used by USDOT, at https://usg.valideval.com/teams/USDOT_SMART_/signup. USDOT will not accept or review application materials submitted via Grants.gov. ': 0.0,\n",
              " ' Tribal Transportation Program Safety Fund: Eligible projects described in section (a)() are strategies, activities, and projects on a public road that are consistent with a transportation safety plan and that (i) correct or improve a hazardous road location or feature, or (ii) address a highway safety problem. TTPSF emphasizes the development of strategic transportation safety plans using a data-driven process as a means for Tribes to identify transportation safety needs and determine how those needs will be addressed in Tribal communities. FHWA has identified four eligibility categories: transportation safety plans; data assessment, improvement, and analysis activities; systemic roadway departure countermeasures; and infrastructure improvements and other eligible activities as listed in  U.S.C. &#; (a)().': 1.0,\n",
              " 'United States Marine Highway Grants: This funding opportunity solicits applications for fiscal year (FY)  United States Marine Highway Program (USMHP) grants. Funds for FY  USMHP grants will be awarded on a competitive basis to assist in funding eligible projects for the purpose of developing, expanding, or promoting marine highway transportation. This opportunity announces the availability of up to $,, million in funding for grants under this program and establishes selection criteria and application requirements. All USMHP grant recipients must meet all applicable Federal requirements, including domestic content (&#;Buy America&#;) requirements. This program was formerly known as the America&#;s Marine Highway Program': 0.0,\n",
              " 'INFRA Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families. The Infrastructure for Rebuilding America (INFRA) program is an existing competitive program that will see a more than  percent increase in this year&#;s funding due to the Bipartisan Infrastructure Law. These grants advance the Administration&#;s priorities of rebuilding America&#;s infrastructure and creating jobs by funding highway, multimodal freight and rail projects that position America to win the st century. Projects will improve safety, generate economic benefits, reduce congestion, enhance resiliency, and hold the greatest promise to eliminate supply chain bottlenecks and improve critical freight movements. Last year, DOT received over $ billion in application requests, but could only fund around $. billion of projects. To see last year&#;s INFRA grants, click here. The Bipartisan Infrastructure Law provides approximately $ billion for INFRA over  years, of which approximately $. billion will be made available through this NOFO. ': 0.0,\n",
              " 'Mega Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The National Infrastructure Project Assistance (MEGA) program was created in the Bipartisan Infrastructure Law to fund major projects that are too large or complex for traditional funding programs. The program will provide grants on a competitive basis to support multijurisdictional or regional projects of significance that may also cut across multiple modes of transportation. Eligible projects could include highway, bridge, freight, port, passenger rail, and related public transportation projects of national and regional significance. DOT will award approximately  percent of funding to projects greater than $ million in cost, and approximately  percent to projects greater than $ million but less than $ million in cost.For more information about last year&apos;s Mega awards, visit here: https://www.transportation.gov/grants/mega-grant-program/FYawards': 0.0,\n",
              " 'Rural Surface Transportation Grant Progam: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The Rural Surface Transportation Grant Program (RURAL) was created in the Bipartisan Infrastructure Law and will support projects to improve and expand the surface transportation infrastructure in rural areas to increase connectivity, improve the safety and reliability of the movement of people and freight, and generate regional economic growth and improve quality of life. Eligible projects for Rural grants include highway, bridge, and tunnel projects that help improve freight, safety, and provide or increase access to an agricultural, commercial, energy, or transportation facilities that support the economy of a rural area. This year alone, DOT will award up to $ million in grants through the rural program&#;part of the $ billion included in the Bipartisan Infrastructure Law over five years. At least % of rural funding must be awarded in amounts of $ million or more. If you are seeking less than $ million, you are competing for only about $. million nationwide.For more information on last year&apos;s rural awards, see: https://www.transportation.gov/grants/rural--fact-sheets': 0.0,\n",
              " 'FY  Competitive Funding Opportunity: Rail Vehicle Replacement Grant Program (RAIL): The Federal Transit Administration (FTA) announces the opportunity to apply for approximately $ million in Fiscal Year (FY)  Section  State of Good Repair Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected rail vehicle replacement projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to States and local governmental authorities to assist in the funding of capital projects to replace rail rolling stock.': 0.0}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('finetuned_BERT_epoch_1.model', map_location=torch.device('cuda')))\n",
        "\n",
        "_, predictions, true_vals, _ = evaluate(dataloader_validation)\n",
        "accuracy_per_class(predictions, true_vals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkev5tWQwYRI",
        "outputId": "30a8fd3c-acb8-4eb1-8272-4631ac9c5061"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class: FY  Notice of Funding Opportunity: Bipartisan Infrastructure Law (BIL) Airport Terminal Program (ATP) Grants: The Department of Transportation (DOT), Federal Aviation Administration (FAA) announces the opportunity to apply for approximately $ billion in FY  discretionary funds for the Airport Terminal Program (ATP), made available under the Infrastructure Investment and Jobs Act of  (IIJA), Pub. L. -, herein referred to as the Bipartisan Infrastructure Law (BIL). The purpose of the ATP is to make annual grants available to eligible airports for airport terminal and airport-owned Airport Traffic Control Towers development projects that address the aging infrastructure of our nation&#;s airports. In addition, ATP grants will align with DOT&#;s Strategic Framework FY- at https://www.transportation.gov/administrations/office-policy/fy--strategic-framework. The FY  ATP will be implemented consistent with law and in alignment with the priorities in Executive Order , Implementation of the Infrastructure Investments and Jobs Act ( FR ), which are to invest efficiently and equitably; promote the competitiveness of the U.S. economy; improve job opportunities by focusing on high labor standards; strengthen infrastructure resilience to all hazards including climate change; and to effectively coordinate with State, local, Tribal, and territorial government partners. \n",
            "Accuracy: 12/25 (0.4800)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Airport Improvement Program Discretionary Grants: The U.S. Department of Transportation&#;s Federal Aviation Administration (FAA) announces the opportunity to apply for an estimated $. billion in Fiscal Year (FY)  discretionary grants under the Airport Improvement Program (AIP), per  U.S.C. &#; . The FAA awards these annually appropriated discretionary funds through the FAA&#;s long-standing iterative, competitive grant process. Prior to the publication of this Notice of Funding Opportunity (NOFO), the FAA identified eligible applicants in its National Plan of Integrated Airport Systems (NPIAS) and compiled potentially eligible projects through the -year Airports Capital Improvement Plan (ACIP). Both of these processes are described in FAA Order ., &#;Formulation of NPIAS and ACIP,&#; adhering to  U.S.C. &#; , which authorizes discretionary funds. The AIP funds airport capital improvements and rehabilitation projects. All discretionary grant funding is subject to appropriations, statutory requirements, and related program funding availability. \n",
            "Accuracy: 163/163 (1.0000)\n",
            "\n",
            "Class: Grants for Buses and Bus Facilities Program: The Federal Transit Administration (FTA) announces the availability of approximately $. million of Fiscal Year  funds for buses, bus facilities, and bus equipment. The purpose of the Grants for Buses and Bus Facilities Program is to assist in the financing of buses and bus facilities capital projects, including replacing, rehabilitating, purchasing or leasing buses or related equipment, and rehabilitating, purchasing, constructing or leasing bus-related facilities. \n",
            "Accuracy: 11/14 (0.7857)\n",
            "\n",
            "Class: Solicitation of Project Proposals for the Low or No Emission (Low-No) Program:  Notice of Funding Opportunity (NOFO): Solicitation of Project Proposals for the Low or No Emission Program (Low-No) Program. The Federal Transit Administration (FTA) announces the availability of $ million of Fiscal Year  funds for the purchase or lease of low or no emission vehicles as well as related equipment or facilities. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time on //. The main purpose of the Low-No Program is to support the transition of the nation&#;s transit fleet to the lowest polluting and most energy efficient transit vehicles. The Low-No Program provides funding to State and local governmental authorities for the purchase or lease of zero-emission and low-emission transit buses, including acquisition, construction, and leasing of required supporting facilities. \n",
            "Accuracy: 0/6 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in Fiscal Year (FY)  Section  Urbanized Area Formula Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected passenger ferry projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to designated recipients or eligible direct recipients of Section  funds to assist in the financing of capital projects to support existing passenger ferry service, establish new ferry service, and to repair and modernize ferry boats, terminals, and related facilities and equipment. \n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: Pilot Program for Transit-Oriented Development (TOD) Planning: Notice of Funding Opportunity for the Pilot Program for Transit-Oriented Development (TOD) Planning. The Federal Transit Administration (FTA) announces the availability of approximately $. million in Pilot Program for TOD Planning funding to support comprehensive planning associated with new fixed guideway and core capacity improvement projects. FTA may award amounts ranging from $, to $,,. The Pilot Program for TOD Planning provides funding to local communities to integrate land use and transportation planning in new fixed guideway and core capacity transit project corridors. As required by statute, any comprehensive planning funded through the pilot program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations. The statute also requires that the planning work be associated with a new fixed guideway or core capacity transit project as defined in Federal transit statute ( USC (a); also see the NOFO for the definitions). Consistent with statutory direction, FTA is seeking comprehensive planning projects covering an entire transit capital project corridor, rather than proposals that involve planning for individual station areas or only a small section of the corridor. To ensure that planning work reflects the needs and aspirations of the local community and results in concrete, specific deliverables and outcomes, FTA is requiring that transit project sponsors partner with entities with land use planning authority in the transit project corridor. \n",
            "Accuracy: 4/4 (1.0000)\n",
            "\n",
            "Class: Public Transportation on Indian Reservations Program: The Federal Transit Administration (FTA) announces the availability of approximately $ million in funding provided by the Public Transportation on Indian Reservations Program (Tribal Transit Program (TTP)), as authorized by  U.S.C. (c)()(A), as amended by the Fixing America&apos;s Surface Transportation Act (FAST), Public Law - (December , ). The primary purpose of these competitively selected grants is to support planning, capital, and, in limited circumstances, operating assistance for tribal public transit services. Funds distributed to Indian tribes under the TTP should not replace or reduce funds that Indian tribes receive from states through FTA&#;s Section  program. \n",
            "Accuracy: 0/6 (0.0000)\n",
            "\n",
            "Class: INFRA Grants: The Nationally Significant Freight and Highway Projects (INFRA) program provides Federal financial assistance to highway and freight projects of national or regional significance. This notice solicits applications for awards under the program&#;s fiscal year (FY)  funding, subject to the availability of appropriated funds. The Department is focusing the competition on transportation infrastructure projects that support four key objectives, each of which is discussed in greater detail in the Notice of Funding Opportunity: () Supporting economic vitality at the national and regional level; () Leveraging Federal funding to attract non-Federal sources of infrastructure investment; () Deploying innovative technology, encouraging innovative approaches to project delivery, and incentivizing the use of innovative financing; and () Holding grant recipients accountable for their performance. \n",
            "Accuracy: 0/3 (0.0000)\n",
            "\n",
            "Class:  Tribal Transportation Program Safety Fund: A Notice of Funding Availability (NOFA) for Tribal Transportation Program Safety Funds (TTPSF) has been published. This notice announces the availability of $. million for projects in three categories: safety plans; data assessment, improvement, and analysis activities; and infrastructure improvement and other eligible activities as listed in  U.S.C.(a)() The deadline for submissions is //. For complete instructions on preparing and submitting an application for TTPSF, please download the application information and NOFO at https://flh.fhwa.dot.gov/programs/ttp/safety/ttpsf.htm Please submit your application directly to FHWA through the &quot;apply now&quot; button at: https://flh.fhwa.dot.gov/programs/ttp/safety/ttpsf.htm \n",
            "Accuracy: 14/14 (1.0000)\n",
            "\n",
            "Class: Grants for Buses and Bus Facilities Program: The purpose of the Grants for Buses and Bus Facilities Program is to assist in the financing of buses and bus facilities capital projects, including replacing, rehabilitating, purchasing or leasing buses or related equipment, and rehabilitating, purchasing, constructing or leasing bus-related facilities.\n",
            "Accuracy: 13/14 (0.9286)\n",
            "\n",
            "Class: Low or No Emission Program (Low-No Program): The main purpose of the Low-No Program is to support the transition of the nation&#;s transit fleet to the lowest polluting and most energy efficient transit vehicles. The Low-No Program provides funding to State and local governmental authorities for the purchase or lease of zero-emission and low-emission transit buses, including acquisition, construction, and leasing of required supporting facilities.\n",
            "Accuracy: 14/14 (1.0000)\n",
            "\n",
            "Class: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in Fiscal Year (FY)  Section  Urbanized Area Formula Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected passenger ferry projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to designated recipients or eligible direct recipients of Section  funds to assist in the financing of capital projects to support existing passenger ferry service, establish new ferry service, and to repair and modernize ferry boats, terminals, and related facilities and equipment. \n",
            "Accuracy: 0/2 (0.0000)\n",
            "\n",
            "Class: Pilot Program for Transit-Oriented Development (TOD) Planning: Notice of Funding Opportunity for PILOT PROGRAM FOR TRANSIT-ORIENTED DEVELOPMENT (TOD) PLANNING. The Federal Transit Administration (FTA) announces the opportunity to apply for approximately $. million of funding under the Pilot Program for TOD Planning (Catalog of Federal Domestic Assistance #.). As required by federal transit law and subject to funding availability, funds will be awarded competitively to support comprehensive planning associated with new fixed guideway and core capacity improvement projects. FTA anticipates minimum grant awards of $, and maximum grant awards of $,,. The Pilot Program for TOD Planning is intended to fund comprehensive planning that supports economic development, ridership, multimodal connectivity and accessibility, increased transit access for pedestrian and bicycle traffic, and mixed-use development near transit stations. The program also encourages identification of infrastructure needs and engagement with the private sector. Consistent with statutory direction, FTA is seeking comprehensive planning projects covering an entire transit capital project corridor, rather than proposals that involve planning for individual station areas or only a small section of the corridor. To ensure any proposed planning work reflects the needs and aspirations of the local community and results in concrete, specific deliverables and outcomes, transit project sponsors must partner with entities with land use planning authority in the transit project corridor to conduct the planning work.The Pilot Program for TOD Planning helps support FTA&#;s mission of improving public transportation for America&#;s communities by providing funding to local communities to integrate land use and transportation planning around a new fixed guideway or core capacity improvement project. Per statute, any comprehensive planning funded through the program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations.\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Public Transportation on Indian Reservations Program: The Federal Transit Administration (FTA) announces the availability of approximately $ million in funding provided by the Public Transportation on Indian Reservations Program (Tribal Transit Program (TTP)), as authorized by  U.S.C. (c)()(A), as amended by the Fixing America&apos;s Surface Transportation Act (FAST), Public Law - (December , ). This is a national solicitation for project proposals and includes the selection criteria and program eligibility information for Fiscal Year  projects. The primary purpose of these competitively selected grants is to support planning, capital, and, in limited circumstances, operating assistance for tribal public transit services. Funds distributed to Indian tribes under the TTP should not replace or reduce funds that Indian tribes receive from states through FTA&#;s Section  program.\n",
            "Accuracy: 0/4 (0.0000)\n",
            "\n",
            "Class: *INFRA Grants: The U.S. Department of Transportation (USDOT) is seeking applicants for the FY  round of the Infrastructure for Rebuilding America (INFRA) discretionary grant program to fund transportation projects of national and regional significance that result in good-paying jobs, improve safety, apply transformative technology, and explicitly address climate change and racial equity. The funding available for this year&#;s grants totals approximately $ million.The Department recognizes the role that infrastructure investment plays in economic development and job creation, and the added urgency of this funding at time when the COVID- pandemic has put stress on state and local budgets.For the first time, the USDOT seeks INFRA projects that address climate change and environmental justice. Projects will be evaluated on whether they were planned as part of a comprehensive strategy to address climate change, or whether they support strategies to reduce greenhouse gas emissions such as deploying zero-emission-vehicle infrastructure or encouraging modal shift and a reduction in vehicle-miles-traveled. Racial equity will also be considered as a selection criterion, to the extent that project sponsors have completed equity-focused community outreach, and projects are designed to benefit underserved communities. The Department will also consider whether the project is located in a federally designated community development zone, including qualified Opportunity Zones, Empowerment Zones, Promise Zones, or Choice Neighborhoods.USDOT seeks projects that apply innovative technology, delivery, or financing methods with proven outcomes to deliver projects in a cost effective manner. The Department will make awards under the INFRA program to both large and small projects. For a large project, the INFRA grant must be at least $ million. For a small project, the grant must be at least $ million. Under statutory requirements,  percent of available funds are reserved for small projects, and the Department must award at least  percent of funding for rural projects. INFRA grants may be used to fund a variety of components of an infrastructure project, however, the Department is specifically focused on projects in which the local sponsor is significantly invested and is positioned to proceed rapidly to construction. Eligible INFRA project costs may include: reconstruction, rehabilitation, acquisition of property (including land related to the project and improvements to the land), environmental mitigation, construction contingencies, equipment acquisition, and operational improvements directly related to system performance. The INFRA NOFO also announces the creation of the &#;INFRA Extra&#; Program, which will identify competitive INFRA applicants who do not receive an INFRA award and authorize them to seek a Transportation Infrastructure Finance and Innovation Act of  (TIFIA) loan up to  percent of their project cost.\n",
            "Accuracy: 1/3 (0.3333)\n",
            "\n",
            "Class: Fiscal Year  Competitive Funding Opportunity; Grants for Buses and Bus Facilities Program: The Federal Transit Administration (FTA) announces the availability of approximately $. million of Fiscal Year  funds for buses, bus facilities, and bus equipment. Synopses and full announcement will be posted on Grants.gov as opportunity FTA---BUS. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time November ,.\n",
            "Accuracy: 0/10 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  Passenger Ferry Grant Program (Ferry Program) (Federal Assistance Listing #.). Of that amount, $ million is available only for low or zero-emission ferries or ferries using electric battery or fuel cell components and the infrastructure to support such ferries. As required by Federal public transportation law, funds will be awarded competitively to designated recipients or eligible direct recipients of Urbanized Area Formula funds to support capital projects to improve existing passenger ferry service, establish new ferry service, and repair and modernize ferry boats, terminals, and related facilities and equipment.\n",
            "Accuracy: 0/2 (0.0000)\n",
            "\n",
            "Class:  Competitive Funding Opportunity: Pilot Program for Transit-Oriented Development (TOD) Planning: The Pilot Program for TOD Planning helps support FTA&#;s mission of improving public transportation for America&#;s communities by providing funding to local communities to integrate land use and transportation planning around a new fixed guideway or core capacity improvement project. Per statute, any comprehensive planning funded through the program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations. Additional information is available at: https://www.transit.dot.gov/TODPilot.Notice of Funding Opportunity (NOFO): Notice of Funding Opportunity for PILOT PROGRAM FOR TRANSIT-ORIENTED DEVELOPMENT (TOD) PLANNING. The Federal Transit Administration (FTA) announces the opportunity to apply for $,, of funding under the Pilot Program for TOD Planning (Catalog of Federal Domestic Assistance #.). As required by federal transit law and subject to funding availability, funds will be awarded competitively to support comprehensive planning associated with new fixed guideway and core capacity improvement projects. FTA anticipates minimum grant awards of $, and maximum grant awards of $,,. Synopses and full announcement are posted on Grants.gov as opportunity FTA---TPE-TODP. The Pilot Program for TOD Planning is intended to fund comprehensive planning that supports economic development, ridership, multimodal connectivity and accessibility, increased transit access for pedestrian and bicycle traffic, and mixed-use development near transit stations. The program also encourages identification of infrastructure needs and engagement with the private sector.Consistent with statutory direction, FTA is seeking comprehensive planning projects covering an entire transit capital project corridor, rather than proposals that involve planning for individual station areas or only a small section of the corridor. To ensure any proposed planning work reflects the needs and aspirations of the local community and results in concrete, specific deliverables and outcomes, transit project sponsors must partner with entities with land use planning authority in the transit project corridor to conduct the planning work.Link and Instructions for attaching the supplemental form to the SF-: All applicants must complete a supplemental form (PDF) specific to the Pilot Program for TOD Planning and attach it to their submission in Grants.gov. See section D of the NOFO for information about additional required application contents.Webinar: FTA will conduct a webinar for prospective applicants after the NOFO is published in the Federal Register. Further information will be posted on FTA&#;s website (https://www.transit.dot.gov/TODPilot) when available. Dates: An applicant must submit a proposal electronically by : p.m. Eastern Daylight Time on June , . Any agency intending to apply should initiate the process of registering on the Grants.gov site immediately to ensure completion of registration before the submission deadline. \n",
            "Accuracy: 0/3 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity:  Public Transportation on Indian Reservations Program; Tribal Transit Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants for the Fiscal Year (FY)  Public Transportation on Indian Reservations (Tribal Transit) Program (Federal Assistance Listing: .). As required by Federal public transportation law, funds will be awarded competitively for any purpose eligible under FTA&#;s Formula Grants for Rural Areas Program,  U.S.C. , including planning, capital, and operating assistance for tribal public transit services in rural areas. \n",
            "Accuracy: 6/6 (1.0000)\n",
            "\n",
            "Class: INFRA Grants: The U.S. Department of Transportation (USDOT) is seeking applicants for the FY  round of the Infrastructure for Rebuilding America (INFRA) discretionary grant program to fund transportation projects of national and regional significance that result in good-paying jobs, improve safety, apply transformative technology, and explicitly address climate change and racial equity. The funding available for this year&#;s grants totals approximately $ million.The Department recognizes the role that infrastructure investment plays in economic development and job creation, and the added urgency of this funding at time when the COVID- pandemic has put stress on state and local budgets.For the first time, the USDOT seeks INFRA projects that address climate change and environmental justice. Projects will be evaluated on whether they were planned as part of a comprehensive strategy to address climate change, or whether they support strategies to reduce greenhouse gas emissions such as deploying zero-emission-vehicle infrastructure or encouraging modal shift and a reduction in vehicle-miles-traveled. Racial equity will also be considered as a selection criterion, to the extent that project sponsors have completed equity-focused community outreach, and projects are designed to benefit underserved communities. The Department will also consider whether the project is located in a federally designated community development zone, including qualified Opportunity Zones, Empowerment Zones, Promise Zones, or Choice Neighborhoods.USDOT seeks projects that apply innovative technology, delivery, or financing methods with proven outcomes to deliver projects in a cost effective manner. The Department will make awards under the INFRA program to both large and small projects. For a large project, the INFRA grant must be at least $ million. For a small project, the grant must be at least $ million. Under statutory requirements,  percent of available funds are reserved for small projects, and the Department must award at least  percent of funding for rural projects. INFRA grants may be used to fund a variety of components of an infrastructure project, however, the Department is specifically focused on projects in which the local sponsor is significantly invested and is positioned to proceed rapidly to construction. Eligible INFRA project costs may include: reconstruction, rehabilitation, acquisition of property (including land related to the project and improvements to the land), environmental mitigation, construction contingencies, equipment acquisition, and operational improvements directly related to system performance. The INFRA NOFO also announces the creation of the &#;INFRA Extra&#; Program, which will identify competitive INFRA applicants who do not receive an INFRA award and authorize them to seek a Transportation Infrastructure Finance and Innovation Act of  (TIFIA) loan up to  percent of their project cost.\n",
            "Accuracy: 1/17 (0.0588)\n",
            "\n",
            "Class: FY Tribal Transportation Program Safety Fund (TTPSF): A Notice of Funding Opportunity (NOFO) for Tribal Transportation Program Safety Funds (TTPSF) has been published. This notice announces the availability of approximately $. million for projects in four categories: safety plans; data assessment, improvement, and analysis activities; systemic roadway departure countermeasures; and infrastructure improvement and other eligible activities as listed in  U.S.C.(a)(). The goal of these projects is to reduce fatalities and serious injuries from transportation incidents, such as motor vehicle crashes. For complete instructions on preparing and submitting an application for TTPSF, please download the application, application information, systemic roadway departure risk assessment form (if applicable) and NOFO at https://highways.dot.gov/federal-lands/programs-tribal/safety/funds. Applications will NOT be submitted on Grants.gov. Instead, please use the &quot;apply now&quot; button on the program web page.\n",
            "Accuracy: 2/9 (0.2222)\n",
            "\n",
            "Class: Advanced Transportation Technologies and  Innovative Mobility Deployment (ATTIMD) Program: AMENDMENT  (//): This amendment is for the purpose of correcting a typo contained in Table D. on page  of . This table, contained in Section D. for CONTENT AND FORM OF APPLICATION SUBMISSION, erroneously omitted the Project Outcome Criteria from that table&#;s submission requirements for Volume . This amendment also revises the verbiage included on page  of the NOFO to clarify that all resumes for key personnel should be included in item f) Appendix &#; R&#;sum&#;s for Key Personnel. Please note that Resumes do not count against the page limitations for Volume . Please refer to Amendment  in the &#;Related Documents&#; tab for more information. -- END OF AMENDMENT  -- AMENDMENT  (//): This amendment adds the weblink to the recording of the October , , informational webinar for this opportunity. The weblink is included in the attachments page for this opportunity as well as below: LINK: https://usdot.zoomgov.com/rec/share/eQNNRGISy-UxfqGpSDzINRbzvcdkAjDdoulZvwRCt-sARp.kdAJIUFsFZZEpf PASSCODE: c@X*&amp;@ -- END OF AMENDMENT  -- AMENDMENT  (//): This amendment adds the registration information for the October , , informational webinar for this opportunity. Please refer to the &quot;Related Documents&quot; tab for more information. -- END OF AMENDMENT  -- BASE NOFO (//): The Advanced Transportation Technologies and Innovative Mobility Deployment Program (ATTIMD), established in Section (c)() of title , United States Code (U.S.C.), also known as the Advanced Transportation Technology and Innovation (ATTAIN) Program, directs FHWA to award grants to eligible entities to deploy, install, and operate advanced transportation technologies to improve safety, mobility, efficiency, system performance, intermodal connectivity, and infrastructure return on investment. FHWA intends for these model technology deployments to help demonstrate how emerging transportation technologies, data, and their applications can be effectively deployed and integrated with existing systems to provide access to essential services and other destinations. This also includes efforts to increase connectivity to employment, education, services, and other opportunities; support workforce development; or contribute to increased mobility, particularly for persons with visible and hidden disabilities and elderly individuals. The Assistance Listing Number for this opportunity is . &#; Highway Research and Development. The Federal Highway Administration (FHWA) hereby requests applications to result in awards to eligible entities to deploy, install, and operate advanced transportation technologies to improve safety, mobility, efficiency, system performance, intermodal connectivity, and infrastructure return on investment. These model deployments are expected to provide benefits in the form of: reduced traffic-related fatalities and injuries; reduced traffic congestion and improved travel time reliability; reduced transportation-related emissions; optimized multimodal system performance; improved access to transportation alternatives, including for underserved populations; improved integration of payment systems; public access to real-time integrated traffic, transit, and multimodal transportation information to make informed travel decisions; cost savings to transportation agencies, businesses, and the traveling public; or other benefits to transportation users and the general public. This competitive ATTAIN Program will promote the use of innovative transportation solutions. The deployment of these technologies will provide Congress and FHWA with valuable real-life data and feedback to inform future decision-making. Cost Sharing: Cost-sharing or matching is required, with the maximum Federal share being  percent; hence, this NOFO requires a minimum non-Federal cost share of  percent. Cost sharing or matching means the portion of project costs not paid by Federal funds. NOTE: FHWA previously issued six NOFOs for the Advanced Transportation and Congestion Management Technologies Deployment (ATCMTD) Grant Program before the program was amended and renamed to the Advanced Transportation Technologies and Innovative Mobility Deployment Program by Section  of the Infrastructure Investment and Jobs Act (IIJA) (Public Law -, also known as the &#;Bipartisan Infrastructure Law&#; (BIL)).\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: FY  and FY  Competitive Funding Opportunity: Competitive Grants for Rail Vehicle Replacement Program (Rail Program): The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in Fiscal Year (FY)  and  Section  State of Good Repair Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected rail vehicle replacement projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to States and local governmental authorities to assist in the funding of capital projects to replace rail rolling stock.\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: Bridge Investment Program - Planning, Bridge Projects, and Large Bridge Projects: This Notice of Funding Opportunity (NOFO) is to solicit applications for three categories of Bridge Investment Program (BIP) funding opportunities: () Planning; () Bridge Projects (projects with eligible costs less than $ million); and () Large Bridge Projects (projects with eligible costs greater than $ million). Eligible applicants may submit applications for any of the three funding categories, but each category has distinct eligibility and selection criteria and application deadlines.These funds will be awarded on a competitive basis for planning, feasibility analysis, and revenue forecasting associated with the development of a project that would subsequently be eligible to apply for BIP funding under either the Bridge Projects or Large Bridge Projects funding categories.A total of $. billion in  BIP funds are available for the Bridge Projects and Large Bridge Projects funding opportunities. These funds will be awarded on a competitive basis for bridge replacement, rehabilitation, preservation, and protection projects that: () improve the safety, efficiency, and reliability of the movement of people and freight over bridges; and () improve the condition of bridges in the United States by reducing (a) the number of bridges, and total person miles traveled over bridges, that are in poor condition or that are in fair condition and at risk of falling into poor condition within the next three years, or (b) the number of bridges, and total person miles traveled over bridges, that do not meet current geometric design standards or cannot meet the load and traffic requirements typical of the regional transportation network. In addition, Large Bridge Projects that receive a BIP award of not less than $ million are eligible for multiyear grants, in which DOT can award available funds to a project over the course of several years in accordance with an agreement and in alignment with its schedule. In selecting Bridge Projects and Large Bridge Projects, FHWA will consider the extent to which BIP funds leverage non-Federal contributions from sponsors and stakeholders involved in the planning, design, and construction of eligible projects. \n",
            "Accuracy: 4/6 (0.6667)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity; Low or No Emission Grant Program: Low-No ProgramThe Federal Transit Administration (FTA) announces the opportunity to apply for approximately $. billion in competitive grants under the fiscal year (FY)  Low or No Emission Grant Program (Low-No Program) (Federal Assistance Listing: .) and approximately $ million in FY  funds under the Grants for Buses and Bus Facilities Program (Buses and Bus Facilities Program) (Federal Assistance Listing .), subject to availability of appropriated funding. Synopses and full announcement will be posted on Grants.gov as funding opportunity ID FTA--TPM for Low-No applications and FTA--TPM for Buses and Bus Facilities applications. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time May , .\n",
            "Accuracy: 14/15 (0.9333)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: All Stations Accessibility Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  All Stations Accessibility Program (ASAP) (Federal Assistance Listing #.). As required by Federal public transportation law, funds will be awarded competitively to eligible designated recipients that operate or allocate funds to inaccessible pre-ADA&#;or &#;legacy&#; &#; rail fixed guideway public transportation systems, and States (including territories and Washington, D.C.) and local governmental entities that operate or financially support legacy rail fixed guideway public transportation systems and corresponding legacy stations/facilities for capital projects to repair, improve, modify, retrofit, or relocate infrastructure of stations or facilities for passenger use, including load-bearing members that are an essential part of the structural frame; or () for planning projects to develop or modify a plan for pursuing public transportation accessibility projects, assessments of accessibility, or assessments of planned modifications to stations or facilities for passenger use projects; or programs of projects in an eligible area.\n",
            "Accuracy: 2/2 (1.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Buses and Bus Facilities Competitive Program: Bus and Bus Facilities ProgramThe Federal Transit Administration (FTA) announces the opportunity to apply for approximately $. billion in competitive grants under the fiscal year (FY)  Low or No Emission Grant Program (Low-No Program) (Federal Assistance Listing: .) and approximately $ million in FY  funds under the Grants for Buses and Bus Facilities Program (Buses and Bus Facilities Program) (Federal Assistance Listing .), subject to availability of appropriated funding. Synopses and full announcement will be posted on Grants.gov as funding opportunity ID FTA--TPM for Low-No applications and FTA--TPM for Buses and Bus Facilities applications. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time May , \n",
            "Accuracy: 0/7 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Electric or Low-Emitting Ferry Pilot Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  Electric or Low-Emitting Ferry Pilot Program (ELEF) (Federal Assistance Listing #.). ELEF makes funding available competitively to assist in the financing of capital projects for the purchase of electric or low-emitting ferry vessels that reduce emissions by using alternative fuels or on-board energy storage systems and related charging infrastructure to reduce emissions or produce zero onboard emissions under normal operation.\n",
            "Accuracy: 2/2 (1.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $. million in competitive grants under the Fiscal Year (FY)  Passenger Ferry Grant Program (PFG) (Federal Assistance Listing #.). Of that amount, $. million is available only for low or zero-emission ferries or ferries using electric battery or fuel cell components and the infrastructure to support such ferries. As required by Federal public transportation law, funds will be awarded competitively to designated recipients or eligible direct recipients of Urbanized Area Formula funds to support capital projects to improve existing passenger ferry service, establish new ferry service, and repair and modernize ferry boats, terminals, and related facilities and equipment. \n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: FY  Pilot Program for Transit-Oriented Development (TOD) Planning: The Pilot Program for TOD Planning helps support FTA&#;s mission of improving public transportation for America&#;s communities by providing funding to local communities to integrate land use and transportation planning around a new fixed guideway or core capacity improvement project. Per statute, any comprehensive or site specific planning funded through the program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations.\n",
            "Accuracy: 0/3 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Public Transportation on Indian Reservations Program; Tribal Transit Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $. million in competitive grants for the Fiscal Year (FY)  Public Transportation on Indian Reservations (Tribal Transit) Program. As required by Federal public transportation law, funds will be awarded competitively for any purpose eligible under FTA&#;s Formula Grants for Rural Areas Program, including planning, capital, and operating assistance for tribal public transit services in rural areas. FTA may award additional funding that is made available to the program prior to the announcement of project selections.\n",
            "Accuracy: 0/4 (0.0000)\n",
            "\n",
            "Class: FY Consolidated Rail Infrastructure and Safety Improvements Grant Program: This program funds projects that improve the safety, efficiency, and reliability of intercity passenger and freight rail.\n",
            "Accuracy: 9/10 (0.9000)\n",
            "\n",
            "Class: FY Corridor Identification and Development Grant Program: This program facilitates the development of intercity passenger rail corridors.\n",
            "Accuracy: 10/10 (1.0000)\n",
            "\n",
            "Class: FY Federal-State Partnership for Intercity Passenger Rail Grant Program for projects not located on the Northeast Corridor: This program funds capital projects that reduce the state of good repair backlog, improve performance, or expand or establish new intercity passenger rail service.\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: Fiscal Year  National Culvert Removal, Replacement, and Restoration Grant Program (Culvert AOP Program): -- ORIGINAL NOFO POSTING - // -- The primary goal of the Culvert AOP Program is to improve or restore anadromous fish passage through the replacement, removal, repair, or improvement of culverts or weirs. The grant program prioritizes projects that would improve fish passage for: (A) anadromous fish stocks listed as an endangered species or a threatened species under section  of the Endangered Species Act of  ( U.S.C. &#; ); (B) anadromous fish stocks identified by the National Marine Fisheries Service (NMFS) or the U.S. Fish and Wildlife Service (USFWS) that could reasonably become listed as an endangered species or a threatened species under that section; (C) anadromous fish stocks identified by the NMFS or the USFWS as prey for endangered species, threatened species, or protected species, including Southern resident orcas (Orcinus orca); or (D) anadromous fish stocks identified by the NMFS or the USFWS as climate resilient stocks ( U.S.C. &#; (e)()). The program also prioritizes projects that would open up more than  meters of upstream (anadromous) habitat before the end of the natural habitat ( U.S.C. &#; (e)()). \n",
            "Accuracy: 9/9 (1.0000)\n",
            "\n",
            "Class: FY  National Infrastructure Investments: Agency Name: Department of TransportationDescription: The Infrastructure Investment and Jobs Act (Pub. L. -, November , , &#;Bipartisan Infrastructure Law,&#; or &#;BIL&#;) authorized and appropriated $. billion to be awarded by the Department of Transportation (&#;DOT&#;) for FY  for Local and Regional Project Assistance Program Grants under National Infrastructure Investments. This Notice of Funding Opportunity (NOFO) solicits applications for projects to be funded under the Local and Regional Project Assistance Program, known as the RAISE Grants program, including any additional funding appropriated for the RAISE Grants program under the FY  Appropriations Act. On March , , the Consolidated Appropriations Act,  (Pub. L. -, &#;FY  Appropriations Act&#;) appropriated an additional $ million for the FY  RAISE Grant Program. Therefore, a total of $. billion in funding is now available for the FY  RAISE Grant Program. Applicants should note that the two funding streams (BIL funding and FY  Appropriations Act funding) have slightly different funding restrictions and requirements that may affect competitiveness. Please see the Notice of Funding Opportunity Amendment No. for details.As with previous rounds, funds for the FY  RAISE Transportation program are to be awarded on a competitive basis for projects that will have a significant local or regional impact.\n",
            "Accuracy: 10/25 (0.4000)\n",
            "\n",
            "Class: FY  Natural Gas Distribution Infrastructure Safety and Modernization Grant: FY  Natural Gas Distribution Infrastructure Safety and Modernization Grant\n",
            "Accuracy: 6/6 (1.0000)\n",
            "\n",
            "Class: The Infrastructure Investment and Jobs Act (IIJA) Notice of Funding Opportunity for America&#;s Marine Highway Program: This notice announces the availability of funding for grants and establishes selection criteria and application requirements for the America&#;s Marine Highway Program (&#;AMHP&#;). The purpose of this program is to make grants available to previously designated Marine Highway Projects that support the development and expansion of documented vessels or port and landside infrastructure. The Department also seeks eligible grant projects that will strengthen American supply chains. The U.S. Department of Transportation (&#;DOT&#; or &#;Department&#;) will award Marine Highway Grants to implement projects or components of projects previously designated by the Secretary of Transportation (&#;Secretary&#;) under the AMHP. Only Marine Highway Projects the Secretary designates before the Notice of Funding Opportunity (&#;NOFO&#;) closing date are eligible for funding as described in this notice. TIMING OF GRANT APPLICATIONS: Applications must be received by the Maritime Administration (&#;MARAD&#;) by : p.m. E.D.T. on April , . ADDRESSES: Grant applications must be submitted electronically using Grants.gov (https://www.grants.gov). Please be aware that you must complete the Grants.gov registration process before submitting your application and that the registration process usually takes  to  weeks to complete. Applicants are strongly encouraged to make submissions in advance of the deadline. FOR FURTHER INFORMATION CONTACT: Fred Jones, Office of Ports &amp; Waterways Planning, Room W&#;, Maritime Administration, U.S. Department of Transportation,  New Jersey Avenue S.E., Washington, D.C. , phone --, or email Fred.Jones@dot.gov. Persons who use a telecommunications device for the deaf (TDD) may call the Federal Information Relay Service (FIRS) at --- to contact the above individual during business hours. The FIRS is available twenty-four hours a day, seven days a week, to leave a message or question with the above individual. You will receive a reply during regular business hours.\n",
            "Accuracy: 1/2 (0.5000)\n",
            "\n",
            "Class: *FY  Notice of Funding Opportunity: Bipartisan Infrastructure Law (BIL) Airport Terminal Program (ATP) Grants: The Department of Transportation (DOT), Federal Aviation Administration (FAA) announces the opportunity to apply for approximately $ billion in FY  discretionary funds for the Airport Terminal Program (ATP), made available under the Infrastructure Investment and Jobs Act of  (IIJA), Pub. L. -, herein referred to as the Bipartisan Infrastructure Law (BIL). The purpose of the ATP is to make annual grants available to eligible airports for airport terminal and airport-owned Airport Traffic Control Towers development projects that address the aging infrastructure of our nation&#;s airports. In addition, ATP grants will align with DOT&#;s Strategic Framework FY- at https://www.transportation.gov/administrations/office-policy/fy--strategic-framework. The FY  ATP will be implemented consistent with law and in alignment with the priorities in Executive Order , Implementation of the Infrastructure Investments and Jobs Act ( FR ), which are to invest efficiently and equitably; promote the competitiveness of the U.S. economy; improve job opportunities by focusing on high labor standards; strengthen infrastructure resilience to all hazards including climate change; and to effectively coordinate with State, local, Tribal, and territorial government partners. \n",
            "Accuracy: 28/28 (1.0000)\n",
            "\n",
            "Class: INFRA Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families. The Infrastructure for Rebuilding America (INFRA) program is an existing competitive program that will see a more than  percent increase in this year&#;s funding due to the Bipartisan Infrastructure Law. These grants advance the Administration&#;s priorities of rebuilding America&#;s infrastructure and creating jobs by funding highway, multimodal freight and rail projects that position America to win the st century. Projects will improve safety, generate economic benefits, reduce congestion, enhance resiliency, and hold the greatest promise to eliminate supply chain bottlenecks and improve critical freight movements. Last year, DOT received over $ billion of project applications, but could only fund around $ billion of projects. To see last year&#;s INFRA grants, click here. The Bipartisan Infrastructure Law provides approximately $ billion for INFRA over  years, of which approximately $. billion will be made available through this NOFO. \n",
            "Accuracy: 0/4 (0.0000)\n",
            "\n",
            "Class: Mega Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The National Infrastructure Project Assistance (MEGA) program was created in the Bipartisan Infrastructure Law to fund major projects that are too large or complex for traditional funding programs. The program will provide grants on a competitive basis to support multijurisdictional or regional projects of significance that may also cut across multiple modes of transportation. Eligible projects could include highway, bridge, freight, port, passenger rail, and public transportation projects of national and regional significance. These could be bridges or tunnels connecting two states; new rail and transit lines that improve equity and reduce emissions; and freight hubs integrating ship, train and truck traffic while improving environmental justice. DOT will award  percent of funding to projects greater than $ million in cost, and  percent to projects greater than $ million but less than $ million in cost. The program will receive up to $ billion this year alone and be able to provide multi-year funding to projects.\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: Rural Surface Transportation Grant Program: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The Rural Surface Transportation Grant Program (RURAL) was created in the Bipartisan Infrastructure Law and will support projects to improve and expand the surface transportation infrastructure in rural areas to increase connectivity, improve the safety and reliability of the movement of people and freight, and generate regional economic growth and improve quality of life. Eligible projects for Rural grants include highway, bridge, and tunnel projects that help improve freight, safety, and provide or increase access to an agricultural, commercial, energy, or transportation facilities that support the economy of a rural area. This year alone, DOT will award up to $ million in grants through the rural program&#;part of the $ billion included in the Bipartisan Infrastructure Law over five years.\n",
            "Accuracy: 0/2 (0.0000)\n",
            "\n",
            "Class: *FY  Notice of Funding Opportunity: Bipartisan Infrastructure Law (BIL) FAA Contract Tower (FCT) Competitive Grant Program: The Department of Transportation (DOT), Federal Aviation Administration (FAA) announces the opportunity to apply for $ million in FY  Airport Infrastructure Grant funds for the FCT competitive grant program, made available under the Infrastructure Investment and Jobs Act of  (IIJA), Pub. L. -, herein referred to as the Bipartisan Infrastructure Law (BIL). The purpose of the FCT competitive grant program is to make annual grants available to eligible airports for airport-owned airport traffic control tower (ATCT) projects that address the aging infrastructure of the nation&#;s airports. The FAA will consider ATCT projects that sustain, construct, repair, improve, rehabilitate, modernize, replace, or relocate non-approach control towers; or acquire and install air traffic control, communications, and related equipment to be used in those towers; or construct a remote tower certified by the FAA including acquisition and installation of air traffic control, communications, or related equipment. In addition, FT grants will align with DOT&#;s Strategic Framework FY- at https://www.transportation.gov/administrations/office-policy/fy--strategic-framework. The FY  FCT will be implemented consistent with law and in alignment with the priorities in Executive Order , Implementation of the Infrastructure Investments and Jobs Act ( FR ), which are to invest efficiently and equitably; promote the competitiveness of the U.S. economy; improve job opportunities by focusing on high labor standards; strengthen infrastructure resilience to all hazards including climate change; and to effectively coordinate with State, local, Tribal, and territorial government partners. \n",
            "Accuracy: 8/8 (1.0000)\n",
            "\n",
            "Class: FY Railroad Crossing Elimination Grant Program: The purpose of the RCE Program is to provide funding for highway-rail and pathway-rail grade crossing improvement projects that focus on improving the safety and mobility of people and goods.\n",
            "Accuracy: 9/9 (1.0000)\n",
            "\n",
            "Class: Reconnecting Communities Pilot Discretionary Grant Program: The Reconnecting Communities Pilot Program notice of funding opportunity was amended on September ,  to include technical corrections to the Key Information Table. See updated materials in the &quot;Manage Related Documents&quot; section, as well as on our website at https://www.transportation.gov/grants/reconnecting-communitiesThe purpose of this notice is to solicit applications for Reconnecting Communities Pilot (RCP) Program grants. Funds for the Fiscal Year (FY)  RCP Program are to be awarded on a competitive basis for projects that reconnect communities by removing, retrofitting, or mitigating highways or other transportation facilities that create barriers to community connectivity, including to mobility, access, or economic development.The variety of transformative solutions to knit communities back together can include: high-quality public transportation, infrastructure removal, pedestrian walkways and overpasses, capping over roadways, linear parks and trail connectors, roadway redesigns and complete streets conversions, and main street revitalization. The program will award two types of grants: Planning Grants and Capital Construction Grants. Please consult the full Notice of Funding Opportunity for full details about the application process, requirements, and forms.\n",
            "Accuracy: 6/7 (0.8571)\n",
            "\n",
            "Class: Safe Streets and Roads for All Discretionary Grant Program: THIS NOTICE OF FUNDING OPPORTUNITY WAS AMENDED ON AUGUST , . PLEASE SEE THE GRANTS.GOV PACKAGE UPDATES, AND THE SAFE STREETS AND ROADS FOR ALL (SSA) WEBSITE IN THE LINK PROVIDED BELOW FOR MORE INFORMATION. DEADLINE REMAINS SEPTEMBER , .The purpose of this notice is to solicit applications for SSA grants. Funds for the fiscal year (FY)  SSA grant program are to be awarded on a competitive basis to support planning, infrastructure, behavioral, and operational initiatives to prevent death and serious injury on roads and streets involving all roadway users, including pedestrians; bicyclists; public transportation, personal conveyance, and micromobility users; motorists; and commercial vehicle operators. Those interested in applying for Implementation Grants should use the application materials in PKG; those interested in applying for Action Plan Grants should use the application materials in PKG. See application instructions to determine which grant to apply for in this funding round.\n",
            "Accuracy: 3/6 (0.5000)\n",
            "\n",
            "Class: Strengthening Mobility and Revolutionizing Transportation (SMART) Grants Program: The purpose of this notice is to solicit applications for Strengthening Mobility and Revolutionizing Transportation (SMART) grants. Funds for the fiscal year (FY)  SMART Grants Program are to be awarded on a competitive basis to conduct demonstration projects focused on advanced smart city or community technologies and systems to improve transportation efficiency and safety.\n",
            "Accuracy: 9/9 (1.0000)\n",
            "\n",
            "Class: FY- Federal-State Partnership for Intercity Passenger Rail Grant Program for projects located on the Northeast Corridor: This program funds capital projects on the Northeast Corridor that reduce the state of good repair backlog, improve performance, or expand or establish new intercity passenger rail service.\n",
            "Accuracy: 0/4 (0.0000)\n",
            "\n",
            "Class: Fiscal Years - Wildlife Crossings Pilot Program (WCPP): The primary goals of the WCPP are to save lives, prevent serious injuries, and protect motorists and wildlife by reducing WVCs, and improve habitat connectivity for terrestrial and aquatic species. Reduction of wildlife vehicle collisions and improvement of terrestrial and aquatic habitat connectivity are the primary merit criteria that will be used in reviewing applications, and each of the primary merit criteria are of equal importance.\n",
            "Accuracy: 0/3 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity; Buses and Bus Facilities Program: The Federal Transit Administration (FTA) announces the availability of approximately $ million in competitive grants under the Buses and Bus Facilities Program is to assist in the financing of buses and bus facilities capital projects, including replacing, rehabilitating, purchasing or leasing buses or related equipment, and rehabilitating, purchasing, constructing or leasing bus-related facilities. Synopses and full announcement will be posted on Grants.gov as opportunity FTA---TPM-BUS. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time April , .\n",
            "Accuracy: 7/7 (1.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity; Low or No Emission Vehicle Program: The Federal Transit Administration (FTA) announces the availability of approximately $. billion in competitive grants under the Low or No Emission Grant Program (Low-No Program) for the purchase or lease of zero-emission and low-emission transit buses, including acquisition, construction, and leasing of required supporting facilities. Synopses and full announcement will be posted on Grants.gov as opportunity FTA---TPM-LWNO. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time April , .\n",
            "Accuracy: 0/12 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Areas of Persistent Poverty Program: To create new opportunities for those experiencing poverty, the U.S. Department of Transportation&#;s Federal Transit Administration (FTA) today announced a new Notice of Funding Opportunity (NOFO) to help improve transit in areas experiencing long-term economic distress. The $ million in competitive grant funding through FTA&#;s Areas of Persistent Poverty (AoPP) Program provides more resources to underserved and disadvantaged communities seeking to expand or improve transit. \n",
            "Accuracy: 7/7 (1.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Ferry Service for Rural Communities Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  Ferry Service for Rural Communities Program (FSRC) (Federal Assistance Listing #.). FSRC makes funding available competitively to assist in the financing of capital, planning, and operating assistance for eligible ferry services.\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $. million in competitive grants under the Fiscal Year (FY)  Passenger Ferry Grant Program (PFG) (Federal Assistance Listing #.). Of that amount, $ million is available only for low or zero-emission ferries or ferries using electric battery or fuel cell components and the infrastructure to support such ferries. As required by Federal public transportation law, funds will be awarded competitively to designated recipients or eligible direct recipients of Urbanized Area Formula funds to support capital projects to improve existing passenger ferry service, establish new ferry service, and repair and modernize ferry boats, terminals, and related facilities and equipment. \n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: FY  National Infrastructure Investments: The Infrastructure Investment and Jobs Act of  (&#;Bipartisan Infrastructure Law,&#; or &#;BIL&#;) authorized and appropriated $. billion to be awarded by the Department of Transportation (&#;DOT&#;) for Local and Regional Project Assistance Program Grants under National Infrastructure Investments in FY . This Notice of Funding Opportunity (NOFO) solicits applications for projects funded under the Local and Regional Project Assistance Program, known as the RAISE Grants program. If the FY  Appropriations Act provides additional funding for the RAISE grants program and/or significantly different requirements for National Infrastructure Investment funds, the Department will amend this Notice with guidance on additional requirements.As with previous rounds, funds for the FY  RAISE Transportation program are to be awarded on a competitive basis for projects that will have a significant local or regional impact.\n",
            "Accuracy: 19/24 (0.7917)\n",
            "\n",
            "Class: Neighborhood Access and Equity (NAE) Program: The Department of Transportation is combining two major discretionary grant programs, the Reconnecting Communities Pilot (RCP) and Neighborhood Access and Equity (NAE) programs, into one Notice of Funding Opportunity (NOFO). Together, this combined program is known as the Reconnecting Communities and Neighborhoods (RCN) Program. The primary goal of the NAE Program is to assist economically disadvantaged or undeserved communities for planning and construction activities. Funds for the fiscal year (FY)  NAE grant program are to be awarded on a competitive basis to support planning, capital construction, and regional partnership activities that aim to restore community connectivity through the removal, retrofit, mitigation or replacement of highways, roadways, or other infrastructure facilities that create barriers to mobility, access or economic development. Applicants must submit their applications via Valid Eval at the links below: Community Planning Grants: https://usg.valideval.com/teams/rcn_planning/signup Capital Construction Grants: https://usg.valideval.com/teams/rcn_capitalconstruction/signup Do not submit applications through Grants.gov. Applications must be submitted by : PM Eastern Time on Thursday, September , . Late applications will not be accepted. \n",
            "Accuracy: 21/21 (1.0000)\n",
            "\n",
            "Class: FY  Notice of Funding Opportunity for Small Shipyard Grants: This notice announces the intention of the Maritime Administration to provide grants to small shipyards. Under the Small Shipyard Grant Program, there is currently $,, available for grants for capital and related improvements to qualified shipyard facilities that will be effective in fostering efficiency, competitive operations, and quality ship construction, repair, and reconfiguration. Potential applicants are advised that it is expected, based on experience, that the number of applications will far exceed the funds available and that only a small percentage of applicants will be funded. Applications (SF- and the Addendum) will be accepted online through Grants.gov by  pm on the closing date (February , ). See full text under Related Documents Tab (FY  SSG NOFO) or the MARAD Web Page for detailed submission instructions.\n",
            "Accuracy: 4/4 (1.0000)\n",
            "\n",
            "Class: Safe Streets and Roads for All Funding Opportunity: The purpose of this notice is to solicit applications for Safe Streets and Roads for All (SSA) grants. Funds for the fiscal year (FY)  SSA grant program are to be awarded on a competitive basis to support planning and demonstration activities, as well as projects and strategies to prevent death and serious injury on roads and streets involving all roadway users, including pedestrians; bicyclists; public transportation, personal conveyance, and micromobility users; motorists; and commercial vehicle operators. Applicants must submit their applications via Valid Eval at https://usg.valideval.com/teams/usdot_ssa__implementation/signup for Implementation Grant applicants, and https://usg.valideval.com/teams/usdot_ssa__planning_demo/signup for Planning and Demonstration Grants. Do not submit your applications through Grants.Gov. Applications must be submitted by : PM Eastern Time on Monday, July , . Late applications will not be accepted.\n",
            "Accuracy: 57/58 (0.9828)\n",
            "\n",
            "Class: SMART Grants Notice of Funding: The purpose of this notice is to solicit applications for Strengthening Mobility and Revolutionizing Transportation (SMART) Stage  Planning and Prototyping grants. Funds for the fiscal year (FY)  SMART Grants Program are to be awarded on a competitive basis to conduct demonstration projects focused on advanced smart city or community technologies and systems to improve transportation efficiency and safety. Applications must be submitted via Valid Eval, an online submission proposal system used by USDOT, at https://usg.valideval.com/teams/USDOT_SMART_/signup. USDOT will not accept or review application materials submitted via Grants.gov. \n",
            "Accuracy: 0/5 (0.0000)\n",
            "\n",
            "Class:  Tribal Transportation Program Safety Fund: Eligible projects described in section (a)() are strategies, activities, and projects on a public road that are consistent with a transportation safety plan and that (i) correct or improve a hazardous road location or feature, or (ii) address a highway safety problem. TTPSF emphasizes the development of strategic transportation safety plans using a data-driven process as a means for Tribes to identify transportation safety needs and determine how those needs will be addressed in Tribal communities. FHWA has identified four eligibility categories: transportation safety plans; data assessment, improvement, and analysis activities; systemic roadway departure countermeasures; and infrastructure improvements and other eligible activities as listed in  U.S.C. &#; (a)().\n",
            "Accuracy: 13/13 (1.0000)\n",
            "\n",
            "Class: United States Marine Highway Grants: This funding opportunity solicits applications for fiscal year (FY)  United States Marine Highway Program (USMHP) grants. Funds for FY  USMHP grants will be awarded on a competitive basis to assist in funding eligible projects for the purpose of developing, expanding, or promoting marine highway transportation. This opportunity announces the availability of up to $,, million in funding for grants under this program and establishes selection criteria and application requirements. All USMHP grant recipients must meet all applicable Federal requirements, including domestic content (&#;Buy America&#;) requirements. This program was formerly known as the America&#;s Marine Highway Program\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: INFRA Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families. The Infrastructure for Rebuilding America (INFRA) program is an existing competitive program that will see a more than  percent increase in this year&#;s funding due to the Bipartisan Infrastructure Law. These grants advance the Administration&#;s priorities of rebuilding America&#;s infrastructure and creating jobs by funding highway, multimodal freight and rail projects that position America to win the st century. Projects will improve safety, generate economic benefits, reduce congestion, enhance resiliency, and hold the greatest promise to eliminate supply chain bottlenecks and improve critical freight movements. Last year, DOT received over $ billion in application requests, but could only fund around $. billion of projects. To see last year&#;s INFRA grants, click here. The Bipartisan Infrastructure Law provides approximately $ billion for INFRA over  years, of which approximately $. billion will be made available through this NOFO. \n",
            "Accuracy: 0/4 (0.0000)\n",
            "\n",
            "Class: Mega Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The National Infrastructure Project Assistance (MEGA) program was created in the Bipartisan Infrastructure Law to fund major projects that are too large or complex for traditional funding programs. The program will provide grants on a competitive basis to support multijurisdictional or regional projects of significance that may also cut across multiple modes of transportation. Eligible projects could include highway, bridge, freight, port, passenger rail, and related public transportation projects of national and regional significance. DOT will award approximately  percent of funding to projects greater than $ million in cost, and approximately  percent to projects greater than $ million but less than $ million in cost.For more information about last year&apos;s Mega awards, visit here: https://www.transportation.gov/grants/mega-grant-program/FYawards\n",
            "Accuracy: 0/2 (0.0000)\n",
            "\n",
            "Class: Rural Surface Transportation Grant Progam: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The Rural Surface Transportation Grant Program (RURAL) was created in the Bipartisan Infrastructure Law and will support projects to improve and expand the surface transportation infrastructure in rural areas to increase connectivity, improve the safety and reliability of the movement of people and freight, and generate regional economic growth and improve quality of life. Eligible projects for Rural grants include highway, bridge, and tunnel projects that help improve freight, safety, and provide or increase access to an agricultural, commercial, energy, or transportation facilities that support the economy of a rural area. This year alone, DOT will award up to $ million in grants through the rural program&#;part of the $ billion included in the Bipartisan Infrastructure Law over five years. At least % of rural funding must be awarded in amounts of $ million or more. If you are seeking less than $ million, you are competing for only about $. million nationwide.For more information on last year&apos;s rural awards, see: https://www.transportation.gov/grants/rural--fact-sheets\n",
            "Accuracy: 0/5 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Rail Vehicle Replacement Grant Program (RAIL): The Federal Transit Administration (FTA) announces the opportunity to apply for approximately $ million in Fiscal Year (FY)  Section  State of Good Repair Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected rail vehicle replacement projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to States and local governmental authorities to assist in the funding of capital projects to replace rail rolling stock.\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'FY  Notice of Funding Opportunity: Bipartisan Infrastructure Law (BIL) Airport Terminal Program (ATP) Grants: The Department of Transportation (DOT), Federal Aviation Administration (FAA) announces the opportunity to apply for approximately $ billion in FY  discretionary funds for the Airport Terminal Program (ATP), made available under the Infrastructure Investment and Jobs Act of  (IIJA), Pub. L. -, herein referred to as the Bipartisan Infrastructure Law (BIL). The purpose of the ATP is to make annual grants available to eligible airports for airport terminal and airport-owned Airport Traffic Control Towers development projects that address the aging infrastructure of our nation&#;s airports. In addition, ATP grants will align with DOT&#;s Strategic Framework FY- at https://www.transportation.gov/administrations/office-policy/fy--strategic-framework. The FY  ATP will be implemented consistent with law and in alignment with the priorities in Executive Order , Implementation of the Infrastructure Investments and Jobs Act ( FR ), which are to invest efficiently and equitably; promote the competitiveness of the U.S. economy; improve job opportunities by focusing on high labor standards; strengthen infrastructure resilience to all hazards including climate change; and to effectively coordinate with State, local, Tribal, and territorial government partners. ': 0.48,\n",
              " 'FY  Competitive Funding Opportunity: Airport Improvement Program Discretionary Grants: The U.S. Department of Transportation&#;s Federal Aviation Administration (FAA) announces the opportunity to apply for an estimated $. billion in Fiscal Year (FY)  discretionary grants under the Airport Improvement Program (AIP), per  U.S.C. &#; . The FAA awards these annually appropriated discretionary funds through the FAA&#;s long-standing iterative, competitive grant process. Prior to the publication of this Notice of Funding Opportunity (NOFO), the FAA identified eligible applicants in its National Plan of Integrated Airport Systems (NPIAS) and compiled potentially eligible projects through the -year Airports Capital Improvement Plan (ACIP). Both of these processes are described in FAA Order ., &#;Formulation of NPIAS and ACIP,&#; adhering to  U.S.C. &#; , which authorizes discretionary funds. The AIP funds airport capital improvements and rehabilitation projects. All discretionary grant funding is subject to appropriations, statutory requirements, and related program funding availability. ': 1.0,\n",
              " 'Grants for Buses and Bus Facilities Program: The Federal Transit Administration (FTA) announces the availability of approximately $. million of Fiscal Year  funds for buses, bus facilities, and bus equipment. The purpose of the Grants for Buses and Bus Facilities Program is to assist in the financing of buses and bus facilities capital projects, including replacing, rehabilitating, purchasing or leasing buses or related equipment, and rehabilitating, purchasing, constructing or leasing bus-related facilities. ': 0.7857142857142857,\n",
              " 'Solicitation of Project Proposals for the Low or No Emission (Low-No) Program:  Notice of Funding Opportunity (NOFO): Solicitation of Project Proposals for the Low or No Emission Program (Low-No) Program. The Federal Transit Administration (FTA) announces the availability of $ million of Fiscal Year  funds for the purchase or lease of low or no emission vehicles as well as related equipment or facilities. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time on //. The main purpose of the Low-No Program is to support the transition of the nation&#;s transit fleet to the lowest polluting and most energy efficient transit vehicles. The Low-No Program provides funding to State and local governmental authorities for the purchase or lease of zero-emission and low-emission transit buses, including acquisition, construction, and leasing of required supporting facilities. ': 0.0,\n",
              " 'FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in Fiscal Year (FY)  Section  Urbanized Area Formula Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected passenger ferry projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to designated recipients or eligible direct recipients of Section  funds to assist in the financing of capital projects to support existing passenger ferry service, establish new ferry service, and to repair and modernize ferry boats, terminals, and related facilities and equipment. ': 0.0,\n",
              " 'Pilot Program for Transit-Oriented Development (TOD) Planning: Notice of Funding Opportunity for the Pilot Program for Transit-Oriented Development (TOD) Planning. The Federal Transit Administration (FTA) announces the availability of approximately $. million in Pilot Program for TOD Planning funding to support comprehensive planning associated with new fixed guideway and core capacity improvement projects. FTA may award amounts ranging from $, to $,,. The Pilot Program for TOD Planning provides funding to local communities to integrate land use and transportation planning in new fixed guideway and core capacity transit project corridors. As required by statute, any comprehensive planning funded through the pilot program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations. The statute also requires that the planning work be associated with a new fixed guideway or core capacity transit project as defined in Federal transit statute ( USC (a); also see the NOFO for the definitions). Consistent with statutory direction, FTA is seeking comprehensive planning projects covering an entire transit capital project corridor, rather than proposals that involve planning for individual station areas or only a small section of the corridor. To ensure that planning work reflects the needs and aspirations of the local community and results in concrete, specific deliverables and outcomes, FTA is requiring that transit project sponsors partner with entities with land use planning authority in the transit project corridor. ': 1.0,\n",
              " 'Public Transportation on Indian Reservations Program: The Federal Transit Administration (FTA) announces the availability of approximately $ million in funding provided by the Public Transportation on Indian Reservations Program (Tribal Transit Program (TTP)), as authorized by  U.S.C. (c)()(A), as amended by the Fixing America&apos;s Surface Transportation Act (FAST), Public Law - (December , ). The primary purpose of these competitively selected grants is to support planning, capital, and, in limited circumstances, operating assistance for tribal public transit services. Funds distributed to Indian tribes under the TTP should not replace or reduce funds that Indian tribes receive from states through FTA&#;s Section  program. ': 0.0,\n",
              " 'INFRA Grants: The Nationally Significant Freight and Highway Projects (INFRA) program provides Federal financial assistance to highway and freight projects of national or regional significance. This notice solicits applications for awards under the program&#;s fiscal year (FY)  funding, subject to the availability of appropriated funds. The Department is focusing the competition on transportation infrastructure projects that support four key objectives, each of which is discussed in greater detail in the Notice of Funding Opportunity: () Supporting economic vitality at the national and regional level; () Leveraging Federal funding to attract non-Federal sources of infrastructure investment; () Deploying innovative technology, encouraging innovative approaches to project delivery, and incentivizing the use of innovative financing; and () Holding grant recipients accountable for their performance. ': 0.0,\n",
              " ' Tribal Transportation Program Safety Fund: A Notice of Funding Availability (NOFA) for Tribal Transportation Program Safety Funds (TTPSF) has been published. This notice announces the availability of $. million for projects in three categories: safety plans; data assessment, improvement, and analysis activities; and infrastructure improvement and other eligible activities as listed in  U.S.C.(a)() The deadline for submissions is //. For complete instructions on preparing and submitting an application for TTPSF, please download the application information and NOFO at https://flh.fhwa.dot.gov/programs/ttp/safety/ttpsf.htm Please submit your application directly to FHWA through the &quot;apply now&quot; button at: https://flh.fhwa.dot.gov/programs/ttp/safety/ttpsf.htm ': 1.0,\n",
              " 'Grants for Buses and Bus Facilities Program: The purpose of the Grants for Buses and Bus Facilities Program is to assist in the financing of buses and bus facilities capital projects, including replacing, rehabilitating, purchasing or leasing buses or related equipment, and rehabilitating, purchasing, constructing or leasing bus-related facilities.': 0.9285714285714286,\n",
              " 'Low or No Emission Program (Low-No Program): The main purpose of the Low-No Program is to support the transition of the nation&#;s transit fleet to the lowest polluting and most energy efficient transit vehicles. The Low-No Program provides funding to State and local governmental authorities for the purchase or lease of zero-emission and low-emission transit buses, including acquisition, construction, and leasing of required supporting facilities.': 1.0,\n",
              " 'Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in Fiscal Year (FY)  Section  Urbanized Area Formula Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected passenger ferry projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to designated recipients or eligible direct recipients of Section  funds to assist in the financing of capital projects to support existing passenger ferry service, establish new ferry service, and to repair and modernize ferry boats, terminals, and related facilities and equipment. ': 0.0,\n",
              " 'Pilot Program for Transit-Oriented Development (TOD) Planning: Notice of Funding Opportunity for PILOT PROGRAM FOR TRANSIT-ORIENTED DEVELOPMENT (TOD) PLANNING. The Federal Transit Administration (FTA) announces the opportunity to apply for approximately $. million of funding under the Pilot Program for TOD Planning (Catalog of Federal Domestic Assistance #.). As required by federal transit law and subject to funding availability, funds will be awarded competitively to support comprehensive planning associated with new fixed guideway and core capacity improvement projects. FTA anticipates minimum grant awards of $, and maximum grant awards of $,,. The Pilot Program for TOD Planning is intended to fund comprehensive planning that supports economic development, ridership, multimodal connectivity and accessibility, increased transit access for pedestrian and bicycle traffic, and mixed-use development near transit stations. The program also encourages identification of infrastructure needs and engagement with the private sector. Consistent with statutory direction, FTA is seeking comprehensive planning projects covering an entire transit capital project corridor, rather than proposals that involve planning for individual station areas or only a small section of the corridor. To ensure any proposed planning work reflects the needs and aspirations of the local community and results in concrete, specific deliverables and outcomes, transit project sponsors must partner with entities with land use planning authority in the transit project corridor to conduct the planning work.The Pilot Program for TOD Planning helps support FTA&#;s mission of improving public transportation for America&#;s communities by providing funding to local communities to integrate land use and transportation planning around a new fixed guideway or core capacity improvement project. Per statute, any comprehensive planning funded through the program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations.': 0.0,\n",
              " 'FY  Competitive Funding Opportunity: Public Transportation on Indian Reservations Program: The Federal Transit Administration (FTA) announces the availability of approximately $ million in funding provided by the Public Transportation on Indian Reservations Program (Tribal Transit Program (TTP)), as authorized by  U.S.C. (c)()(A), as amended by the Fixing America&apos;s Surface Transportation Act (FAST), Public Law - (December , ). This is a national solicitation for project proposals and includes the selection criteria and program eligibility information for Fiscal Year  projects. The primary purpose of these competitively selected grants is to support planning, capital, and, in limited circumstances, operating assistance for tribal public transit services. Funds distributed to Indian tribes under the TTP should not replace or reduce funds that Indian tribes receive from states through FTA&#;s Section  program.': 0.0,\n",
              " '*INFRA Grants: The U.S. Department of Transportation (USDOT) is seeking applicants for the FY  round of the Infrastructure for Rebuilding America (INFRA) discretionary grant program to fund transportation projects of national and regional significance that result in good-paying jobs, improve safety, apply transformative technology, and explicitly address climate change and racial equity. The funding available for this year&#;s grants totals approximately $ million.The Department recognizes the role that infrastructure investment plays in economic development and job creation, and the added urgency of this funding at time when the COVID- pandemic has put stress on state and local budgets.For the first time, the USDOT seeks INFRA projects that address climate change and environmental justice. Projects will be evaluated on whether they were planned as part of a comprehensive strategy to address climate change, or whether they support strategies to reduce greenhouse gas emissions such as deploying zero-emission-vehicle infrastructure or encouraging modal shift and a reduction in vehicle-miles-traveled. Racial equity will also be considered as a selection criterion, to the extent that project sponsors have completed equity-focused community outreach, and projects are designed to benefit underserved communities. The Department will also consider whether the project is located in a federally designated community development zone, including qualified Opportunity Zones, Empowerment Zones, Promise Zones, or Choice Neighborhoods.USDOT seeks projects that apply innovative technology, delivery, or financing methods with proven outcomes to deliver projects in a cost effective manner. The Department will make awards under the INFRA program to both large and small projects. For a large project, the INFRA grant must be at least $ million. For a small project, the grant must be at least $ million. Under statutory requirements,  percent of available funds are reserved for small projects, and the Department must award at least  percent of funding for rural projects. INFRA grants may be used to fund a variety of components of an infrastructure project, however, the Department is specifically focused on projects in which the local sponsor is significantly invested and is positioned to proceed rapidly to construction. Eligible INFRA project costs may include: reconstruction, rehabilitation, acquisition of property (including land related to the project and improvements to the land), environmental mitigation, construction contingencies, equipment acquisition, and operational improvements directly related to system performance. The INFRA NOFO also announces the creation of the &#;INFRA Extra&#; Program, which will identify competitive INFRA applicants who do not receive an INFRA award and authorize them to seek a Transportation Infrastructure Finance and Innovation Act of  (TIFIA) loan up to  percent of their project cost.': 0.3333333333333333,\n",
              " 'Fiscal Year  Competitive Funding Opportunity; Grants for Buses and Bus Facilities Program: The Federal Transit Administration (FTA) announces the availability of approximately $. million of Fiscal Year  funds for buses, bus facilities, and bus equipment. Synopses and full announcement will be posted on Grants.gov as opportunity FTA---BUS. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time November ,.': 0.0,\n",
              " 'FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  Passenger Ferry Grant Program (Ferry Program) (Federal Assistance Listing #.). Of that amount, $ million is available only for low or zero-emission ferries or ferries using electric battery or fuel cell components and the infrastructure to support such ferries. As required by Federal public transportation law, funds will be awarded competitively to designated recipients or eligible direct recipients of Urbanized Area Formula funds to support capital projects to improve existing passenger ferry service, establish new ferry service, and repair and modernize ferry boats, terminals, and related facilities and equipment.': 0.0,\n",
              " ' Competitive Funding Opportunity: Pilot Program for Transit-Oriented Development (TOD) Planning: The Pilot Program for TOD Planning helps support FTA&#;s mission of improving public transportation for America&#;s communities by providing funding to local communities to integrate land use and transportation planning around a new fixed guideway or core capacity improvement project. Per statute, any comprehensive planning funded through the program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations. Additional information is available at: https://www.transit.dot.gov/TODPilot.Notice of Funding Opportunity (NOFO): Notice of Funding Opportunity for PILOT PROGRAM FOR TRANSIT-ORIENTED DEVELOPMENT (TOD) PLANNING. The Federal Transit Administration (FTA) announces the opportunity to apply for $,, of funding under the Pilot Program for TOD Planning (Catalog of Federal Domestic Assistance #.). As required by federal transit law and subject to funding availability, funds will be awarded competitively to support comprehensive planning associated with new fixed guideway and core capacity improvement projects. FTA anticipates minimum grant awards of $, and maximum grant awards of $,,. Synopses and full announcement are posted on Grants.gov as opportunity FTA---TPE-TODP. The Pilot Program for TOD Planning is intended to fund comprehensive planning that supports economic development, ridership, multimodal connectivity and accessibility, increased transit access for pedestrian and bicycle traffic, and mixed-use development near transit stations. The program also encourages identification of infrastructure needs and engagement with the private sector.Consistent with statutory direction, FTA is seeking comprehensive planning projects covering an entire transit capital project corridor, rather than proposals that involve planning for individual station areas or only a small section of the corridor. To ensure any proposed planning work reflects the needs and aspirations of the local community and results in concrete, specific deliverables and outcomes, transit project sponsors must partner with entities with land use planning authority in the transit project corridor to conduct the planning work.Link and Instructions for attaching the supplemental form to the SF-: All applicants must complete a supplemental form (PDF) specific to the Pilot Program for TOD Planning and attach it to their submission in Grants.gov. See section D of the NOFO for information about additional required application contents.Webinar: FTA will conduct a webinar for prospective applicants after the NOFO is published in the Federal Register. Further information will be posted on FTA&#;s website (https://www.transit.dot.gov/TODPilot) when available. Dates: An applicant must submit a proposal electronically by : p.m. Eastern Daylight Time on June , . Any agency intending to apply should initiate the process of registering on the Grants.gov site immediately to ensure completion of registration before the submission deadline. ': 0.0,\n",
              " 'FY  Competitive Funding Opportunity:  Public Transportation on Indian Reservations Program; Tribal Transit Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants for the Fiscal Year (FY)  Public Transportation on Indian Reservations (Tribal Transit) Program (Federal Assistance Listing: .). As required by Federal public transportation law, funds will be awarded competitively for any purpose eligible under FTA&#;s Formula Grants for Rural Areas Program,  U.S.C. , including planning, capital, and operating assistance for tribal public transit services in rural areas. ': 1.0,\n",
              " 'INFRA Grants: The U.S. Department of Transportation (USDOT) is seeking applicants for the FY  round of the Infrastructure for Rebuilding America (INFRA) discretionary grant program to fund transportation projects of national and regional significance that result in good-paying jobs, improve safety, apply transformative technology, and explicitly address climate change and racial equity. The funding available for this year&#;s grants totals approximately $ million.The Department recognizes the role that infrastructure investment plays in economic development and job creation, and the added urgency of this funding at time when the COVID- pandemic has put stress on state and local budgets.For the first time, the USDOT seeks INFRA projects that address climate change and environmental justice. Projects will be evaluated on whether they were planned as part of a comprehensive strategy to address climate change, or whether they support strategies to reduce greenhouse gas emissions such as deploying zero-emission-vehicle infrastructure or encouraging modal shift and a reduction in vehicle-miles-traveled. Racial equity will also be considered as a selection criterion, to the extent that project sponsors have completed equity-focused community outreach, and projects are designed to benefit underserved communities. The Department will also consider whether the project is located in a federally designated community development zone, including qualified Opportunity Zones, Empowerment Zones, Promise Zones, or Choice Neighborhoods.USDOT seeks projects that apply innovative technology, delivery, or financing methods with proven outcomes to deliver projects in a cost effective manner. The Department will make awards under the INFRA program to both large and small projects. For a large project, the INFRA grant must be at least $ million. For a small project, the grant must be at least $ million. Under statutory requirements,  percent of available funds are reserved for small projects, and the Department must award at least  percent of funding for rural projects. INFRA grants may be used to fund a variety of components of an infrastructure project, however, the Department is specifically focused on projects in which the local sponsor is significantly invested and is positioned to proceed rapidly to construction. Eligible INFRA project costs may include: reconstruction, rehabilitation, acquisition of property (including land related to the project and improvements to the land), environmental mitigation, construction contingencies, equipment acquisition, and operational improvements directly related to system performance. The INFRA NOFO also announces the creation of the &#;INFRA Extra&#; Program, which will identify competitive INFRA applicants who do not receive an INFRA award and authorize them to seek a Transportation Infrastructure Finance and Innovation Act of  (TIFIA) loan up to  percent of their project cost.': 0.058823529411764705,\n",
              " 'FY Tribal Transportation Program Safety Fund (TTPSF): A Notice of Funding Opportunity (NOFO) for Tribal Transportation Program Safety Funds (TTPSF) has been published. This notice announces the availability of approximately $. million for projects in four categories: safety plans; data assessment, improvement, and analysis activities; systemic roadway departure countermeasures; and infrastructure improvement and other eligible activities as listed in  U.S.C.(a)(). The goal of these projects is to reduce fatalities and serious injuries from transportation incidents, such as motor vehicle crashes. For complete instructions on preparing and submitting an application for TTPSF, please download the application, application information, systemic roadway departure risk assessment form (if applicable) and NOFO at https://highways.dot.gov/federal-lands/programs-tribal/safety/funds. Applications will NOT be submitted on Grants.gov. Instead, please use the &quot;apply now&quot; button on the program web page.': 0.2222222222222222,\n",
              " 'Advanced Transportation Technologies and  Innovative Mobility Deployment (ATTIMD) Program: AMENDMENT  (//): This amendment is for the purpose of correcting a typo contained in Table D. on page  of . This table, contained in Section D. for CONTENT AND FORM OF APPLICATION SUBMISSION, erroneously omitted the Project Outcome Criteria from that table&#;s submission requirements for Volume . This amendment also revises the verbiage included on page  of the NOFO to clarify that all resumes for key personnel should be included in item f) Appendix &#; R&#;sum&#;s for Key Personnel. Please note that Resumes do not count against the page limitations for Volume . Please refer to Amendment  in the &#;Related Documents&#; tab for more information. -- END OF AMENDMENT  -- AMENDMENT  (//): This amendment adds the weblink to the recording of the October , , informational webinar for this opportunity. The weblink is included in the attachments page for this opportunity as well as below: LINK: https://usdot.zoomgov.com/rec/share/eQNNRGISy-UxfqGpSDzINRbzvcdkAjDdoulZvwRCt-sARp.kdAJIUFsFZZEpf PASSCODE: c@X*&amp;@ -- END OF AMENDMENT  -- AMENDMENT  (//): This amendment adds the registration information for the October , , informational webinar for this opportunity. Please refer to the &quot;Related Documents&quot; tab for more information. -- END OF AMENDMENT  -- BASE NOFO (//): The Advanced Transportation Technologies and Innovative Mobility Deployment Program (ATTIMD), established in Section (c)() of title , United States Code (U.S.C.), also known as the Advanced Transportation Technology and Innovation (ATTAIN) Program, directs FHWA to award grants to eligible entities to deploy, install, and operate advanced transportation technologies to improve safety, mobility, efficiency, system performance, intermodal connectivity, and infrastructure return on investment. FHWA intends for these model technology deployments to help demonstrate how emerging transportation technologies, data, and their applications can be effectively deployed and integrated with existing systems to provide access to essential services and other destinations. This also includes efforts to increase connectivity to employment, education, services, and other opportunities; support workforce development; or contribute to increased mobility, particularly for persons with visible and hidden disabilities and elderly individuals. The Assistance Listing Number for this opportunity is . &#; Highway Research and Development. The Federal Highway Administration (FHWA) hereby requests applications to result in awards to eligible entities to deploy, install, and operate advanced transportation technologies to improve safety, mobility, efficiency, system performance, intermodal connectivity, and infrastructure return on investment. These model deployments are expected to provide benefits in the form of: reduced traffic-related fatalities and injuries; reduced traffic congestion and improved travel time reliability; reduced transportation-related emissions; optimized multimodal system performance; improved access to transportation alternatives, including for underserved populations; improved integration of payment systems; public access to real-time integrated traffic, transit, and multimodal transportation information to make informed travel decisions; cost savings to transportation agencies, businesses, and the traveling public; or other benefits to transportation users and the general public. This competitive ATTAIN Program will promote the use of innovative transportation solutions. The deployment of these technologies will provide Congress and FHWA with valuable real-life data and feedback to inform future decision-making. Cost Sharing: Cost-sharing or matching is required, with the maximum Federal share being  percent; hence, this NOFO requires a minimum non-Federal cost share of  percent. Cost sharing or matching means the portion of project costs not paid by Federal funds. NOTE: FHWA previously issued six NOFOs for the Advanced Transportation and Congestion Management Technologies Deployment (ATCMTD) Grant Program before the program was amended and renamed to the Advanced Transportation Technologies and Innovative Mobility Deployment Program by Section  of the Infrastructure Investment and Jobs Act (IIJA) (Public Law -, also known as the &#;Bipartisan Infrastructure Law&#; (BIL)).': 0.0,\n",
              " 'FY  and FY  Competitive Funding Opportunity: Competitive Grants for Rail Vehicle Replacement Program (Rail Program): The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in Fiscal Year (FY)  and  Section  State of Good Repair Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected rail vehicle replacement projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to States and local governmental authorities to assist in the funding of capital projects to replace rail rolling stock.': 0.0,\n",
              " 'Bridge Investment Program - Planning, Bridge Projects, and Large Bridge Projects: This Notice of Funding Opportunity (NOFO) is to solicit applications for three categories of Bridge Investment Program (BIP) funding opportunities: () Planning; () Bridge Projects (projects with eligible costs less than $ million); and () Large Bridge Projects (projects with eligible costs greater than $ million). Eligible applicants may submit applications for any of the three funding categories, but each category has distinct eligibility and selection criteria and application deadlines.These funds will be awarded on a competitive basis for planning, feasibility analysis, and revenue forecasting associated with the development of a project that would subsequently be eligible to apply for BIP funding under either the Bridge Projects or Large Bridge Projects funding categories.A total of $. billion in  BIP funds are available for the Bridge Projects and Large Bridge Projects funding opportunities. These funds will be awarded on a competitive basis for bridge replacement, rehabilitation, preservation, and protection projects that: () improve the safety, efficiency, and reliability of the movement of people and freight over bridges; and () improve the condition of bridges in the United States by reducing (a) the number of bridges, and total person miles traveled over bridges, that are in poor condition or that are in fair condition and at risk of falling into poor condition within the next three years, or (b) the number of bridges, and total person miles traveled over bridges, that do not meet current geometric design standards or cannot meet the load and traffic requirements typical of the regional transportation network. In addition, Large Bridge Projects that receive a BIP award of not less than $ million are eligible for multiyear grants, in which DOT can award available funds to a project over the course of several years in accordance with an agreement and in alignment with its schedule. In selecting Bridge Projects and Large Bridge Projects, FHWA will consider the extent to which BIP funds leverage non-Federal contributions from sponsors and stakeholders involved in the planning, design, and construction of eligible projects. ': 0.6666666666666666,\n",
              " 'FY  Competitive Funding Opportunity; Low or No Emission Grant Program: Low-No ProgramThe Federal Transit Administration (FTA) announces the opportunity to apply for approximately $. billion in competitive grants under the fiscal year (FY)  Low or No Emission Grant Program (Low-No Program) (Federal Assistance Listing: .) and approximately $ million in FY  funds under the Grants for Buses and Bus Facilities Program (Buses and Bus Facilities Program) (Federal Assistance Listing .), subject to availability of appropriated funding. Synopses and full announcement will be posted on Grants.gov as funding opportunity ID FTA--TPM for Low-No applications and FTA--TPM for Buses and Bus Facilities applications. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time May , .': 0.9333333333333333,\n",
              " 'FY  Competitive Funding Opportunity: All Stations Accessibility Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  All Stations Accessibility Program (ASAP) (Federal Assistance Listing #.). As required by Federal public transportation law, funds will be awarded competitively to eligible designated recipients that operate or allocate funds to inaccessible pre-ADA&#;or &#;legacy&#; &#; rail fixed guideway public transportation systems, and States (including territories and Washington, D.C.) and local governmental entities that operate or financially support legacy rail fixed guideway public transportation systems and corresponding legacy stations/facilities for capital projects to repair, improve, modify, retrofit, or relocate infrastructure of stations or facilities for passenger use, including load-bearing members that are an essential part of the structural frame; or () for planning projects to develop or modify a plan for pursuing public transportation accessibility projects, assessments of accessibility, or assessments of planned modifications to stations or facilities for passenger use projects; or programs of projects in an eligible area.': 1.0,\n",
              " 'FY  Competitive Funding Opportunity: Buses and Bus Facilities Competitive Program: Bus and Bus Facilities ProgramThe Federal Transit Administration (FTA) announces the opportunity to apply for approximately $. billion in competitive grants under the fiscal year (FY)  Low or No Emission Grant Program (Low-No Program) (Federal Assistance Listing: .) and approximately $ million in FY  funds under the Grants for Buses and Bus Facilities Program (Buses and Bus Facilities Program) (Federal Assistance Listing .), subject to availability of appropriated funding. Synopses and full announcement will be posted on Grants.gov as funding opportunity ID FTA--TPM for Low-No applications and FTA--TPM for Buses and Bus Facilities applications. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time May , ': 0.0,\n",
              " 'FY  Competitive Funding Opportunity: Electric or Low-Emitting Ferry Pilot Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  Electric or Low-Emitting Ferry Pilot Program (ELEF) (Federal Assistance Listing #.). ELEF makes funding available competitively to assist in the financing of capital projects for the purchase of electric or low-emitting ferry vessels that reduce emissions by using alternative fuels or on-board energy storage systems and related charging infrastructure to reduce emissions or produce zero onboard emissions under normal operation.': 1.0,\n",
              " 'FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $. million in competitive grants under the Fiscal Year (FY)  Passenger Ferry Grant Program (PFG) (Federal Assistance Listing #.). Of that amount, $. million is available only for low or zero-emission ferries or ferries using electric battery or fuel cell components and the infrastructure to support such ferries. As required by Federal public transportation law, funds will be awarded competitively to designated recipients or eligible direct recipients of Urbanized Area Formula funds to support capital projects to improve existing passenger ferry service, establish new ferry service, and repair and modernize ferry boats, terminals, and related facilities and equipment. ': 0.0,\n",
              " 'FY  Pilot Program for Transit-Oriented Development (TOD) Planning: The Pilot Program for TOD Planning helps support FTA&#;s mission of improving public transportation for America&#;s communities by providing funding to local communities to integrate land use and transportation planning around a new fixed guideway or core capacity improvement project. Per statute, any comprehensive or site specific planning funded through the program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations.': 0.0,\n",
              " 'FY  Competitive Funding Opportunity: Public Transportation on Indian Reservations Program; Tribal Transit Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $. million in competitive grants for the Fiscal Year (FY)  Public Transportation on Indian Reservations (Tribal Transit) Program. As required by Federal public transportation law, funds will be awarded competitively for any purpose eligible under FTA&#;s Formula Grants for Rural Areas Program, including planning, capital, and operating assistance for tribal public transit services in rural areas. FTA may award additional funding that is made available to the program prior to the announcement of project selections.': 0.0,\n",
              " 'FY Consolidated Rail Infrastructure and Safety Improvements Grant Program: This program funds projects that improve the safety, efficiency, and reliability of intercity passenger and freight rail.': 0.9,\n",
              " 'FY Corridor Identification and Development Grant Program: This program facilitates the development of intercity passenger rail corridors.': 1.0,\n",
              " 'FY Federal-State Partnership for Intercity Passenger Rail Grant Program for projects not located on the Northeast Corridor: This program funds capital projects that reduce the state of good repair backlog, improve performance, or expand or establish new intercity passenger rail service.': 0.0,\n",
              " 'Fiscal Year  National Culvert Removal, Replacement, and Restoration Grant Program (Culvert AOP Program): -- ORIGINAL NOFO POSTING - // -- The primary goal of the Culvert AOP Program is to improve or restore anadromous fish passage through the replacement, removal, repair, or improvement of culverts or weirs. The grant program prioritizes projects that would improve fish passage for: (A) anadromous fish stocks listed as an endangered species or a threatened species under section  of the Endangered Species Act of  ( U.S.C. &#; ); (B) anadromous fish stocks identified by the National Marine Fisheries Service (NMFS) or the U.S. Fish and Wildlife Service (USFWS) that could reasonably become listed as an endangered species or a threatened species under that section; (C) anadromous fish stocks identified by the NMFS or the USFWS as prey for endangered species, threatened species, or protected species, including Southern resident orcas (Orcinus orca); or (D) anadromous fish stocks identified by the NMFS or the USFWS as climate resilient stocks ( U.S.C. &#; (e)()). The program also prioritizes projects that would open up more than  meters of upstream (anadromous) habitat before the end of the natural habitat ( U.S.C. &#; (e)()). ': 1.0,\n",
              " 'FY  National Infrastructure Investments: Agency Name: Department of TransportationDescription: The Infrastructure Investment and Jobs Act (Pub. L. -, November , , &#;Bipartisan Infrastructure Law,&#; or &#;BIL&#;) authorized and appropriated $. billion to be awarded by the Department of Transportation (&#;DOT&#;) for FY  for Local and Regional Project Assistance Program Grants under National Infrastructure Investments. This Notice of Funding Opportunity (NOFO) solicits applications for projects to be funded under the Local and Regional Project Assistance Program, known as the RAISE Grants program, including any additional funding appropriated for the RAISE Grants program under the FY  Appropriations Act. On March , , the Consolidated Appropriations Act,  (Pub. L. -, &#;FY  Appropriations Act&#;) appropriated an additional $ million for the FY  RAISE Grant Program. Therefore, a total of $. billion in funding is now available for the FY  RAISE Grant Program. Applicants should note that the two funding streams (BIL funding and FY  Appropriations Act funding) have slightly different funding restrictions and requirements that may affect competitiveness. Please see the Notice of Funding Opportunity Amendment No. for details.As with previous rounds, funds for the FY  RAISE Transportation program are to be awarded on a competitive basis for projects that will have a significant local or regional impact.': 0.4,\n",
              " 'FY  Natural Gas Distribution Infrastructure Safety and Modernization Grant: FY  Natural Gas Distribution Infrastructure Safety and Modernization Grant': 1.0,\n",
              " 'The Infrastructure Investment and Jobs Act (IIJA) Notice of Funding Opportunity for America&#;s Marine Highway Program: This notice announces the availability of funding for grants and establishes selection criteria and application requirements for the America&#;s Marine Highway Program (&#;AMHP&#;). The purpose of this program is to make grants available to previously designated Marine Highway Projects that support the development and expansion of documented vessels or port and landside infrastructure. The Department also seeks eligible grant projects that will strengthen American supply chains. The U.S. Department of Transportation (&#;DOT&#; or &#;Department&#;) will award Marine Highway Grants to implement projects or components of projects previously designated by the Secretary of Transportation (&#;Secretary&#;) under the AMHP. Only Marine Highway Projects the Secretary designates before the Notice of Funding Opportunity (&#;NOFO&#;) closing date are eligible for funding as described in this notice. TIMING OF GRANT APPLICATIONS: Applications must be received by the Maritime Administration (&#;MARAD&#;) by : p.m. E.D.T. on April , . ADDRESSES: Grant applications must be submitted electronically using Grants.gov (https://www.grants.gov). Please be aware that you must complete the Grants.gov registration process before submitting your application and that the registration process usually takes  to  weeks to complete. Applicants are strongly encouraged to make submissions in advance of the deadline. FOR FURTHER INFORMATION CONTACT: Fred Jones, Office of Ports &amp; Waterways Planning, Room W&#;, Maritime Administration, U.S. Department of Transportation,  New Jersey Avenue S.E., Washington, D.C. , phone --, or email Fred.Jones@dot.gov. Persons who use a telecommunications device for the deaf (TDD) may call the Federal Information Relay Service (FIRS) at --- to contact the above individual during business hours. The FIRS is available twenty-four hours a day, seven days a week, to leave a message or question with the above individual. You will receive a reply during regular business hours.': 0.5,\n",
              " '*FY  Notice of Funding Opportunity: Bipartisan Infrastructure Law (BIL) Airport Terminal Program (ATP) Grants: The Department of Transportation (DOT), Federal Aviation Administration (FAA) announces the opportunity to apply for approximately $ billion in FY  discretionary funds for the Airport Terminal Program (ATP), made available under the Infrastructure Investment and Jobs Act of  (IIJA), Pub. L. -, herein referred to as the Bipartisan Infrastructure Law (BIL). The purpose of the ATP is to make annual grants available to eligible airports for airport terminal and airport-owned Airport Traffic Control Towers development projects that address the aging infrastructure of our nation&#;s airports. In addition, ATP grants will align with DOT&#;s Strategic Framework FY- at https://www.transportation.gov/administrations/office-policy/fy--strategic-framework. The FY  ATP will be implemented consistent with law and in alignment with the priorities in Executive Order , Implementation of the Infrastructure Investments and Jobs Act ( FR ), which are to invest efficiently and equitably; promote the competitiveness of the U.S. economy; improve job opportunities by focusing on high labor standards; strengthen infrastructure resilience to all hazards including climate change; and to effectively coordinate with State, local, Tribal, and territorial government partners. ': 1.0,\n",
              " 'INFRA Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families. The Infrastructure for Rebuilding America (INFRA) program is an existing competitive program that will see a more than  percent increase in this year&#;s funding due to the Bipartisan Infrastructure Law. These grants advance the Administration&#;s priorities of rebuilding America&#;s infrastructure and creating jobs by funding highway, multimodal freight and rail projects that position America to win the st century. Projects will improve safety, generate economic benefits, reduce congestion, enhance resiliency, and hold the greatest promise to eliminate supply chain bottlenecks and improve critical freight movements. Last year, DOT received over $ billion of project applications, but could only fund around $ billion of projects. To see last year&#;s INFRA grants, click here. The Bipartisan Infrastructure Law provides approximately $ billion for INFRA over  years, of which approximately $. billion will be made available through this NOFO. ': 0.0,\n",
              " 'Mega Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The National Infrastructure Project Assistance (MEGA) program was created in the Bipartisan Infrastructure Law to fund major projects that are too large or complex for traditional funding programs. The program will provide grants on a competitive basis to support multijurisdictional or regional projects of significance that may also cut across multiple modes of transportation. Eligible projects could include highway, bridge, freight, port, passenger rail, and public transportation projects of national and regional significance. These could be bridges or tunnels connecting two states; new rail and transit lines that improve equity and reduce emissions; and freight hubs integrating ship, train and truck traffic while improving environmental justice. DOT will award  percent of funding to projects greater than $ million in cost, and  percent to projects greater than $ million but less than $ million in cost. The program will receive up to $ billion this year alone and be able to provide multi-year funding to projects.': 0.0,\n",
              " 'Rural Surface Transportation Grant Program: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The Rural Surface Transportation Grant Program (RURAL) was created in the Bipartisan Infrastructure Law and will support projects to improve and expand the surface transportation infrastructure in rural areas to increase connectivity, improve the safety and reliability of the movement of people and freight, and generate regional economic growth and improve quality of life. Eligible projects for Rural grants include highway, bridge, and tunnel projects that help improve freight, safety, and provide or increase access to an agricultural, commercial, energy, or transportation facilities that support the economy of a rural area. This year alone, DOT will award up to $ million in grants through the rural program&#;part of the $ billion included in the Bipartisan Infrastructure Law over five years.': 0.0,\n",
              " '*FY  Notice of Funding Opportunity: Bipartisan Infrastructure Law (BIL) FAA Contract Tower (FCT) Competitive Grant Program: The Department of Transportation (DOT), Federal Aviation Administration (FAA) announces the opportunity to apply for $ million in FY  Airport Infrastructure Grant funds for the FCT competitive grant program, made available under the Infrastructure Investment and Jobs Act of  (IIJA), Pub. L. -, herein referred to as the Bipartisan Infrastructure Law (BIL). The purpose of the FCT competitive grant program is to make annual grants available to eligible airports for airport-owned airport traffic control tower (ATCT) projects that address the aging infrastructure of the nation&#;s airports. The FAA will consider ATCT projects that sustain, construct, repair, improve, rehabilitate, modernize, replace, or relocate non-approach control towers; or acquire and install air traffic control, communications, and related equipment to be used in those towers; or construct a remote tower certified by the FAA including acquisition and installation of air traffic control, communications, or related equipment. In addition, FT grants will align with DOT&#;s Strategic Framework FY- at https://www.transportation.gov/administrations/office-policy/fy--strategic-framework. The FY  FCT will be implemented consistent with law and in alignment with the priorities in Executive Order , Implementation of the Infrastructure Investments and Jobs Act ( FR ), which are to invest efficiently and equitably; promote the competitiveness of the U.S. economy; improve job opportunities by focusing on high labor standards; strengthen infrastructure resilience to all hazards including climate change; and to effectively coordinate with State, local, Tribal, and territorial government partners. ': 1.0,\n",
              " 'FY Railroad Crossing Elimination Grant Program: The purpose of the RCE Program is to provide funding for highway-rail and pathway-rail grade crossing improvement projects that focus on improving the safety and mobility of people and goods.': 1.0,\n",
              " 'Reconnecting Communities Pilot Discretionary Grant Program: The Reconnecting Communities Pilot Program notice of funding opportunity was amended on September ,  to include technical corrections to the Key Information Table. See updated materials in the &quot;Manage Related Documents&quot; section, as well as on our website at https://www.transportation.gov/grants/reconnecting-communitiesThe purpose of this notice is to solicit applications for Reconnecting Communities Pilot (RCP) Program grants. Funds for the Fiscal Year (FY)  RCP Program are to be awarded on a competitive basis for projects that reconnect communities by removing, retrofitting, or mitigating highways or other transportation facilities that create barriers to community connectivity, including to mobility, access, or economic development.The variety of transformative solutions to knit communities back together can include: high-quality public transportation, infrastructure removal, pedestrian walkways and overpasses, capping over roadways, linear parks and trail connectors, roadway redesigns and complete streets conversions, and main street revitalization. The program will award two types of grants: Planning Grants and Capital Construction Grants. Please consult the full Notice of Funding Opportunity for full details about the application process, requirements, and forms.': 0.8571428571428571,\n",
              " 'Safe Streets and Roads for All Discretionary Grant Program: THIS NOTICE OF FUNDING OPPORTUNITY WAS AMENDED ON AUGUST , . PLEASE SEE THE GRANTS.GOV PACKAGE UPDATES, AND THE SAFE STREETS AND ROADS FOR ALL (SSA) WEBSITE IN THE LINK PROVIDED BELOW FOR MORE INFORMATION. DEADLINE REMAINS SEPTEMBER , .The purpose of this notice is to solicit applications for SSA grants. Funds for the fiscal year (FY)  SSA grant program are to be awarded on a competitive basis to support planning, infrastructure, behavioral, and operational initiatives to prevent death and serious injury on roads and streets involving all roadway users, including pedestrians; bicyclists; public transportation, personal conveyance, and micromobility users; motorists; and commercial vehicle operators. Those interested in applying for Implementation Grants should use the application materials in PKG; those interested in applying for Action Plan Grants should use the application materials in PKG. See application instructions to determine which grant to apply for in this funding round.': 0.5,\n",
              " 'Strengthening Mobility and Revolutionizing Transportation (SMART) Grants Program: The purpose of this notice is to solicit applications for Strengthening Mobility and Revolutionizing Transportation (SMART) grants. Funds for the fiscal year (FY)  SMART Grants Program are to be awarded on a competitive basis to conduct demonstration projects focused on advanced smart city or community technologies and systems to improve transportation efficiency and safety.': 1.0,\n",
              " 'FY- Federal-State Partnership for Intercity Passenger Rail Grant Program for projects located on the Northeast Corridor: This program funds capital projects on the Northeast Corridor that reduce the state of good repair backlog, improve performance, or expand or establish new intercity passenger rail service.': 0.0,\n",
              " 'Fiscal Years - Wildlife Crossings Pilot Program (WCPP): The primary goals of the WCPP are to save lives, prevent serious injuries, and protect motorists and wildlife by reducing WVCs, and improve habitat connectivity for terrestrial and aquatic species. Reduction of wildlife vehicle collisions and improvement of terrestrial and aquatic habitat connectivity are the primary merit criteria that will be used in reviewing applications, and each of the primary merit criteria are of equal importance.': 0.0,\n",
              " 'FY  Competitive Funding Opportunity; Buses and Bus Facilities Program: The Federal Transit Administration (FTA) announces the availability of approximately $ million in competitive grants under the Buses and Bus Facilities Program is to assist in the financing of buses and bus facilities capital projects, including replacing, rehabilitating, purchasing or leasing buses or related equipment, and rehabilitating, purchasing, constructing or leasing bus-related facilities. Synopses and full announcement will be posted on Grants.gov as opportunity FTA---TPM-BUS. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time April , .': 1.0,\n",
              " 'FY  Competitive Funding Opportunity; Low or No Emission Vehicle Program: The Federal Transit Administration (FTA) announces the availability of approximately $. billion in competitive grants under the Low or No Emission Grant Program (Low-No Program) for the purchase or lease of zero-emission and low-emission transit buses, including acquisition, construction, and leasing of required supporting facilities. Synopses and full announcement will be posted on Grants.gov as opportunity FTA---TPM-LWNO. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time April , .': 0.0,\n",
              " 'FY  Competitive Funding Opportunity: Areas of Persistent Poverty Program: To create new opportunities for those experiencing poverty, the U.S. Department of Transportation&#;s Federal Transit Administration (FTA) today announced a new Notice of Funding Opportunity (NOFO) to help improve transit in areas experiencing long-term economic distress. The $ million in competitive grant funding through FTA&#;s Areas of Persistent Poverty (AoPP) Program provides more resources to underserved and disadvantaged communities seeking to expand or improve transit. ': 1.0,\n",
              " 'FY  Competitive Funding Opportunity: Ferry Service for Rural Communities Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  Ferry Service for Rural Communities Program (FSRC) (Federal Assistance Listing #.). FSRC makes funding available competitively to assist in the financing of capital, planning, and operating assistance for eligible ferry services.': 0.0,\n",
              " 'FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $. million in competitive grants under the Fiscal Year (FY)  Passenger Ferry Grant Program (PFG) (Federal Assistance Listing #.). Of that amount, $ million is available only for low or zero-emission ferries or ferries using electric battery or fuel cell components and the infrastructure to support such ferries. As required by Federal public transportation law, funds will be awarded competitively to designated recipients or eligible direct recipients of Urbanized Area Formula funds to support capital projects to improve existing passenger ferry service, establish new ferry service, and repair and modernize ferry boats, terminals, and related facilities and equipment. ': 0.0,\n",
              " 'FY  National Infrastructure Investments: The Infrastructure Investment and Jobs Act of  (&#;Bipartisan Infrastructure Law,&#; or &#;BIL&#;) authorized and appropriated $. billion to be awarded by the Department of Transportation (&#;DOT&#;) for Local and Regional Project Assistance Program Grants under National Infrastructure Investments in FY . This Notice of Funding Opportunity (NOFO) solicits applications for projects funded under the Local and Regional Project Assistance Program, known as the RAISE Grants program. If the FY  Appropriations Act provides additional funding for the RAISE grants program and/or significantly different requirements for National Infrastructure Investment funds, the Department will amend this Notice with guidance on additional requirements.As with previous rounds, funds for the FY  RAISE Transportation program are to be awarded on a competitive basis for projects that will have a significant local or regional impact.': 0.7916666666666666,\n",
              " 'Neighborhood Access and Equity (NAE) Program: The Department of Transportation is combining two major discretionary grant programs, the Reconnecting Communities Pilot (RCP) and Neighborhood Access and Equity (NAE) programs, into one Notice of Funding Opportunity (NOFO). Together, this combined program is known as the Reconnecting Communities and Neighborhoods (RCN) Program. The primary goal of the NAE Program is to assist economically disadvantaged or undeserved communities for planning and construction activities. Funds for the fiscal year (FY)  NAE grant program are to be awarded on a competitive basis to support planning, capital construction, and regional partnership activities that aim to restore community connectivity through the removal, retrofit, mitigation or replacement of highways, roadways, or other infrastructure facilities that create barriers to mobility, access or economic development. Applicants must submit their applications via Valid Eval at the links below: Community Planning Grants: https://usg.valideval.com/teams/rcn_planning/signup Capital Construction Grants: https://usg.valideval.com/teams/rcn_capitalconstruction/signup Do not submit applications through Grants.gov. Applications must be submitted by : PM Eastern Time on Thursday, September , . Late applications will not be accepted. ': 1.0,\n",
              " 'FY  Notice of Funding Opportunity for Small Shipyard Grants: This notice announces the intention of the Maritime Administration to provide grants to small shipyards. Under the Small Shipyard Grant Program, there is currently $,, available for grants for capital and related improvements to qualified shipyard facilities that will be effective in fostering efficiency, competitive operations, and quality ship construction, repair, and reconfiguration. Potential applicants are advised that it is expected, based on experience, that the number of applications will far exceed the funds available and that only a small percentage of applicants will be funded. Applications (SF- and the Addendum) will be accepted online through Grants.gov by  pm on the closing date (February , ). See full text under Related Documents Tab (FY  SSG NOFO) or the MARAD Web Page for detailed submission instructions.': 1.0,\n",
              " 'Safe Streets and Roads for All Funding Opportunity: The purpose of this notice is to solicit applications for Safe Streets and Roads for All (SSA) grants. Funds for the fiscal year (FY)  SSA grant program are to be awarded on a competitive basis to support planning and demonstration activities, as well as projects and strategies to prevent death and serious injury on roads and streets involving all roadway users, including pedestrians; bicyclists; public transportation, personal conveyance, and micromobility users; motorists; and commercial vehicle operators. Applicants must submit their applications via Valid Eval at https://usg.valideval.com/teams/usdot_ssa__implementation/signup for Implementation Grant applicants, and https://usg.valideval.com/teams/usdot_ssa__planning_demo/signup for Planning and Demonstration Grants. Do not submit your applications through Grants.Gov. Applications must be submitted by : PM Eastern Time on Monday, July , . Late applications will not be accepted.': 0.9827586206896551,\n",
              " 'SMART Grants Notice of Funding: The purpose of this notice is to solicit applications for Strengthening Mobility and Revolutionizing Transportation (SMART) Stage  Planning and Prototyping grants. Funds for the fiscal year (FY)  SMART Grants Program are to be awarded on a competitive basis to conduct demonstration projects focused on advanced smart city or community technologies and systems to improve transportation efficiency and safety. Applications must be submitted via Valid Eval, an online submission proposal system used by USDOT, at https://usg.valideval.com/teams/USDOT_SMART_/signup. USDOT will not accept or review application materials submitted via Grants.gov. ': 0.0,\n",
              " ' Tribal Transportation Program Safety Fund: Eligible projects described in section (a)() are strategies, activities, and projects on a public road that are consistent with a transportation safety plan and that (i) correct or improve a hazardous road location or feature, or (ii) address a highway safety problem. TTPSF emphasizes the development of strategic transportation safety plans using a data-driven process as a means for Tribes to identify transportation safety needs and determine how those needs will be addressed in Tribal communities. FHWA has identified four eligibility categories: transportation safety plans; data assessment, improvement, and analysis activities; systemic roadway departure countermeasures; and infrastructure improvements and other eligible activities as listed in  U.S.C. &#; (a)().': 1.0,\n",
              " 'United States Marine Highway Grants: This funding opportunity solicits applications for fiscal year (FY)  United States Marine Highway Program (USMHP) grants. Funds for FY  USMHP grants will be awarded on a competitive basis to assist in funding eligible projects for the purpose of developing, expanding, or promoting marine highway transportation. This opportunity announces the availability of up to $,, million in funding for grants under this program and establishes selection criteria and application requirements. All USMHP grant recipients must meet all applicable Federal requirements, including domestic content (&#;Buy America&#;) requirements. This program was formerly known as the America&#;s Marine Highway Program': 0.0,\n",
              " 'INFRA Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families. The Infrastructure for Rebuilding America (INFRA) program is an existing competitive program that will see a more than  percent increase in this year&#;s funding due to the Bipartisan Infrastructure Law. These grants advance the Administration&#;s priorities of rebuilding America&#;s infrastructure and creating jobs by funding highway, multimodal freight and rail projects that position America to win the st century. Projects will improve safety, generate economic benefits, reduce congestion, enhance resiliency, and hold the greatest promise to eliminate supply chain bottlenecks and improve critical freight movements. Last year, DOT received over $ billion in application requests, but could only fund around $. billion of projects. To see last year&#;s INFRA grants, click here. The Bipartisan Infrastructure Law provides approximately $ billion for INFRA over  years, of which approximately $. billion will be made available through this NOFO. ': 0.0,\n",
              " 'Mega Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The National Infrastructure Project Assistance (MEGA) program was created in the Bipartisan Infrastructure Law to fund major projects that are too large or complex for traditional funding programs. The program will provide grants on a competitive basis to support multijurisdictional or regional projects of significance that may also cut across multiple modes of transportation. Eligible projects could include highway, bridge, freight, port, passenger rail, and related public transportation projects of national and regional significance. DOT will award approximately  percent of funding to projects greater than $ million in cost, and approximately  percent to projects greater than $ million but less than $ million in cost.For more information about last year&apos;s Mega awards, visit here: https://www.transportation.gov/grants/mega-grant-program/FYawards': 0.0,\n",
              " 'Rural Surface Transportation Grant Progam: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The Rural Surface Transportation Grant Program (RURAL) was created in the Bipartisan Infrastructure Law and will support projects to improve and expand the surface transportation infrastructure in rural areas to increase connectivity, improve the safety and reliability of the movement of people and freight, and generate regional economic growth and improve quality of life. Eligible projects for Rural grants include highway, bridge, and tunnel projects that help improve freight, safety, and provide or increase access to an agricultural, commercial, energy, or transportation facilities that support the economy of a rural area. This year alone, DOT will award up to $ million in grants through the rural program&#;part of the $ billion included in the Bipartisan Infrastructure Law over five years. At least % of rural funding must be awarded in amounts of $ million or more. If you are seeking less than $ million, you are competing for only about $. million nationwide.For more information on last year&apos;s rural awards, see: https://www.transportation.gov/grants/rural--fact-sheets': 0.0,\n",
              " 'FY  Competitive Funding Opportunity: Rail Vehicle Replacement Grant Program (RAIL): The Federal Transit Administration (FTA) announces the opportunity to apply for approximately $ million in Fiscal Year (FY)  Section  State of Good Repair Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected rail vehicle replacement projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to States and local governmental authorities to assist in the funding of capital projects to replace rail rolling stock.': 0.0}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('finetuned_BERT_epoch_2.model', map_location=torch.device('cuda')))\n",
        "\n",
        "_, predictions, true_vals, _ = evaluate(dataloader_validation)\n",
        "accuracy_per_class(predictions, true_vals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4i-QKwmiwa-s",
        "outputId": "cf8e522b-3803-432d-fdad-904c9bbf937b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class: FY  Notice of Funding Opportunity: Bipartisan Infrastructure Law (BIL) Airport Terminal Program (ATP) Grants: The Department of Transportation (DOT), Federal Aviation Administration (FAA) announces the opportunity to apply for approximately $ billion in FY  discretionary funds for the Airport Terminal Program (ATP), made available under the Infrastructure Investment and Jobs Act of  (IIJA), Pub. L. -, herein referred to as the Bipartisan Infrastructure Law (BIL). The purpose of the ATP is to make annual grants available to eligible airports for airport terminal and airport-owned Airport Traffic Control Towers development projects that address the aging infrastructure of our nation&#;s airports. In addition, ATP grants will align with DOT&#;s Strategic Framework FY- at https://www.transportation.gov/administrations/office-policy/fy--strategic-framework. The FY  ATP will be implemented consistent with law and in alignment with the priorities in Executive Order , Implementation of the Infrastructure Investments and Jobs Act ( FR ), which are to invest efficiently and equitably; promote the competitiveness of the U.S. economy; improve job opportunities by focusing on high labor standards; strengthen infrastructure resilience to all hazards including climate change; and to effectively coordinate with State, local, Tribal, and territorial government partners. \n",
            "Accuracy: 12/25 (0.4800)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Airport Improvement Program Discretionary Grants: The U.S. Department of Transportation&#;s Federal Aviation Administration (FAA) announces the opportunity to apply for an estimated $. billion in Fiscal Year (FY)  discretionary grants under the Airport Improvement Program (AIP), per  U.S.C. &#; . The FAA awards these annually appropriated discretionary funds through the FAA&#;s long-standing iterative, competitive grant process. Prior to the publication of this Notice of Funding Opportunity (NOFO), the FAA identified eligible applicants in its National Plan of Integrated Airport Systems (NPIAS) and compiled potentially eligible projects through the -year Airports Capital Improvement Plan (ACIP). Both of these processes are described in FAA Order ., &#;Formulation of NPIAS and ACIP,&#; adhering to  U.S.C. &#; , which authorizes discretionary funds. The AIP funds airport capital improvements and rehabilitation projects. All discretionary grant funding is subject to appropriations, statutory requirements, and related program funding availability. \n",
            "Accuracy: 163/163 (1.0000)\n",
            "\n",
            "Class: Grants for Buses and Bus Facilities Program: The Federal Transit Administration (FTA) announces the availability of approximately $. million of Fiscal Year  funds for buses, bus facilities, and bus equipment. The purpose of the Grants for Buses and Bus Facilities Program is to assist in the financing of buses and bus facilities capital projects, including replacing, rehabilitating, purchasing or leasing buses or related equipment, and rehabilitating, purchasing, constructing or leasing bus-related facilities. \n",
            "Accuracy: 13/14 (0.9286)\n",
            "\n",
            "Class: Solicitation of Project Proposals for the Low or No Emission (Low-No) Program:  Notice of Funding Opportunity (NOFO): Solicitation of Project Proposals for the Low or No Emission Program (Low-No) Program. The Federal Transit Administration (FTA) announces the availability of $ million of Fiscal Year  funds for the purchase or lease of low or no emission vehicles as well as related equipment or facilities. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time on //. The main purpose of the Low-No Program is to support the transition of the nation&#;s transit fleet to the lowest polluting and most energy efficient transit vehicles. The Low-No Program provides funding to State and local governmental authorities for the purchase or lease of zero-emission and low-emission transit buses, including acquisition, construction, and leasing of required supporting facilities. \n",
            "Accuracy: 0/6 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in Fiscal Year (FY)  Section  Urbanized Area Formula Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected passenger ferry projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to designated recipients or eligible direct recipients of Section  funds to assist in the financing of capital projects to support existing passenger ferry service, establish new ferry service, and to repair and modernize ferry boats, terminals, and related facilities and equipment. \n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: Pilot Program for Transit-Oriented Development (TOD) Planning: Notice of Funding Opportunity for the Pilot Program for Transit-Oriented Development (TOD) Planning. The Federal Transit Administration (FTA) announces the availability of approximately $. million in Pilot Program for TOD Planning funding to support comprehensive planning associated with new fixed guideway and core capacity improvement projects. FTA may award amounts ranging from $, to $,,. The Pilot Program for TOD Planning provides funding to local communities to integrate land use and transportation planning in new fixed guideway and core capacity transit project corridors. As required by statute, any comprehensive planning funded through the pilot program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations. The statute also requires that the planning work be associated with a new fixed guideway or core capacity transit project as defined in Federal transit statute ( USC (a); also see the NOFO for the definitions). Consistent with statutory direction, FTA is seeking comprehensive planning projects covering an entire transit capital project corridor, rather than proposals that involve planning for individual station areas or only a small section of the corridor. To ensure that planning work reflects the needs and aspirations of the local community and results in concrete, specific deliverables and outcomes, FTA is requiring that transit project sponsors partner with entities with land use planning authority in the transit project corridor. \n",
            "Accuracy: 4/4 (1.0000)\n",
            "\n",
            "Class: Public Transportation on Indian Reservations Program: The Federal Transit Administration (FTA) announces the availability of approximately $ million in funding provided by the Public Transportation on Indian Reservations Program (Tribal Transit Program (TTP)), as authorized by  U.S.C. (c)()(A), as amended by the Fixing America&apos;s Surface Transportation Act (FAST), Public Law - (December , ). The primary purpose of these competitively selected grants is to support planning, capital, and, in limited circumstances, operating assistance for tribal public transit services. Funds distributed to Indian tribes under the TTP should not replace or reduce funds that Indian tribes receive from states through FTA&#;s Section  program. \n",
            "Accuracy: 0/6 (0.0000)\n",
            "\n",
            "Class: INFRA Grants: The Nationally Significant Freight and Highway Projects (INFRA) program provides Federal financial assistance to highway and freight projects of national or regional significance. This notice solicits applications for awards under the program&#;s fiscal year (FY)  funding, subject to the availability of appropriated funds. The Department is focusing the competition on transportation infrastructure projects that support four key objectives, each of which is discussed in greater detail in the Notice of Funding Opportunity: () Supporting economic vitality at the national and regional level; () Leveraging Federal funding to attract non-Federal sources of infrastructure investment; () Deploying innovative technology, encouraging innovative approaches to project delivery, and incentivizing the use of innovative financing; and () Holding grant recipients accountable for their performance. \n",
            "Accuracy: 2/3 (0.6667)\n",
            "\n",
            "Class:  Tribal Transportation Program Safety Fund: A Notice of Funding Availability (NOFA) for Tribal Transportation Program Safety Funds (TTPSF) has been published. This notice announces the availability of $. million for projects in three categories: safety plans; data assessment, improvement, and analysis activities; and infrastructure improvement and other eligible activities as listed in  U.S.C.(a)() The deadline for submissions is //. For complete instructions on preparing and submitting an application for TTPSF, please download the application information and NOFO at https://flh.fhwa.dot.gov/programs/ttp/safety/ttpsf.htm Please submit your application directly to FHWA through the &quot;apply now&quot; button at: https://flh.fhwa.dot.gov/programs/ttp/safety/ttpsf.htm \n",
            "Accuracy: 14/14 (1.0000)\n",
            "\n",
            "Class: Grants for Buses and Bus Facilities Program: The purpose of the Grants for Buses and Bus Facilities Program is to assist in the financing of buses and bus facilities capital projects, including replacing, rehabilitating, purchasing or leasing buses or related equipment, and rehabilitating, purchasing, constructing or leasing bus-related facilities.\n",
            "Accuracy: 13/14 (0.9286)\n",
            "\n",
            "Class: Low or No Emission Program (Low-No Program): The main purpose of the Low-No Program is to support the transition of the nation&#;s transit fleet to the lowest polluting and most energy efficient transit vehicles. The Low-No Program provides funding to State and local governmental authorities for the purchase or lease of zero-emission and low-emission transit buses, including acquisition, construction, and leasing of required supporting facilities.\n",
            "Accuracy: 14/14 (1.0000)\n",
            "\n",
            "Class: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in Fiscal Year (FY)  Section  Urbanized Area Formula Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected passenger ferry projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to designated recipients or eligible direct recipients of Section  funds to assist in the financing of capital projects to support existing passenger ferry service, establish new ferry service, and to repair and modernize ferry boats, terminals, and related facilities and equipment. \n",
            "Accuracy: 0/2 (0.0000)\n",
            "\n",
            "Class: Pilot Program for Transit-Oriented Development (TOD) Planning: Notice of Funding Opportunity for PILOT PROGRAM FOR TRANSIT-ORIENTED DEVELOPMENT (TOD) PLANNING. The Federal Transit Administration (FTA) announces the opportunity to apply for approximately $. million of funding under the Pilot Program for TOD Planning (Catalog of Federal Domestic Assistance #.). As required by federal transit law and subject to funding availability, funds will be awarded competitively to support comprehensive planning associated with new fixed guideway and core capacity improvement projects. FTA anticipates minimum grant awards of $, and maximum grant awards of $,,. The Pilot Program for TOD Planning is intended to fund comprehensive planning that supports economic development, ridership, multimodal connectivity and accessibility, increased transit access for pedestrian and bicycle traffic, and mixed-use development near transit stations. The program also encourages identification of infrastructure needs and engagement with the private sector. Consistent with statutory direction, FTA is seeking comprehensive planning projects covering an entire transit capital project corridor, rather than proposals that involve planning for individual station areas or only a small section of the corridor. To ensure any proposed planning work reflects the needs and aspirations of the local community and results in concrete, specific deliverables and outcomes, transit project sponsors must partner with entities with land use planning authority in the transit project corridor to conduct the planning work.The Pilot Program for TOD Planning helps support FTA&#;s mission of improving public transportation for America&#;s communities by providing funding to local communities to integrate land use and transportation planning around a new fixed guideway or core capacity improvement project. Per statute, any comprehensive planning funded through the program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations.\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Public Transportation on Indian Reservations Program: The Federal Transit Administration (FTA) announces the availability of approximately $ million in funding provided by the Public Transportation on Indian Reservations Program (Tribal Transit Program (TTP)), as authorized by  U.S.C. (c)()(A), as amended by the Fixing America&apos;s Surface Transportation Act (FAST), Public Law - (December , ). This is a national solicitation for project proposals and includes the selection criteria and program eligibility information for Fiscal Year  projects. The primary purpose of these competitively selected grants is to support planning, capital, and, in limited circumstances, operating assistance for tribal public transit services. Funds distributed to Indian tribes under the TTP should not replace or reduce funds that Indian tribes receive from states through FTA&#;s Section  program.\n",
            "Accuracy: 0/4 (0.0000)\n",
            "\n",
            "Class: *INFRA Grants: The U.S. Department of Transportation (USDOT) is seeking applicants for the FY  round of the Infrastructure for Rebuilding America (INFRA) discretionary grant program to fund transportation projects of national and regional significance that result in good-paying jobs, improve safety, apply transformative technology, and explicitly address climate change and racial equity. The funding available for this year&#;s grants totals approximately $ million.The Department recognizes the role that infrastructure investment plays in economic development and job creation, and the added urgency of this funding at time when the COVID- pandemic has put stress on state and local budgets.For the first time, the USDOT seeks INFRA projects that address climate change and environmental justice. Projects will be evaluated on whether they were planned as part of a comprehensive strategy to address climate change, or whether they support strategies to reduce greenhouse gas emissions such as deploying zero-emission-vehicle infrastructure or encouraging modal shift and a reduction in vehicle-miles-traveled. Racial equity will also be considered as a selection criterion, to the extent that project sponsors have completed equity-focused community outreach, and projects are designed to benefit underserved communities. The Department will also consider whether the project is located in a federally designated community development zone, including qualified Opportunity Zones, Empowerment Zones, Promise Zones, or Choice Neighborhoods.USDOT seeks projects that apply innovative technology, delivery, or financing methods with proven outcomes to deliver projects in a cost effective manner. The Department will make awards under the INFRA program to both large and small projects. For a large project, the INFRA grant must be at least $ million. For a small project, the grant must be at least $ million. Under statutory requirements,  percent of available funds are reserved for small projects, and the Department must award at least  percent of funding for rural projects. INFRA grants may be used to fund a variety of components of an infrastructure project, however, the Department is specifically focused on projects in which the local sponsor is significantly invested and is positioned to proceed rapidly to construction. Eligible INFRA project costs may include: reconstruction, rehabilitation, acquisition of property (including land related to the project and improvements to the land), environmental mitigation, construction contingencies, equipment acquisition, and operational improvements directly related to system performance. The INFRA NOFO also announces the creation of the &#;INFRA Extra&#; Program, which will identify competitive INFRA applicants who do not receive an INFRA award and authorize them to seek a Transportation Infrastructure Finance and Innovation Act of  (TIFIA) loan up to  percent of their project cost.\n",
            "Accuracy: 2/3 (0.6667)\n",
            "\n",
            "Class: Fiscal Year  Competitive Funding Opportunity; Grants for Buses and Bus Facilities Program: The Federal Transit Administration (FTA) announces the availability of approximately $. million of Fiscal Year  funds for buses, bus facilities, and bus equipment. Synopses and full announcement will be posted on Grants.gov as opportunity FTA---BUS. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time November ,.\n",
            "Accuracy: 0/10 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  Passenger Ferry Grant Program (Ferry Program) (Federal Assistance Listing #.). Of that amount, $ million is available only for low or zero-emission ferries or ferries using electric battery or fuel cell components and the infrastructure to support such ferries. As required by Federal public transportation law, funds will be awarded competitively to designated recipients or eligible direct recipients of Urbanized Area Formula funds to support capital projects to improve existing passenger ferry service, establish new ferry service, and repair and modernize ferry boats, terminals, and related facilities and equipment.\n",
            "Accuracy: 0/2 (0.0000)\n",
            "\n",
            "Class:  Competitive Funding Opportunity: Pilot Program for Transit-Oriented Development (TOD) Planning: The Pilot Program for TOD Planning helps support FTA&#;s mission of improving public transportation for America&#;s communities by providing funding to local communities to integrate land use and transportation planning around a new fixed guideway or core capacity improvement project. Per statute, any comprehensive planning funded through the program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations. Additional information is available at: https://www.transit.dot.gov/TODPilot.Notice of Funding Opportunity (NOFO): Notice of Funding Opportunity for PILOT PROGRAM FOR TRANSIT-ORIENTED DEVELOPMENT (TOD) PLANNING. The Federal Transit Administration (FTA) announces the opportunity to apply for $,, of funding under the Pilot Program for TOD Planning (Catalog of Federal Domestic Assistance #.). As required by federal transit law and subject to funding availability, funds will be awarded competitively to support comprehensive planning associated with new fixed guideway and core capacity improvement projects. FTA anticipates minimum grant awards of $, and maximum grant awards of $,,. Synopses and full announcement are posted on Grants.gov as opportunity FTA---TPE-TODP. The Pilot Program for TOD Planning is intended to fund comprehensive planning that supports economic development, ridership, multimodal connectivity and accessibility, increased transit access for pedestrian and bicycle traffic, and mixed-use development near transit stations. The program also encourages identification of infrastructure needs and engagement with the private sector.Consistent with statutory direction, FTA is seeking comprehensive planning projects covering an entire transit capital project corridor, rather than proposals that involve planning for individual station areas or only a small section of the corridor. To ensure any proposed planning work reflects the needs and aspirations of the local community and results in concrete, specific deliverables and outcomes, transit project sponsors must partner with entities with land use planning authority in the transit project corridor to conduct the planning work.Link and Instructions for attaching the supplemental form to the SF-: All applicants must complete a supplemental form (PDF) specific to the Pilot Program for TOD Planning and attach it to their submission in Grants.gov. See section D of the NOFO for information about additional required application contents.Webinar: FTA will conduct a webinar for prospective applicants after the NOFO is published in the Federal Register. Further information will be posted on FTA&#;s website (https://www.transit.dot.gov/TODPilot) when available. Dates: An applicant must submit a proposal electronically by : p.m. Eastern Daylight Time on June , . Any agency intending to apply should initiate the process of registering on the Grants.gov site immediately to ensure completion of registration before the submission deadline. \n",
            "Accuracy: 0/3 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity:  Public Transportation on Indian Reservations Program; Tribal Transit Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants for the Fiscal Year (FY)  Public Transportation on Indian Reservations (Tribal Transit) Program (Federal Assistance Listing: .). As required by Federal public transportation law, funds will be awarded competitively for any purpose eligible under FTA&#;s Formula Grants for Rural Areas Program,  U.S.C. , including planning, capital, and operating assistance for tribal public transit services in rural areas. \n",
            "Accuracy: 6/6 (1.0000)\n",
            "\n",
            "Class: INFRA Grants: The U.S. Department of Transportation (USDOT) is seeking applicants for the FY  round of the Infrastructure for Rebuilding America (INFRA) discretionary grant program to fund transportation projects of national and regional significance that result in good-paying jobs, improve safety, apply transformative technology, and explicitly address climate change and racial equity. The funding available for this year&#;s grants totals approximately $ million.The Department recognizes the role that infrastructure investment plays in economic development and job creation, and the added urgency of this funding at time when the COVID- pandemic has put stress on state and local budgets.For the first time, the USDOT seeks INFRA projects that address climate change and environmental justice. Projects will be evaluated on whether they were planned as part of a comprehensive strategy to address climate change, or whether they support strategies to reduce greenhouse gas emissions such as deploying zero-emission-vehicle infrastructure or encouraging modal shift and a reduction in vehicle-miles-traveled. Racial equity will also be considered as a selection criterion, to the extent that project sponsors have completed equity-focused community outreach, and projects are designed to benefit underserved communities. The Department will also consider whether the project is located in a federally designated community development zone, including qualified Opportunity Zones, Empowerment Zones, Promise Zones, or Choice Neighborhoods.USDOT seeks projects that apply innovative technology, delivery, or financing methods with proven outcomes to deliver projects in a cost effective manner. The Department will make awards under the INFRA program to both large and small projects. For a large project, the INFRA grant must be at least $ million. For a small project, the grant must be at least $ million. Under statutory requirements,  percent of available funds are reserved for small projects, and the Department must award at least  percent of funding for rural projects. INFRA grants may be used to fund a variety of components of an infrastructure project, however, the Department is specifically focused on projects in which the local sponsor is significantly invested and is positioned to proceed rapidly to construction. Eligible INFRA project costs may include: reconstruction, rehabilitation, acquisition of property (including land related to the project and improvements to the land), environmental mitigation, construction contingencies, equipment acquisition, and operational improvements directly related to system performance. The INFRA NOFO also announces the creation of the &#;INFRA Extra&#; Program, which will identify competitive INFRA applicants who do not receive an INFRA award and authorize them to seek a Transportation Infrastructure Finance and Innovation Act of  (TIFIA) loan up to  percent of their project cost.\n",
            "Accuracy: 0/17 (0.0000)\n",
            "\n",
            "Class: FY Tribal Transportation Program Safety Fund (TTPSF): A Notice of Funding Opportunity (NOFO) for Tribal Transportation Program Safety Funds (TTPSF) has been published. This notice announces the availability of approximately $. million for projects in four categories: safety plans; data assessment, improvement, and analysis activities; systemic roadway departure countermeasures; and infrastructure improvement and other eligible activities as listed in  U.S.C.(a)(). The goal of these projects is to reduce fatalities and serious injuries from transportation incidents, such as motor vehicle crashes. For complete instructions on preparing and submitting an application for TTPSF, please download the application, application information, systemic roadway departure risk assessment form (if applicable) and NOFO at https://highways.dot.gov/federal-lands/programs-tribal/safety/funds. Applications will NOT be submitted on Grants.gov. Instead, please use the &quot;apply now&quot; button on the program web page.\n",
            "Accuracy: 5/9 (0.5556)\n",
            "\n",
            "Class: Advanced Transportation Technologies and  Innovative Mobility Deployment (ATTIMD) Program: AMENDMENT  (//): This amendment is for the purpose of correcting a typo contained in Table D. on page  of . This table, contained in Section D. for CONTENT AND FORM OF APPLICATION SUBMISSION, erroneously omitted the Project Outcome Criteria from that table&#;s submission requirements for Volume . This amendment also revises the verbiage included on page  of the NOFO to clarify that all resumes for key personnel should be included in item f) Appendix &#; R&#;sum&#;s for Key Personnel. Please note that Resumes do not count against the page limitations for Volume . Please refer to Amendment  in the &#;Related Documents&#; tab for more information. -- END OF AMENDMENT  -- AMENDMENT  (//): This amendment adds the weblink to the recording of the October , , informational webinar for this opportunity. The weblink is included in the attachments page for this opportunity as well as below: LINK: https://usdot.zoomgov.com/rec/share/eQNNRGISy-UxfqGpSDzINRbzvcdkAjDdoulZvwRCt-sARp.kdAJIUFsFZZEpf PASSCODE: c@X*&amp;@ -- END OF AMENDMENT  -- AMENDMENT  (//): This amendment adds the registration information for the October , , informational webinar for this opportunity. Please refer to the &quot;Related Documents&quot; tab for more information. -- END OF AMENDMENT  -- BASE NOFO (//): The Advanced Transportation Technologies and Innovative Mobility Deployment Program (ATTIMD), established in Section (c)() of title , United States Code (U.S.C.), also known as the Advanced Transportation Technology and Innovation (ATTAIN) Program, directs FHWA to award grants to eligible entities to deploy, install, and operate advanced transportation technologies to improve safety, mobility, efficiency, system performance, intermodal connectivity, and infrastructure return on investment. FHWA intends for these model technology deployments to help demonstrate how emerging transportation technologies, data, and their applications can be effectively deployed and integrated with existing systems to provide access to essential services and other destinations. This also includes efforts to increase connectivity to employment, education, services, and other opportunities; support workforce development; or contribute to increased mobility, particularly for persons with visible and hidden disabilities and elderly individuals. The Assistance Listing Number for this opportunity is . &#; Highway Research and Development. The Federal Highway Administration (FHWA) hereby requests applications to result in awards to eligible entities to deploy, install, and operate advanced transportation technologies to improve safety, mobility, efficiency, system performance, intermodal connectivity, and infrastructure return on investment. These model deployments are expected to provide benefits in the form of: reduced traffic-related fatalities and injuries; reduced traffic congestion and improved travel time reliability; reduced transportation-related emissions; optimized multimodal system performance; improved access to transportation alternatives, including for underserved populations; improved integration of payment systems; public access to real-time integrated traffic, transit, and multimodal transportation information to make informed travel decisions; cost savings to transportation agencies, businesses, and the traveling public; or other benefits to transportation users and the general public. This competitive ATTAIN Program will promote the use of innovative transportation solutions. The deployment of these technologies will provide Congress and FHWA with valuable real-life data and feedback to inform future decision-making. Cost Sharing: Cost-sharing or matching is required, with the maximum Federal share being  percent; hence, this NOFO requires a minimum non-Federal cost share of  percent. Cost sharing or matching means the portion of project costs not paid by Federal funds. NOTE: FHWA previously issued six NOFOs for the Advanced Transportation and Congestion Management Technologies Deployment (ATCMTD) Grant Program before the program was amended and renamed to the Advanced Transportation Technologies and Innovative Mobility Deployment Program by Section  of the Infrastructure Investment and Jobs Act (IIJA) (Public Law -, also known as the &#;Bipartisan Infrastructure Law&#; (BIL)).\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: FY  and FY  Competitive Funding Opportunity: Competitive Grants for Rail Vehicle Replacement Program (Rail Program): The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in Fiscal Year (FY)  and  Section  State of Good Repair Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected rail vehicle replacement projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to States and local governmental authorities to assist in the funding of capital projects to replace rail rolling stock.\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: Bridge Investment Program - Planning, Bridge Projects, and Large Bridge Projects: This Notice of Funding Opportunity (NOFO) is to solicit applications for three categories of Bridge Investment Program (BIP) funding opportunities: () Planning; () Bridge Projects (projects with eligible costs less than $ million); and () Large Bridge Projects (projects with eligible costs greater than $ million). Eligible applicants may submit applications for any of the three funding categories, but each category has distinct eligibility and selection criteria and application deadlines.These funds will be awarded on a competitive basis for planning, feasibility analysis, and revenue forecasting associated with the development of a project that would subsequently be eligible to apply for BIP funding under either the Bridge Projects or Large Bridge Projects funding categories.A total of $. billion in  BIP funds are available for the Bridge Projects and Large Bridge Projects funding opportunities. These funds will be awarded on a competitive basis for bridge replacement, rehabilitation, preservation, and protection projects that: () improve the safety, efficiency, and reliability of the movement of people and freight over bridges; and () improve the condition of bridges in the United States by reducing (a) the number of bridges, and total person miles traveled over bridges, that are in poor condition or that are in fair condition and at risk of falling into poor condition within the next three years, or (b) the number of bridges, and total person miles traveled over bridges, that do not meet current geometric design standards or cannot meet the load and traffic requirements typical of the regional transportation network. In addition, Large Bridge Projects that receive a BIP award of not less than $ million are eligible for multiyear grants, in which DOT can award available funds to a project over the course of several years in accordance with an agreement and in alignment with its schedule. In selecting Bridge Projects and Large Bridge Projects, FHWA will consider the extent to which BIP funds leverage non-Federal contributions from sponsors and stakeholders involved in the planning, design, and construction of eligible projects. \n",
            "Accuracy: 5/6 (0.8333)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity; Low or No Emission Grant Program: Low-No ProgramThe Federal Transit Administration (FTA) announces the opportunity to apply for approximately $. billion in competitive grants under the fiscal year (FY)  Low or No Emission Grant Program (Low-No Program) (Federal Assistance Listing: .) and approximately $ million in FY  funds under the Grants for Buses and Bus Facilities Program (Buses and Bus Facilities Program) (Federal Assistance Listing .), subject to availability of appropriated funding. Synopses and full announcement will be posted on Grants.gov as funding opportunity ID FTA--TPM for Low-No applications and FTA--TPM for Buses and Bus Facilities applications. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time May , .\n",
            "Accuracy: 14/15 (0.9333)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: All Stations Accessibility Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  All Stations Accessibility Program (ASAP) (Federal Assistance Listing #.). As required by Federal public transportation law, funds will be awarded competitively to eligible designated recipients that operate or allocate funds to inaccessible pre-ADA&#;or &#;legacy&#; &#; rail fixed guideway public transportation systems, and States (including territories and Washington, D.C.) and local governmental entities that operate or financially support legacy rail fixed guideway public transportation systems and corresponding legacy stations/facilities for capital projects to repair, improve, modify, retrofit, or relocate infrastructure of stations or facilities for passenger use, including load-bearing members that are an essential part of the structural frame; or () for planning projects to develop or modify a plan for pursuing public transportation accessibility projects, assessments of accessibility, or assessments of planned modifications to stations or facilities for passenger use projects; or programs of projects in an eligible area.\n",
            "Accuracy: 2/2 (1.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Buses and Bus Facilities Competitive Program: Bus and Bus Facilities ProgramThe Federal Transit Administration (FTA) announces the opportunity to apply for approximately $. billion in competitive grants under the fiscal year (FY)  Low or No Emission Grant Program (Low-No Program) (Federal Assistance Listing: .) and approximately $ million in FY  funds under the Grants for Buses and Bus Facilities Program (Buses and Bus Facilities Program) (Federal Assistance Listing .), subject to availability of appropriated funding. Synopses and full announcement will be posted on Grants.gov as funding opportunity ID FTA--TPM for Low-No applications and FTA--TPM for Buses and Bus Facilities applications. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time May , \n",
            "Accuracy: 0/7 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Electric or Low-Emitting Ferry Pilot Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  Electric or Low-Emitting Ferry Pilot Program (ELEF) (Federal Assistance Listing #.). ELEF makes funding available competitively to assist in the financing of capital projects for the purchase of electric or low-emitting ferry vessels that reduce emissions by using alternative fuels or on-board energy storage systems and related charging infrastructure to reduce emissions or produce zero onboard emissions under normal operation.\n",
            "Accuracy: 0/2 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $. million in competitive grants under the Fiscal Year (FY)  Passenger Ferry Grant Program (PFG) (Federal Assistance Listing #.). Of that amount, $. million is available only for low or zero-emission ferries or ferries using electric battery or fuel cell components and the infrastructure to support such ferries. As required by Federal public transportation law, funds will be awarded competitively to designated recipients or eligible direct recipients of Urbanized Area Formula funds to support capital projects to improve existing passenger ferry service, establish new ferry service, and repair and modernize ferry boats, terminals, and related facilities and equipment. \n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: FY  Pilot Program for Transit-Oriented Development (TOD) Planning: The Pilot Program for TOD Planning helps support FTA&#;s mission of improving public transportation for America&#;s communities by providing funding to local communities to integrate land use and transportation planning around a new fixed guideway or core capacity improvement project. Per statute, any comprehensive or site specific planning funded through the program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations.\n",
            "Accuracy: 3/3 (1.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Public Transportation on Indian Reservations Program; Tribal Transit Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $. million in competitive grants for the Fiscal Year (FY)  Public Transportation on Indian Reservations (Tribal Transit) Program. As required by Federal public transportation law, funds will be awarded competitively for any purpose eligible under FTA&#;s Formula Grants for Rural Areas Program, including planning, capital, and operating assistance for tribal public transit services in rural areas. FTA may award additional funding that is made available to the program prior to the announcement of project selections.\n",
            "Accuracy: 0/4 (0.0000)\n",
            "\n",
            "Class: FY Consolidated Rail Infrastructure and Safety Improvements Grant Program: This program funds projects that improve the safety, efficiency, and reliability of intercity passenger and freight rail.\n",
            "Accuracy: 10/10 (1.0000)\n",
            "\n",
            "Class: FY Corridor Identification and Development Grant Program: This program facilitates the development of intercity passenger rail corridors.\n",
            "Accuracy: 10/10 (1.0000)\n",
            "\n",
            "Class: FY Federal-State Partnership for Intercity Passenger Rail Grant Program for projects not located on the Northeast Corridor: This program funds capital projects that reduce the state of good repair backlog, improve performance, or expand or establish new intercity passenger rail service.\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: Fiscal Year  National Culvert Removal, Replacement, and Restoration Grant Program (Culvert AOP Program): -- ORIGINAL NOFO POSTING - // -- The primary goal of the Culvert AOP Program is to improve or restore anadromous fish passage through the replacement, removal, repair, or improvement of culverts or weirs. The grant program prioritizes projects that would improve fish passage for: (A) anadromous fish stocks listed as an endangered species or a threatened species under section  of the Endangered Species Act of  ( U.S.C. &#; ); (B) anadromous fish stocks identified by the National Marine Fisheries Service (NMFS) or the U.S. Fish and Wildlife Service (USFWS) that could reasonably become listed as an endangered species or a threatened species under that section; (C) anadromous fish stocks identified by the NMFS or the USFWS as prey for endangered species, threatened species, or protected species, including Southern resident orcas (Orcinus orca); or (D) anadromous fish stocks identified by the NMFS or the USFWS as climate resilient stocks ( U.S.C. &#; (e)()). The program also prioritizes projects that would open up more than  meters of upstream (anadromous) habitat before the end of the natural habitat ( U.S.C. &#; (e)()). \n",
            "Accuracy: 9/9 (1.0000)\n",
            "\n",
            "Class: FY  National Infrastructure Investments: Agency Name: Department of TransportationDescription: The Infrastructure Investment and Jobs Act (Pub. L. -, November , , &#;Bipartisan Infrastructure Law,&#; or &#;BIL&#;) authorized and appropriated $. billion to be awarded by the Department of Transportation (&#;DOT&#;) for FY  for Local and Regional Project Assistance Program Grants under National Infrastructure Investments. This Notice of Funding Opportunity (NOFO) solicits applications for projects to be funded under the Local and Regional Project Assistance Program, known as the RAISE Grants program, including any additional funding appropriated for the RAISE Grants program under the FY  Appropriations Act. On March , , the Consolidated Appropriations Act,  (Pub. L. -, &#;FY  Appropriations Act&#;) appropriated an additional $ million for the FY  RAISE Grant Program. Therefore, a total of $. billion in funding is now available for the FY  RAISE Grant Program. Applicants should note that the two funding streams (BIL funding and FY  Appropriations Act funding) have slightly different funding restrictions and requirements that may affect competitiveness. Please see the Notice of Funding Opportunity Amendment No. for details.As with previous rounds, funds for the FY  RAISE Transportation program are to be awarded on a competitive basis for projects that will have a significant local or regional impact.\n",
            "Accuracy: 22/25 (0.8800)\n",
            "\n",
            "Class: FY  Natural Gas Distribution Infrastructure Safety and Modernization Grant: FY  Natural Gas Distribution Infrastructure Safety and Modernization Grant\n",
            "Accuracy: 6/6 (1.0000)\n",
            "\n",
            "Class: The Infrastructure Investment and Jobs Act (IIJA) Notice of Funding Opportunity for America&#;s Marine Highway Program: This notice announces the availability of funding for grants and establishes selection criteria and application requirements for the America&#;s Marine Highway Program (&#;AMHP&#;). The purpose of this program is to make grants available to previously designated Marine Highway Projects that support the development and expansion of documented vessels or port and landside infrastructure. The Department also seeks eligible grant projects that will strengthen American supply chains. The U.S. Department of Transportation (&#;DOT&#; or &#;Department&#;) will award Marine Highway Grants to implement projects or components of projects previously designated by the Secretary of Transportation (&#;Secretary&#;) under the AMHP. Only Marine Highway Projects the Secretary designates before the Notice of Funding Opportunity (&#;NOFO&#;) closing date are eligible for funding as described in this notice. TIMING OF GRANT APPLICATIONS: Applications must be received by the Maritime Administration (&#;MARAD&#;) by : p.m. E.D.T. on April , . ADDRESSES: Grant applications must be submitted electronically using Grants.gov (https://www.grants.gov). Please be aware that you must complete the Grants.gov registration process before submitting your application and that the registration process usually takes  to  weeks to complete. Applicants are strongly encouraged to make submissions in advance of the deadline. FOR FURTHER INFORMATION CONTACT: Fred Jones, Office of Ports &amp; Waterways Planning, Room W&#;, Maritime Administration, U.S. Department of Transportation,  New Jersey Avenue S.E., Washington, D.C. , phone --, or email Fred.Jones@dot.gov. Persons who use a telecommunications device for the deaf (TDD) may call the Federal Information Relay Service (FIRS) at --- to contact the above individual during business hours. The FIRS is available twenty-four hours a day, seven days a week, to leave a message or question with the above individual. You will receive a reply during regular business hours.\n",
            "Accuracy: 1/2 (0.5000)\n",
            "\n",
            "Class: *FY  Notice of Funding Opportunity: Bipartisan Infrastructure Law (BIL) Airport Terminal Program (ATP) Grants: The Department of Transportation (DOT), Federal Aviation Administration (FAA) announces the opportunity to apply for approximately $ billion in FY  discretionary funds for the Airport Terminal Program (ATP), made available under the Infrastructure Investment and Jobs Act of  (IIJA), Pub. L. -, herein referred to as the Bipartisan Infrastructure Law (BIL). The purpose of the ATP is to make annual grants available to eligible airports for airport terminal and airport-owned Airport Traffic Control Towers development projects that address the aging infrastructure of our nation&#;s airports. In addition, ATP grants will align with DOT&#;s Strategic Framework FY- at https://www.transportation.gov/administrations/office-policy/fy--strategic-framework. The FY  ATP will be implemented consistent with law and in alignment with the priorities in Executive Order , Implementation of the Infrastructure Investments and Jobs Act ( FR ), which are to invest efficiently and equitably; promote the competitiveness of the U.S. economy; improve job opportunities by focusing on high labor standards; strengthen infrastructure resilience to all hazards including climate change; and to effectively coordinate with State, local, Tribal, and territorial government partners. \n",
            "Accuracy: 28/28 (1.0000)\n",
            "\n",
            "Class: INFRA Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families. The Infrastructure for Rebuilding America (INFRA) program is an existing competitive program that will see a more than  percent increase in this year&#;s funding due to the Bipartisan Infrastructure Law. These grants advance the Administration&#;s priorities of rebuilding America&#;s infrastructure and creating jobs by funding highway, multimodal freight and rail projects that position America to win the st century. Projects will improve safety, generate economic benefits, reduce congestion, enhance resiliency, and hold the greatest promise to eliminate supply chain bottlenecks and improve critical freight movements. Last year, DOT received over $ billion of project applications, but could only fund around $ billion of projects. To see last year&#;s INFRA grants, click here. The Bipartisan Infrastructure Law provides approximately $ billion for INFRA over  years, of which approximately $. billion will be made available through this NOFO. \n",
            "Accuracy: 0/4 (0.0000)\n",
            "\n",
            "Class: Mega Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The National Infrastructure Project Assistance (MEGA) program was created in the Bipartisan Infrastructure Law to fund major projects that are too large or complex for traditional funding programs. The program will provide grants on a competitive basis to support multijurisdictional or regional projects of significance that may also cut across multiple modes of transportation. Eligible projects could include highway, bridge, freight, port, passenger rail, and public transportation projects of national and regional significance. These could be bridges or tunnels connecting two states; new rail and transit lines that improve equity and reduce emissions; and freight hubs integrating ship, train and truck traffic while improving environmental justice. DOT will award  percent of funding to projects greater than $ million in cost, and  percent to projects greater than $ million but less than $ million in cost. The program will receive up to $ billion this year alone and be able to provide multi-year funding to projects.\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: Rural Surface Transportation Grant Program: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The Rural Surface Transportation Grant Program (RURAL) was created in the Bipartisan Infrastructure Law and will support projects to improve and expand the surface transportation infrastructure in rural areas to increase connectivity, improve the safety and reliability of the movement of people and freight, and generate regional economic growth and improve quality of life. Eligible projects for Rural grants include highway, bridge, and tunnel projects that help improve freight, safety, and provide or increase access to an agricultural, commercial, energy, or transportation facilities that support the economy of a rural area. This year alone, DOT will award up to $ million in grants through the rural program&#;part of the $ billion included in the Bipartisan Infrastructure Law over five years.\n",
            "Accuracy: 0/2 (0.0000)\n",
            "\n",
            "Class: *FY  Notice of Funding Opportunity: Bipartisan Infrastructure Law (BIL) FAA Contract Tower (FCT) Competitive Grant Program: The Department of Transportation (DOT), Federal Aviation Administration (FAA) announces the opportunity to apply for $ million in FY  Airport Infrastructure Grant funds for the FCT competitive grant program, made available under the Infrastructure Investment and Jobs Act of  (IIJA), Pub. L. -, herein referred to as the Bipartisan Infrastructure Law (BIL). The purpose of the FCT competitive grant program is to make annual grants available to eligible airports for airport-owned airport traffic control tower (ATCT) projects that address the aging infrastructure of the nation&#;s airports. The FAA will consider ATCT projects that sustain, construct, repair, improve, rehabilitate, modernize, replace, or relocate non-approach control towers; or acquire and install air traffic control, communications, and related equipment to be used in those towers; or construct a remote tower certified by the FAA including acquisition and installation of air traffic control, communications, or related equipment. In addition, FT grants will align with DOT&#;s Strategic Framework FY- at https://www.transportation.gov/administrations/office-policy/fy--strategic-framework. The FY  FCT will be implemented consistent with law and in alignment with the priorities in Executive Order , Implementation of the Infrastructure Investments and Jobs Act ( FR ), which are to invest efficiently and equitably; promote the competitiveness of the U.S. economy; improve job opportunities by focusing on high labor standards; strengthen infrastructure resilience to all hazards including climate change; and to effectively coordinate with State, local, Tribal, and territorial government partners. \n",
            "Accuracy: 8/8 (1.0000)\n",
            "\n",
            "Class: FY Railroad Crossing Elimination Grant Program: The purpose of the RCE Program is to provide funding for highway-rail and pathway-rail grade crossing improvement projects that focus on improving the safety and mobility of people and goods.\n",
            "Accuracy: 9/9 (1.0000)\n",
            "\n",
            "Class: Reconnecting Communities Pilot Discretionary Grant Program: The Reconnecting Communities Pilot Program notice of funding opportunity was amended on September ,  to include technical corrections to the Key Information Table. See updated materials in the &quot;Manage Related Documents&quot; section, as well as on our website at https://www.transportation.gov/grants/reconnecting-communitiesThe purpose of this notice is to solicit applications for Reconnecting Communities Pilot (RCP) Program grants. Funds for the Fiscal Year (FY)  RCP Program are to be awarded on a competitive basis for projects that reconnect communities by removing, retrofitting, or mitigating highways or other transportation facilities that create barriers to community connectivity, including to mobility, access, or economic development.The variety of transformative solutions to knit communities back together can include: high-quality public transportation, infrastructure removal, pedestrian walkways and overpasses, capping over roadways, linear parks and trail connectors, roadway redesigns and complete streets conversions, and main street revitalization. The program will award two types of grants: Planning Grants and Capital Construction Grants. Please consult the full Notice of Funding Opportunity for full details about the application process, requirements, and forms.\n",
            "Accuracy: 6/7 (0.8571)\n",
            "\n",
            "Class: Safe Streets and Roads for All Discretionary Grant Program: THIS NOTICE OF FUNDING OPPORTUNITY WAS AMENDED ON AUGUST , . PLEASE SEE THE GRANTS.GOV PACKAGE UPDATES, AND THE SAFE STREETS AND ROADS FOR ALL (SSA) WEBSITE IN THE LINK PROVIDED BELOW FOR MORE INFORMATION. DEADLINE REMAINS SEPTEMBER , .The purpose of this notice is to solicit applications for SSA grants. Funds for the fiscal year (FY)  SSA grant program are to be awarded on a competitive basis to support planning, infrastructure, behavioral, and operational initiatives to prevent death and serious injury on roads and streets involving all roadway users, including pedestrians; bicyclists; public transportation, personal conveyance, and micromobility users; motorists; and commercial vehicle operators. Those interested in applying for Implementation Grants should use the application materials in PKG; those interested in applying for Action Plan Grants should use the application materials in PKG. See application instructions to determine which grant to apply for in this funding round.\n",
            "Accuracy: 4/6 (0.6667)\n",
            "\n",
            "Class: Strengthening Mobility and Revolutionizing Transportation (SMART) Grants Program: The purpose of this notice is to solicit applications for Strengthening Mobility and Revolutionizing Transportation (SMART) grants. Funds for the fiscal year (FY)  SMART Grants Program are to be awarded on a competitive basis to conduct demonstration projects focused on advanced smart city or community technologies and systems to improve transportation efficiency and safety.\n",
            "Accuracy: 9/9 (1.0000)\n",
            "\n",
            "Class: FY- Federal-State Partnership for Intercity Passenger Rail Grant Program for projects located on the Northeast Corridor: This program funds capital projects on the Northeast Corridor that reduce the state of good repair backlog, improve performance, or expand or establish new intercity passenger rail service.\n",
            "Accuracy: 4/4 (1.0000)\n",
            "\n",
            "Class: Fiscal Years - Wildlife Crossings Pilot Program (WCPP): The primary goals of the WCPP are to save lives, prevent serious injuries, and protect motorists and wildlife by reducing WVCs, and improve habitat connectivity for terrestrial and aquatic species. Reduction of wildlife vehicle collisions and improvement of terrestrial and aquatic habitat connectivity are the primary merit criteria that will be used in reviewing applications, and each of the primary merit criteria are of equal importance.\n",
            "Accuracy: 3/3 (1.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity; Buses and Bus Facilities Program: The Federal Transit Administration (FTA) announces the availability of approximately $ million in competitive grants under the Buses and Bus Facilities Program is to assist in the financing of buses and bus facilities capital projects, including replacing, rehabilitating, purchasing or leasing buses or related equipment, and rehabilitating, purchasing, constructing or leasing bus-related facilities. Synopses and full announcement will be posted on Grants.gov as opportunity FTA---TPM-BUS. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time April , .\n",
            "Accuracy: 7/7 (1.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity; Low or No Emission Vehicle Program: The Federal Transit Administration (FTA) announces the availability of approximately $. billion in competitive grants under the Low or No Emission Grant Program (Low-No Program) for the purchase or lease of zero-emission and low-emission transit buses, including acquisition, construction, and leasing of required supporting facilities. Synopses and full announcement will be posted on Grants.gov as opportunity FTA---TPM-LWNO. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time April , .\n",
            "Accuracy: 0/12 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Areas of Persistent Poverty Program: To create new opportunities for those experiencing poverty, the U.S. Department of Transportation&#;s Federal Transit Administration (FTA) today announced a new Notice of Funding Opportunity (NOFO) to help improve transit in areas experiencing long-term economic distress. The $ million in competitive grant funding through FTA&#;s Areas of Persistent Poverty (AoPP) Program provides more resources to underserved and disadvantaged communities seeking to expand or improve transit. \n",
            "Accuracy: 7/7 (1.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Ferry Service for Rural Communities Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  Ferry Service for Rural Communities Program (FSRC) (Federal Assistance Listing #.). FSRC makes funding available competitively to assist in the financing of capital, planning, and operating assistance for eligible ferry services.\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $. million in competitive grants under the Fiscal Year (FY)  Passenger Ferry Grant Program (PFG) (Federal Assistance Listing #.). Of that amount, $ million is available only for low or zero-emission ferries or ferries using electric battery or fuel cell components and the infrastructure to support such ferries. As required by Federal public transportation law, funds will be awarded competitively to designated recipients or eligible direct recipients of Urbanized Area Formula funds to support capital projects to improve existing passenger ferry service, establish new ferry service, and repair and modernize ferry boats, terminals, and related facilities and equipment. \n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: FY  National Infrastructure Investments: The Infrastructure Investment and Jobs Act of  (&#;Bipartisan Infrastructure Law,&#; or &#;BIL&#;) authorized and appropriated $. billion to be awarded by the Department of Transportation (&#;DOT&#;) for Local and Regional Project Assistance Program Grants under National Infrastructure Investments in FY . This Notice of Funding Opportunity (NOFO) solicits applications for projects funded under the Local and Regional Project Assistance Program, known as the RAISE Grants program. If the FY  Appropriations Act provides additional funding for the RAISE grants program and/or significantly different requirements for National Infrastructure Investment funds, the Department will amend this Notice with guidance on additional requirements.As with previous rounds, funds for the FY  RAISE Transportation program are to be awarded on a competitive basis for projects that will have a significant local or regional impact.\n",
            "Accuracy: 12/24 (0.5000)\n",
            "\n",
            "Class: Neighborhood Access and Equity (NAE) Program: The Department of Transportation is combining two major discretionary grant programs, the Reconnecting Communities Pilot (RCP) and Neighborhood Access and Equity (NAE) programs, into one Notice of Funding Opportunity (NOFO). Together, this combined program is known as the Reconnecting Communities and Neighborhoods (RCN) Program. The primary goal of the NAE Program is to assist economically disadvantaged or undeserved communities for planning and construction activities. Funds for the fiscal year (FY)  NAE grant program are to be awarded on a competitive basis to support planning, capital construction, and regional partnership activities that aim to restore community connectivity through the removal, retrofit, mitigation or replacement of highways, roadways, or other infrastructure facilities that create barriers to mobility, access or economic development. Applicants must submit their applications via Valid Eval at the links below: Community Planning Grants: https://usg.valideval.com/teams/rcn_planning/signup Capital Construction Grants: https://usg.valideval.com/teams/rcn_capitalconstruction/signup Do not submit applications through Grants.gov. Applications must be submitted by : PM Eastern Time on Thursday, September , . Late applications will not be accepted. \n",
            "Accuracy: 21/21 (1.0000)\n",
            "\n",
            "Class: FY  Notice of Funding Opportunity for Small Shipyard Grants: This notice announces the intention of the Maritime Administration to provide grants to small shipyards. Under the Small Shipyard Grant Program, there is currently $,, available for grants for capital and related improvements to qualified shipyard facilities that will be effective in fostering efficiency, competitive operations, and quality ship construction, repair, and reconfiguration. Potential applicants are advised that it is expected, based on experience, that the number of applications will far exceed the funds available and that only a small percentage of applicants will be funded. Applications (SF- and the Addendum) will be accepted online through Grants.gov by  pm on the closing date (February , ). See full text under Related Documents Tab (FY  SSG NOFO) or the MARAD Web Page for detailed submission instructions.\n",
            "Accuracy: 4/4 (1.0000)\n",
            "\n",
            "Class: Safe Streets and Roads for All Funding Opportunity: The purpose of this notice is to solicit applications for Safe Streets and Roads for All (SSA) grants. Funds for the fiscal year (FY)  SSA grant program are to be awarded on a competitive basis to support planning and demonstration activities, as well as projects and strategies to prevent death and serious injury on roads and streets involving all roadway users, including pedestrians; bicyclists; public transportation, personal conveyance, and micromobility users; motorists; and commercial vehicle operators. Applicants must submit their applications via Valid Eval at https://usg.valideval.com/teams/usdot_ssa__implementation/signup for Implementation Grant applicants, and https://usg.valideval.com/teams/usdot_ssa__planning_demo/signup for Planning and Demonstration Grants. Do not submit your applications through Grants.Gov. Applications must be submitted by : PM Eastern Time on Monday, July , . Late applications will not be accepted.\n",
            "Accuracy: 57/58 (0.9828)\n",
            "\n",
            "Class: SMART Grants Notice of Funding: The purpose of this notice is to solicit applications for Strengthening Mobility and Revolutionizing Transportation (SMART) Stage  Planning and Prototyping grants. Funds for the fiscal year (FY)  SMART Grants Program are to be awarded on a competitive basis to conduct demonstration projects focused on advanced smart city or community technologies and systems to improve transportation efficiency and safety. Applications must be submitted via Valid Eval, an online submission proposal system used by USDOT, at https://usg.valideval.com/teams/USDOT_SMART_/signup. USDOT will not accept or review application materials submitted via Grants.gov. \n",
            "Accuracy: 0/5 (0.0000)\n",
            "\n",
            "Class:  Tribal Transportation Program Safety Fund: Eligible projects described in section (a)() are strategies, activities, and projects on a public road that are consistent with a transportation safety plan and that (i) correct or improve a hazardous road location or feature, or (ii) address a highway safety problem. TTPSF emphasizes the development of strategic transportation safety plans using a data-driven process as a means for Tribes to identify transportation safety needs and determine how those needs will be addressed in Tribal communities. FHWA has identified four eligibility categories: transportation safety plans; data assessment, improvement, and analysis activities; systemic roadway departure countermeasures; and infrastructure improvements and other eligible activities as listed in  U.S.C. &#; (a)().\n",
            "Accuracy: 13/13 (1.0000)\n",
            "\n",
            "Class: United States Marine Highway Grants: This funding opportunity solicits applications for fiscal year (FY)  United States Marine Highway Program (USMHP) grants. Funds for FY  USMHP grants will be awarded on a competitive basis to assist in funding eligible projects for the purpose of developing, expanding, or promoting marine highway transportation. This opportunity announces the availability of up to $,, million in funding for grants under this program and establishes selection criteria and application requirements. All USMHP grant recipients must meet all applicable Federal requirements, including domestic content (&#;Buy America&#;) requirements. This program was formerly known as the America&#;s Marine Highway Program\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: INFRA Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families. The Infrastructure for Rebuilding America (INFRA) program is an existing competitive program that will see a more than  percent increase in this year&#;s funding due to the Bipartisan Infrastructure Law. These grants advance the Administration&#;s priorities of rebuilding America&#;s infrastructure and creating jobs by funding highway, multimodal freight and rail projects that position America to win the st century. Projects will improve safety, generate economic benefits, reduce congestion, enhance resiliency, and hold the greatest promise to eliminate supply chain bottlenecks and improve critical freight movements. Last year, DOT received over $ billion in application requests, but could only fund around $. billion of projects. To see last year&#;s INFRA grants, click here. The Bipartisan Infrastructure Law provides approximately $ billion for INFRA over  years, of which approximately $. billion will be made available through this NOFO. \n",
            "Accuracy: 0/4 (0.0000)\n",
            "\n",
            "Class: Mega Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The National Infrastructure Project Assistance (MEGA) program was created in the Bipartisan Infrastructure Law to fund major projects that are too large or complex for traditional funding programs. The program will provide grants on a competitive basis to support multijurisdictional or regional projects of significance that may also cut across multiple modes of transportation. Eligible projects could include highway, bridge, freight, port, passenger rail, and related public transportation projects of national and regional significance. DOT will award approximately  percent of funding to projects greater than $ million in cost, and approximately  percent to projects greater than $ million but less than $ million in cost.For more information about last year&apos;s Mega awards, visit here: https://www.transportation.gov/grants/mega-grant-program/FYawards\n",
            "Accuracy: 0/2 (0.0000)\n",
            "\n",
            "Class: Rural Surface Transportation Grant Progam: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The Rural Surface Transportation Grant Program (RURAL) was created in the Bipartisan Infrastructure Law and will support projects to improve and expand the surface transportation infrastructure in rural areas to increase connectivity, improve the safety and reliability of the movement of people and freight, and generate regional economic growth and improve quality of life. Eligible projects for Rural grants include highway, bridge, and tunnel projects that help improve freight, safety, and provide or increase access to an agricultural, commercial, energy, or transportation facilities that support the economy of a rural area. This year alone, DOT will award up to $ million in grants through the rural program&#;part of the $ billion included in the Bipartisan Infrastructure Law over five years. At least % of rural funding must be awarded in amounts of $ million or more. If you are seeking less than $ million, you are competing for only about $. million nationwide.For more information on last year&apos;s rural awards, see: https://www.transportation.gov/grants/rural--fact-sheets\n",
            "Accuracy: 0/5 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Rail Vehicle Replacement Grant Program (RAIL): The Federal Transit Administration (FTA) announces the opportunity to apply for approximately $ million in Fiscal Year (FY)  Section  State of Good Repair Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected rail vehicle replacement projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to States and local governmental authorities to assist in the funding of capital projects to replace rail rolling stock.\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'FY  Notice of Funding Opportunity: Bipartisan Infrastructure Law (BIL) Airport Terminal Program (ATP) Grants: The Department of Transportation (DOT), Federal Aviation Administration (FAA) announces the opportunity to apply for approximately $ billion in FY  discretionary funds for the Airport Terminal Program (ATP), made available under the Infrastructure Investment and Jobs Act of  (IIJA), Pub. L. -, herein referred to as the Bipartisan Infrastructure Law (BIL). The purpose of the ATP is to make annual grants available to eligible airports for airport terminal and airport-owned Airport Traffic Control Towers development projects that address the aging infrastructure of our nation&#;s airports. In addition, ATP grants will align with DOT&#;s Strategic Framework FY- at https://www.transportation.gov/administrations/office-policy/fy--strategic-framework. The FY  ATP will be implemented consistent with law and in alignment with the priorities in Executive Order , Implementation of the Infrastructure Investments and Jobs Act ( FR ), which are to invest efficiently and equitably; promote the competitiveness of the U.S. economy; improve job opportunities by focusing on high labor standards; strengthen infrastructure resilience to all hazards including climate change; and to effectively coordinate with State, local, Tribal, and territorial government partners. ': 0.48,\n",
              " 'FY  Competitive Funding Opportunity: Airport Improvement Program Discretionary Grants: The U.S. Department of Transportation&#;s Federal Aviation Administration (FAA) announces the opportunity to apply for an estimated $. billion in Fiscal Year (FY)  discretionary grants under the Airport Improvement Program (AIP), per  U.S.C. &#; . The FAA awards these annually appropriated discretionary funds through the FAA&#;s long-standing iterative, competitive grant process. Prior to the publication of this Notice of Funding Opportunity (NOFO), the FAA identified eligible applicants in its National Plan of Integrated Airport Systems (NPIAS) and compiled potentially eligible projects through the -year Airports Capital Improvement Plan (ACIP). Both of these processes are described in FAA Order ., &#;Formulation of NPIAS and ACIP,&#; adhering to  U.S.C. &#; , which authorizes discretionary funds. The AIP funds airport capital improvements and rehabilitation projects. All discretionary grant funding is subject to appropriations, statutory requirements, and related program funding availability. ': 1.0,\n",
              " 'Grants for Buses and Bus Facilities Program: The Federal Transit Administration (FTA) announces the availability of approximately $. million of Fiscal Year  funds for buses, bus facilities, and bus equipment. The purpose of the Grants for Buses and Bus Facilities Program is to assist in the financing of buses and bus facilities capital projects, including replacing, rehabilitating, purchasing or leasing buses or related equipment, and rehabilitating, purchasing, constructing or leasing bus-related facilities. ': 0.9285714285714286,\n",
              " 'Solicitation of Project Proposals for the Low or No Emission (Low-No) Program:  Notice of Funding Opportunity (NOFO): Solicitation of Project Proposals for the Low or No Emission Program (Low-No) Program. The Federal Transit Administration (FTA) announces the availability of $ million of Fiscal Year  funds for the purchase or lease of low or no emission vehicles as well as related equipment or facilities. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time on //. The main purpose of the Low-No Program is to support the transition of the nation&#;s transit fleet to the lowest polluting and most energy efficient transit vehicles. The Low-No Program provides funding to State and local governmental authorities for the purchase or lease of zero-emission and low-emission transit buses, including acquisition, construction, and leasing of required supporting facilities. ': 0.0,\n",
              " 'FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in Fiscal Year (FY)  Section  Urbanized Area Formula Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected passenger ferry projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to designated recipients or eligible direct recipients of Section  funds to assist in the financing of capital projects to support existing passenger ferry service, establish new ferry service, and to repair and modernize ferry boats, terminals, and related facilities and equipment. ': 0.0,\n",
              " 'Pilot Program for Transit-Oriented Development (TOD) Planning: Notice of Funding Opportunity for the Pilot Program for Transit-Oriented Development (TOD) Planning. The Federal Transit Administration (FTA) announces the availability of approximately $. million in Pilot Program for TOD Planning funding to support comprehensive planning associated with new fixed guideway and core capacity improvement projects. FTA may award amounts ranging from $, to $,,. The Pilot Program for TOD Planning provides funding to local communities to integrate land use and transportation planning in new fixed guideway and core capacity transit project corridors. As required by statute, any comprehensive planning funded through the pilot program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations. The statute also requires that the planning work be associated with a new fixed guideway or core capacity transit project as defined in Federal transit statute ( USC (a); also see the NOFO for the definitions). Consistent with statutory direction, FTA is seeking comprehensive planning projects covering an entire transit capital project corridor, rather than proposals that involve planning for individual station areas or only a small section of the corridor. To ensure that planning work reflects the needs and aspirations of the local community and results in concrete, specific deliverables and outcomes, FTA is requiring that transit project sponsors partner with entities with land use planning authority in the transit project corridor. ': 1.0,\n",
              " 'Public Transportation on Indian Reservations Program: The Federal Transit Administration (FTA) announces the availability of approximately $ million in funding provided by the Public Transportation on Indian Reservations Program (Tribal Transit Program (TTP)), as authorized by  U.S.C. (c)()(A), as amended by the Fixing America&apos;s Surface Transportation Act (FAST), Public Law - (December , ). The primary purpose of these competitively selected grants is to support planning, capital, and, in limited circumstances, operating assistance for tribal public transit services. Funds distributed to Indian tribes under the TTP should not replace or reduce funds that Indian tribes receive from states through FTA&#;s Section  program. ': 0.0,\n",
              " 'INFRA Grants: The Nationally Significant Freight and Highway Projects (INFRA) program provides Federal financial assistance to highway and freight projects of national or regional significance. This notice solicits applications for awards under the program&#;s fiscal year (FY)  funding, subject to the availability of appropriated funds. The Department is focusing the competition on transportation infrastructure projects that support four key objectives, each of which is discussed in greater detail in the Notice of Funding Opportunity: () Supporting economic vitality at the national and regional level; () Leveraging Federal funding to attract non-Federal sources of infrastructure investment; () Deploying innovative technology, encouraging innovative approaches to project delivery, and incentivizing the use of innovative financing; and () Holding grant recipients accountable for their performance. ': 0.6666666666666666,\n",
              " ' Tribal Transportation Program Safety Fund: A Notice of Funding Availability (NOFA) for Tribal Transportation Program Safety Funds (TTPSF) has been published. This notice announces the availability of $. million for projects in three categories: safety plans; data assessment, improvement, and analysis activities; and infrastructure improvement and other eligible activities as listed in  U.S.C.(a)() The deadline for submissions is //. For complete instructions on preparing and submitting an application for TTPSF, please download the application information and NOFO at https://flh.fhwa.dot.gov/programs/ttp/safety/ttpsf.htm Please submit your application directly to FHWA through the &quot;apply now&quot; button at: https://flh.fhwa.dot.gov/programs/ttp/safety/ttpsf.htm ': 1.0,\n",
              " 'Grants for Buses and Bus Facilities Program: The purpose of the Grants for Buses and Bus Facilities Program is to assist in the financing of buses and bus facilities capital projects, including replacing, rehabilitating, purchasing or leasing buses or related equipment, and rehabilitating, purchasing, constructing or leasing bus-related facilities.': 0.9285714285714286,\n",
              " 'Low or No Emission Program (Low-No Program): The main purpose of the Low-No Program is to support the transition of the nation&#;s transit fleet to the lowest polluting and most energy efficient transit vehicles. The Low-No Program provides funding to State and local governmental authorities for the purchase or lease of zero-emission and low-emission transit buses, including acquisition, construction, and leasing of required supporting facilities.': 1.0,\n",
              " 'Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in Fiscal Year (FY)  Section  Urbanized Area Formula Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected passenger ferry projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to designated recipients or eligible direct recipients of Section  funds to assist in the financing of capital projects to support existing passenger ferry service, establish new ferry service, and to repair and modernize ferry boats, terminals, and related facilities and equipment. ': 0.0,\n",
              " 'Pilot Program for Transit-Oriented Development (TOD) Planning: Notice of Funding Opportunity for PILOT PROGRAM FOR TRANSIT-ORIENTED DEVELOPMENT (TOD) PLANNING. The Federal Transit Administration (FTA) announces the opportunity to apply for approximately $. million of funding under the Pilot Program for TOD Planning (Catalog of Federal Domestic Assistance #.). As required by federal transit law and subject to funding availability, funds will be awarded competitively to support comprehensive planning associated with new fixed guideway and core capacity improvement projects. FTA anticipates minimum grant awards of $, and maximum grant awards of $,,. The Pilot Program for TOD Planning is intended to fund comprehensive planning that supports economic development, ridership, multimodal connectivity and accessibility, increased transit access for pedestrian and bicycle traffic, and mixed-use development near transit stations. The program also encourages identification of infrastructure needs and engagement with the private sector. Consistent with statutory direction, FTA is seeking comprehensive planning projects covering an entire transit capital project corridor, rather than proposals that involve planning for individual station areas or only a small section of the corridor. To ensure any proposed planning work reflects the needs and aspirations of the local community and results in concrete, specific deliverables and outcomes, transit project sponsors must partner with entities with land use planning authority in the transit project corridor to conduct the planning work.The Pilot Program for TOD Planning helps support FTA&#;s mission of improving public transportation for America&#;s communities by providing funding to local communities to integrate land use and transportation planning around a new fixed guideway or core capacity improvement project. Per statute, any comprehensive planning funded through the program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations.': 0.0,\n",
              " 'FY  Competitive Funding Opportunity: Public Transportation on Indian Reservations Program: The Federal Transit Administration (FTA) announces the availability of approximately $ million in funding provided by the Public Transportation on Indian Reservations Program (Tribal Transit Program (TTP)), as authorized by  U.S.C. (c)()(A), as amended by the Fixing America&apos;s Surface Transportation Act (FAST), Public Law - (December , ). This is a national solicitation for project proposals and includes the selection criteria and program eligibility information for Fiscal Year  projects. The primary purpose of these competitively selected grants is to support planning, capital, and, in limited circumstances, operating assistance for tribal public transit services. Funds distributed to Indian tribes under the TTP should not replace or reduce funds that Indian tribes receive from states through FTA&#;s Section  program.': 0.0,\n",
              " '*INFRA Grants: The U.S. Department of Transportation (USDOT) is seeking applicants for the FY  round of the Infrastructure for Rebuilding America (INFRA) discretionary grant program to fund transportation projects of national and regional significance that result in good-paying jobs, improve safety, apply transformative technology, and explicitly address climate change and racial equity. The funding available for this year&#;s grants totals approximately $ million.The Department recognizes the role that infrastructure investment plays in economic development and job creation, and the added urgency of this funding at time when the COVID- pandemic has put stress on state and local budgets.For the first time, the USDOT seeks INFRA projects that address climate change and environmental justice. Projects will be evaluated on whether they were planned as part of a comprehensive strategy to address climate change, or whether they support strategies to reduce greenhouse gas emissions such as deploying zero-emission-vehicle infrastructure or encouraging modal shift and a reduction in vehicle-miles-traveled. Racial equity will also be considered as a selection criterion, to the extent that project sponsors have completed equity-focused community outreach, and projects are designed to benefit underserved communities. The Department will also consider whether the project is located in a federally designated community development zone, including qualified Opportunity Zones, Empowerment Zones, Promise Zones, or Choice Neighborhoods.USDOT seeks projects that apply innovative technology, delivery, or financing methods with proven outcomes to deliver projects in a cost effective manner. The Department will make awards under the INFRA program to both large and small projects. For a large project, the INFRA grant must be at least $ million. For a small project, the grant must be at least $ million. Under statutory requirements,  percent of available funds are reserved for small projects, and the Department must award at least  percent of funding for rural projects. INFRA grants may be used to fund a variety of components of an infrastructure project, however, the Department is specifically focused on projects in which the local sponsor is significantly invested and is positioned to proceed rapidly to construction. Eligible INFRA project costs may include: reconstruction, rehabilitation, acquisition of property (including land related to the project and improvements to the land), environmental mitigation, construction contingencies, equipment acquisition, and operational improvements directly related to system performance. The INFRA NOFO also announces the creation of the &#;INFRA Extra&#; Program, which will identify competitive INFRA applicants who do not receive an INFRA award and authorize them to seek a Transportation Infrastructure Finance and Innovation Act of  (TIFIA) loan up to  percent of their project cost.': 0.6666666666666666,\n",
              " 'Fiscal Year  Competitive Funding Opportunity; Grants for Buses and Bus Facilities Program: The Federal Transit Administration (FTA) announces the availability of approximately $. million of Fiscal Year  funds for buses, bus facilities, and bus equipment. Synopses and full announcement will be posted on Grants.gov as opportunity FTA---BUS. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time November ,.': 0.0,\n",
              " 'FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  Passenger Ferry Grant Program (Ferry Program) (Federal Assistance Listing #.). Of that amount, $ million is available only for low or zero-emission ferries or ferries using electric battery or fuel cell components and the infrastructure to support such ferries. As required by Federal public transportation law, funds will be awarded competitively to designated recipients or eligible direct recipients of Urbanized Area Formula funds to support capital projects to improve existing passenger ferry service, establish new ferry service, and repair and modernize ferry boats, terminals, and related facilities and equipment.': 0.0,\n",
              " ' Competitive Funding Opportunity: Pilot Program for Transit-Oriented Development (TOD) Planning: The Pilot Program for TOD Planning helps support FTA&#;s mission of improving public transportation for America&#;s communities by providing funding to local communities to integrate land use and transportation planning around a new fixed guideway or core capacity improvement project. Per statute, any comprehensive planning funded through the program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations. Additional information is available at: https://www.transit.dot.gov/TODPilot.Notice of Funding Opportunity (NOFO): Notice of Funding Opportunity for PILOT PROGRAM FOR TRANSIT-ORIENTED DEVELOPMENT (TOD) PLANNING. The Federal Transit Administration (FTA) announces the opportunity to apply for $,, of funding under the Pilot Program for TOD Planning (Catalog of Federal Domestic Assistance #.). As required by federal transit law and subject to funding availability, funds will be awarded competitively to support comprehensive planning associated with new fixed guideway and core capacity improvement projects. FTA anticipates minimum grant awards of $, and maximum grant awards of $,,. Synopses and full announcement are posted on Grants.gov as opportunity FTA---TPE-TODP. The Pilot Program for TOD Planning is intended to fund comprehensive planning that supports economic development, ridership, multimodal connectivity and accessibility, increased transit access for pedestrian and bicycle traffic, and mixed-use development near transit stations. The program also encourages identification of infrastructure needs and engagement with the private sector.Consistent with statutory direction, FTA is seeking comprehensive planning projects covering an entire transit capital project corridor, rather than proposals that involve planning for individual station areas or only a small section of the corridor. To ensure any proposed planning work reflects the needs and aspirations of the local community and results in concrete, specific deliverables and outcomes, transit project sponsors must partner with entities with land use planning authority in the transit project corridor to conduct the planning work.Link and Instructions for attaching the supplemental form to the SF-: All applicants must complete a supplemental form (PDF) specific to the Pilot Program for TOD Planning and attach it to their submission in Grants.gov. See section D of the NOFO for information about additional required application contents.Webinar: FTA will conduct a webinar for prospective applicants after the NOFO is published in the Federal Register. Further information will be posted on FTA&#;s website (https://www.transit.dot.gov/TODPilot) when available. Dates: An applicant must submit a proposal electronically by : p.m. Eastern Daylight Time on June , . Any agency intending to apply should initiate the process of registering on the Grants.gov site immediately to ensure completion of registration before the submission deadline. ': 0.0,\n",
              " 'FY  Competitive Funding Opportunity:  Public Transportation on Indian Reservations Program; Tribal Transit Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants for the Fiscal Year (FY)  Public Transportation on Indian Reservations (Tribal Transit) Program (Federal Assistance Listing: .). As required by Federal public transportation law, funds will be awarded competitively for any purpose eligible under FTA&#;s Formula Grants for Rural Areas Program,  U.S.C. , including planning, capital, and operating assistance for tribal public transit services in rural areas. ': 1.0,\n",
              " 'INFRA Grants: The U.S. Department of Transportation (USDOT) is seeking applicants for the FY  round of the Infrastructure for Rebuilding America (INFRA) discretionary grant program to fund transportation projects of national and regional significance that result in good-paying jobs, improve safety, apply transformative technology, and explicitly address climate change and racial equity. The funding available for this year&#;s grants totals approximately $ million.The Department recognizes the role that infrastructure investment plays in economic development and job creation, and the added urgency of this funding at time when the COVID- pandemic has put stress on state and local budgets.For the first time, the USDOT seeks INFRA projects that address climate change and environmental justice. Projects will be evaluated on whether they were planned as part of a comprehensive strategy to address climate change, or whether they support strategies to reduce greenhouse gas emissions such as deploying zero-emission-vehicle infrastructure or encouraging modal shift and a reduction in vehicle-miles-traveled. Racial equity will also be considered as a selection criterion, to the extent that project sponsors have completed equity-focused community outreach, and projects are designed to benefit underserved communities. The Department will also consider whether the project is located in a federally designated community development zone, including qualified Opportunity Zones, Empowerment Zones, Promise Zones, or Choice Neighborhoods.USDOT seeks projects that apply innovative technology, delivery, or financing methods with proven outcomes to deliver projects in a cost effective manner. The Department will make awards under the INFRA program to both large and small projects. For a large project, the INFRA grant must be at least $ million. For a small project, the grant must be at least $ million. Under statutory requirements,  percent of available funds are reserved for small projects, and the Department must award at least  percent of funding for rural projects. INFRA grants may be used to fund a variety of components of an infrastructure project, however, the Department is specifically focused on projects in which the local sponsor is significantly invested and is positioned to proceed rapidly to construction. Eligible INFRA project costs may include: reconstruction, rehabilitation, acquisition of property (including land related to the project and improvements to the land), environmental mitigation, construction contingencies, equipment acquisition, and operational improvements directly related to system performance. The INFRA NOFO also announces the creation of the &#;INFRA Extra&#; Program, which will identify competitive INFRA applicants who do not receive an INFRA award and authorize them to seek a Transportation Infrastructure Finance and Innovation Act of  (TIFIA) loan up to  percent of their project cost.': 0.0,\n",
              " 'FY Tribal Transportation Program Safety Fund (TTPSF): A Notice of Funding Opportunity (NOFO) for Tribal Transportation Program Safety Funds (TTPSF) has been published. This notice announces the availability of approximately $. million for projects in four categories: safety plans; data assessment, improvement, and analysis activities; systemic roadway departure countermeasures; and infrastructure improvement and other eligible activities as listed in  U.S.C.(a)(). The goal of these projects is to reduce fatalities and serious injuries from transportation incidents, such as motor vehicle crashes. For complete instructions on preparing and submitting an application for TTPSF, please download the application, application information, systemic roadway departure risk assessment form (if applicable) and NOFO at https://highways.dot.gov/federal-lands/programs-tribal/safety/funds. Applications will NOT be submitted on Grants.gov. Instead, please use the &quot;apply now&quot; button on the program web page.': 0.5555555555555556,\n",
              " 'Advanced Transportation Technologies and  Innovative Mobility Deployment (ATTIMD) Program: AMENDMENT  (//): This amendment is for the purpose of correcting a typo contained in Table D. on page  of . This table, contained in Section D. for CONTENT AND FORM OF APPLICATION SUBMISSION, erroneously omitted the Project Outcome Criteria from that table&#;s submission requirements for Volume . This amendment also revises the verbiage included on page  of the NOFO to clarify that all resumes for key personnel should be included in item f) Appendix &#; R&#;sum&#;s for Key Personnel. Please note that Resumes do not count against the page limitations for Volume . Please refer to Amendment  in the &#;Related Documents&#; tab for more information. -- END OF AMENDMENT  -- AMENDMENT  (//): This amendment adds the weblink to the recording of the October , , informational webinar for this opportunity. The weblink is included in the attachments page for this opportunity as well as below: LINK: https://usdot.zoomgov.com/rec/share/eQNNRGISy-UxfqGpSDzINRbzvcdkAjDdoulZvwRCt-sARp.kdAJIUFsFZZEpf PASSCODE: c@X*&amp;@ -- END OF AMENDMENT  -- AMENDMENT  (//): This amendment adds the registration information for the October , , informational webinar for this opportunity. Please refer to the &quot;Related Documents&quot; tab for more information. -- END OF AMENDMENT  -- BASE NOFO (//): The Advanced Transportation Technologies and Innovative Mobility Deployment Program (ATTIMD), established in Section (c)() of title , United States Code (U.S.C.), also known as the Advanced Transportation Technology and Innovation (ATTAIN) Program, directs FHWA to award grants to eligible entities to deploy, install, and operate advanced transportation technologies to improve safety, mobility, efficiency, system performance, intermodal connectivity, and infrastructure return on investment. FHWA intends for these model technology deployments to help demonstrate how emerging transportation technologies, data, and their applications can be effectively deployed and integrated with existing systems to provide access to essential services and other destinations. This also includes efforts to increase connectivity to employment, education, services, and other opportunities; support workforce development; or contribute to increased mobility, particularly for persons with visible and hidden disabilities and elderly individuals. The Assistance Listing Number for this opportunity is . &#; Highway Research and Development. The Federal Highway Administration (FHWA) hereby requests applications to result in awards to eligible entities to deploy, install, and operate advanced transportation technologies to improve safety, mobility, efficiency, system performance, intermodal connectivity, and infrastructure return on investment. These model deployments are expected to provide benefits in the form of: reduced traffic-related fatalities and injuries; reduced traffic congestion and improved travel time reliability; reduced transportation-related emissions; optimized multimodal system performance; improved access to transportation alternatives, including for underserved populations; improved integration of payment systems; public access to real-time integrated traffic, transit, and multimodal transportation information to make informed travel decisions; cost savings to transportation agencies, businesses, and the traveling public; or other benefits to transportation users and the general public. This competitive ATTAIN Program will promote the use of innovative transportation solutions. The deployment of these technologies will provide Congress and FHWA with valuable real-life data and feedback to inform future decision-making. Cost Sharing: Cost-sharing or matching is required, with the maximum Federal share being  percent; hence, this NOFO requires a minimum non-Federal cost share of  percent. Cost sharing or matching means the portion of project costs not paid by Federal funds. NOTE: FHWA previously issued six NOFOs for the Advanced Transportation and Congestion Management Technologies Deployment (ATCMTD) Grant Program before the program was amended and renamed to the Advanced Transportation Technologies and Innovative Mobility Deployment Program by Section  of the Infrastructure Investment and Jobs Act (IIJA) (Public Law -, also known as the &#;Bipartisan Infrastructure Law&#; (BIL)).': 0.0,\n",
              " 'FY  and FY  Competitive Funding Opportunity: Competitive Grants for Rail Vehicle Replacement Program (Rail Program): The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in Fiscal Year (FY)  and  Section  State of Good Repair Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected rail vehicle replacement projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to States and local governmental authorities to assist in the funding of capital projects to replace rail rolling stock.': 0.0,\n",
              " 'Bridge Investment Program - Planning, Bridge Projects, and Large Bridge Projects: This Notice of Funding Opportunity (NOFO) is to solicit applications for three categories of Bridge Investment Program (BIP) funding opportunities: () Planning; () Bridge Projects (projects with eligible costs less than $ million); and () Large Bridge Projects (projects with eligible costs greater than $ million). Eligible applicants may submit applications for any of the three funding categories, but each category has distinct eligibility and selection criteria and application deadlines.These funds will be awarded on a competitive basis for planning, feasibility analysis, and revenue forecasting associated with the development of a project that would subsequently be eligible to apply for BIP funding under either the Bridge Projects or Large Bridge Projects funding categories.A total of $. billion in  BIP funds are available for the Bridge Projects and Large Bridge Projects funding opportunities. These funds will be awarded on a competitive basis for bridge replacement, rehabilitation, preservation, and protection projects that: () improve the safety, efficiency, and reliability of the movement of people and freight over bridges; and () improve the condition of bridges in the United States by reducing (a) the number of bridges, and total person miles traveled over bridges, that are in poor condition or that are in fair condition and at risk of falling into poor condition within the next three years, or (b) the number of bridges, and total person miles traveled over bridges, that do not meet current geometric design standards or cannot meet the load and traffic requirements typical of the regional transportation network. In addition, Large Bridge Projects that receive a BIP award of not less than $ million are eligible for multiyear grants, in which DOT can award available funds to a project over the course of several years in accordance with an agreement and in alignment with its schedule. In selecting Bridge Projects and Large Bridge Projects, FHWA will consider the extent to which BIP funds leverage non-Federal contributions from sponsors and stakeholders involved in the planning, design, and construction of eligible projects. ': 0.8333333333333334,\n",
              " 'FY  Competitive Funding Opportunity; Low or No Emission Grant Program: Low-No ProgramThe Federal Transit Administration (FTA) announces the opportunity to apply for approximately $. billion in competitive grants under the fiscal year (FY)  Low or No Emission Grant Program (Low-No Program) (Federal Assistance Listing: .) and approximately $ million in FY  funds under the Grants for Buses and Bus Facilities Program (Buses and Bus Facilities Program) (Federal Assistance Listing .), subject to availability of appropriated funding. Synopses and full announcement will be posted on Grants.gov as funding opportunity ID FTA--TPM for Low-No applications and FTA--TPM for Buses and Bus Facilities applications. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time May , .': 0.9333333333333333,\n",
              " 'FY  Competitive Funding Opportunity: All Stations Accessibility Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  All Stations Accessibility Program (ASAP) (Federal Assistance Listing #.). As required by Federal public transportation law, funds will be awarded competitively to eligible designated recipients that operate or allocate funds to inaccessible pre-ADA&#;or &#;legacy&#; &#; rail fixed guideway public transportation systems, and States (including territories and Washington, D.C.) and local governmental entities that operate or financially support legacy rail fixed guideway public transportation systems and corresponding legacy stations/facilities for capital projects to repair, improve, modify, retrofit, or relocate infrastructure of stations or facilities for passenger use, including load-bearing members that are an essential part of the structural frame; or () for planning projects to develop or modify a plan for pursuing public transportation accessibility projects, assessments of accessibility, or assessments of planned modifications to stations or facilities for passenger use projects; or programs of projects in an eligible area.': 1.0,\n",
              " 'FY  Competitive Funding Opportunity: Buses and Bus Facilities Competitive Program: Bus and Bus Facilities ProgramThe Federal Transit Administration (FTA) announces the opportunity to apply for approximately $. billion in competitive grants under the fiscal year (FY)  Low or No Emission Grant Program (Low-No Program) (Federal Assistance Listing: .) and approximately $ million in FY  funds under the Grants for Buses and Bus Facilities Program (Buses and Bus Facilities Program) (Federal Assistance Listing .), subject to availability of appropriated funding. Synopses and full announcement will be posted on Grants.gov as funding opportunity ID FTA--TPM for Low-No applications and FTA--TPM for Buses and Bus Facilities applications. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time May , ': 0.0,\n",
              " 'FY  Competitive Funding Opportunity: Electric or Low-Emitting Ferry Pilot Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  Electric or Low-Emitting Ferry Pilot Program (ELEF) (Federal Assistance Listing #.). ELEF makes funding available competitively to assist in the financing of capital projects for the purchase of electric or low-emitting ferry vessels that reduce emissions by using alternative fuels or on-board energy storage systems and related charging infrastructure to reduce emissions or produce zero onboard emissions under normal operation.': 0.0,\n",
              " 'FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $. million in competitive grants under the Fiscal Year (FY)  Passenger Ferry Grant Program (PFG) (Federal Assistance Listing #.). Of that amount, $. million is available only for low or zero-emission ferries or ferries using electric battery or fuel cell components and the infrastructure to support such ferries. As required by Federal public transportation law, funds will be awarded competitively to designated recipients or eligible direct recipients of Urbanized Area Formula funds to support capital projects to improve existing passenger ferry service, establish new ferry service, and repair and modernize ferry boats, terminals, and related facilities and equipment. ': 0.0,\n",
              " 'FY  Pilot Program for Transit-Oriented Development (TOD) Planning: The Pilot Program for TOD Planning helps support FTA&#;s mission of improving public transportation for America&#;s communities by providing funding to local communities to integrate land use and transportation planning around a new fixed guideway or core capacity improvement project. Per statute, any comprehensive or site specific planning funded through the program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations.': 1.0,\n",
              " 'FY  Competitive Funding Opportunity: Public Transportation on Indian Reservations Program; Tribal Transit Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $. million in competitive grants for the Fiscal Year (FY)  Public Transportation on Indian Reservations (Tribal Transit) Program. As required by Federal public transportation law, funds will be awarded competitively for any purpose eligible under FTA&#;s Formula Grants for Rural Areas Program, including planning, capital, and operating assistance for tribal public transit services in rural areas. FTA may award additional funding that is made available to the program prior to the announcement of project selections.': 0.0,\n",
              " 'FY Consolidated Rail Infrastructure and Safety Improvements Grant Program: This program funds projects that improve the safety, efficiency, and reliability of intercity passenger and freight rail.': 1.0,\n",
              " 'FY Corridor Identification and Development Grant Program: This program facilitates the development of intercity passenger rail corridors.': 1.0,\n",
              " 'FY Federal-State Partnership for Intercity Passenger Rail Grant Program for projects not located on the Northeast Corridor: This program funds capital projects that reduce the state of good repair backlog, improve performance, or expand or establish new intercity passenger rail service.': 0.0,\n",
              " 'Fiscal Year  National Culvert Removal, Replacement, and Restoration Grant Program (Culvert AOP Program): -- ORIGINAL NOFO POSTING - // -- The primary goal of the Culvert AOP Program is to improve or restore anadromous fish passage through the replacement, removal, repair, or improvement of culverts or weirs. The grant program prioritizes projects that would improve fish passage for: (A) anadromous fish stocks listed as an endangered species or a threatened species under section  of the Endangered Species Act of  ( U.S.C. &#; ); (B) anadromous fish stocks identified by the National Marine Fisheries Service (NMFS) or the U.S. Fish and Wildlife Service (USFWS) that could reasonably become listed as an endangered species or a threatened species under that section; (C) anadromous fish stocks identified by the NMFS or the USFWS as prey for endangered species, threatened species, or protected species, including Southern resident orcas (Orcinus orca); or (D) anadromous fish stocks identified by the NMFS or the USFWS as climate resilient stocks ( U.S.C. &#; (e)()). The program also prioritizes projects that would open up more than  meters of upstream (anadromous) habitat before the end of the natural habitat ( U.S.C. &#; (e)()). ': 1.0,\n",
              " 'FY  National Infrastructure Investments: Agency Name: Department of TransportationDescription: The Infrastructure Investment and Jobs Act (Pub. L. -, November , , &#;Bipartisan Infrastructure Law,&#; or &#;BIL&#;) authorized and appropriated $. billion to be awarded by the Department of Transportation (&#;DOT&#;) for FY  for Local and Regional Project Assistance Program Grants under National Infrastructure Investments. This Notice of Funding Opportunity (NOFO) solicits applications for projects to be funded under the Local and Regional Project Assistance Program, known as the RAISE Grants program, including any additional funding appropriated for the RAISE Grants program under the FY  Appropriations Act. On March , , the Consolidated Appropriations Act,  (Pub. L. -, &#;FY  Appropriations Act&#;) appropriated an additional $ million for the FY  RAISE Grant Program. Therefore, a total of $. billion in funding is now available for the FY  RAISE Grant Program. Applicants should note that the two funding streams (BIL funding and FY  Appropriations Act funding) have slightly different funding restrictions and requirements that may affect competitiveness. Please see the Notice of Funding Opportunity Amendment No. for details.As with previous rounds, funds for the FY  RAISE Transportation program are to be awarded on a competitive basis for projects that will have a significant local or regional impact.': 0.88,\n",
              " 'FY  Natural Gas Distribution Infrastructure Safety and Modernization Grant: FY  Natural Gas Distribution Infrastructure Safety and Modernization Grant': 1.0,\n",
              " 'The Infrastructure Investment and Jobs Act (IIJA) Notice of Funding Opportunity for America&#;s Marine Highway Program: This notice announces the availability of funding for grants and establishes selection criteria and application requirements for the America&#;s Marine Highway Program (&#;AMHP&#;). The purpose of this program is to make grants available to previously designated Marine Highway Projects that support the development and expansion of documented vessels or port and landside infrastructure. The Department also seeks eligible grant projects that will strengthen American supply chains. The U.S. Department of Transportation (&#;DOT&#; or &#;Department&#;) will award Marine Highway Grants to implement projects or components of projects previously designated by the Secretary of Transportation (&#;Secretary&#;) under the AMHP. Only Marine Highway Projects the Secretary designates before the Notice of Funding Opportunity (&#;NOFO&#;) closing date are eligible for funding as described in this notice. TIMING OF GRANT APPLICATIONS: Applications must be received by the Maritime Administration (&#;MARAD&#;) by : p.m. E.D.T. on April , . ADDRESSES: Grant applications must be submitted electronically using Grants.gov (https://www.grants.gov). Please be aware that you must complete the Grants.gov registration process before submitting your application and that the registration process usually takes  to  weeks to complete. Applicants are strongly encouraged to make submissions in advance of the deadline. FOR FURTHER INFORMATION CONTACT: Fred Jones, Office of Ports &amp; Waterways Planning, Room W&#;, Maritime Administration, U.S. Department of Transportation,  New Jersey Avenue S.E., Washington, D.C. , phone --, or email Fred.Jones@dot.gov. Persons who use a telecommunications device for the deaf (TDD) may call the Federal Information Relay Service (FIRS) at --- to contact the above individual during business hours. The FIRS is available twenty-four hours a day, seven days a week, to leave a message or question with the above individual. You will receive a reply during regular business hours.': 0.5,\n",
              " '*FY  Notice of Funding Opportunity: Bipartisan Infrastructure Law (BIL) Airport Terminal Program (ATP) Grants: The Department of Transportation (DOT), Federal Aviation Administration (FAA) announces the opportunity to apply for approximately $ billion in FY  discretionary funds for the Airport Terminal Program (ATP), made available under the Infrastructure Investment and Jobs Act of  (IIJA), Pub. L. -, herein referred to as the Bipartisan Infrastructure Law (BIL). The purpose of the ATP is to make annual grants available to eligible airports for airport terminal and airport-owned Airport Traffic Control Towers development projects that address the aging infrastructure of our nation&#;s airports. In addition, ATP grants will align with DOT&#;s Strategic Framework FY- at https://www.transportation.gov/administrations/office-policy/fy--strategic-framework. The FY  ATP will be implemented consistent with law and in alignment with the priorities in Executive Order , Implementation of the Infrastructure Investments and Jobs Act ( FR ), which are to invest efficiently and equitably; promote the competitiveness of the U.S. economy; improve job opportunities by focusing on high labor standards; strengthen infrastructure resilience to all hazards including climate change; and to effectively coordinate with State, local, Tribal, and territorial government partners. ': 1.0,\n",
              " 'INFRA Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families. The Infrastructure for Rebuilding America (INFRA) program is an existing competitive program that will see a more than  percent increase in this year&#;s funding due to the Bipartisan Infrastructure Law. These grants advance the Administration&#;s priorities of rebuilding America&#;s infrastructure and creating jobs by funding highway, multimodal freight and rail projects that position America to win the st century. Projects will improve safety, generate economic benefits, reduce congestion, enhance resiliency, and hold the greatest promise to eliminate supply chain bottlenecks and improve critical freight movements. Last year, DOT received over $ billion of project applications, but could only fund around $ billion of projects. To see last year&#;s INFRA grants, click here. The Bipartisan Infrastructure Law provides approximately $ billion for INFRA over  years, of which approximately $. billion will be made available through this NOFO. ': 0.0,\n",
              " 'Mega Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The National Infrastructure Project Assistance (MEGA) program was created in the Bipartisan Infrastructure Law to fund major projects that are too large or complex for traditional funding programs. The program will provide grants on a competitive basis to support multijurisdictional or regional projects of significance that may also cut across multiple modes of transportation. Eligible projects could include highway, bridge, freight, port, passenger rail, and public transportation projects of national and regional significance. These could be bridges or tunnels connecting two states; new rail and transit lines that improve equity and reduce emissions; and freight hubs integrating ship, train and truck traffic while improving environmental justice. DOT will award  percent of funding to projects greater than $ million in cost, and  percent to projects greater than $ million but less than $ million in cost. The program will receive up to $ billion this year alone and be able to provide multi-year funding to projects.': 0.0,\n",
              " 'Rural Surface Transportation Grant Program: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The Rural Surface Transportation Grant Program (RURAL) was created in the Bipartisan Infrastructure Law and will support projects to improve and expand the surface transportation infrastructure in rural areas to increase connectivity, improve the safety and reliability of the movement of people and freight, and generate regional economic growth and improve quality of life. Eligible projects for Rural grants include highway, bridge, and tunnel projects that help improve freight, safety, and provide or increase access to an agricultural, commercial, energy, or transportation facilities that support the economy of a rural area. This year alone, DOT will award up to $ million in grants through the rural program&#;part of the $ billion included in the Bipartisan Infrastructure Law over five years.': 0.0,\n",
              " '*FY  Notice of Funding Opportunity: Bipartisan Infrastructure Law (BIL) FAA Contract Tower (FCT) Competitive Grant Program: The Department of Transportation (DOT), Federal Aviation Administration (FAA) announces the opportunity to apply for $ million in FY  Airport Infrastructure Grant funds for the FCT competitive grant program, made available under the Infrastructure Investment and Jobs Act of  (IIJA), Pub. L. -, herein referred to as the Bipartisan Infrastructure Law (BIL). The purpose of the FCT competitive grant program is to make annual grants available to eligible airports for airport-owned airport traffic control tower (ATCT) projects that address the aging infrastructure of the nation&#;s airports. The FAA will consider ATCT projects that sustain, construct, repair, improve, rehabilitate, modernize, replace, or relocate non-approach control towers; or acquire and install air traffic control, communications, and related equipment to be used in those towers; or construct a remote tower certified by the FAA including acquisition and installation of air traffic control, communications, or related equipment. In addition, FT grants will align with DOT&#;s Strategic Framework FY- at https://www.transportation.gov/administrations/office-policy/fy--strategic-framework. The FY  FCT will be implemented consistent with law and in alignment with the priorities in Executive Order , Implementation of the Infrastructure Investments and Jobs Act ( FR ), which are to invest efficiently and equitably; promote the competitiveness of the U.S. economy; improve job opportunities by focusing on high labor standards; strengthen infrastructure resilience to all hazards including climate change; and to effectively coordinate with State, local, Tribal, and territorial government partners. ': 1.0,\n",
              " 'FY Railroad Crossing Elimination Grant Program: The purpose of the RCE Program is to provide funding for highway-rail and pathway-rail grade crossing improvement projects that focus on improving the safety and mobility of people and goods.': 1.0,\n",
              " 'Reconnecting Communities Pilot Discretionary Grant Program: The Reconnecting Communities Pilot Program notice of funding opportunity was amended on September ,  to include technical corrections to the Key Information Table. See updated materials in the &quot;Manage Related Documents&quot; section, as well as on our website at https://www.transportation.gov/grants/reconnecting-communitiesThe purpose of this notice is to solicit applications for Reconnecting Communities Pilot (RCP) Program grants. Funds for the Fiscal Year (FY)  RCP Program are to be awarded on a competitive basis for projects that reconnect communities by removing, retrofitting, or mitigating highways or other transportation facilities that create barriers to community connectivity, including to mobility, access, or economic development.The variety of transformative solutions to knit communities back together can include: high-quality public transportation, infrastructure removal, pedestrian walkways and overpasses, capping over roadways, linear parks and trail connectors, roadway redesigns and complete streets conversions, and main street revitalization. The program will award two types of grants: Planning Grants and Capital Construction Grants. Please consult the full Notice of Funding Opportunity for full details about the application process, requirements, and forms.': 0.8571428571428571,\n",
              " 'Safe Streets and Roads for All Discretionary Grant Program: THIS NOTICE OF FUNDING OPPORTUNITY WAS AMENDED ON AUGUST , . PLEASE SEE THE GRANTS.GOV PACKAGE UPDATES, AND THE SAFE STREETS AND ROADS FOR ALL (SSA) WEBSITE IN THE LINK PROVIDED BELOW FOR MORE INFORMATION. DEADLINE REMAINS SEPTEMBER , .The purpose of this notice is to solicit applications for SSA grants. Funds for the fiscal year (FY)  SSA grant program are to be awarded on a competitive basis to support planning, infrastructure, behavioral, and operational initiatives to prevent death and serious injury on roads and streets involving all roadway users, including pedestrians; bicyclists; public transportation, personal conveyance, and micromobility users; motorists; and commercial vehicle operators. Those interested in applying for Implementation Grants should use the application materials in PKG; those interested in applying for Action Plan Grants should use the application materials in PKG. See application instructions to determine which grant to apply for in this funding round.': 0.6666666666666666,\n",
              " 'Strengthening Mobility and Revolutionizing Transportation (SMART) Grants Program: The purpose of this notice is to solicit applications for Strengthening Mobility and Revolutionizing Transportation (SMART) grants. Funds for the fiscal year (FY)  SMART Grants Program are to be awarded on a competitive basis to conduct demonstration projects focused on advanced smart city or community technologies and systems to improve transportation efficiency and safety.': 1.0,\n",
              " 'FY- Federal-State Partnership for Intercity Passenger Rail Grant Program for projects located on the Northeast Corridor: This program funds capital projects on the Northeast Corridor that reduce the state of good repair backlog, improve performance, or expand or establish new intercity passenger rail service.': 1.0,\n",
              " 'Fiscal Years - Wildlife Crossings Pilot Program (WCPP): The primary goals of the WCPP are to save lives, prevent serious injuries, and protect motorists and wildlife by reducing WVCs, and improve habitat connectivity for terrestrial and aquatic species. Reduction of wildlife vehicle collisions and improvement of terrestrial and aquatic habitat connectivity are the primary merit criteria that will be used in reviewing applications, and each of the primary merit criteria are of equal importance.': 1.0,\n",
              " 'FY  Competitive Funding Opportunity; Buses and Bus Facilities Program: The Federal Transit Administration (FTA) announces the availability of approximately $ million in competitive grants under the Buses and Bus Facilities Program is to assist in the financing of buses and bus facilities capital projects, including replacing, rehabilitating, purchasing or leasing buses or related equipment, and rehabilitating, purchasing, constructing or leasing bus-related facilities. Synopses and full announcement will be posted on Grants.gov as opportunity FTA---TPM-BUS. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time April , .': 1.0,\n",
              " 'FY  Competitive Funding Opportunity; Low or No Emission Vehicle Program: The Federal Transit Administration (FTA) announces the availability of approximately $. billion in competitive grants under the Low or No Emission Grant Program (Low-No Program) for the purchase or lease of zero-emission and low-emission transit buses, including acquisition, construction, and leasing of required supporting facilities. Synopses and full announcement will be posted on Grants.gov as opportunity FTA---TPM-LWNO. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time April , .': 0.0,\n",
              " 'FY  Competitive Funding Opportunity: Areas of Persistent Poverty Program: To create new opportunities for those experiencing poverty, the U.S. Department of Transportation&#;s Federal Transit Administration (FTA) today announced a new Notice of Funding Opportunity (NOFO) to help improve transit in areas experiencing long-term economic distress. The $ million in competitive grant funding through FTA&#;s Areas of Persistent Poverty (AoPP) Program provides more resources to underserved and disadvantaged communities seeking to expand or improve transit. ': 1.0,\n",
              " 'FY  Competitive Funding Opportunity: Ferry Service for Rural Communities Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  Ferry Service for Rural Communities Program (FSRC) (Federal Assistance Listing #.). FSRC makes funding available competitively to assist in the financing of capital, planning, and operating assistance for eligible ferry services.': 0.0,\n",
              " 'FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $. million in competitive grants under the Fiscal Year (FY)  Passenger Ferry Grant Program (PFG) (Federal Assistance Listing #.). Of that amount, $ million is available only for low or zero-emission ferries or ferries using electric battery or fuel cell components and the infrastructure to support such ferries. As required by Federal public transportation law, funds will be awarded competitively to designated recipients or eligible direct recipients of Urbanized Area Formula funds to support capital projects to improve existing passenger ferry service, establish new ferry service, and repair and modernize ferry boats, terminals, and related facilities and equipment. ': 0.0,\n",
              " 'FY  National Infrastructure Investments: The Infrastructure Investment and Jobs Act of  (&#;Bipartisan Infrastructure Law,&#; or &#;BIL&#;) authorized and appropriated $. billion to be awarded by the Department of Transportation (&#;DOT&#;) for Local and Regional Project Assistance Program Grants under National Infrastructure Investments in FY . This Notice of Funding Opportunity (NOFO) solicits applications for projects funded under the Local and Regional Project Assistance Program, known as the RAISE Grants program. If the FY  Appropriations Act provides additional funding for the RAISE grants program and/or significantly different requirements for National Infrastructure Investment funds, the Department will amend this Notice with guidance on additional requirements.As with previous rounds, funds for the FY  RAISE Transportation program are to be awarded on a competitive basis for projects that will have a significant local or regional impact.': 0.5,\n",
              " 'Neighborhood Access and Equity (NAE) Program: The Department of Transportation is combining two major discretionary grant programs, the Reconnecting Communities Pilot (RCP) and Neighborhood Access and Equity (NAE) programs, into one Notice of Funding Opportunity (NOFO). Together, this combined program is known as the Reconnecting Communities and Neighborhoods (RCN) Program. The primary goal of the NAE Program is to assist economically disadvantaged or undeserved communities for planning and construction activities. Funds for the fiscal year (FY)  NAE grant program are to be awarded on a competitive basis to support planning, capital construction, and regional partnership activities that aim to restore community connectivity through the removal, retrofit, mitigation or replacement of highways, roadways, or other infrastructure facilities that create barriers to mobility, access or economic development. Applicants must submit their applications via Valid Eval at the links below: Community Planning Grants: https://usg.valideval.com/teams/rcn_planning/signup Capital Construction Grants: https://usg.valideval.com/teams/rcn_capitalconstruction/signup Do not submit applications through Grants.gov. Applications must be submitted by : PM Eastern Time on Thursday, September , . Late applications will not be accepted. ': 1.0,\n",
              " 'FY  Notice of Funding Opportunity for Small Shipyard Grants: This notice announces the intention of the Maritime Administration to provide grants to small shipyards. Under the Small Shipyard Grant Program, there is currently $,, available for grants for capital and related improvements to qualified shipyard facilities that will be effective in fostering efficiency, competitive operations, and quality ship construction, repair, and reconfiguration. Potential applicants are advised that it is expected, based on experience, that the number of applications will far exceed the funds available and that only a small percentage of applicants will be funded. Applications (SF- and the Addendum) will be accepted online through Grants.gov by  pm on the closing date (February , ). See full text under Related Documents Tab (FY  SSG NOFO) or the MARAD Web Page for detailed submission instructions.': 1.0,\n",
              " 'Safe Streets and Roads for All Funding Opportunity: The purpose of this notice is to solicit applications for Safe Streets and Roads for All (SSA) grants. Funds for the fiscal year (FY)  SSA grant program are to be awarded on a competitive basis to support planning and demonstration activities, as well as projects and strategies to prevent death and serious injury on roads and streets involving all roadway users, including pedestrians; bicyclists; public transportation, personal conveyance, and micromobility users; motorists; and commercial vehicle operators. Applicants must submit their applications via Valid Eval at https://usg.valideval.com/teams/usdot_ssa__implementation/signup for Implementation Grant applicants, and https://usg.valideval.com/teams/usdot_ssa__planning_demo/signup for Planning and Demonstration Grants. Do not submit your applications through Grants.Gov. Applications must be submitted by : PM Eastern Time on Monday, July , . Late applications will not be accepted.': 0.9827586206896551,\n",
              " 'SMART Grants Notice of Funding: The purpose of this notice is to solicit applications for Strengthening Mobility and Revolutionizing Transportation (SMART) Stage  Planning and Prototyping grants. Funds for the fiscal year (FY)  SMART Grants Program are to be awarded on a competitive basis to conduct demonstration projects focused on advanced smart city or community technologies and systems to improve transportation efficiency and safety. Applications must be submitted via Valid Eval, an online submission proposal system used by USDOT, at https://usg.valideval.com/teams/USDOT_SMART_/signup. USDOT will not accept or review application materials submitted via Grants.gov. ': 0.0,\n",
              " ' Tribal Transportation Program Safety Fund: Eligible projects described in section (a)() are strategies, activities, and projects on a public road that are consistent with a transportation safety plan and that (i) correct or improve a hazardous road location or feature, or (ii) address a highway safety problem. TTPSF emphasizes the development of strategic transportation safety plans using a data-driven process as a means for Tribes to identify transportation safety needs and determine how those needs will be addressed in Tribal communities. FHWA has identified four eligibility categories: transportation safety plans; data assessment, improvement, and analysis activities; systemic roadway departure countermeasures; and infrastructure improvements and other eligible activities as listed in  U.S.C. &#; (a)().': 1.0,\n",
              " 'United States Marine Highway Grants: This funding opportunity solicits applications for fiscal year (FY)  United States Marine Highway Program (USMHP) grants. Funds for FY  USMHP grants will be awarded on a competitive basis to assist in funding eligible projects for the purpose of developing, expanding, or promoting marine highway transportation. This opportunity announces the availability of up to $,, million in funding for grants under this program and establishes selection criteria and application requirements. All USMHP grant recipients must meet all applicable Federal requirements, including domestic content (&#;Buy America&#;) requirements. This program was formerly known as the America&#;s Marine Highway Program': 0.0,\n",
              " 'INFRA Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families. The Infrastructure for Rebuilding America (INFRA) program is an existing competitive program that will see a more than  percent increase in this year&#;s funding due to the Bipartisan Infrastructure Law. These grants advance the Administration&#;s priorities of rebuilding America&#;s infrastructure and creating jobs by funding highway, multimodal freight and rail projects that position America to win the st century. Projects will improve safety, generate economic benefits, reduce congestion, enhance resiliency, and hold the greatest promise to eliminate supply chain bottlenecks and improve critical freight movements. Last year, DOT received over $ billion in application requests, but could only fund around $. billion of projects. To see last year&#;s INFRA grants, click here. The Bipartisan Infrastructure Law provides approximately $ billion for INFRA over  years, of which approximately $. billion will be made available through this NOFO. ': 0.0,\n",
              " 'Mega Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The National Infrastructure Project Assistance (MEGA) program was created in the Bipartisan Infrastructure Law to fund major projects that are too large or complex for traditional funding programs. The program will provide grants on a competitive basis to support multijurisdictional or regional projects of significance that may also cut across multiple modes of transportation. Eligible projects could include highway, bridge, freight, port, passenger rail, and related public transportation projects of national and regional significance. DOT will award approximately  percent of funding to projects greater than $ million in cost, and approximately  percent to projects greater than $ million but less than $ million in cost.For more information about last year&apos;s Mega awards, visit here: https://www.transportation.gov/grants/mega-grant-program/FYawards': 0.0,\n",
              " 'Rural Surface Transportation Grant Progam: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The Rural Surface Transportation Grant Program (RURAL) was created in the Bipartisan Infrastructure Law and will support projects to improve and expand the surface transportation infrastructure in rural areas to increase connectivity, improve the safety and reliability of the movement of people and freight, and generate regional economic growth and improve quality of life. Eligible projects for Rural grants include highway, bridge, and tunnel projects that help improve freight, safety, and provide or increase access to an agricultural, commercial, energy, or transportation facilities that support the economy of a rural area. This year alone, DOT will award up to $ million in grants through the rural program&#;part of the $ billion included in the Bipartisan Infrastructure Law over five years. At least % of rural funding must be awarded in amounts of $ million or more. If you are seeking less than $ million, you are competing for only about $. million nationwide.For more information on last year&apos;s rural awards, see: https://www.transportation.gov/grants/rural--fact-sheets': 0.0,\n",
              " 'FY  Competitive Funding Opportunity: Rail Vehicle Replacement Grant Program (RAIL): The Federal Transit Administration (FTA) announces the opportunity to apply for approximately $ million in Fiscal Year (FY)  Section  State of Good Repair Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected rail vehicle replacement projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to States and local governmental authorities to assist in the funding of capital projects to replace rail rolling stock.': 0.0}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('finetuned_BERT_epoch_3.model', map_location=torch.device('cuda')))\n",
        "\n",
        "_, predictions, true_vals, _ = evaluate(dataloader_validation)\n",
        "accuracy_per_class(predictions, true_vals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-R5arLWNwbVj",
        "outputId": "546630e4-9c85-41a6-ffd1-2c98babbb714"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class: FY  Notice of Funding Opportunity: Bipartisan Infrastructure Law (BIL) Airport Terminal Program (ATP) Grants: The Department of Transportation (DOT), Federal Aviation Administration (FAA) announces the opportunity to apply for approximately $ billion in FY  discretionary funds for the Airport Terminal Program (ATP), made available under the Infrastructure Investment and Jobs Act of  (IIJA), Pub. L. -, herein referred to as the Bipartisan Infrastructure Law (BIL). The purpose of the ATP is to make annual grants available to eligible airports for airport terminal and airport-owned Airport Traffic Control Towers development projects that address the aging infrastructure of our nation&#;s airports. In addition, ATP grants will align with DOT&#;s Strategic Framework FY- at https://www.transportation.gov/administrations/office-policy/fy--strategic-framework. The FY  ATP will be implemented consistent with law and in alignment with the priorities in Executive Order , Implementation of the Infrastructure Investments and Jobs Act ( FR ), which are to invest efficiently and equitably; promote the competitiveness of the U.S. economy; improve job opportunities by focusing on high labor standards; strengthen infrastructure resilience to all hazards including climate change; and to effectively coordinate with State, local, Tribal, and territorial government partners. \n",
            "Accuracy: 12/25 (0.4800)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Airport Improvement Program Discretionary Grants: The U.S. Department of Transportation&#;s Federal Aviation Administration (FAA) announces the opportunity to apply for an estimated $. billion in Fiscal Year (FY)  discretionary grants under the Airport Improvement Program (AIP), per  U.S.C. &#; . The FAA awards these annually appropriated discretionary funds through the FAA&#;s long-standing iterative, competitive grant process. Prior to the publication of this Notice of Funding Opportunity (NOFO), the FAA identified eligible applicants in its National Plan of Integrated Airport Systems (NPIAS) and compiled potentially eligible projects through the -year Airports Capital Improvement Plan (ACIP). Both of these processes are described in FAA Order ., &#;Formulation of NPIAS and ACIP,&#; adhering to  U.S.C. &#; , which authorizes discretionary funds. The AIP funds airport capital improvements and rehabilitation projects. All discretionary grant funding is subject to appropriations, statutory requirements, and related program funding availability. \n",
            "Accuracy: 163/163 (1.0000)\n",
            "\n",
            "Class: Grants for Buses and Bus Facilities Program: The Federal Transit Administration (FTA) announces the availability of approximately $. million of Fiscal Year  funds for buses, bus facilities, and bus equipment. The purpose of the Grants for Buses and Bus Facilities Program is to assist in the financing of buses and bus facilities capital projects, including replacing, rehabilitating, purchasing or leasing buses or related equipment, and rehabilitating, purchasing, constructing or leasing bus-related facilities. \n",
            "Accuracy: 14/14 (1.0000)\n",
            "\n",
            "Class: Solicitation of Project Proposals for the Low or No Emission (Low-No) Program:  Notice of Funding Opportunity (NOFO): Solicitation of Project Proposals for the Low or No Emission Program (Low-No) Program. The Federal Transit Administration (FTA) announces the availability of $ million of Fiscal Year  funds for the purchase or lease of low or no emission vehicles as well as related equipment or facilities. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time on //. The main purpose of the Low-No Program is to support the transition of the nation&#;s transit fleet to the lowest polluting and most energy efficient transit vehicles. The Low-No Program provides funding to State and local governmental authorities for the purchase or lease of zero-emission and low-emission transit buses, including acquisition, construction, and leasing of required supporting facilities. \n",
            "Accuracy: 0/6 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in Fiscal Year (FY)  Section  Urbanized Area Formula Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected passenger ferry projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to designated recipients or eligible direct recipients of Section  funds to assist in the financing of capital projects to support existing passenger ferry service, establish new ferry service, and to repair and modernize ferry boats, terminals, and related facilities and equipment. \n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: Pilot Program for Transit-Oriented Development (TOD) Planning: Notice of Funding Opportunity for the Pilot Program for Transit-Oriented Development (TOD) Planning. The Federal Transit Administration (FTA) announces the availability of approximately $. million in Pilot Program for TOD Planning funding to support comprehensive planning associated with new fixed guideway and core capacity improvement projects. FTA may award amounts ranging from $, to $,,. The Pilot Program for TOD Planning provides funding to local communities to integrate land use and transportation planning in new fixed guideway and core capacity transit project corridors. As required by statute, any comprehensive planning funded through the pilot program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations. The statute also requires that the planning work be associated with a new fixed guideway or core capacity transit project as defined in Federal transit statute ( USC (a); also see the NOFO for the definitions). Consistent with statutory direction, FTA is seeking comprehensive planning projects covering an entire transit capital project corridor, rather than proposals that involve planning for individual station areas or only a small section of the corridor. To ensure that planning work reflects the needs and aspirations of the local community and results in concrete, specific deliverables and outcomes, FTA is requiring that transit project sponsors partner with entities with land use planning authority in the transit project corridor. \n",
            "Accuracy: 4/4 (1.0000)\n",
            "\n",
            "Class: Public Transportation on Indian Reservations Program: The Federal Transit Administration (FTA) announces the availability of approximately $ million in funding provided by the Public Transportation on Indian Reservations Program (Tribal Transit Program (TTP)), as authorized by  U.S.C. (c)()(A), as amended by the Fixing America&apos;s Surface Transportation Act (FAST), Public Law - (December , ). The primary purpose of these competitively selected grants is to support planning, capital, and, in limited circumstances, operating assistance for tribal public transit services. Funds distributed to Indian tribes under the TTP should not replace or reduce funds that Indian tribes receive from states through FTA&#;s Section  program. \n",
            "Accuracy: 1/6 (0.1667)\n",
            "\n",
            "Class: INFRA Grants: The Nationally Significant Freight and Highway Projects (INFRA) program provides Federal financial assistance to highway and freight projects of national or regional significance. This notice solicits applications for awards under the program&#;s fiscal year (FY)  funding, subject to the availability of appropriated funds. The Department is focusing the competition on transportation infrastructure projects that support four key objectives, each of which is discussed in greater detail in the Notice of Funding Opportunity: () Supporting economic vitality at the national and regional level; () Leveraging Federal funding to attract non-Federal sources of infrastructure investment; () Deploying innovative technology, encouraging innovative approaches to project delivery, and incentivizing the use of innovative financing; and () Holding grant recipients accountable for their performance. \n",
            "Accuracy: 2/3 (0.6667)\n",
            "\n",
            "Class:  Tribal Transportation Program Safety Fund: A Notice of Funding Availability (NOFA) for Tribal Transportation Program Safety Funds (TTPSF) has been published. This notice announces the availability of $. million for projects in three categories: safety plans; data assessment, improvement, and analysis activities; and infrastructure improvement and other eligible activities as listed in  U.S.C.(a)() The deadline for submissions is //. For complete instructions on preparing and submitting an application for TTPSF, please download the application information and NOFO at https://flh.fhwa.dot.gov/programs/ttp/safety/ttpsf.htm Please submit your application directly to FHWA through the &quot;apply now&quot; button at: https://flh.fhwa.dot.gov/programs/ttp/safety/ttpsf.htm \n",
            "Accuracy: 14/14 (1.0000)\n",
            "\n",
            "Class: Grants for Buses and Bus Facilities Program: The purpose of the Grants for Buses and Bus Facilities Program is to assist in the financing of buses and bus facilities capital projects, including replacing, rehabilitating, purchasing or leasing buses or related equipment, and rehabilitating, purchasing, constructing or leasing bus-related facilities.\n",
            "Accuracy: 12/14 (0.8571)\n",
            "\n",
            "Class: Low or No Emission Program (Low-No Program): The main purpose of the Low-No Program is to support the transition of the nation&#;s transit fleet to the lowest polluting and most energy efficient transit vehicles. The Low-No Program provides funding to State and local governmental authorities for the purchase or lease of zero-emission and low-emission transit buses, including acquisition, construction, and leasing of required supporting facilities.\n",
            "Accuracy: 14/14 (1.0000)\n",
            "\n",
            "Class: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in Fiscal Year (FY)  Section  Urbanized Area Formula Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected passenger ferry projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to designated recipients or eligible direct recipients of Section  funds to assist in the financing of capital projects to support existing passenger ferry service, establish new ferry service, and to repair and modernize ferry boats, terminals, and related facilities and equipment. \n",
            "Accuracy: 0/2 (0.0000)\n",
            "\n",
            "Class: Pilot Program for Transit-Oriented Development (TOD) Planning: Notice of Funding Opportunity for PILOT PROGRAM FOR TRANSIT-ORIENTED DEVELOPMENT (TOD) PLANNING. The Federal Transit Administration (FTA) announces the opportunity to apply for approximately $. million of funding under the Pilot Program for TOD Planning (Catalog of Federal Domestic Assistance #.). As required by federal transit law and subject to funding availability, funds will be awarded competitively to support comprehensive planning associated with new fixed guideway and core capacity improvement projects. FTA anticipates minimum grant awards of $, and maximum grant awards of $,,. The Pilot Program for TOD Planning is intended to fund comprehensive planning that supports economic development, ridership, multimodal connectivity and accessibility, increased transit access for pedestrian and bicycle traffic, and mixed-use development near transit stations. The program also encourages identification of infrastructure needs and engagement with the private sector. Consistent with statutory direction, FTA is seeking comprehensive planning projects covering an entire transit capital project corridor, rather than proposals that involve planning for individual station areas or only a small section of the corridor. To ensure any proposed planning work reflects the needs and aspirations of the local community and results in concrete, specific deliverables and outcomes, transit project sponsors must partner with entities with land use planning authority in the transit project corridor to conduct the planning work.The Pilot Program for TOD Planning helps support FTA&#;s mission of improving public transportation for America&#;s communities by providing funding to local communities to integrate land use and transportation planning around a new fixed guideway or core capacity improvement project. Per statute, any comprehensive planning funded through the program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations.\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Public Transportation on Indian Reservations Program: The Federal Transit Administration (FTA) announces the availability of approximately $ million in funding provided by the Public Transportation on Indian Reservations Program (Tribal Transit Program (TTP)), as authorized by  U.S.C. (c)()(A), as amended by the Fixing America&apos;s Surface Transportation Act (FAST), Public Law - (December , ). This is a national solicitation for project proposals and includes the selection criteria and program eligibility information for Fiscal Year  projects. The primary purpose of these competitively selected grants is to support planning, capital, and, in limited circumstances, operating assistance for tribal public transit services. Funds distributed to Indian tribes under the TTP should not replace or reduce funds that Indian tribes receive from states through FTA&#;s Section  program.\n",
            "Accuracy: 0/4 (0.0000)\n",
            "\n",
            "Class: *INFRA Grants: The U.S. Department of Transportation (USDOT) is seeking applicants for the FY  round of the Infrastructure for Rebuilding America (INFRA) discretionary grant program to fund transportation projects of national and regional significance that result in good-paying jobs, improve safety, apply transformative technology, and explicitly address climate change and racial equity. The funding available for this year&#;s grants totals approximately $ million.The Department recognizes the role that infrastructure investment plays in economic development and job creation, and the added urgency of this funding at time when the COVID- pandemic has put stress on state and local budgets.For the first time, the USDOT seeks INFRA projects that address climate change and environmental justice. Projects will be evaluated on whether they were planned as part of a comprehensive strategy to address climate change, or whether they support strategies to reduce greenhouse gas emissions such as deploying zero-emission-vehicle infrastructure or encouraging modal shift and a reduction in vehicle-miles-traveled. Racial equity will also be considered as a selection criterion, to the extent that project sponsors have completed equity-focused community outreach, and projects are designed to benefit underserved communities. The Department will also consider whether the project is located in a federally designated community development zone, including qualified Opportunity Zones, Empowerment Zones, Promise Zones, or Choice Neighborhoods.USDOT seeks projects that apply innovative technology, delivery, or financing methods with proven outcomes to deliver projects in a cost effective manner. The Department will make awards under the INFRA program to both large and small projects. For a large project, the INFRA grant must be at least $ million. For a small project, the grant must be at least $ million. Under statutory requirements,  percent of available funds are reserved for small projects, and the Department must award at least  percent of funding for rural projects. INFRA grants may be used to fund a variety of components of an infrastructure project, however, the Department is specifically focused on projects in which the local sponsor is significantly invested and is positioned to proceed rapidly to construction. Eligible INFRA project costs may include: reconstruction, rehabilitation, acquisition of property (including land related to the project and improvements to the land), environmental mitigation, construction contingencies, equipment acquisition, and operational improvements directly related to system performance. The INFRA NOFO also announces the creation of the &#;INFRA Extra&#; Program, which will identify competitive INFRA applicants who do not receive an INFRA award and authorize them to seek a Transportation Infrastructure Finance and Innovation Act of  (TIFIA) loan up to  percent of their project cost.\n",
            "Accuracy: 0/3 (0.0000)\n",
            "\n",
            "Class: Fiscal Year  Competitive Funding Opportunity; Grants for Buses and Bus Facilities Program: The Federal Transit Administration (FTA) announces the availability of approximately $. million of Fiscal Year  funds for buses, bus facilities, and bus equipment. Synopses and full announcement will be posted on Grants.gov as opportunity FTA---BUS. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time November ,.\n",
            "Accuracy: 1/10 (0.1000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  Passenger Ferry Grant Program (Ferry Program) (Federal Assistance Listing #.). Of that amount, $ million is available only for low or zero-emission ferries or ferries using electric battery or fuel cell components and the infrastructure to support such ferries. As required by Federal public transportation law, funds will be awarded competitively to designated recipients or eligible direct recipients of Urbanized Area Formula funds to support capital projects to improve existing passenger ferry service, establish new ferry service, and repair and modernize ferry boats, terminals, and related facilities and equipment.\n",
            "Accuracy: 0/2 (0.0000)\n",
            "\n",
            "Class:  Competitive Funding Opportunity: Pilot Program for Transit-Oriented Development (TOD) Planning: The Pilot Program for TOD Planning helps support FTA&#;s mission of improving public transportation for America&#;s communities by providing funding to local communities to integrate land use and transportation planning around a new fixed guideway or core capacity improvement project. Per statute, any comprehensive planning funded through the program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations. Additional information is available at: https://www.transit.dot.gov/TODPilot.Notice of Funding Opportunity (NOFO): Notice of Funding Opportunity for PILOT PROGRAM FOR TRANSIT-ORIENTED DEVELOPMENT (TOD) PLANNING. The Federal Transit Administration (FTA) announces the opportunity to apply for $,, of funding under the Pilot Program for TOD Planning (Catalog of Federal Domestic Assistance #.). As required by federal transit law and subject to funding availability, funds will be awarded competitively to support comprehensive planning associated with new fixed guideway and core capacity improvement projects. FTA anticipates minimum grant awards of $, and maximum grant awards of $,,. Synopses and full announcement are posted on Grants.gov as opportunity FTA---TPE-TODP. The Pilot Program for TOD Planning is intended to fund comprehensive planning that supports economic development, ridership, multimodal connectivity and accessibility, increased transit access for pedestrian and bicycle traffic, and mixed-use development near transit stations. The program also encourages identification of infrastructure needs and engagement with the private sector.Consistent with statutory direction, FTA is seeking comprehensive planning projects covering an entire transit capital project corridor, rather than proposals that involve planning for individual station areas or only a small section of the corridor. To ensure any proposed planning work reflects the needs and aspirations of the local community and results in concrete, specific deliverables and outcomes, transit project sponsors must partner with entities with land use planning authority in the transit project corridor to conduct the planning work.Link and Instructions for attaching the supplemental form to the SF-: All applicants must complete a supplemental form (PDF) specific to the Pilot Program for TOD Planning and attach it to their submission in Grants.gov. See section D of the NOFO for information about additional required application contents.Webinar: FTA will conduct a webinar for prospective applicants after the NOFO is published in the Federal Register. Further information will be posted on FTA&#;s website (https://www.transit.dot.gov/TODPilot) when available. Dates: An applicant must submit a proposal electronically by : p.m. Eastern Daylight Time on June , . Any agency intending to apply should initiate the process of registering on the Grants.gov site immediately to ensure completion of registration before the submission deadline. \n",
            "Accuracy: 0/3 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity:  Public Transportation on Indian Reservations Program; Tribal Transit Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants for the Fiscal Year (FY)  Public Transportation on Indian Reservations (Tribal Transit) Program (Federal Assistance Listing: .). As required by Federal public transportation law, funds will be awarded competitively for any purpose eligible under FTA&#;s Formula Grants for Rural Areas Program,  U.S.C. , including planning, capital, and operating assistance for tribal public transit services in rural areas. \n",
            "Accuracy: 6/6 (1.0000)\n",
            "\n",
            "Class: INFRA Grants: The U.S. Department of Transportation (USDOT) is seeking applicants for the FY  round of the Infrastructure for Rebuilding America (INFRA) discretionary grant program to fund transportation projects of national and regional significance that result in good-paying jobs, improve safety, apply transformative technology, and explicitly address climate change and racial equity. The funding available for this year&#;s grants totals approximately $ million.The Department recognizes the role that infrastructure investment plays in economic development and job creation, and the added urgency of this funding at time when the COVID- pandemic has put stress on state and local budgets.For the first time, the USDOT seeks INFRA projects that address climate change and environmental justice. Projects will be evaluated on whether they were planned as part of a comprehensive strategy to address climate change, or whether they support strategies to reduce greenhouse gas emissions such as deploying zero-emission-vehicle infrastructure or encouraging modal shift and a reduction in vehicle-miles-traveled. Racial equity will also be considered as a selection criterion, to the extent that project sponsors have completed equity-focused community outreach, and projects are designed to benefit underserved communities. The Department will also consider whether the project is located in a federally designated community development zone, including qualified Opportunity Zones, Empowerment Zones, Promise Zones, or Choice Neighborhoods.USDOT seeks projects that apply innovative technology, delivery, or financing methods with proven outcomes to deliver projects in a cost effective manner. The Department will make awards under the INFRA program to both large and small projects. For a large project, the INFRA grant must be at least $ million. For a small project, the grant must be at least $ million. Under statutory requirements,  percent of available funds are reserved for small projects, and the Department must award at least  percent of funding for rural projects. INFRA grants may be used to fund a variety of components of an infrastructure project, however, the Department is specifically focused on projects in which the local sponsor is significantly invested and is positioned to proceed rapidly to construction. Eligible INFRA project costs may include: reconstruction, rehabilitation, acquisition of property (including land related to the project and improvements to the land), environmental mitigation, construction contingencies, equipment acquisition, and operational improvements directly related to system performance. The INFRA NOFO also announces the creation of the &#;INFRA Extra&#; Program, which will identify competitive INFRA applicants who do not receive an INFRA award and authorize them to seek a Transportation Infrastructure Finance and Innovation Act of  (TIFIA) loan up to  percent of their project cost.\n",
            "Accuracy: 2/17 (0.1176)\n",
            "\n",
            "Class: FY Tribal Transportation Program Safety Fund (TTPSF): A Notice of Funding Opportunity (NOFO) for Tribal Transportation Program Safety Funds (TTPSF) has been published. This notice announces the availability of approximately $. million for projects in four categories: safety plans; data assessment, improvement, and analysis activities; systemic roadway departure countermeasures; and infrastructure improvement and other eligible activities as listed in  U.S.C.(a)(). The goal of these projects is to reduce fatalities and serious injuries from transportation incidents, such as motor vehicle crashes. For complete instructions on preparing and submitting an application for TTPSF, please download the application, application information, systemic roadway departure risk assessment form (if applicable) and NOFO at https://highways.dot.gov/federal-lands/programs-tribal/safety/funds. Applications will NOT be submitted on Grants.gov. Instead, please use the &quot;apply now&quot; button on the program web page.\n",
            "Accuracy: 7/9 (0.7778)\n",
            "\n",
            "Class: Advanced Transportation Technologies and  Innovative Mobility Deployment (ATTIMD) Program: AMENDMENT  (//): This amendment is for the purpose of correcting a typo contained in Table D. on page  of . This table, contained in Section D. for CONTENT AND FORM OF APPLICATION SUBMISSION, erroneously omitted the Project Outcome Criteria from that table&#;s submission requirements for Volume . This amendment also revises the verbiage included on page  of the NOFO to clarify that all resumes for key personnel should be included in item f) Appendix &#; R&#;sum&#;s for Key Personnel. Please note that Resumes do not count against the page limitations for Volume . Please refer to Amendment  in the &#;Related Documents&#; tab for more information. -- END OF AMENDMENT  -- AMENDMENT  (//): This amendment adds the weblink to the recording of the October , , informational webinar for this opportunity. The weblink is included in the attachments page for this opportunity as well as below: LINK: https://usdot.zoomgov.com/rec/share/eQNNRGISy-UxfqGpSDzINRbzvcdkAjDdoulZvwRCt-sARp.kdAJIUFsFZZEpf PASSCODE: c@X*&amp;@ -- END OF AMENDMENT  -- AMENDMENT  (//): This amendment adds the registration information for the October , , informational webinar for this opportunity. Please refer to the &quot;Related Documents&quot; tab for more information. -- END OF AMENDMENT  -- BASE NOFO (//): The Advanced Transportation Technologies and Innovative Mobility Deployment Program (ATTIMD), established in Section (c)() of title , United States Code (U.S.C.), also known as the Advanced Transportation Technology and Innovation (ATTAIN) Program, directs FHWA to award grants to eligible entities to deploy, install, and operate advanced transportation technologies to improve safety, mobility, efficiency, system performance, intermodal connectivity, and infrastructure return on investment. FHWA intends for these model technology deployments to help demonstrate how emerging transportation technologies, data, and their applications can be effectively deployed and integrated with existing systems to provide access to essential services and other destinations. This also includes efforts to increase connectivity to employment, education, services, and other opportunities; support workforce development; or contribute to increased mobility, particularly for persons with visible and hidden disabilities and elderly individuals. The Assistance Listing Number for this opportunity is . &#; Highway Research and Development. The Federal Highway Administration (FHWA) hereby requests applications to result in awards to eligible entities to deploy, install, and operate advanced transportation technologies to improve safety, mobility, efficiency, system performance, intermodal connectivity, and infrastructure return on investment. These model deployments are expected to provide benefits in the form of: reduced traffic-related fatalities and injuries; reduced traffic congestion and improved travel time reliability; reduced transportation-related emissions; optimized multimodal system performance; improved access to transportation alternatives, including for underserved populations; improved integration of payment systems; public access to real-time integrated traffic, transit, and multimodal transportation information to make informed travel decisions; cost savings to transportation agencies, businesses, and the traveling public; or other benefits to transportation users and the general public. This competitive ATTAIN Program will promote the use of innovative transportation solutions. The deployment of these technologies will provide Congress and FHWA with valuable real-life data and feedback to inform future decision-making. Cost Sharing: Cost-sharing or matching is required, with the maximum Federal share being  percent; hence, this NOFO requires a minimum non-Federal cost share of  percent. Cost sharing or matching means the portion of project costs not paid by Federal funds. NOTE: FHWA previously issued six NOFOs for the Advanced Transportation and Congestion Management Technologies Deployment (ATCMTD) Grant Program before the program was amended and renamed to the Advanced Transportation Technologies and Innovative Mobility Deployment Program by Section  of the Infrastructure Investment and Jobs Act (IIJA) (Public Law -, also known as the &#;Bipartisan Infrastructure Law&#; (BIL)).\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: FY  and FY  Competitive Funding Opportunity: Competitive Grants for Rail Vehicle Replacement Program (Rail Program): The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in Fiscal Year (FY)  and  Section  State of Good Repair Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected rail vehicle replacement projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to States and local governmental authorities to assist in the funding of capital projects to replace rail rolling stock.\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: Bridge Investment Program - Planning, Bridge Projects, and Large Bridge Projects: This Notice of Funding Opportunity (NOFO) is to solicit applications for three categories of Bridge Investment Program (BIP) funding opportunities: () Planning; () Bridge Projects (projects with eligible costs less than $ million); and () Large Bridge Projects (projects with eligible costs greater than $ million). Eligible applicants may submit applications for any of the three funding categories, but each category has distinct eligibility and selection criteria and application deadlines.These funds will be awarded on a competitive basis for planning, feasibility analysis, and revenue forecasting associated with the development of a project that would subsequently be eligible to apply for BIP funding under either the Bridge Projects or Large Bridge Projects funding categories.A total of $. billion in  BIP funds are available for the Bridge Projects and Large Bridge Projects funding opportunities. These funds will be awarded on a competitive basis for bridge replacement, rehabilitation, preservation, and protection projects that: () improve the safety, efficiency, and reliability of the movement of people and freight over bridges; and () improve the condition of bridges in the United States by reducing (a) the number of bridges, and total person miles traveled over bridges, that are in poor condition or that are in fair condition and at risk of falling into poor condition within the next three years, or (b) the number of bridges, and total person miles traveled over bridges, that do not meet current geometric design standards or cannot meet the load and traffic requirements typical of the regional transportation network. In addition, Large Bridge Projects that receive a BIP award of not less than $ million are eligible for multiyear grants, in which DOT can award available funds to a project over the course of several years in accordance with an agreement and in alignment with its schedule. In selecting Bridge Projects and Large Bridge Projects, FHWA will consider the extent to which BIP funds leverage non-Federal contributions from sponsors and stakeholders involved in the planning, design, and construction of eligible projects. \n",
            "Accuracy: 5/6 (0.8333)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity; Low or No Emission Grant Program: Low-No ProgramThe Federal Transit Administration (FTA) announces the opportunity to apply for approximately $. billion in competitive grants under the fiscal year (FY)  Low or No Emission Grant Program (Low-No Program) (Federal Assistance Listing: .) and approximately $ million in FY  funds under the Grants for Buses and Bus Facilities Program (Buses and Bus Facilities Program) (Federal Assistance Listing .), subject to availability of appropriated funding. Synopses and full announcement will be posted on Grants.gov as funding opportunity ID FTA--TPM for Low-No applications and FTA--TPM for Buses and Bus Facilities applications. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time May , .\n",
            "Accuracy: 14/15 (0.9333)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: All Stations Accessibility Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  All Stations Accessibility Program (ASAP) (Federal Assistance Listing #.). As required by Federal public transportation law, funds will be awarded competitively to eligible designated recipients that operate or allocate funds to inaccessible pre-ADA&#;or &#;legacy&#; &#; rail fixed guideway public transportation systems, and States (including territories and Washington, D.C.) and local governmental entities that operate or financially support legacy rail fixed guideway public transportation systems and corresponding legacy stations/facilities for capital projects to repair, improve, modify, retrofit, or relocate infrastructure of stations or facilities for passenger use, including load-bearing members that are an essential part of the structural frame; or () for planning projects to develop or modify a plan for pursuing public transportation accessibility projects, assessments of accessibility, or assessments of planned modifications to stations or facilities for passenger use projects; or programs of projects in an eligible area.\n",
            "Accuracy: 2/2 (1.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Buses and Bus Facilities Competitive Program: Bus and Bus Facilities ProgramThe Federal Transit Administration (FTA) announces the opportunity to apply for approximately $. billion in competitive grants under the fiscal year (FY)  Low or No Emission Grant Program (Low-No Program) (Federal Assistance Listing: .) and approximately $ million in FY  funds under the Grants for Buses and Bus Facilities Program (Buses and Bus Facilities Program) (Federal Assistance Listing .), subject to availability of appropriated funding. Synopses and full announcement will be posted on Grants.gov as funding opportunity ID FTA--TPM for Low-No applications and FTA--TPM for Buses and Bus Facilities applications. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time May , \n",
            "Accuracy: 0/7 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Electric or Low-Emitting Ferry Pilot Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  Electric or Low-Emitting Ferry Pilot Program (ELEF) (Federal Assistance Listing #.). ELEF makes funding available competitively to assist in the financing of capital projects for the purchase of electric or low-emitting ferry vessels that reduce emissions by using alternative fuels or on-board energy storage systems and related charging infrastructure to reduce emissions or produce zero onboard emissions under normal operation.\n",
            "Accuracy: 2/2 (1.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $. million in competitive grants under the Fiscal Year (FY)  Passenger Ferry Grant Program (PFG) (Federal Assistance Listing #.). Of that amount, $. million is available only for low or zero-emission ferries or ferries using electric battery or fuel cell components and the infrastructure to support such ferries. As required by Federal public transportation law, funds will be awarded competitively to designated recipients or eligible direct recipients of Urbanized Area Formula funds to support capital projects to improve existing passenger ferry service, establish new ferry service, and repair and modernize ferry boats, terminals, and related facilities and equipment. \n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: FY  Pilot Program for Transit-Oriented Development (TOD) Planning: The Pilot Program for TOD Planning helps support FTA&#;s mission of improving public transportation for America&#;s communities by providing funding to local communities to integrate land use and transportation planning around a new fixed guideway or core capacity improvement project. Per statute, any comprehensive or site specific planning funded through the program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations.\n",
            "Accuracy: 3/3 (1.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Public Transportation on Indian Reservations Program; Tribal Transit Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $. million in competitive grants for the Fiscal Year (FY)  Public Transportation on Indian Reservations (Tribal Transit) Program. As required by Federal public transportation law, funds will be awarded competitively for any purpose eligible under FTA&#;s Formula Grants for Rural Areas Program, including planning, capital, and operating assistance for tribal public transit services in rural areas. FTA may award additional funding that is made available to the program prior to the announcement of project selections.\n",
            "Accuracy: 0/4 (0.0000)\n",
            "\n",
            "Class: FY Consolidated Rail Infrastructure and Safety Improvements Grant Program: This program funds projects that improve the safety, efficiency, and reliability of intercity passenger and freight rail.\n",
            "Accuracy: 10/10 (1.0000)\n",
            "\n",
            "Class: FY Corridor Identification and Development Grant Program: This program facilitates the development of intercity passenger rail corridors.\n",
            "Accuracy: 10/10 (1.0000)\n",
            "\n",
            "Class: FY Federal-State Partnership for Intercity Passenger Rail Grant Program for projects not located on the Northeast Corridor: This program funds capital projects that reduce the state of good repair backlog, improve performance, or expand or establish new intercity passenger rail service.\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: Fiscal Year  National Culvert Removal, Replacement, and Restoration Grant Program (Culvert AOP Program): -- ORIGINAL NOFO POSTING - // -- The primary goal of the Culvert AOP Program is to improve or restore anadromous fish passage through the replacement, removal, repair, or improvement of culverts or weirs. The grant program prioritizes projects that would improve fish passage for: (A) anadromous fish stocks listed as an endangered species or a threatened species under section  of the Endangered Species Act of  ( U.S.C. &#; ); (B) anadromous fish stocks identified by the National Marine Fisheries Service (NMFS) or the U.S. Fish and Wildlife Service (USFWS) that could reasonably become listed as an endangered species or a threatened species under that section; (C) anadromous fish stocks identified by the NMFS or the USFWS as prey for endangered species, threatened species, or protected species, including Southern resident orcas (Orcinus orca); or (D) anadromous fish stocks identified by the NMFS or the USFWS as climate resilient stocks ( U.S.C. &#; (e)()). The program also prioritizes projects that would open up more than  meters of upstream (anadromous) habitat before the end of the natural habitat ( U.S.C. &#; (e)()). \n",
            "Accuracy: 9/9 (1.0000)\n",
            "\n",
            "Class: FY  National Infrastructure Investments: Agency Name: Department of TransportationDescription: The Infrastructure Investment and Jobs Act (Pub. L. -, November , , &#;Bipartisan Infrastructure Law,&#; or &#;BIL&#;) authorized and appropriated $. billion to be awarded by the Department of Transportation (&#;DOT&#;) for FY  for Local and Regional Project Assistance Program Grants under National Infrastructure Investments. This Notice of Funding Opportunity (NOFO) solicits applications for projects to be funded under the Local and Regional Project Assistance Program, known as the RAISE Grants program, including any additional funding appropriated for the RAISE Grants program under the FY  Appropriations Act. On March , , the Consolidated Appropriations Act,  (Pub. L. -, &#;FY  Appropriations Act&#;) appropriated an additional $ million for the FY  RAISE Grant Program. Therefore, a total of $. billion in funding is now available for the FY  RAISE Grant Program. Applicants should note that the two funding streams (BIL funding and FY  Appropriations Act funding) have slightly different funding restrictions and requirements that may affect competitiveness. Please see the Notice of Funding Opportunity Amendment No. for details.As with previous rounds, funds for the FY  RAISE Transportation program are to be awarded on a competitive basis for projects that will have a significant local or regional impact.\n",
            "Accuracy: 11/25 (0.4400)\n",
            "\n",
            "Class: FY  Natural Gas Distribution Infrastructure Safety and Modernization Grant: FY  Natural Gas Distribution Infrastructure Safety and Modernization Grant\n",
            "Accuracy: 6/6 (1.0000)\n",
            "\n",
            "Class: The Infrastructure Investment and Jobs Act (IIJA) Notice of Funding Opportunity for America&#;s Marine Highway Program: This notice announces the availability of funding for grants and establishes selection criteria and application requirements for the America&#;s Marine Highway Program (&#;AMHP&#;). The purpose of this program is to make grants available to previously designated Marine Highway Projects that support the development and expansion of documented vessels or port and landside infrastructure. The Department also seeks eligible grant projects that will strengthen American supply chains. The U.S. Department of Transportation (&#;DOT&#; or &#;Department&#;) will award Marine Highway Grants to implement projects or components of projects previously designated by the Secretary of Transportation (&#;Secretary&#;) under the AMHP. Only Marine Highway Projects the Secretary designates before the Notice of Funding Opportunity (&#;NOFO&#;) closing date are eligible for funding as described in this notice. TIMING OF GRANT APPLICATIONS: Applications must be received by the Maritime Administration (&#;MARAD&#;) by : p.m. E.D.T. on April , . ADDRESSES: Grant applications must be submitted electronically using Grants.gov (https://www.grants.gov). Please be aware that you must complete the Grants.gov registration process before submitting your application and that the registration process usually takes  to  weeks to complete. Applicants are strongly encouraged to make submissions in advance of the deadline. FOR FURTHER INFORMATION CONTACT: Fred Jones, Office of Ports &amp; Waterways Planning, Room W&#;, Maritime Administration, U.S. Department of Transportation,  New Jersey Avenue S.E., Washington, D.C. , phone --, or email Fred.Jones@dot.gov. Persons who use a telecommunications device for the deaf (TDD) may call the Federal Information Relay Service (FIRS) at --- to contact the above individual during business hours. The FIRS is available twenty-four hours a day, seven days a week, to leave a message or question with the above individual. You will receive a reply during regular business hours.\n",
            "Accuracy: 1/2 (0.5000)\n",
            "\n",
            "Class: *FY  Notice of Funding Opportunity: Bipartisan Infrastructure Law (BIL) Airport Terminal Program (ATP) Grants: The Department of Transportation (DOT), Federal Aviation Administration (FAA) announces the opportunity to apply for approximately $ billion in FY  discretionary funds for the Airport Terminal Program (ATP), made available under the Infrastructure Investment and Jobs Act of  (IIJA), Pub. L. -, herein referred to as the Bipartisan Infrastructure Law (BIL). The purpose of the ATP is to make annual grants available to eligible airports for airport terminal and airport-owned Airport Traffic Control Towers development projects that address the aging infrastructure of our nation&#;s airports. In addition, ATP grants will align with DOT&#;s Strategic Framework FY- at https://www.transportation.gov/administrations/office-policy/fy--strategic-framework. The FY  ATP will be implemented consistent with law and in alignment with the priorities in Executive Order , Implementation of the Infrastructure Investments and Jobs Act ( FR ), which are to invest efficiently and equitably; promote the competitiveness of the U.S. economy; improve job opportunities by focusing on high labor standards; strengthen infrastructure resilience to all hazards including climate change; and to effectively coordinate with State, local, Tribal, and territorial government partners. \n",
            "Accuracy: 28/28 (1.0000)\n",
            "\n",
            "Class: INFRA Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families. The Infrastructure for Rebuilding America (INFRA) program is an existing competitive program that will see a more than  percent increase in this year&#;s funding due to the Bipartisan Infrastructure Law. These grants advance the Administration&#;s priorities of rebuilding America&#;s infrastructure and creating jobs by funding highway, multimodal freight and rail projects that position America to win the st century. Projects will improve safety, generate economic benefits, reduce congestion, enhance resiliency, and hold the greatest promise to eliminate supply chain bottlenecks and improve critical freight movements. Last year, DOT received over $ billion of project applications, but could only fund around $ billion of projects. To see last year&#;s INFRA grants, click here. The Bipartisan Infrastructure Law provides approximately $ billion for INFRA over  years, of which approximately $. billion will be made available through this NOFO. \n",
            "Accuracy: 0/4 (0.0000)\n",
            "\n",
            "Class: Mega Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The National Infrastructure Project Assistance (MEGA) program was created in the Bipartisan Infrastructure Law to fund major projects that are too large or complex for traditional funding programs. The program will provide grants on a competitive basis to support multijurisdictional or regional projects of significance that may also cut across multiple modes of transportation. Eligible projects could include highway, bridge, freight, port, passenger rail, and public transportation projects of national and regional significance. These could be bridges or tunnels connecting two states; new rail and transit lines that improve equity and reduce emissions; and freight hubs integrating ship, train and truck traffic while improving environmental justice. DOT will award  percent of funding to projects greater than $ million in cost, and  percent to projects greater than $ million but less than $ million in cost. The program will receive up to $ billion this year alone and be able to provide multi-year funding to projects.\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: Rural Surface Transportation Grant Program: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The Rural Surface Transportation Grant Program (RURAL) was created in the Bipartisan Infrastructure Law and will support projects to improve and expand the surface transportation infrastructure in rural areas to increase connectivity, improve the safety and reliability of the movement of people and freight, and generate regional economic growth and improve quality of life. Eligible projects for Rural grants include highway, bridge, and tunnel projects that help improve freight, safety, and provide or increase access to an agricultural, commercial, energy, or transportation facilities that support the economy of a rural area. This year alone, DOT will award up to $ million in grants through the rural program&#;part of the $ billion included in the Bipartisan Infrastructure Law over five years.\n",
            "Accuracy: 0/2 (0.0000)\n",
            "\n",
            "Class: *FY  Notice of Funding Opportunity: Bipartisan Infrastructure Law (BIL) FAA Contract Tower (FCT) Competitive Grant Program: The Department of Transportation (DOT), Federal Aviation Administration (FAA) announces the opportunity to apply for $ million in FY  Airport Infrastructure Grant funds for the FCT competitive grant program, made available under the Infrastructure Investment and Jobs Act of  (IIJA), Pub. L. -, herein referred to as the Bipartisan Infrastructure Law (BIL). The purpose of the FCT competitive grant program is to make annual grants available to eligible airports for airport-owned airport traffic control tower (ATCT) projects that address the aging infrastructure of the nation&#;s airports. The FAA will consider ATCT projects that sustain, construct, repair, improve, rehabilitate, modernize, replace, or relocate non-approach control towers; or acquire and install air traffic control, communications, and related equipment to be used in those towers; or construct a remote tower certified by the FAA including acquisition and installation of air traffic control, communications, or related equipment. In addition, FT grants will align with DOT&#;s Strategic Framework FY- at https://www.transportation.gov/administrations/office-policy/fy--strategic-framework. The FY  FCT will be implemented consistent with law and in alignment with the priorities in Executive Order , Implementation of the Infrastructure Investments and Jobs Act ( FR ), which are to invest efficiently and equitably; promote the competitiveness of the U.S. economy; improve job opportunities by focusing on high labor standards; strengthen infrastructure resilience to all hazards including climate change; and to effectively coordinate with State, local, Tribal, and territorial government partners. \n",
            "Accuracy: 8/8 (1.0000)\n",
            "\n",
            "Class: FY Railroad Crossing Elimination Grant Program: The purpose of the RCE Program is to provide funding for highway-rail and pathway-rail grade crossing improvement projects that focus on improving the safety and mobility of people and goods.\n",
            "Accuracy: 9/9 (1.0000)\n",
            "\n",
            "Class: Reconnecting Communities Pilot Discretionary Grant Program: The Reconnecting Communities Pilot Program notice of funding opportunity was amended on September ,  to include technical corrections to the Key Information Table. See updated materials in the &quot;Manage Related Documents&quot; section, as well as on our website at https://www.transportation.gov/grants/reconnecting-communitiesThe purpose of this notice is to solicit applications for Reconnecting Communities Pilot (RCP) Program grants. Funds for the Fiscal Year (FY)  RCP Program are to be awarded on a competitive basis for projects that reconnect communities by removing, retrofitting, or mitigating highways or other transportation facilities that create barriers to community connectivity, including to mobility, access, or economic development.The variety of transformative solutions to knit communities back together can include: high-quality public transportation, infrastructure removal, pedestrian walkways and overpasses, capping over roadways, linear parks and trail connectors, roadway redesigns and complete streets conversions, and main street revitalization. The program will award two types of grants: Planning Grants and Capital Construction Grants. Please consult the full Notice of Funding Opportunity for full details about the application process, requirements, and forms.\n",
            "Accuracy: 6/7 (0.8571)\n",
            "\n",
            "Class: Safe Streets and Roads for All Discretionary Grant Program: THIS NOTICE OF FUNDING OPPORTUNITY WAS AMENDED ON AUGUST , . PLEASE SEE THE GRANTS.GOV PACKAGE UPDATES, AND THE SAFE STREETS AND ROADS FOR ALL (SSA) WEBSITE IN THE LINK PROVIDED BELOW FOR MORE INFORMATION. DEADLINE REMAINS SEPTEMBER , .The purpose of this notice is to solicit applications for SSA grants. Funds for the fiscal year (FY)  SSA grant program are to be awarded on a competitive basis to support planning, infrastructure, behavioral, and operational initiatives to prevent death and serious injury on roads and streets involving all roadway users, including pedestrians; bicyclists; public transportation, personal conveyance, and micromobility users; motorists; and commercial vehicle operators. Those interested in applying for Implementation Grants should use the application materials in PKG; those interested in applying for Action Plan Grants should use the application materials in PKG. See application instructions to determine which grant to apply for in this funding round.\n",
            "Accuracy: 4/6 (0.6667)\n",
            "\n",
            "Class: Strengthening Mobility and Revolutionizing Transportation (SMART) Grants Program: The purpose of this notice is to solicit applications for Strengthening Mobility and Revolutionizing Transportation (SMART) grants. Funds for the fiscal year (FY)  SMART Grants Program are to be awarded on a competitive basis to conduct demonstration projects focused on advanced smart city or community technologies and systems to improve transportation efficiency and safety.\n",
            "Accuracy: 9/9 (1.0000)\n",
            "\n",
            "Class: FY- Federal-State Partnership for Intercity Passenger Rail Grant Program for projects located on the Northeast Corridor: This program funds capital projects on the Northeast Corridor that reduce the state of good repair backlog, improve performance, or expand or establish new intercity passenger rail service.\n",
            "Accuracy: 4/4 (1.0000)\n",
            "\n",
            "Class: Fiscal Years - Wildlife Crossings Pilot Program (WCPP): The primary goals of the WCPP are to save lives, prevent serious injuries, and protect motorists and wildlife by reducing WVCs, and improve habitat connectivity for terrestrial and aquatic species. Reduction of wildlife vehicle collisions and improvement of terrestrial and aquatic habitat connectivity are the primary merit criteria that will be used in reviewing applications, and each of the primary merit criteria are of equal importance.\n",
            "Accuracy: 3/3 (1.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity; Buses and Bus Facilities Program: The Federal Transit Administration (FTA) announces the availability of approximately $ million in competitive grants under the Buses and Bus Facilities Program is to assist in the financing of buses and bus facilities capital projects, including replacing, rehabilitating, purchasing or leasing buses or related equipment, and rehabilitating, purchasing, constructing or leasing bus-related facilities. Synopses and full announcement will be posted on Grants.gov as opportunity FTA---TPM-BUS. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time April , .\n",
            "Accuracy: 7/7 (1.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity; Low or No Emission Vehicle Program: The Federal Transit Administration (FTA) announces the availability of approximately $. billion in competitive grants under the Low or No Emission Grant Program (Low-No Program) for the purchase or lease of zero-emission and low-emission transit buses, including acquisition, construction, and leasing of required supporting facilities. Synopses and full announcement will be posted on Grants.gov as opportunity FTA---TPM-LWNO. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time April , .\n",
            "Accuracy: 0/12 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Areas of Persistent Poverty Program: To create new opportunities for those experiencing poverty, the U.S. Department of Transportation&#;s Federal Transit Administration (FTA) today announced a new Notice of Funding Opportunity (NOFO) to help improve transit in areas experiencing long-term economic distress. The $ million in competitive grant funding through FTA&#;s Areas of Persistent Poverty (AoPP) Program provides more resources to underserved and disadvantaged communities seeking to expand or improve transit. \n",
            "Accuracy: 7/7 (1.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Ferry Service for Rural Communities Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  Ferry Service for Rural Communities Program (FSRC) (Federal Assistance Listing #.). FSRC makes funding available competitively to assist in the financing of capital, planning, and operating assistance for eligible ferry services.\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $. million in competitive grants under the Fiscal Year (FY)  Passenger Ferry Grant Program (PFG) (Federal Assistance Listing #.). Of that amount, $ million is available only for low or zero-emission ferries or ferries using electric battery or fuel cell components and the infrastructure to support such ferries. As required by Federal public transportation law, funds will be awarded competitively to designated recipients or eligible direct recipients of Urbanized Area Formula funds to support capital projects to improve existing passenger ferry service, establish new ferry service, and repair and modernize ferry boats, terminals, and related facilities and equipment. \n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: FY  National Infrastructure Investments: The Infrastructure Investment and Jobs Act of  (&#;Bipartisan Infrastructure Law,&#; or &#;BIL&#;) authorized and appropriated $. billion to be awarded by the Department of Transportation (&#;DOT&#;) for Local and Regional Project Assistance Program Grants under National Infrastructure Investments in FY . This Notice of Funding Opportunity (NOFO) solicits applications for projects funded under the Local and Regional Project Assistance Program, known as the RAISE Grants program. If the FY  Appropriations Act provides additional funding for the RAISE grants program and/or significantly different requirements for National Infrastructure Investment funds, the Department will amend this Notice with guidance on additional requirements.As with previous rounds, funds for the FY  RAISE Transportation program are to be awarded on a competitive basis for projects that will have a significant local or regional impact.\n",
            "Accuracy: 18/24 (0.7500)\n",
            "\n",
            "Class: Neighborhood Access and Equity (NAE) Program: The Department of Transportation is combining two major discretionary grant programs, the Reconnecting Communities Pilot (RCP) and Neighborhood Access and Equity (NAE) programs, into one Notice of Funding Opportunity (NOFO). Together, this combined program is known as the Reconnecting Communities and Neighborhoods (RCN) Program. The primary goal of the NAE Program is to assist economically disadvantaged or undeserved communities for planning and construction activities. Funds for the fiscal year (FY)  NAE grant program are to be awarded on a competitive basis to support planning, capital construction, and regional partnership activities that aim to restore community connectivity through the removal, retrofit, mitigation or replacement of highways, roadways, or other infrastructure facilities that create barriers to mobility, access or economic development. Applicants must submit their applications via Valid Eval at the links below: Community Planning Grants: https://usg.valideval.com/teams/rcn_planning/signup Capital Construction Grants: https://usg.valideval.com/teams/rcn_capitalconstruction/signup Do not submit applications through Grants.gov. Applications must be submitted by : PM Eastern Time on Thursday, September , . Late applications will not be accepted. \n",
            "Accuracy: 21/21 (1.0000)\n",
            "\n",
            "Class: FY  Notice of Funding Opportunity for Small Shipyard Grants: This notice announces the intention of the Maritime Administration to provide grants to small shipyards. Under the Small Shipyard Grant Program, there is currently $,, available for grants for capital and related improvements to qualified shipyard facilities that will be effective in fostering efficiency, competitive operations, and quality ship construction, repair, and reconfiguration. Potential applicants are advised that it is expected, based on experience, that the number of applications will far exceed the funds available and that only a small percentage of applicants will be funded. Applications (SF- and the Addendum) will be accepted online through Grants.gov by  pm on the closing date (February , ). See full text under Related Documents Tab (FY  SSG NOFO) or the MARAD Web Page for detailed submission instructions.\n",
            "Accuracy: 4/4 (1.0000)\n",
            "\n",
            "Class: Safe Streets and Roads for All Funding Opportunity: The purpose of this notice is to solicit applications for Safe Streets and Roads for All (SSA) grants. Funds for the fiscal year (FY)  SSA grant program are to be awarded on a competitive basis to support planning and demonstration activities, as well as projects and strategies to prevent death and serious injury on roads and streets involving all roadway users, including pedestrians; bicyclists; public transportation, personal conveyance, and micromobility users; motorists; and commercial vehicle operators. Applicants must submit their applications via Valid Eval at https://usg.valideval.com/teams/usdot_ssa__implementation/signup for Implementation Grant applicants, and https://usg.valideval.com/teams/usdot_ssa__planning_demo/signup for Planning and Demonstration Grants. Do not submit your applications through Grants.Gov. Applications must be submitted by : PM Eastern Time on Monday, July , . Late applications will not be accepted.\n",
            "Accuracy: 57/58 (0.9828)\n",
            "\n",
            "Class: SMART Grants Notice of Funding: The purpose of this notice is to solicit applications for Strengthening Mobility and Revolutionizing Transportation (SMART) Stage  Planning and Prototyping grants. Funds for the fiscal year (FY)  SMART Grants Program are to be awarded on a competitive basis to conduct demonstration projects focused on advanced smart city or community technologies and systems to improve transportation efficiency and safety. Applications must be submitted via Valid Eval, an online submission proposal system used by USDOT, at https://usg.valideval.com/teams/USDOT_SMART_/signup. USDOT will not accept or review application materials submitted via Grants.gov. \n",
            "Accuracy: 0/5 (0.0000)\n",
            "\n",
            "Class:  Tribal Transportation Program Safety Fund: Eligible projects described in section (a)() are strategies, activities, and projects on a public road that are consistent with a transportation safety plan and that (i) correct or improve a hazardous road location or feature, or (ii) address a highway safety problem. TTPSF emphasizes the development of strategic transportation safety plans using a data-driven process as a means for Tribes to identify transportation safety needs and determine how those needs will be addressed in Tribal communities. FHWA has identified four eligibility categories: transportation safety plans; data assessment, improvement, and analysis activities; systemic roadway departure countermeasures; and infrastructure improvements and other eligible activities as listed in  U.S.C. &#; (a)().\n",
            "Accuracy: 13/13 (1.0000)\n",
            "\n",
            "Class: United States Marine Highway Grants: This funding opportunity solicits applications for fiscal year (FY)  United States Marine Highway Program (USMHP) grants. Funds for FY  USMHP grants will be awarded on a competitive basis to assist in funding eligible projects for the purpose of developing, expanding, or promoting marine highway transportation. This opportunity announces the availability of up to $,, million in funding for grants under this program and establishes selection criteria and application requirements. All USMHP grant recipients must meet all applicable Federal requirements, including domestic content (&#;Buy America&#;) requirements. This program was formerly known as the America&#;s Marine Highway Program\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: INFRA Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families. The Infrastructure for Rebuilding America (INFRA) program is an existing competitive program that will see a more than  percent increase in this year&#;s funding due to the Bipartisan Infrastructure Law. These grants advance the Administration&#;s priorities of rebuilding America&#;s infrastructure and creating jobs by funding highway, multimodal freight and rail projects that position America to win the st century. Projects will improve safety, generate economic benefits, reduce congestion, enhance resiliency, and hold the greatest promise to eliminate supply chain bottlenecks and improve critical freight movements. Last year, DOT received over $ billion in application requests, but could only fund around $. billion of projects. To see last year&#;s INFRA grants, click here. The Bipartisan Infrastructure Law provides approximately $ billion for INFRA over  years, of which approximately $. billion will be made available through this NOFO. \n",
            "Accuracy: 0/4 (0.0000)\n",
            "\n",
            "Class: Mega Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The National Infrastructure Project Assistance (MEGA) program was created in the Bipartisan Infrastructure Law to fund major projects that are too large or complex for traditional funding programs. The program will provide grants on a competitive basis to support multijurisdictional or regional projects of significance that may also cut across multiple modes of transportation. Eligible projects could include highway, bridge, freight, port, passenger rail, and related public transportation projects of national and regional significance. DOT will award approximately  percent of funding to projects greater than $ million in cost, and approximately  percent to projects greater than $ million but less than $ million in cost.For more information about last year&apos;s Mega awards, visit here: https://www.transportation.gov/grants/mega-grant-program/FYawards\n",
            "Accuracy: 0/2 (0.0000)\n",
            "\n",
            "Class: Rural Surface Transportation Grant Progam: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The Rural Surface Transportation Grant Program (RURAL) was created in the Bipartisan Infrastructure Law and will support projects to improve and expand the surface transportation infrastructure in rural areas to increase connectivity, improve the safety and reliability of the movement of people and freight, and generate regional economic growth and improve quality of life. Eligible projects for Rural grants include highway, bridge, and tunnel projects that help improve freight, safety, and provide or increase access to an agricultural, commercial, energy, or transportation facilities that support the economy of a rural area. This year alone, DOT will award up to $ million in grants through the rural program&#;part of the $ billion included in the Bipartisan Infrastructure Law over five years. At least % of rural funding must be awarded in amounts of $ million or more. If you are seeking less than $ million, you are competing for only about $. million nationwide.For more information on last year&apos;s rural awards, see: https://www.transportation.gov/grants/rural--fact-sheets\n",
            "Accuracy: 0/5 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Rail Vehicle Replacement Grant Program (RAIL): The Federal Transit Administration (FTA) announces the opportunity to apply for approximately $ million in Fiscal Year (FY)  Section  State of Good Repair Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected rail vehicle replacement projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to States and local governmental authorities to assist in the funding of capital projects to replace rail rolling stock.\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'FY  Notice of Funding Opportunity: Bipartisan Infrastructure Law (BIL) Airport Terminal Program (ATP) Grants: The Department of Transportation (DOT), Federal Aviation Administration (FAA) announces the opportunity to apply for approximately $ billion in FY  discretionary funds for the Airport Terminal Program (ATP), made available under the Infrastructure Investment and Jobs Act of  (IIJA), Pub. L. -, herein referred to as the Bipartisan Infrastructure Law (BIL). The purpose of the ATP is to make annual grants available to eligible airports for airport terminal and airport-owned Airport Traffic Control Towers development projects that address the aging infrastructure of our nation&#;s airports. In addition, ATP grants will align with DOT&#;s Strategic Framework FY- at https://www.transportation.gov/administrations/office-policy/fy--strategic-framework. The FY  ATP will be implemented consistent with law and in alignment with the priorities in Executive Order , Implementation of the Infrastructure Investments and Jobs Act ( FR ), which are to invest efficiently and equitably; promote the competitiveness of the U.S. economy; improve job opportunities by focusing on high labor standards; strengthen infrastructure resilience to all hazards including climate change; and to effectively coordinate with State, local, Tribal, and territorial government partners. ': 0.48,\n",
              " 'FY  Competitive Funding Opportunity: Airport Improvement Program Discretionary Grants: The U.S. Department of Transportation&#;s Federal Aviation Administration (FAA) announces the opportunity to apply for an estimated $. billion in Fiscal Year (FY)  discretionary grants under the Airport Improvement Program (AIP), per  U.S.C. &#; . The FAA awards these annually appropriated discretionary funds through the FAA&#;s long-standing iterative, competitive grant process. Prior to the publication of this Notice of Funding Opportunity (NOFO), the FAA identified eligible applicants in its National Plan of Integrated Airport Systems (NPIAS) and compiled potentially eligible projects through the -year Airports Capital Improvement Plan (ACIP). Both of these processes are described in FAA Order ., &#;Formulation of NPIAS and ACIP,&#; adhering to  U.S.C. &#; , which authorizes discretionary funds. The AIP funds airport capital improvements and rehabilitation projects. All discretionary grant funding is subject to appropriations, statutory requirements, and related program funding availability. ': 1.0,\n",
              " 'Grants for Buses and Bus Facilities Program: The Federal Transit Administration (FTA) announces the availability of approximately $. million of Fiscal Year  funds for buses, bus facilities, and bus equipment. The purpose of the Grants for Buses and Bus Facilities Program is to assist in the financing of buses and bus facilities capital projects, including replacing, rehabilitating, purchasing or leasing buses or related equipment, and rehabilitating, purchasing, constructing or leasing bus-related facilities. ': 1.0,\n",
              " 'Solicitation of Project Proposals for the Low or No Emission (Low-No) Program:  Notice of Funding Opportunity (NOFO): Solicitation of Project Proposals for the Low or No Emission Program (Low-No) Program. The Federal Transit Administration (FTA) announces the availability of $ million of Fiscal Year  funds for the purchase or lease of low or no emission vehicles as well as related equipment or facilities. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time on //. The main purpose of the Low-No Program is to support the transition of the nation&#;s transit fleet to the lowest polluting and most energy efficient transit vehicles. The Low-No Program provides funding to State and local governmental authorities for the purchase or lease of zero-emission and low-emission transit buses, including acquisition, construction, and leasing of required supporting facilities. ': 0.0,\n",
              " 'FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in Fiscal Year (FY)  Section  Urbanized Area Formula Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected passenger ferry projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to designated recipients or eligible direct recipients of Section  funds to assist in the financing of capital projects to support existing passenger ferry service, establish new ferry service, and to repair and modernize ferry boats, terminals, and related facilities and equipment. ': 0.0,\n",
              " 'Pilot Program for Transit-Oriented Development (TOD) Planning: Notice of Funding Opportunity for the Pilot Program for Transit-Oriented Development (TOD) Planning. The Federal Transit Administration (FTA) announces the availability of approximately $. million in Pilot Program for TOD Planning funding to support comprehensive planning associated with new fixed guideway and core capacity improvement projects. FTA may award amounts ranging from $, to $,,. The Pilot Program for TOD Planning provides funding to local communities to integrate land use and transportation planning in new fixed guideway and core capacity transit project corridors. As required by statute, any comprehensive planning funded through the pilot program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations. The statute also requires that the planning work be associated with a new fixed guideway or core capacity transit project as defined in Federal transit statute ( USC (a); also see the NOFO for the definitions). Consistent with statutory direction, FTA is seeking comprehensive planning projects covering an entire transit capital project corridor, rather than proposals that involve planning for individual station areas or only a small section of the corridor. To ensure that planning work reflects the needs and aspirations of the local community and results in concrete, specific deliverables and outcomes, FTA is requiring that transit project sponsors partner with entities with land use planning authority in the transit project corridor. ': 1.0,\n",
              " 'Public Transportation on Indian Reservations Program: The Federal Transit Administration (FTA) announces the availability of approximately $ million in funding provided by the Public Transportation on Indian Reservations Program (Tribal Transit Program (TTP)), as authorized by  U.S.C. (c)()(A), as amended by the Fixing America&apos;s Surface Transportation Act (FAST), Public Law - (December , ). The primary purpose of these competitively selected grants is to support planning, capital, and, in limited circumstances, operating assistance for tribal public transit services. Funds distributed to Indian tribes under the TTP should not replace or reduce funds that Indian tribes receive from states through FTA&#;s Section  program. ': 0.16666666666666666,\n",
              " 'INFRA Grants: The Nationally Significant Freight and Highway Projects (INFRA) program provides Federal financial assistance to highway and freight projects of national or regional significance. This notice solicits applications for awards under the program&#;s fiscal year (FY)  funding, subject to the availability of appropriated funds. The Department is focusing the competition on transportation infrastructure projects that support four key objectives, each of which is discussed in greater detail in the Notice of Funding Opportunity: () Supporting economic vitality at the national and regional level; () Leveraging Federal funding to attract non-Federal sources of infrastructure investment; () Deploying innovative technology, encouraging innovative approaches to project delivery, and incentivizing the use of innovative financing; and () Holding grant recipients accountable for their performance. ': 0.6666666666666666,\n",
              " ' Tribal Transportation Program Safety Fund: A Notice of Funding Availability (NOFA) for Tribal Transportation Program Safety Funds (TTPSF) has been published. This notice announces the availability of $. million for projects in three categories: safety plans; data assessment, improvement, and analysis activities; and infrastructure improvement and other eligible activities as listed in  U.S.C.(a)() The deadline for submissions is //. For complete instructions on preparing and submitting an application for TTPSF, please download the application information and NOFO at https://flh.fhwa.dot.gov/programs/ttp/safety/ttpsf.htm Please submit your application directly to FHWA through the &quot;apply now&quot; button at: https://flh.fhwa.dot.gov/programs/ttp/safety/ttpsf.htm ': 1.0,\n",
              " 'Grants for Buses and Bus Facilities Program: The purpose of the Grants for Buses and Bus Facilities Program is to assist in the financing of buses and bus facilities capital projects, including replacing, rehabilitating, purchasing or leasing buses or related equipment, and rehabilitating, purchasing, constructing or leasing bus-related facilities.': 0.8571428571428571,\n",
              " 'Low or No Emission Program (Low-No Program): The main purpose of the Low-No Program is to support the transition of the nation&#;s transit fleet to the lowest polluting and most energy efficient transit vehicles. The Low-No Program provides funding to State and local governmental authorities for the purchase or lease of zero-emission and low-emission transit buses, including acquisition, construction, and leasing of required supporting facilities.': 1.0,\n",
              " 'Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in Fiscal Year (FY)  Section  Urbanized Area Formula Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected passenger ferry projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to designated recipients or eligible direct recipients of Section  funds to assist in the financing of capital projects to support existing passenger ferry service, establish new ferry service, and to repair and modernize ferry boats, terminals, and related facilities and equipment. ': 0.0,\n",
              " 'Pilot Program for Transit-Oriented Development (TOD) Planning: Notice of Funding Opportunity for PILOT PROGRAM FOR TRANSIT-ORIENTED DEVELOPMENT (TOD) PLANNING. The Federal Transit Administration (FTA) announces the opportunity to apply for approximately $. million of funding under the Pilot Program for TOD Planning (Catalog of Federal Domestic Assistance #.). As required by federal transit law and subject to funding availability, funds will be awarded competitively to support comprehensive planning associated with new fixed guideway and core capacity improvement projects. FTA anticipates minimum grant awards of $, and maximum grant awards of $,,. The Pilot Program for TOD Planning is intended to fund comprehensive planning that supports economic development, ridership, multimodal connectivity and accessibility, increased transit access for pedestrian and bicycle traffic, and mixed-use development near transit stations. The program also encourages identification of infrastructure needs and engagement with the private sector. Consistent with statutory direction, FTA is seeking comprehensive planning projects covering an entire transit capital project corridor, rather than proposals that involve planning for individual station areas or only a small section of the corridor. To ensure any proposed planning work reflects the needs and aspirations of the local community and results in concrete, specific deliverables and outcomes, transit project sponsors must partner with entities with land use planning authority in the transit project corridor to conduct the planning work.The Pilot Program for TOD Planning helps support FTA&#;s mission of improving public transportation for America&#;s communities by providing funding to local communities to integrate land use and transportation planning around a new fixed guideway or core capacity improvement project. Per statute, any comprehensive planning funded through the program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations.': 0.0,\n",
              " 'FY  Competitive Funding Opportunity: Public Transportation on Indian Reservations Program: The Federal Transit Administration (FTA) announces the availability of approximately $ million in funding provided by the Public Transportation on Indian Reservations Program (Tribal Transit Program (TTP)), as authorized by  U.S.C. (c)()(A), as amended by the Fixing America&apos;s Surface Transportation Act (FAST), Public Law - (December , ). This is a national solicitation for project proposals and includes the selection criteria and program eligibility information for Fiscal Year  projects. The primary purpose of these competitively selected grants is to support planning, capital, and, in limited circumstances, operating assistance for tribal public transit services. Funds distributed to Indian tribes under the TTP should not replace or reduce funds that Indian tribes receive from states through FTA&#;s Section  program.': 0.0,\n",
              " '*INFRA Grants: The U.S. Department of Transportation (USDOT) is seeking applicants for the FY  round of the Infrastructure for Rebuilding America (INFRA) discretionary grant program to fund transportation projects of national and regional significance that result in good-paying jobs, improve safety, apply transformative technology, and explicitly address climate change and racial equity. The funding available for this year&#;s grants totals approximately $ million.The Department recognizes the role that infrastructure investment plays in economic development and job creation, and the added urgency of this funding at time when the COVID- pandemic has put stress on state and local budgets.For the first time, the USDOT seeks INFRA projects that address climate change and environmental justice. Projects will be evaluated on whether they were planned as part of a comprehensive strategy to address climate change, or whether they support strategies to reduce greenhouse gas emissions such as deploying zero-emission-vehicle infrastructure or encouraging modal shift and a reduction in vehicle-miles-traveled. Racial equity will also be considered as a selection criterion, to the extent that project sponsors have completed equity-focused community outreach, and projects are designed to benefit underserved communities. The Department will also consider whether the project is located in a federally designated community development zone, including qualified Opportunity Zones, Empowerment Zones, Promise Zones, or Choice Neighborhoods.USDOT seeks projects that apply innovative technology, delivery, or financing methods with proven outcomes to deliver projects in a cost effective manner. The Department will make awards under the INFRA program to both large and small projects. For a large project, the INFRA grant must be at least $ million. For a small project, the grant must be at least $ million. Under statutory requirements,  percent of available funds are reserved for small projects, and the Department must award at least  percent of funding for rural projects. INFRA grants may be used to fund a variety of components of an infrastructure project, however, the Department is specifically focused on projects in which the local sponsor is significantly invested and is positioned to proceed rapidly to construction. Eligible INFRA project costs may include: reconstruction, rehabilitation, acquisition of property (including land related to the project and improvements to the land), environmental mitigation, construction contingencies, equipment acquisition, and operational improvements directly related to system performance. The INFRA NOFO also announces the creation of the &#;INFRA Extra&#; Program, which will identify competitive INFRA applicants who do not receive an INFRA award and authorize them to seek a Transportation Infrastructure Finance and Innovation Act of  (TIFIA) loan up to  percent of their project cost.': 0.0,\n",
              " 'Fiscal Year  Competitive Funding Opportunity; Grants for Buses and Bus Facilities Program: The Federal Transit Administration (FTA) announces the availability of approximately $. million of Fiscal Year  funds for buses, bus facilities, and bus equipment. Synopses and full announcement will be posted on Grants.gov as opportunity FTA---BUS. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time November ,.': 0.1,\n",
              " 'FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  Passenger Ferry Grant Program (Ferry Program) (Federal Assistance Listing #.). Of that amount, $ million is available only for low or zero-emission ferries or ferries using electric battery or fuel cell components and the infrastructure to support such ferries. As required by Federal public transportation law, funds will be awarded competitively to designated recipients or eligible direct recipients of Urbanized Area Formula funds to support capital projects to improve existing passenger ferry service, establish new ferry service, and repair and modernize ferry boats, terminals, and related facilities and equipment.': 0.0,\n",
              " ' Competitive Funding Opportunity: Pilot Program for Transit-Oriented Development (TOD) Planning: The Pilot Program for TOD Planning helps support FTA&#;s mission of improving public transportation for America&#;s communities by providing funding to local communities to integrate land use and transportation planning around a new fixed guideway or core capacity improvement project. Per statute, any comprehensive planning funded through the program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations. Additional information is available at: https://www.transit.dot.gov/TODPilot.Notice of Funding Opportunity (NOFO): Notice of Funding Opportunity for PILOT PROGRAM FOR TRANSIT-ORIENTED DEVELOPMENT (TOD) PLANNING. The Federal Transit Administration (FTA) announces the opportunity to apply for $,, of funding under the Pilot Program for TOD Planning (Catalog of Federal Domestic Assistance #.). As required by federal transit law and subject to funding availability, funds will be awarded competitively to support comprehensive planning associated with new fixed guideway and core capacity improvement projects. FTA anticipates minimum grant awards of $, and maximum grant awards of $,,. Synopses and full announcement are posted on Grants.gov as opportunity FTA---TPE-TODP. The Pilot Program for TOD Planning is intended to fund comprehensive planning that supports economic development, ridership, multimodal connectivity and accessibility, increased transit access for pedestrian and bicycle traffic, and mixed-use development near transit stations. The program also encourages identification of infrastructure needs and engagement with the private sector.Consistent with statutory direction, FTA is seeking comprehensive planning projects covering an entire transit capital project corridor, rather than proposals that involve planning for individual station areas or only a small section of the corridor. To ensure any proposed planning work reflects the needs and aspirations of the local community and results in concrete, specific deliverables and outcomes, transit project sponsors must partner with entities with land use planning authority in the transit project corridor to conduct the planning work.Link and Instructions for attaching the supplemental form to the SF-: All applicants must complete a supplemental form (PDF) specific to the Pilot Program for TOD Planning and attach it to their submission in Grants.gov. See section D of the NOFO for information about additional required application contents.Webinar: FTA will conduct a webinar for prospective applicants after the NOFO is published in the Federal Register. Further information will be posted on FTA&#;s website (https://www.transit.dot.gov/TODPilot) when available. Dates: An applicant must submit a proposal electronically by : p.m. Eastern Daylight Time on June , . Any agency intending to apply should initiate the process of registering on the Grants.gov site immediately to ensure completion of registration before the submission deadline. ': 0.0,\n",
              " 'FY  Competitive Funding Opportunity:  Public Transportation on Indian Reservations Program; Tribal Transit Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants for the Fiscal Year (FY)  Public Transportation on Indian Reservations (Tribal Transit) Program (Federal Assistance Listing: .). As required by Federal public transportation law, funds will be awarded competitively for any purpose eligible under FTA&#;s Formula Grants for Rural Areas Program,  U.S.C. , including planning, capital, and operating assistance for tribal public transit services in rural areas. ': 1.0,\n",
              " 'INFRA Grants: The U.S. Department of Transportation (USDOT) is seeking applicants for the FY  round of the Infrastructure for Rebuilding America (INFRA) discretionary grant program to fund transportation projects of national and regional significance that result in good-paying jobs, improve safety, apply transformative technology, and explicitly address climate change and racial equity. The funding available for this year&#;s grants totals approximately $ million.The Department recognizes the role that infrastructure investment plays in economic development and job creation, and the added urgency of this funding at time when the COVID- pandemic has put stress on state and local budgets.For the first time, the USDOT seeks INFRA projects that address climate change and environmental justice. Projects will be evaluated on whether they were planned as part of a comprehensive strategy to address climate change, or whether they support strategies to reduce greenhouse gas emissions such as deploying zero-emission-vehicle infrastructure or encouraging modal shift and a reduction in vehicle-miles-traveled. Racial equity will also be considered as a selection criterion, to the extent that project sponsors have completed equity-focused community outreach, and projects are designed to benefit underserved communities. The Department will also consider whether the project is located in a federally designated community development zone, including qualified Opportunity Zones, Empowerment Zones, Promise Zones, or Choice Neighborhoods.USDOT seeks projects that apply innovative technology, delivery, or financing methods with proven outcomes to deliver projects in a cost effective manner. The Department will make awards under the INFRA program to both large and small projects. For a large project, the INFRA grant must be at least $ million. For a small project, the grant must be at least $ million. Under statutory requirements,  percent of available funds are reserved for small projects, and the Department must award at least  percent of funding for rural projects. INFRA grants may be used to fund a variety of components of an infrastructure project, however, the Department is specifically focused on projects in which the local sponsor is significantly invested and is positioned to proceed rapidly to construction. Eligible INFRA project costs may include: reconstruction, rehabilitation, acquisition of property (including land related to the project and improvements to the land), environmental mitigation, construction contingencies, equipment acquisition, and operational improvements directly related to system performance. The INFRA NOFO also announces the creation of the &#;INFRA Extra&#; Program, which will identify competitive INFRA applicants who do not receive an INFRA award and authorize them to seek a Transportation Infrastructure Finance and Innovation Act of  (TIFIA) loan up to  percent of their project cost.': 0.11764705882352941,\n",
              " 'FY Tribal Transportation Program Safety Fund (TTPSF): A Notice of Funding Opportunity (NOFO) for Tribal Transportation Program Safety Funds (TTPSF) has been published. This notice announces the availability of approximately $. million for projects in four categories: safety plans; data assessment, improvement, and analysis activities; systemic roadway departure countermeasures; and infrastructure improvement and other eligible activities as listed in  U.S.C.(a)(). The goal of these projects is to reduce fatalities and serious injuries from transportation incidents, such as motor vehicle crashes. For complete instructions on preparing and submitting an application for TTPSF, please download the application, application information, systemic roadway departure risk assessment form (if applicable) and NOFO at https://highways.dot.gov/federal-lands/programs-tribal/safety/funds. Applications will NOT be submitted on Grants.gov. Instead, please use the &quot;apply now&quot; button on the program web page.': 0.7777777777777778,\n",
              " 'Advanced Transportation Technologies and  Innovative Mobility Deployment (ATTIMD) Program: AMENDMENT  (//): This amendment is for the purpose of correcting a typo contained in Table D. on page  of . This table, contained in Section D. for CONTENT AND FORM OF APPLICATION SUBMISSION, erroneously omitted the Project Outcome Criteria from that table&#;s submission requirements for Volume . This amendment also revises the verbiage included on page  of the NOFO to clarify that all resumes for key personnel should be included in item f) Appendix &#; R&#;sum&#;s for Key Personnel. Please note that Resumes do not count against the page limitations for Volume . Please refer to Amendment  in the &#;Related Documents&#; tab for more information. -- END OF AMENDMENT  -- AMENDMENT  (//): This amendment adds the weblink to the recording of the October , , informational webinar for this opportunity. The weblink is included in the attachments page for this opportunity as well as below: LINK: https://usdot.zoomgov.com/rec/share/eQNNRGISy-UxfqGpSDzINRbzvcdkAjDdoulZvwRCt-sARp.kdAJIUFsFZZEpf PASSCODE: c@X*&amp;@ -- END OF AMENDMENT  -- AMENDMENT  (//): This amendment adds the registration information for the October , , informational webinar for this opportunity. Please refer to the &quot;Related Documents&quot; tab for more information. -- END OF AMENDMENT  -- BASE NOFO (//): The Advanced Transportation Technologies and Innovative Mobility Deployment Program (ATTIMD), established in Section (c)() of title , United States Code (U.S.C.), also known as the Advanced Transportation Technology and Innovation (ATTAIN) Program, directs FHWA to award grants to eligible entities to deploy, install, and operate advanced transportation technologies to improve safety, mobility, efficiency, system performance, intermodal connectivity, and infrastructure return on investment. FHWA intends for these model technology deployments to help demonstrate how emerging transportation technologies, data, and their applications can be effectively deployed and integrated with existing systems to provide access to essential services and other destinations. This also includes efforts to increase connectivity to employment, education, services, and other opportunities; support workforce development; or contribute to increased mobility, particularly for persons with visible and hidden disabilities and elderly individuals. The Assistance Listing Number for this opportunity is . &#; Highway Research and Development. The Federal Highway Administration (FHWA) hereby requests applications to result in awards to eligible entities to deploy, install, and operate advanced transportation technologies to improve safety, mobility, efficiency, system performance, intermodal connectivity, and infrastructure return on investment. These model deployments are expected to provide benefits in the form of: reduced traffic-related fatalities and injuries; reduced traffic congestion and improved travel time reliability; reduced transportation-related emissions; optimized multimodal system performance; improved access to transportation alternatives, including for underserved populations; improved integration of payment systems; public access to real-time integrated traffic, transit, and multimodal transportation information to make informed travel decisions; cost savings to transportation agencies, businesses, and the traveling public; or other benefits to transportation users and the general public. This competitive ATTAIN Program will promote the use of innovative transportation solutions. The deployment of these technologies will provide Congress and FHWA with valuable real-life data and feedback to inform future decision-making. Cost Sharing: Cost-sharing or matching is required, with the maximum Federal share being  percent; hence, this NOFO requires a minimum non-Federal cost share of  percent. Cost sharing or matching means the portion of project costs not paid by Federal funds. NOTE: FHWA previously issued six NOFOs for the Advanced Transportation and Congestion Management Technologies Deployment (ATCMTD) Grant Program before the program was amended and renamed to the Advanced Transportation Technologies and Innovative Mobility Deployment Program by Section  of the Infrastructure Investment and Jobs Act (IIJA) (Public Law -, also known as the &#;Bipartisan Infrastructure Law&#; (BIL)).': 0.0,\n",
              " 'FY  and FY  Competitive Funding Opportunity: Competitive Grants for Rail Vehicle Replacement Program (Rail Program): The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in Fiscal Year (FY)  and  Section  State of Good Repair Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected rail vehicle replacement projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to States and local governmental authorities to assist in the funding of capital projects to replace rail rolling stock.': 0.0,\n",
              " 'Bridge Investment Program - Planning, Bridge Projects, and Large Bridge Projects: This Notice of Funding Opportunity (NOFO) is to solicit applications for three categories of Bridge Investment Program (BIP) funding opportunities: () Planning; () Bridge Projects (projects with eligible costs less than $ million); and () Large Bridge Projects (projects with eligible costs greater than $ million). Eligible applicants may submit applications for any of the three funding categories, but each category has distinct eligibility and selection criteria and application deadlines.These funds will be awarded on a competitive basis for planning, feasibility analysis, and revenue forecasting associated with the development of a project that would subsequently be eligible to apply for BIP funding under either the Bridge Projects or Large Bridge Projects funding categories.A total of $. billion in  BIP funds are available for the Bridge Projects and Large Bridge Projects funding opportunities. These funds will be awarded on a competitive basis for bridge replacement, rehabilitation, preservation, and protection projects that: () improve the safety, efficiency, and reliability of the movement of people and freight over bridges; and () improve the condition of bridges in the United States by reducing (a) the number of bridges, and total person miles traveled over bridges, that are in poor condition or that are in fair condition and at risk of falling into poor condition within the next three years, or (b) the number of bridges, and total person miles traveled over bridges, that do not meet current geometric design standards or cannot meet the load and traffic requirements typical of the regional transportation network. In addition, Large Bridge Projects that receive a BIP award of not less than $ million are eligible for multiyear grants, in which DOT can award available funds to a project over the course of several years in accordance with an agreement and in alignment with its schedule. In selecting Bridge Projects and Large Bridge Projects, FHWA will consider the extent to which BIP funds leverage non-Federal contributions from sponsors and stakeholders involved in the planning, design, and construction of eligible projects. ': 0.8333333333333334,\n",
              " 'FY  Competitive Funding Opportunity; Low or No Emission Grant Program: Low-No ProgramThe Federal Transit Administration (FTA) announces the opportunity to apply for approximately $. billion in competitive grants under the fiscal year (FY)  Low or No Emission Grant Program (Low-No Program) (Federal Assistance Listing: .) and approximately $ million in FY  funds under the Grants for Buses and Bus Facilities Program (Buses and Bus Facilities Program) (Federal Assistance Listing .), subject to availability of appropriated funding. Synopses and full announcement will be posted on Grants.gov as funding opportunity ID FTA--TPM for Low-No applications and FTA--TPM for Buses and Bus Facilities applications. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time May , .': 0.9333333333333333,\n",
              " 'FY  Competitive Funding Opportunity: All Stations Accessibility Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  All Stations Accessibility Program (ASAP) (Federal Assistance Listing #.). As required by Federal public transportation law, funds will be awarded competitively to eligible designated recipients that operate or allocate funds to inaccessible pre-ADA&#;or &#;legacy&#; &#; rail fixed guideway public transportation systems, and States (including territories and Washington, D.C.) and local governmental entities that operate or financially support legacy rail fixed guideway public transportation systems and corresponding legacy stations/facilities for capital projects to repair, improve, modify, retrofit, or relocate infrastructure of stations or facilities for passenger use, including load-bearing members that are an essential part of the structural frame; or () for planning projects to develop or modify a plan for pursuing public transportation accessibility projects, assessments of accessibility, or assessments of planned modifications to stations or facilities for passenger use projects; or programs of projects in an eligible area.': 1.0,\n",
              " 'FY  Competitive Funding Opportunity: Buses and Bus Facilities Competitive Program: Bus and Bus Facilities ProgramThe Federal Transit Administration (FTA) announces the opportunity to apply for approximately $. billion in competitive grants under the fiscal year (FY)  Low or No Emission Grant Program (Low-No Program) (Federal Assistance Listing: .) and approximately $ million in FY  funds under the Grants for Buses and Bus Facilities Program (Buses and Bus Facilities Program) (Federal Assistance Listing .), subject to availability of appropriated funding. Synopses and full announcement will be posted on Grants.gov as funding opportunity ID FTA--TPM for Low-No applications and FTA--TPM for Buses and Bus Facilities applications. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time May , ': 0.0,\n",
              " 'FY  Competitive Funding Opportunity: Electric or Low-Emitting Ferry Pilot Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  Electric or Low-Emitting Ferry Pilot Program (ELEF) (Federal Assistance Listing #.). ELEF makes funding available competitively to assist in the financing of capital projects for the purchase of electric or low-emitting ferry vessels that reduce emissions by using alternative fuels or on-board energy storage systems and related charging infrastructure to reduce emissions or produce zero onboard emissions under normal operation.': 1.0,\n",
              " 'FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $. million in competitive grants under the Fiscal Year (FY)  Passenger Ferry Grant Program (PFG) (Federal Assistance Listing #.). Of that amount, $. million is available only for low or zero-emission ferries or ferries using electric battery or fuel cell components and the infrastructure to support such ferries. As required by Federal public transportation law, funds will be awarded competitively to designated recipients or eligible direct recipients of Urbanized Area Formula funds to support capital projects to improve existing passenger ferry service, establish new ferry service, and repair and modernize ferry boats, terminals, and related facilities and equipment. ': 0.0,\n",
              " 'FY  Pilot Program for Transit-Oriented Development (TOD) Planning: The Pilot Program for TOD Planning helps support FTA&#;s mission of improving public transportation for America&#;s communities by providing funding to local communities to integrate land use and transportation planning around a new fixed guideway or core capacity improvement project. Per statute, any comprehensive or site specific planning funded through the program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations.': 1.0,\n",
              " 'FY  Competitive Funding Opportunity: Public Transportation on Indian Reservations Program; Tribal Transit Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $. million in competitive grants for the Fiscal Year (FY)  Public Transportation on Indian Reservations (Tribal Transit) Program. As required by Federal public transportation law, funds will be awarded competitively for any purpose eligible under FTA&#;s Formula Grants for Rural Areas Program, including planning, capital, and operating assistance for tribal public transit services in rural areas. FTA may award additional funding that is made available to the program prior to the announcement of project selections.': 0.0,\n",
              " 'FY Consolidated Rail Infrastructure and Safety Improvements Grant Program: This program funds projects that improve the safety, efficiency, and reliability of intercity passenger and freight rail.': 1.0,\n",
              " 'FY Corridor Identification and Development Grant Program: This program facilitates the development of intercity passenger rail corridors.': 1.0,\n",
              " 'FY Federal-State Partnership for Intercity Passenger Rail Grant Program for projects not located on the Northeast Corridor: This program funds capital projects that reduce the state of good repair backlog, improve performance, or expand or establish new intercity passenger rail service.': 0.0,\n",
              " 'Fiscal Year  National Culvert Removal, Replacement, and Restoration Grant Program (Culvert AOP Program): -- ORIGINAL NOFO POSTING - // -- The primary goal of the Culvert AOP Program is to improve or restore anadromous fish passage through the replacement, removal, repair, or improvement of culverts or weirs. The grant program prioritizes projects that would improve fish passage for: (A) anadromous fish stocks listed as an endangered species or a threatened species under section  of the Endangered Species Act of  ( U.S.C. &#; ); (B) anadromous fish stocks identified by the National Marine Fisheries Service (NMFS) or the U.S. Fish and Wildlife Service (USFWS) that could reasonably become listed as an endangered species or a threatened species under that section; (C) anadromous fish stocks identified by the NMFS or the USFWS as prey for endangered species, threatened species, or protected species, including Southern resident orcas (Orcinus orca); or (D) anadromous fish stocks identified by the NMFS or the USFWS as climate resilient stocks ( U.S.C. &#; (e)()). The program also prioritizes projects that would open up more than  meters of upstream (anadromous) habitat before the end of the natural habitat ( U.S.C. &#; (e)()). ': 1.0,\n",
              " 'FY  National Infrastructure Investments: Agency Name: Department of TransportationDescription: The Infrastructure Investment and Jobs Act (Pub. L. -, November , , &#;Bipartisan Infrastructure Law,&#; or &#;BIL&#;) authorized and appropriated $. billion to be awarded by the Department of Transportation (&#;DOT&#;) for FY  for Local and Regional Project Assistance Program Grants under National Infrastructure Investments. This Notice of Funding Opportunity (NOFO) solicits applications for projects to be funded under the Local and Regional Project Assistance Program, known as the RAISE Grants program, including any additional funding appropriated for the RAISE Grants program under the FY  Appropriations Act. On March , , the Consolidated Appropriations Act,  (Pub. L. -, &#;FY  Appropriations Act&#;) appropriated an additional $ million for the FY  RAISE Grant Program. Therefore, a total of $. billion in funding is now available for the FY  RAISE Grant Program. Applicants should note that the two funding streams (BIL funding and FY  Appropriations Act funding) have slightly different funding restrictions and requirements that may affect competitiveness. Please see the Notice of Funding Opportunity Amendment No. for details.As with previous rounds, funds for the FY  RAISE Transportation program are to be awarded on a competitive basis for projects that will have a significant local or regional impact.': 0.44,\n",
              " 'FY  Natural Gas Distribution Infrastructure Safety and Modernization Grant: FY  Natural Gas Distribution Infrastructure Safety and Modernization Grant': 1.0,\n",
              " 'The Infrastructure Investment and Jobs Act (IIJA) Notice of Funding Opportunity for America&#;s Marine Highway Program: This notice announces the availability of funding for grants and establishes selection criteria and application requirements for the America&#;s Marine Highway Program (&#;AMHP&#;). The purpose of this program is to make grants available to previously designated Marine Highway Projects that support the development and expansion of documented vessels or port and landside infrastructure. The Department also seeks eligible grant projects that will strengthen American supply chains. The U.S. Department of Transportation (&#;DOT&#; or &#;Department&#;) will award Marine Highway Grants to implement projects or components of projects previously designated by the Secretary of Transportation (&#;Secretary&#;) under the AMHP. Only Marine Highway Projects the Secretary designates before the Notice of Funding Opportunity (&#;NOFO&#;) closing date are eligible for funding as described in this notice. TIMING OF GRANT APPLICATIONS: Applications must be received by the Maritime Administration (&#;MARAD&#;) by : p.m. E.D.T. on April , . ADDRESSES: Grant applications must be submitted electronically using Grants.gov (https://www.grants.gov). Please be aware that you must complete the Grants.gov registration process before submitting your application and that the registration process usually takes  to  weeks to complete. Applicants are strongly encouraged to make submissions in advance of the deadline. FOR FURTHER INFORMATION CONTACT: Fred Jones, Office of Ports &amp; Waterways Planning, Room W&#;, Maritime Administration, U.S. Department of Transportation,  New Jersey Avenue S.E., Washington, D.C. , phone --, or email Fred.Jones@dot.gov. Persons who use a telecommunications device for the deaf (TDD) may call the Federal Information Relay Service (FIRS) at --- to contact the above individual during business hours. The FIRS is available twenty-four hours a day, seven days a week, to leave a message or question with the above individual. You will receive a reply during regular business hours.': 0.5,\n",
              " '*FY  Notice of Funding Opportunity: Bipartisan Infrastructure Law (BIL) Airport Terminal Program (ATP) Grants: The Department of Transportation (DOT), Federal Aviation Administration (FAA) announces the opportunity to apply for approximately $ billion in FY  discretionary funds for the Airport Terminal Program (ATP), made available under the Infrastructure Investment and Jobs Act of  (IIJA), Pub. L. -, herein referred to as the Bipartisan Infrastructure Law (BIL). The purpose of the ATP is to make annual grants available to eligible airports for airport terminal and airport-owned Airport Traffic Control Towers development projects that address the aging infrastructure of our nation&#;s airports. In addition, ATP grants will align with DOT&#;s Strategic Framework FY- at https://www.transportation.gov/administrations/office-policy/fy--strategic-framework. The FY  ATP will be implemented consistent with law and in alignment with the priorities in Executive Order , Implementation of the Infrastructure Investments and Jobs Act ( FR ), which are to invest efficiently and equitably; promote the competitiveness of the U.S. economy; improve job opportunities by focusing on high labor standards; strengthen infrastructure resilience to all hazards including climate change; and to effectively coordinate with State, local, Tribal, and territorial government partners. ': 1.0,\n",
              " 'INFRA Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families. The Infrastructure for Rebuilding America (INFRA) program is an existing competitive program that will see a more than  percent increase in this year&#;s funding due to the Bipartisan Infrastructure Law. These grants advance the Administration&#;s priorities of rebuilding America&#;s infrastructure and creating jobs by funding highway, multimodal freight and rail projects that position America to win the st century. Projects will improve safety, generate economic benefits, reduce congestion, enhance resiliency, and hold the greatest promise to eliminate supply chain bottlenecks and improve critical freight movements. Last year, DOT received over $ billion of project applications, but could only fund around $ billion of projects. To see last year&#;s INFRA grants, click here. The Bipartisan Infrastructure Law provides approximately $ billion for INFRA over  years, of which approximately $. billion will be made available through this NOFO. ': 0.0,\n",
              " 'Mega Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The National Infrastructure Project Assistance (MEGA) program was created in the Bipartisan Infrastructure Law to fund major projects that are too large or complex for traditional funding programs. The program will provide grants on a competitive basis to support multijurisdictional or regional projects of significance that may also cut across multiple modes of transportation. Eligible projects could include highway, bridge, freight, port, passenger rail, and public transportation projects of national and regional significance. These could be bridges or tunnels connecting two states; new rail and transit lines that improve equity and reduce emissions; and freight hubs integrating ship, train and truck traffic while improving environmental justice. DOT will award  percent of funding to projects greater than $ million in cost, and  percent to projects greater than $ million but less than $ million in cost. The program will receive up to $ billion this year alone and be able to provide multi-year funding to projects.': 0.0,\n",
              " 'Rural Surface Transportation Grant Program: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The Rural Surface Transportation Grant Program (RURAL) was created in the Bipartisan Infrastructure Law and will support projects to improve and expand the surface transportation infrastructure in rural areas to increase connectivity, improve the safety and reliability of the movement of people and freight, and generate regional economic growth and improve quality of life. Eligible projects for Rural grants include highway, bridge, and tunnel projects that help improve freight, safety, and provide or increase access to an agricultural, commercial, energy, or transportation facilities that support the economy of a rural area. This year alone, DOT will award up to $ million in grants through the rural program&#;part of the $ billion included in the Bipartisan Infrastructure Law over five years.': 0.0,\n",
              " '*FY  Notice of Funding Opportunity: Bipartisan Infrastructure Law (BIL) FAA Contract Tower (FCT) Competitive Grant Program: The Department of Transportation (DOT), Federal Aviation Administration (FAA) announces the opportunity to apply for $ million in FY  Airport Infrastructure Grant funds for the FCT competitive grant program, made available under the Infrastructure Investment and Jobs Act of  (IIJA), Pub. L. -, herein referred to as the Bipartisan Infrastructure Law (BIL). The purpose of the FCT competitive grant program is to make annual grants available to eligible airports for airport-owned airport traffic control tower (ATCT) projects that address the aging infrastructure of the nation&#;s airports. The FAA will consider ATCT projects that sustain, construct, repair, improve, rehabilitate, modernize, replace, or relocate non-approach control towers; or acquire and install air traffic control, communications, and related equipment to be used in those towers; or construct a remote tower certified by the FAA including acquisition and installation of air traffic control, communications, or related equipment. In addition, FT grants will align with DOT&#;s Strategic Framework FY- at https://www.transportation.gov/administrations/office-policy/fy--strategic-framework. The FY  FCT will be implemented consistent with law and in alignment with the priorities in Executive Order , Implementation of the Infrastructure Investments and Jobs Act ( FR ), which are to invest efficiently and equitably; promote the competitiveness of the U.S. economy; improve job opportunities by focusing on high labor standards; strengthen infrastructure resilience to all hazards including climate change; and to effectively coordinate with State, local, Tribal, and territorial government partners. ': 1.0,\n",
              " 'FY Railroad Crossing Elimination Grant Program: The purpose of the RCE Program is to provide funding for highway-rail and pathway-rail grade crossing improvement projects that focus on improving the safety and mobility of people and goods.': 1.0,\n",
              " 'Reconnecting Communities Pilot Discretionary Grant Program: The Reconnecting Communities Pilot Program notice of funding opportunity was amended on September ,  to include technical corrections to the Key Information Table. See updated materials in the &quot;Manage Related Documents&quot; section, as well as on our website at https://www.transportation.gov/grants/reconnecting-communitiesThe purpose of this notice is to solicit applications for Reconnecting Communities Pilot (RCP) Program grants. Funds for the Fiscal Year (FY)  RCP Program are to be awarded on a competitive basis for projects that reconnect communities by removing, retrofitting, or mitigating highways or other transportation facilities that create barriers to community connectivity, including to mobility, access, or economic development.The variety of transformative solutions to knit communities back together can include: high-quality public transportation, infrastructure removal, pedestrian walkways and overpasses, capping over roadways, linear parks and trail connectors, roadway redesigns and complete streets conversions, and main street revitalization. The program will award two types of grants: Planning Grants and Capital Construction Grants. Please consult the full Notice of Funding Opportunity for full details about the application process, requirements, and forms.': 0.8571428571428571,\n",
              " 'Safe Streets and Roads for All Discretionary Grant Program: THIS NOTICE OF FUNDING OPPORTUNITY WAS AMENDED ON AUGUST , . PLEASE SEE THE GRANTS.GOV PACKAGE UPDATES, AND THE SAFE STREETS AND ROADS FOR ALL (SSA) WEBSITE IN THE LINK PROVIDED BELOW FOR MORE INFORMATION. DEADLINE REMAINS SEPTEMBER , .The purpose of this notice is to solicit applications for SSA grants. Funds for the fiscal year (FY)  SSA grant program are to be awarded on a competitive basis to support planning, infrastructure, behavioral, and operational initiatives to prevent death and serious injury on roads and streets involving all roadway users, including pedestrians; bicyclists; public transportation, personal conveyance, and micromobility users; motorists; and commercial vehicle operators. Those interested in applying for Implementation Grants should use the application materials in PKG; those interested in applying for Action Plan Grants should use the application materials in PKG. See application instructions to determine which grant to apply for in this funding round.': 0.6666666666666666,\n",
              " 'Strengthening Mobility and Revolutionizing Transportation (SMART) Grants Program: The purpose of this notice is to solicit applications for Strengthening Mobility and Revolutionizing Transportation (SMART) grants. Funds for the fiscal year (FY)  SMART Grants Program are to be awarded on a competitive basis to conduct demonstration projects focused on advanced smart city or community technologies and systems to improve transportation efficiency and safety.': 1.0,\n",
              " 'FY- Federal-State Partnership for Intercity Passenger Rail Grant Program for projects located on the Northeast Corridor: This program funds capital projects on the Northeast Corridor that reduce the state of good repair backlog, improve performance, or expand or establish new intercity passenger rail service.': 1.0,\n",
              " 'Fiscal Years - Wildlife Crossings Pilot Program (WCPP): The primary goals of the WCPP are to save lives, prevent serious injuries, and protect motorists and wildlife by reducing WVCs, and improve habitat connectivity for terrestrial and aquatic species. Reduction of wildlife vehicle collisions and improvement of terrestrial and aquatic habitat connectivity are the primary merit criteria that will be used in reviewing applications, and each of the primary merit criteria are of equal importance.': 1.0,\n",
              " 'FY  Competitive Funding Opportunity; Buses and Bus Facilities Program: The Federal Transit Administration (FTA) announces the availability of approximately $ million in competitive grants under the Buses and Bus Facilities Program is to assist in the financing of buses and bus facilities capital projects, including replacing, rehabilitating, purchasing or leasing buses or related equipment, and rehabilitating, purchasing, constructing or leasing bus-related facilities. Synopses and full announcement will be posted on Grants.gov as opportunity FTA---TPM-BUS. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time April , .': 1.0,\n",
              " 'FY  Competitive Funding Opportunity; Low or No Emission Vehicle Program: The Federal Transit Administration (FTA) announces the availability of approximately $. billion in competitive grants under the Low or No Emission Grant Program (Low-No Program) for the purchase or lease of zero-emission and low-emission transit buses, including acquisition, construction, and leasing of required supporting facilities. Synopses and full announcement will be posted on Grants.gov as opportunity FTA---TPM-LWNO. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time April , .': 0.0,\n",
              " 'FY  Competitive Funding Opportunity: Areas of Persistent Poverty Program: To create new opportunities for those experiencing poverty, the U.S. Department of Transportation&#;s Federal Transit Administration (FTA) today announced a new Notice of Funding Opportunity (NOFO) to help improve transit in areas experiencing long-term economic distress. The $ million in competitive grant funding through FTA&#;s Areas of Persistent Poverty (AoPP) Program provides more resources to underserved and disadvantaged communities seeking to expand or improve transit. ': 1.0,\n",
              " 'FY  Competitive Funding Opportunity: Ferry Service for Rural Communities Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  Ferry Service for Rural Communities Program (FSRC) (Federal Assistance Listing #.). FSRC makes funding available competitively to assist in the financing of capital, planning, and operating assistance for eligible ferry services.': 0.0,\n",
              " 'FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $. million in competitive grants under the Fiscal Year (FY)  Passenger Ferry Grant Program (PFG) (Federal Assistance Listing #.). Of that amount, $ million is available only for low or zero-emission ferries or ferries using electric battery or fuel cell components and the infrastructure to support such ferries. As required by Federal public transportation law, funds will be awarded competitively to designated recipients or eligible direct recipients of Urbanized Area Formula funds to support capital projects to improve existing passenger ferry service, establish new ferry service, and repair and modernize ferry boats, terminals, and related facilities and equipment. ': 0.0,\n",
              " 'FY  National Infrastructure Investments: The Infrastructure Investment and Jobs Act of  (&#;Bipartisan Infrastructure Law,&#; or &#;BIL&#;) authorized and appropriated $. billion to be awarded by the Department of Transportation (&#;DOT&#;) for Local and Regional Project Assistance Program Grants under National Infrastructure Investments in FY . This Notice of Funding Opportunity (NOFO) solicits applications for projects funded under the Local and Regional Project Assistance Program, known as the RAISE Grants program. If the FY  Appropriations Act provides additional funding for the RAISE grants program and/or significantly different requirements for National Infrastructure Investment funds, the Department will amend this Notice with guidance on additional requirements.As with previous rounds, funds for the FY  RAISE Transportation program are to be awarded on a competitive basis for projects that will have a significant local or regional impact.': 0.75,\n",
              " 'Neighborhood Access and Equity (NAE) Program: The Department of Transportation is combining two major discretionary grant programs, the Reconnecting Communities Pilot (RCP) and Neighborhood Access and Equity (NAE) programs, into one Notice of Funding Opportunity (NOFO). Together, this combined program is known as the Reconnecting Communities and Neighborhoods (RCN) Program. The primary goal of the NAE Program is to assist economically disadvantaged or undeserved communities for planning and construction activities. Funds for the fiscal year (FY)  NAE grant program are to be awarded on a competitive basis to support planning, capital construction, and regional partnership activities that aim to restore community connectivity through the removal, retrofit, mitigation or replacement of highways, roadways, or other infrastructure facilities that create barriers to mobility, access or economic development. Applicants must submit their applications via Valid Eval at the links below: Community Planning Grants: https://usg.valideval.com/teams/rcn_planning/signup Capital Construction Grants: https://usg.valideval.com/teams/rcn_capitalconstruction/signup Do not submit applications through Grants.gov. Applications must be submitted by : PM Eastern Time on Thursday, September , . Late applications will not be accepted. ': 1.0,\n",
              " 'FY  Notice of Funding Opportunity for Small Shipyard Grants: This notice announces the intention of the Maritime Administration to provide grants to small shipyards. Under the Small Shipyard Grant Program, there is currently $,, available for grants for capital and related improvements to qualified shipyard facilities that will be effective in fostering efficiency, competitive operations, and quality ship construction, repair, and reconfiguration. Potential applicants are advised that it is expected, based on experience, that the number of applications will far exceed the funds available and that only a small percentage of applicants will be funded. Applications (SF- and the Addendum) will be accepted online through Grants.gov by  pm on the closing date (February , ). See full text under Related Documents Tab (FY  SSG NOFO) or the MARAD Web Page for detailed submission instructions.': 1.0,\n",
              " 'Safe Streets and Roads for All Funding Opportunity: The purpose of this notice is to solicit applications for Safe Streets and Roads for All (SSA) grants. Funds for the fiscal year (FY)  SSA grant program are to be awarded on a competitive basis to support planning and demonstration activities, as well as projects and strategies to prevent death and serious injury on roads and streets involving all roadway users, including pedestrians; bicyclists; public transportation, personal conveyance, and micromobility users; motorists; and commercial vehicle operators. Applicants must submit their applications via Valid Eval at https://usg.valideval.com/teams/usdot_ssa__implementation/signup for Implementation Grant applicants, and https://usg.valideval.com/teams/usdot_ssa__planning_demo/signup for Planning and Demonstration Grants. Do not submit your applications through Grants.Gov. Applications must be submitted by : PM Eastern Time on Monday, July , . Late applications will not be accepted.': 0.9827586206896551,\n",
              " 'SMART Grants Notice of Funding: The purpose of this notice is to solicit applications for Strengthening Mobility and Revolutionizing Transportation (SMART) Stage  Planning and Prototyping grants. Funds for the fiscal year (FY)  SMART Grants Program are to be awarded on a competitive basis to conduct demonstration projects focused on advanced smart city or community technologies and systems to improve transportation efficiency and safety. Applications must be submitted via Valid Eval, an online submission proposal system used by USDOT, at https://usg.valideval.com/teams/USDOT_SMART_/signup. USDOT will not accept or review application materials submitted via Grants.gov. ': 0.0,\n",
              " ' Tribal Transportation Program Safety Fund: Eligible projects described in section (a)() are strategies, activities, and projects on a public road that are consistent with a transportation safety plan and that (i) correct or improve a hazardous road location or feature, or (ii) address a highway safety problem. TTPSF emphasizes the development of strategic transportation safety plans using a data-driven process as a means for Tribes to identify transportation safety needs and determine how those needs will be addressed in Tribal communities. FHWA has identified four eligibility categories: transportation safety plans; data assessment, improvement, and analysis activities; systemic roadway departure countermeasures; and infrastructure improvements and other eligible activities as listed in  U.S.C. &#; (a)().': 1.0,\n",
              " 'United States Marine Highway Grants: This funding opportunity solicits applications for fiscal year (FY)  United States Marine Highway Program (USMHP) grants. Funds for FY  USMHP grants will be awarded on a competitive basis to assist in funding eligible projects for the purpose of developing, expanding, or promoting marine highway transportation. This opportunity announces the availability of up to $,, million in funding for grants under this program and establishes selection criteria and application requirements. All USMHP grant recipients must meet all applicable Federal requirements, including domestic content (&#;Buy America&#;) requirements. This program was formerly known as the America&#;s Marine Highway Program': 0.0,\n",
              " 'INFRA Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families. The Infrastructure for Rebuilding America (INFRA) program is an existing competitive program that will see a more than  percent increase in this year&#;s funding due to the Bipartisan Infrastructure Law. These grants advance the Administration&#;s priorities of rebuilding America&#;s infrastructure and creating jobs by funding highway, multimodal freight and rail projects that position America to win the st century. Projects will improve safety, generate economic benefits, reduce congestion, enhance resiliency, and hold the greatest promise to eliminate supply chain bottlenecks and improve critical freight movements. Last year, DOT received over $ billion in application requests, but could only fund around $. billion of projects. To see last year&#;s INFRA grants, click here. The Bipartisan Infrastructure Law provides approximately $ billion for INFRA over  years, of which approximately $. billion will be made available through this NOFO. ': 0.0,\n",
              " 'Mega Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The National Infrastructure Project Assistance (MEGA) program was created in the Bipartisan Infrastructure Law to fund major projects that are too large or complex for traditional funding programs. The program will provide grants on a competitive basis to support multijurisdictional or regional projects of significance that may also cut across multiple modes of transportation. Eligible projects could include highway, bridge, freight, port, passenger rail, and related public transportation projects of national and regional significance. DOT will award approximately  percent of funding to projects greater than $ million in cost, and approximately  percent to projects greater than $ million but less than $ million in cost.For more information about last year&apos;s Mega awards, visit here: https://www.transportation.gov/grants/mega-grant-program/FYawards': 0.0,\n",
              " 'Rural Surface Transportation Grant Progam: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The Rural Surface Transportation Grant Program (RURAL) was created in the Bipartisan Infrastructure Law and will support projects to improve and expand the surface transportation infrastructure in rural areas to increase connectivity, improve the safety and reliability of the movement of people and freight, and generate regional economic growth and improve quality of life. Eligible projects for Rural grants include highway, bridge, and tunnel projects that help improve freight, safety, and provide or increase access to an agricultural, commercial, energy, or transportation facilities that support the economy of a rural area. This year alone, DOT will award up to $ million in grants through the rural program&#;part of the $ billion included in the Bipartisan Infrastructure Law over five years. At least % of rural funding must be awarded in amounts of $ million or more. If you are seeking less than $ million, you are competing for only about $. million nationwide.For more information on last year&apos;s rural awards, see: https://www.transportation.gov/grants/rural--fact-sheets': 0.0,\n",
              " 'FY  Competitive Funding Opportunity: Rail Vehicle Replacement Grant Program (RAIL): The Federal Transit Administration (FTA) announces the opportunity to apply for approximately $ million in Fiscal Year (FY)  Section  State of Good Repair Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected rail vehicle replacement projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to States and local governmental authorities to assist in the funding of capital projects to replace rail rolling stock.': 0.0}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('finetuned_BERT_epoch_4.model', map_location=torch.device('cuda')))\n",
        "\n",
        "_, predictions, true_vals, _ = evaluate(dataloader_validation)\n",
        "accuracy_per_class(predictions, true_vals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Rm7bySywbk0",
        "outputId": "b140e72b-a117-48b9-bc8d-39adba286724"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class: FY  Notice of Funding Opportunity: Bipartisan Infrastructure Law (BIL) Airport Terminal Program (ATP) Grants: The Department of Transportation (DOT), Federal Aviation Administration (FAA) announces the opportunity to apply for approximately $ billion in FY  discretionary funds for the Airport Terminal Program (ATP), made available under the Infrastructure Investment and Jobs Act of  (IIJA), Pub. L. -, herein referred to as the Bipartisan Infrastructure Law (BIL). The purpose of the ATP is to make annual grants available to eligible airports for airport terminal and airport-owned Airport Traffic Control Towers development projects that address the aging infrastructure of our nation&#;s airports. In addition, ATP grants will align with DOT&#;s Strategic Framework FY- at https://www.transportation.gov/administrations/office-policy/fy--strategic-framework. The FY  ATP will be implemented consistent with law and in alignment with the priorities in Executive Order , Implementation of the Infrastructure Investments and Jobs Act ( FR ), which are to invest efficiently and equitably; promote the competitiveness of the U.S. economy; improve job opportunities by focusing on high labor standards; strengthen infrastructure resilience to all hazards including climate change; and to effectively coordinate with State, local, Tribal, and territorial government partners. \n",
            "Accuracy: 13/25 (0.5200)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Airport Improvement Program Discretionary Grants: The U.S. Department of Transportation&#;s Federal Aviation Administration (FAA) announces the opportunity to apply for an estimated $. billion in Fiscal Year (FY)  discretionary grants under the Airport Improvement Program (AIP), per  U.S.C. &#; . The FAA awards these annually appropriated discretionary funds through the FAA&#;s long-standing iterative, competitive grant process. Prior to the publication of this Notice of Funding Opportunity (NOFO), the FAA identified eligible applicants in its National Plan of Integrated Airport Systems (NPIAS) and compiled potentially eligible projects through the -year Airports Capital Improvement Plan (ACIP). Both of these processes are described in FAA Order ., &#;Formulation of NPIAS and ACIP,&#; adhering to  U.S.C. &#; , which authorizes discretionary funds. The AIP funds airport capital improvements and rehabilitation projects. All discretionary grant funding is subject to appropriations, statutory requirements, and related program funding availability. \n",
            "Accuracy: 162/163 (0.9939)\n",
            "\n",
            "Class: Grants for Buses and Bus Facilities Program: The Federal Transit Administration (FTA) announces the availability of approximately $. million of Fiscal Year  funds for buses, bus facilities, and bus equipment. The purpose of the Grants for Buses and Bus Facilities Program is to assist in the financing of buses and bus facilities capital projects, including replacing, rehabilitating, purchasing or leasing buses or related equipment, and rehabilitating, purchasing, constructing or leasing bus-related facilities. \n",
            "Accuracy: 13/14 (0.9286)\n",
            "\n",
            "Class: Solicitation of Project Proposals for the Low or No Emission (Low-No) Program:  Notice of Funding Opportunity (NOFO): Solicitation of Project Proposals for the Low or No Emission Program (Low-No) Program. The Federal Transit Administration (FTA) announces the availability of $ million of Fiscal Year  funds for the purchase or lease of low or no emission vehicles as well as related equipment or facilities. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time on //. The main purpose of the Low-No Program is to support the transition of the nation&#;s transit fleet to the lowest polluting and most energy efficient transit vehicles. The Low-No Program provides funding to State and local governmental authorities for the purchase or lease of zero-emission and low-emission transit buses, including acquisition, construction, and leasing of required supporting facilities. \n",
            "Accuracy: 0/6 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in Fiscal Year (FY)  Section  Urbanized Area Formula Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected passenger ferry projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to designated recipients or eligible direct recipients of Section  funds to assist in the financing of capital projects to support existing passenger ferry service, establish new ferry service, and to repair and modernize ferry boats, terminals, and related facilities and equipment. \n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: Pilot Program for Transit-Oriented Development (TOD) Planning: Notice of Funding Opportunity for the Pilot Program for Transit-Oriented Development (TOD) Planning. The Federal Transit Administration (FTA) announces the availability of approximately $. million in Pilot Program for TOD Planning funding to support comprehensive planning associated with new fixed guideway and core capacity improvement projects. FTA may award amounts ranging from $, to $,,. The Pilot Program for TOD Planning provides funding to local communities to integrate land use and transportation planning in new fixed guideway and core capacity transit project corridors. As required by statute, any comprehensive planning funded through the pilot program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations. The statute also requires that the planning work be associated with a new fixed guideway or core capacity transit project as defined in Federal transit statute ( USC (a); also see the NOFO for the definitions). Consistent with statutory direction, FTA is seeking comprehensive planning projects covering an entire transit capital project corridor, rather than proposals that involve planning for individual station areas or only a small section of the corridor. To ensure that planning work reflects the needs and aspirations of the local community and results in concrete, specific deliverables and outcomes, FTA is requiring that transit project sponsors partner with entities with land use planning authority in the transit project corridor. \n",
            "Accuracy: 4/4 (1.0000)\n",
            "\n",
            "Class: Public Transportation on Indian Reservations Program: The Federal Transit Administration (FTA) announces the availability of approximately $ million in funding provided by the Public Transportation on Indian Reservations Program (Tribal Transit Program (TTP)), as authorized by  U.S.C. (c)()(A), as amended by the Fixing America&apos;s Surface Transportation Act (FAST), Public Law - (December , ). The primary purpose of these competitively selected grants is to support planning, capital, and, in limited circumstances, operating assistance for tribal public transit services. Funds distributed to Indian tribes under the TTP should not replace or reduce funds that Indian tribes receive from states through FTA&#;s Section  program. \n",
            "Accuracy: 1/6 (0.1667)\n",
            "\n",
            "Class: INFRA Grants: The Nationally Significant Freight and Highway Projects (INFRA) program provides Federal financial assistance to highway and freight projects of national or regional significance. This notice solicits applications for awards under the program&#;s fiscal year (FY)  funding, subject to the availability of appropriated funds. The Department is focusing the competition on transportation infrastructure projects that support four key objectives, each of which is discussed in greater detail in the Notice of Funding Opportunity: () Supporting economic vitality at the national and regional level; () Leveraging Federal funding to attract non-Federal sources of infrastructure investment; () Deploying innovative technology, encouraging innovative approaches to project delivery, and incentivizing the use of innovative financing; and () Holding grant recipients accountable for their performance. \n",
            "Accuracy: 2/3 (0.6667)\n",
            "\n",
            "Class:  Tribal Transportation Program Safety Fund: A Notice of Funding Availability (NOFA) for Tribal Transportation Program Safety Funds (TTPSF) has been published. This notice announces the availability of $. million for projects in three categories: safety plans; data assessment, improvement, and analysis activities; and infrastructure improvement and other eligible activities as listed in  U.S.C.(a)() The deadline for submissions is //. For complete instructions on preparing and submitting an application for TTPSF, please download the application information and NOFO at https://flh.fhwa.dot.gov/programs/ttp/safety/ttpsf.htm Please submit your application directly to FHWA through the &quot;apply now&quot; button at: https://flh.fhwa.dot.gov/programs/ttp/safety/ttpsf.htm \n",
            "Accuracy: 14/14 (1.0000)\n",
            "\n",
            "Class: Grants for Buses and Bus Facilities Program: The purpose of the Grants for Buses and Bus Facilities Program is to assist in the financing of buses and bus facilities capital projects, including replacing, rehabilitating, purchasing or leasing buses or related equipment, and rehabilitating, purchasing, constructing or leasing bus-related facilities.\n",
            "Accuracy: 12/14 (0.8571)\n",
            "\n",
            "Class: Low or No Emission Program (Low-No Program): The main purpose of the Low-No Program is to support the transition of the nation&#;s transit fleet to the lowest polluting and most energy efficient transit vehicles. The Low-No Program provides funding to State and local governmental authorities for the purchase or lease of zero-emission and low-emission transit buses, including acquisition, construction, and leasing of required supporting facilities.\n",
            "Accuracy: 14/14 (1.0000)\n",
            "\n",
            "Class: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in Fiscal Year (FY)  Section  Urbanized Area Formula Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected passenger ferry projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to designated recipients or eligible direct recipients of Section  funds to assist in the financing of capital projects to support existing passenger ferry service, establish new ferry service, and to repair and modernize ferry boats, terminals, and related facilities and equipment. \n",
            "Accuracy: 0/2 (0.0000)\n",
            "\n",
            "Class: Pilot Program for Transit-Oriented Development (TOD) Planning: Notice of Funding Opportunity for PILOT PROGRAM FOR TRANSIT-ORIENTED DEVELOPMENT (TOD) PLANNING. The Federal Transit Administration (FTA) announces the opportunity to apply for approximately $. million of funding under the Pilot Program for TOD Planning (Catalog of Federal Domestic Assistance #.). As required by federal transit law and subject to funding availability, funds will be awarded competitively to support comprehensive planning associated with new fixed guideway and core capacity improvement projects. FTA anticipates minimum grant awards of $, and maximum grant awards of $,,. The Pilot Program for TOD Planning is intended to fund comprehensive planning that supports economic development, ridership, multimodal connectivity and accessibility, increased transit access for pedestrian and bicycle traffic, and mixed-use development near transit stations. The program also encourages identification of infrastructure needs and engagement with the private sector. Consistent with statutory direction, FTA is seeking comprehensive planning projects covering an entire transit capital project corridor, rather than proposals that involve planning for individual station areas or only a small section of the corridor. To ensure any proposed planning work reflects the needs and aspirations of the local community and results in concrete, specific deliverables and outcomes, transit project sponsors must partner with entities with land use planning authority in the transit project corridor to conduct the planning work.The Pilot Program for TOD Planning helps support FTA&#;s mission of improving public transportation for America&#;s communities by providing funding to local communities to integrate land use and transportation planning around a new fixed guideway or core capacity improvement project. Per statute, any comprehensive planning funded through the program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations.\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Public Transportation on Indian Reservations Program: The Federal Transit Administration (FTA) announces the availability of approximately $ million in funding provided by the Public Transportation on Indian Reservations Program (Tribal Transit Program (TTP)), as authorized by  U.S.C. (c)()(A), as amended by the Fixing America&apos;s Surface Transportation Act (FAST), Public Law - (December , ). This is a national solicitation for project proposals and includes the selection criteria and program eligibility information for Fiscal Year  projects. The primary purpose of these competitively selected grants is to support planning, capital, and, in limited circumstances, operating assistance for tribal public transit services. Funds distributed to Indian tribes under the TTP should not replace or reduce funds that Indian tribes receive from states through FTA&#;s Section  program.\n",
            "Accuracy: 0/4 (0.0000)\n",
            "\n",
            "Class: *INFRA Grants: The U.S. Department of Transportation (USDOT) is seeking applicants for the FY  round of the Infrastructure for Rebuilding America (INFRA) discretionary grant program to fund transportation projects of national and regional significance that result in good-paying jobs, improve safety, apply transformative technology, and explicitly address climate change and racial equity. The funding available for this year&#;s grants totals approximately $ million.The Department recognizes the role that infrastructure investment plays in economic development and job creation, and the added urgency of this funding at time when the COVID- pandemic has put stress on state and local budgets.For the first time, the USDOT seeks INFRA projects that address climate change and environmental justice. Projects will be evaluated on whether they were planned as part of a comprehensive strategy to address climate change, or whether they support strategies to reduce greenhouse gas emissions such as deploying zero-emission-vehicle infrastructure or encouraging modal shift and a reduction in vehicle-miles-traveled. Racial equity will also be considered as a selection criterion, to the extent that project sponsors have completed equity-focused community outreach, and projects are designed to benefit underserved communities. The Department will also consider whether the project is located in a federally designated community development zone, including qualified Opportunity Zones, Empowerment Zones, Promise Zones, or Choice Neighborhoods.USDOT seeks projects that apply innovative technology, delivery, or financing methods with proven outcomes to deliver projects in a cost effective manner. The Department will make awards under the INFRA program to both large and small projects. For a large project, the INFRA grant must be at least $ million. For a small project, the grant must be at least $ million. Under statutory requirements,  percent of available funds are reserved for small projects, and the Department must award at least  percent of funding for rural projects. INFRA grants may be used to fund a variety of components of an infrastructure project, however, the Department is specifically focused on projects in which the local sponsor is significantly invested and is positioned to proceed rapidly to construction. Eligible INFRA project costs may include: reconstruction, rehabilitation, acquisition of property (including land related to the project and improvements to the land), environmental mitigation, construction contingencies, equipment acquisition, and operational improvements directly related to system performance. The INFRA NOFO also announces the creation of the &#;INFRA Extra&#; Program, which will identify competitive INFRA applicants who do not receive an INFRA award and authorize them to seek a Transportation Infrastructure Finance and Innovation Act of  (TIFIA) loan up to  percent of their project cost.\n",
            "Accuracy: 0/3 (0.0000)\n",
            "\n",
            "Class: Fiscal Year  Competitive Funding Opportunity; Grants for Buses and Bus Facilities Program: The Federal Transit Administration (FTA) announces the availability of approximately $. million of Fiscal Year  funds for buses, bus facilities, and bus equipment. Synopses and full announcement will be posted on Grants.gov as opportunity FTA---BUS. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time November ,.\n",
            "Accuracy: 1/10 (0.1000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  Passenger Ferry Grant Program (Ferry Program) (Federal Assistance Listing #.). Of that amount, $ million is available only for low or zero-emission ferries or ferries using electric battery or fuel cell components and the infrastructure to support such ferries. As required by Federal public transportation law, funds will be awarded competitively to designated recipients or eligible direct recipients of Urbanized Area Formula funds to support capital projects to improve existing passenger ferry service, establish new ferry service, and repair and modernize ferry boats, terminals, and related facilities and equipment.\n",
            "Accuracy: 0/2 (0.0000)\n",
            "\n",
            "Class:  Competitive Funding Opportunity: Pilot Program for Transit-Oriented Development (TOD) Planning: The Pilot Program for TOD Planning helps support FTA&#;s mission of improving public transportation for America&#;s communities by providing funding to local communities to integrate land use and transportation planning around a new fixed guideway or core capacity improvement project. Per statute, any comprehensive planning funded through the program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations. Additional information is available at: https://www.transit.dot.gov/TODPilot.Notice of Funding Opportunity (NOFO): Notice of Funding Opportunity for PILOT PROGRAM FOR TRANSIT-ORIENTED DEVELOPMENT (TOD) PLANNING. The Federal Transit Administration (FTA) announces the opportunity to apply for $,, of funding under the Pilot Program for TOD Planning (Catalog of Federal Domestic Assistance #.). As required by federal transit law and subject to funding availability, funds will be awarded competitively to support comprehensive planning associated with new fixed guideway and core capacity improvement projects. FTA anticipates minimum grant awards of $, and maximum grant awards of $,,. Synopses and full announcement are posted on Grants.gov as opportunity FTA---TPE-TODP. The Pilot Program for TOD Planning is intended to fund comprehensive planning that supports economic development, ridership, multimodal connectivity and accessibility, increased transit access for pedestrian and bicycle traffic, and mixed-use development near transit stations. The program also encourages identification of infrastructure needs and engagement with the private sector.Consistent with statutory direction, FTA is seeking comprehensive planning projects covering an entire transit capital project corridor, rather than proposals that involve planning for individual station areas or only a small section of the corridor. To ensure any proposed planning work reflects the needs and aspirations of the local community and results in concrete, specific deliverables and outcomes, transit project sponsors must partner with entities with land use planning authority in the transit project corridor to conduct the planning work.Link and Instructions for attaching the supplemental form to the SF-: All applicants must complete a supplemental form (PDF) specific to the Pilot Program for TOD Planning and attach it to their submission in Grants.gov. See section D of the NOFO for information about additional required application contents.Webinar: FTA will conduct a webinar for prospective applicants after the NOFO is published in the Federal Register. Further information will be posted on FTA&#;s website (https://www.transit.dot.gov/TODPilot) when available. Dates: An applicant must submit a proposal electronically by : p.m. Eastern Daylight Time on June , . Any agency intending to apply should initiate the process of registering on the Grants.gov site immediately to ensure completion of registration before the submission deadline. \n",
            "Accuracy: 0/3 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity:  Public Transportation on Indian Reservations Program; Tribal Transit Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants for the Fiscal Year (FY)  Public Transportation on Indian Reservations (Tribal Transit) Program (Federal Assistance Listing: .). As required by Federal public transportation law, funds will be awarded competitively for any purpose eligible under FTA&#;s Formula Grants for Rural Areas Program,  U.S.C. , including planning, capital, and operating assistance for tribal public transit services in rural areas. \n",
            "Accuracy: 4/6 (0.6667)\n",
            "\n",
            "Class: INFRA Grants: The U.S. Department of Transportation (USDOT) is seeking applicants for the FY  round of the Infrastructure for Rebuilding America (INFRA) discretionary grant program to fund transportation projects of national and regional significance that result in good-paying jobs, improve safety, apply transformative technology, and explicitly address climate change and racial equity. The funding available for this year&#;s grants totals approximately $ million.The Department recognizes the role that infrastructure investment plays in economic development and job creation, and the added urgency of this funding at time when the COVID- pandemic has put stress on state and local budgets.For the first time, the USDOT seeks INFRA projects that address climate change and environmental justice. Projects will be evaluated on whether they were planned as part of a comprehensive strategy to address climate change, or whether they support strategies to reduce greenhouse gas emissions such as deploying zero-emission-vehicle infrastructure or encouraging modal shift and a reduction in vehicle-miles-traveled. Racial equity will also be considered as a selection criterion, to the extent that project sponsors have completed equity-focused community outreach, and projects are designed to benefit underserved communities. The Department will also consider whether the project is located in a federally designated community development zone, including qualified Opportunity Zones, Empowerment Zones, Promise Zones, or Choice Neighborhoods.USDOT seeks projects that apply innovative technology, delivery, or financing methods with proven outcomes to deliver projects in a cost effective manner. The Department will make awards under the INFRA program to both large and small projects. For a large project, the INFRA grant must be at least $ million. For a small project, the grant must be at least $ million. Under statutory requirements,  percent of available funds are reserved for small projects, and the Department must award at least  percent of funding for rural projects. INFRA grants may be used to fund a variety of components of an infrastructure project, however, the Department is specifically focused on projects in which the local sponsor is significantly invested and is positioned to proceed rapidly to construction. Eligible INFRA project costs may include: reconstruction, rehabilitation, acquisition of property (including land related to the project and improvements to the land), environmental mitigation, construction contingencies, equipment acquisition, and operational improvements directly related to system performance. The INFRA NOFO also announces the creation of the &#;INFRA Extra&#; Program, which will identify competitive INFRA applicants who do not receive an INFRA award and authorize them to seek a Transportation Infrastructure Finance and Innovation Act of  (TIFIA) loan up to  percent of their project cost.\n",
            "Accuracy: 5/17 (0.2941)\n",
            "\n",
            "Class: FY Tribal Transportation Program Safety Fund (TTPSF): A Notice of Funding Opportunity (NOFO) for Tribal Transportation Program Safety Funds (TTPSF) has been published. This notice announces the availability of approximately $. million for projects in four categories: safety plans; data assessment, improvement, and analysis activities; systemic roadway departure countermeasures; and infrastructure improvement and other eligible activities as listed in  U.S.C.(a)(). The goal of these projects is to reduce fatalities and serious injuries from transportation incidents, such as motor vehicle crashes. For complete instructions on preparing and submitting an application for TTPSF, please download the application, application information, systemic roadway departure risk assessment form (if applicable) and NOFO at https://highways.dot.gov/federal-lands/programs-tribal/safety/funds. Applications will NOT be submitted on Grants.gov. Instead, please use the &quot;apply now&quot; button on the program web page.\n",
            "Accuracy: 7/9 (0.7778)\n",
            "\n",
            "Class: Advanced Transportation Technologies and  Innovative Mobility Deployment (ATTIMD) Program: AMENDMENT  (//): This amendment is for the purpose of correcting a typo contained in Table D. on page  of . This table, contained in Section D. for CONTENT AND FORM OF APPLICATION SUBMISSION, erroneously omitted the Project Outcome Criteria from that table&#;s submission requirements for Volume . This amendment also revises the verbiage included on page  of the NOFO to clarify that all resumes for key personnel should be included in item f) Appendix &#; R&#;sum&#;s for Key Personnel. Please note that Resumes do not count against the page limitations for Volume . Please refer to Amendment  in the &#;Related Documents&#; tab for more information. -- END OF AMENDMENT  -- AMENDMENT  (//): This amendment adds the weblink to the recording of the October , , informational webinar for this opportunity. The weblink is included in the attachments page for this opportunity as well as below: LINK: https://usdot.zoomgov.com/rec/share/eQNNRGISy-UxfqGpSDzINRbzvcdkAjDdoulZvwRCt-sARp.kdAJIUFsFZZEpf PASSCODE: c@X*&amp;@ -- END OF AMENDMENT  -- AMENDMENT  (//): This amendment adds the registration information for the October , , informational webinar for this opportunity. Please refer to the &quot;Related Documents&quot; tab for more information. -- END OF AMENDMENT  -- BASE NOFO (//): The Advanced Transportation Technologies and Innovative Mobility Deployment Program (ATTIMD), established in Section (c)() of title , United States Code (U.S.C.), also known as the Advanced Transportation Technology and Innovation (ATTAIN) Program, directs FHWA to award grants to eligible entities to deploy, install, and operate advanced transportation technologies to improve safety, mobility, efficiency, system performance, intermodal connectivity, and infrastructure return on investment. FHWA intends for these model technology deployments to help demonstrate how emerging transportation technologies, data, and their applications can be effectively deployed and integrated with existing systems to provide access to essential services and other destinations. This also includes efforts to increase connectivity to employment, education, services, and other opportunities; support workforce development; or contribute to increased mobility, particularly for persons with visible and hidden disabilities and elderly individuals. The Assistance Listing Number for this opportunity is . &#; Highway Research and Development. The Federal Highway Administration (FHWA) hereby requests applications to result in awards to eligible entities to deploy, install, and operate advanced transportation technologies to improve safety, mobility, efficiency, system performance, intermodal connectivity, and infrastructure return on investment. These model deployments are expected to provide benefits in the form of: reduced traffic-related fatalities and injuries; reduced traffic congestion and improved travel time reliability; reduced transportation-related emissions; optimized multimodal system performance; improved access to transportation alternatives, including for underserved populations; improved integration of payment systems; public access to real-time integrated traffic, transit, and multimodal transportation information to make informed travel decisions; cost savings to transportation agencies, businesses, and the traveling public; or other benefits to transportation users and the general public. This competitive ATTAIN Program will promote the use of innovative transportation solutions. The deployment of these technologies will provide Congress and FHWA with valuable real-life data and feedback to inform future decision-making. Cost Sharing: Cost-sharing or matching is required, with the maximum Federal share being  percent; hence, this NOFO requires a minimum non-Federal cost share of  percent. Cost sharing or matching means the portion of project costs not paid by Federal funds. NOTE: FHWA previously issued six NOFOs for the Advanced Transportation and Congestion Management Technologies Deployment (ATCMTD) Grant Program before the program was amended and renamed to the Advanced Transportation Technologies and Innovative Mobility Deployment Program by Section  of the Infrastructure Investment and Jobs Act (IIJA) (Public Law -, also known as the &#;Bipartisan Infrastructure Law&#; (BIL)).\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: FY  and FY  Competitive Funding Opportunity: Competitive Grants for Rail Vehicle Replacement Program (Rail Program): The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in Fiscal Year (FY)  and  Section  State of Good Repair Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected rail vehicle replacement projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to States and local governmental authorities to assist in the funding of capital projects to replace rail rolling stock.\n",
            "Accuracy: 1/1 (1.0000)\n",
            "\n",
            "Class: Bridge Investment Program - Planning, Bridge Projects, and Large Bridge Projects: This Notice of Funding Opportunity (NOFO) is to solicit applications for three categories of Bridge Investment Program (BIP) funding opportunities: () Planning; () Bridge Projects (projects with eligible costs less than $ million); and () Large Bridge Projects (projects with eligible costs greater than $ million). Eligible applicants may submit applications for any of the three funding categories, but each category has distinct eligibility and selection criteria and application deadlines.These funds will be awarded on a competitive basis for planning, feasibility analysis, and revenue forecasting associated with the development of a project that would subsequently be eligible to apply for BIP funding under either the Bridge Projects or Large Bridge Projects funding categories.A total of $. billion in  BIP funds are available for the Bridge Projects and Large Bridge Projects funding opportunities. These funds will be awarded on a competitive basis for bridge replacement, rehabilitation, preservation, and protection projects that: () improve the safety, efficiency, and reliability of the movement of people and freight over bridges; and () improve the condition of bridges in the United States by reducing (a) the number of bridges, and total person miles traveled over bridges, that are in poor condition or that are in fair condition and at risk of falling into poor condition within the next three years, or (b) the number of bridges, and total person miles traveled over bridges, that do not meet current geometric design standards or cannot meet the load and traffic requirements typical of the regional transportation network. In addition, Large Bridge Projects that receive a BIP award of not less than $ million are eligible for multiyear grants, in which DOT can award available funds to a project over the course of several years in accordance with an agreement and in alignment with its schedule. In selecting Bridge Projects and Large Bridge Projects, FHWA will consider the extent to which BIP funds leverage non-Federal contributions from sponsors and stakeholders involved in the planning, design, and construction of eligible projects. \n",
            "Accuracy: 5/6 (0.8333)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity; Low or No Emission Grant Program: Low-No ProgramThe Federal Transit Administration (FTA) announces the opportunity to apply for approximately $. billion in competitive grants under the fiscal year (FY)  Low or No Emission Grant Program (Low-No Program) (Federal Assistance Listing: .) and approximately $ million in FY  funds under the Grants for Buses and Bus Facilities Program (Buses and Bus Facilities Program) (Federal Assistance Listing .), subject to availability of appropriated funding. Synopses and full announcement will be posted on Grants.gov as funding opportunity ID FTA--TPM for Low-No applications and FTA--TPM for Buses and Bus Facilities applications. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time May , .\n",
            "Accuracy: 14/15 (0.9333)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: All Stations Accessibility Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  All Stations Accessibility Program (ASAP) (Federal Assistance Listing #.). As required by Federal public transportation law, funds will be awarded competitively to eligible designated recipients that operate or allocate funds to inaccessible pre-ADA&#;or &#;legacy&#; &#; rail fixed guideway public transportation systems, and States (including territories and Washington, D.C.) and local governmental entities that operate or financially support legacy rail fixed guideway public transportation systems and corresponding legacy stations/facilities for capital projects to repair, improve, modify, retrofit, or relocate infrastructure of stations or facilities for passenger use, including load-bearing members that are an essential part of the structural frame; or () for planning projects to develop or modify a plan for pursuing public transportation accessibility projects, assessments of accessibility, or assessments of planned modifications to stations or facilities for passenger use projects; or programs of projects in an eligible area.\n",
            "Accuracy: 2/2 (1.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Buses and Bus Facilities Competitive Program: Bus and Bus Facilities ProgramThe Federal Transit Administration (FTA) announces the opportunity to apply for approximately $. billion in competitive grants under the fiscal year (FY)  Low or No Emission Grant Program (Low-No Program) (Federal Assistance Listing: .) and approximately $ million in FY  funds under the Grants for Buses and Bus Facilities Program (Buses and Bus Facilities Program) (Federal Assistance Listing .), subject to availability of appropriated funding. Synopses and full announcement will be posted on Grants.gov as funding opportunity ID FTA--TPM for Low-No applications and FTA--TPM for Buses and Bus Facilities applications. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time May , \n",
            "Accuracy: 0/7 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Electric or Low-Emitting Ferry Pilot Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  Electric or Low-Emitting Ferry Pilot Program (ELEF) (Federal Assistance Listing #.). ELEF makes funding available competitively to assist in the financing of capital projects for the purchase of electric or low-emitting ferry vessels that reduce emissions by using alternative fuels or on-board energy storage systems and related charging infrastructure to reduce emissions or produce zero onboard emissions under normal operation.\n",
            "Accuracy: 2/2 (1.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $. million in competitive grants under the Fiscal Year (FY)  Passenger Ferry Grant Program (PFG) (Federal Assistance Listing #.). Of that amount, $. million is available only for low or zero-emission ferries or ferries using electric battery or fuel cell components and the infrastructure to support such ferries. As required by Federal public transportation law, funds will be awarded competitively to designated recipients or eligible direct recipients of Urbanized Area Formula funds to support capital projects to improve existing passenger ferry service, establish new ferry service, and repair and modernize ferry boats, terminals, and related facilities and equipment. \n",
            "Accuracy: 1/1 (1.0000)\n",
            "\n",
            "Class: FY  Pilot Program for Transit-Oriented Development (TOD) Planning: The Pilot Program for TOD Planning helps support FTA&#;s mission of improving public transportation for America&#;s communities by providing funding to local communities to integrate land use and transportation planning around a new fixed guideway or core capacity improvement project. Per statute, any comprehensive or site specific planning funded through the program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations.\n",
            "Accuracy: 3/3 (1.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Public Transportation on Indian Reservations Program; Tribal Transit Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $. million in competitive grants for the Fiscal Year (FY)  Public Transportation on Indian Reservations (Tribal Transit) Program. As required by Federal public transportation law, funds will be awarded competitively for any purpose eligible under FTA&#;s Formula Grants for Rural Areas Program, including planning, capital, and operating assistance for tribal public transit services in rural areas. FTA may award additional funding that is made available to the program prior to the announcement of project selections.\n",
            "Accuracy: 0/4 (0.0000)\n",
            "\n",
            "Class: FY Consolidated Rail Infrastructure and Safety Improvements Grant Program: This program funds projects that improve the safety, efficiency, and reliability of intercity passenger and freight rail.\n",
            "Accuracy: 10/10 (1.0000)\n",
            "\n",
            "Class: FY Corridor Identification and Development Grant Program: This program facilitates the development of intercity passenger rail corridors.\n",
            "Accuracy: 10/10 (1.0000)\n",
            "\n",
            "Class: FY Federal-State Partnership for Intercity Passenger Rail Grant Program for projects not located on the Northeast Corridor: This program funds capital projects that reduce the state of good repair backlog, improve performance, or expand or establish new intercity passenger rail service.\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: Fiscal Year  National Culvert Removal, Replacement, and Restoration Grant Program (Culvert AOP Program): -- ORIGINAL NOFO POSTING - // -- The primary goal of the Culvert AOP Program is to improve or restore anadromous fish passage through the replacement, removal, repair, or improvement of culverts or weirs. The grant program prioritizes projects that would improve fish passage for: (A) anadromous fish stocks listed as an endangered species or a threatened species under section  of the Endangered Species Act of  ( U.S.C. &#; ); (B) anadromous fish stocks identified by the National Marine Fisheries Service (NMFS) or the U.S. Fish and Wildlife Service (USFWS) that could reasonably become listed as an endangered species or a threatened species under that section; (C) anadromous fish stocks identified by the NMFS or the USFWS as prey for endangered species, threatened species, or protected species, including Southern resident orcas (Orcinus orca); or (D) anadromous fish stocks identified by the NMFS or the USFWS as climate resilient stocks ( U.S.C. &#; (e)()). The program also prioritizes projects that would open up more than  meters of upstream (anadromous) habitat before the end of the natural habitat ( U.S.C. &#; (e)()). \n",
            "Accuracy: 9/9 (1.0000)\n",
            "\n",
            "Class: FY  National Infrastructure Investments: Agency Name: Department of TransportationDescription: The Infrastructure Investment and Jobs Act (Pub. L. -, November , , &#;Bipartisan Infrastructure Law,&#; or &#;BIL&#;) authorized and appropriated $. billion to be awarded by the Department of Transportation (&#;DOT&#;) for FY  for Local and Regional Project Assistance Program Grants under National Infrastructure Investments. This Notice of Funding Opportunity (NOFO) solicits applications for projects to be funded under the Local and Regional Project Assistance Program, known as the RAISE Grants program, including any additional funding appropriated for the RAISE Grants program under the FY  Appropriations Act. On March , , the Consolidated Appropriations Act,  (Pub. L. -, &#;FY  Appropriations Act&#;) appropriated an additional $ million for the FY  RAISE Grant Program. Therefore, a total of $. billion in funding is now available for the FY  RAISE Grant Program. Applicants should note that the two funding streams (BIL funding and FY  Appropriations Act funding) have slightly different funding restrictions and requirements that may affect competitiveness. Please see the Notice of Funding Opportunity Amendment No. for details.As with previous rounds, funds for the FY  RAISE Transportation program are to be awarded on a competitive basis for projects that will have a significant local or regional impact.\n",
            "Accuracy: 15/25 (0.6000)\n",
            "\n",
            "Class: FY  Natural Gas Distribution Infrastructure Safety and Modernization Grant: FY  Natural Gas Distribution Infrastructure Safety and Modernization Grant\n",
            "Accuracy: 6/6 (1.0000)\n",
            "\n",
            "Class: The Infrastructure Investment and Jobs Act (IIJA) Notice of Funding Opportunity for America&#;s Marine Highway Program: This notice announces the availability of funding for grants and establishes selection criteria and application requirements for the America&#;s Marine Highway Program (&#;AMHP&#;). The purpose of this program is to make grants available to previously designated Marine Highway Projects that support the development and expansion of documented vessels or port and landside infrastructure. The Department also seeks eligible grant projects that will strengthen American supply chains. The U.S. Department of Transportation (&#;DOT&#; or &#;Department&#;) will award Marine Highway Grants to implement projects or components of projects previously designated by the Secretary of Transportation (&#;Secretary&#;) under the AMHP. Only Marine Highway Projects the Secretary designates before the Notice of Funding Opportunity (&#;NOFO&#;) closing date are eligible for funding as described in this notice. TIMING OF GRANT APPLICATIONS: Applications must be received by the Maritime Administration (&#;MARAD&#;) by : p.m. E.D.T. on April , . ADDRESSES: Grant applications must be submitted electronically using Grants.gov (https://www.grants.gov). Please be aware that you must complete the Grants.gov registration process before submitting your application and that the registration process usually takes  to  weeks to complete. Applicants are strongly encouraged to make submissions in advance of the deadline. FOR FURTHER INFORMATION CONTACT: Fred Jones, Office of Ports &amp; Waterways Planning, Room W&#;, Maritime Administration, U.S. Department of Transportation,  New Jersey Avenue S.E., Washington, D.C. , phone --, or email Fred.Jones@dot.gov. Persons who use a telecommunications device for the deaf (TDD) may call the Federal Information Relay Service (FIRS) at --- to contact the above individual during business hours. The FIRS is available twenty-four hours a day, seven days a week, to leave a message or question with the above individual. You will receive a reply during regular business hours.\n",
            "Accuracy: 2/2 (1.0000)\n",
            "\n",
            "Class: *FY  Notice of Funding Opportunity: Bipartisan Infrastructure Law (BIL) Airport Terminal Program (ATP) Grants: The Department of Transportation (DOT), Federal Aviation Administration (FAA) announces the opportunity to apply for approximately $ billion in FY  discretionary funds for the Airport Terminal Program (ATP), made available under the Infrastructure Investment and Jobs Act of  (IIJA), Pub. L. -, herein referred to as the Bipartisan Infrastructure Law (BIL). The purpose of the ATP is to make annual grants available to eligible airports for airport terminal and airport-owned Airport Traffic Control Towers development projects that address the aging infrastructure of our nation&#;s airports. In addition, ATP grants will align with DOT&#;s Strategic Framework FY- at https://www.transportation.gov/administrations/office-policy/fy--strategic-framework. The FY  ATP will be implemented consistent with law and in alignment with the priorities in Executive Order , Implementation of the Infrastructure Investments and Jobs Act ( FR ), which are to invest efficiently and equitably; promote the competitiveness of the U.S. economy; improve job opportunities by focusing on high labor standards; strengthen infrastructure resilience to all hazards including climate change; and to effectively coordinate with State, local, Tribal, and territorial government partners. \n",
            "Accuracy: 28/28 (1.0000)\n",
            "\n",
            "Class: INFRA Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families. The Infrastructure for Rebuilding America (INFRA) program is an existing competitive program that will see a more than  percent increase in this year&#;s funding due to the Bipartisan Infrastructure Law. These grants advance the Administration&#;s priorities of rebuilding America&#;s infrastructure and creating jobs by funding highway, multimodal freight and rail projects that position America to win the st century. Projects will improve safety, generate economic benefits, reduce congestion, enhance resiliency, and hold the greatest promise to eliminate supply chain bottlenecks and improve critical freight movements. Last year, DOT received over $ billion of project applications, but could only fund around $ billion of projects. To see last year&#;s INFRA grants, click here. The Bipartisan Infrastructure Law provides approximately $ billion for INFRA over  years, of which approximately $. billion will be made available through this NOFO. \n",
            "Accuracy: 0/4 (0.0000)\n",
            "\n",
            "Class: Mega Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The National Infrastructure Project Assistance (MEGA) program was created in the Bipartisan Infrastructure Law to fund major projects that are too large or complex for traditional funding programs. The program will provide grants on a competitive basis to support multijurisdictional or regional projects of significance that may also cut across multiple modes of transportation. Eligible projects could include highway, bridge, freight, port, passenger rail, and public transportation projects of national and regional significance. These could be bridges or tunnels connecting two states; new rail and transit lines that improve equity and reduce emissions; and freight hubs integrating ship, train and truck traffic while improving environmental justice. DOT will award  percent of funding to projects greater than $ million in cost, and  percent to projects greater than $ million but less than $ million in cost. The program will receive up to $ billion this year alone and be able to provide multi-year funding to projects.\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: Rural Surface Transportation Grant Program: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The Rural Surface Transportation Grant Program (RURAL) was created in the Bipartisan Infrastructure Law and will support projects to improve and expand the surface transportation infrastructure in rural areas to increase connectivity, improve the safety and reliability of the movement of people and freight, and generate regional economic growth and improve quality of life. Eligible projects for Rural grants include highway, bridge, and tunnel projects that help improve freight, safety, and provide or increase access to an agricultural, commercial, energy, or transportation facilities that support the economy of a rural area. This year alone, DOT will award up to $ million in grants through the rural program&#;part of the $ billion included in the Bipartisan Infrastructure Law over five years.\n",
            "Accuracy: 0/2 (0.0000)\n",
            "\n",
            "Class: *FY  Notice of Funding Opportunity: Bipartisan Infrastructure Law (BIL) FAA Contract Tower (FCT) Competitive Grant Program: The Department of Transportation (DOT), Federal Aviation Administration (FAA) announces the opportunity to apply for $ million in FY  Airport Infrastructure Grant funds for the FCT competitive grant program, made available under the Infrastructure Investment and Jobs Act of  (IIJA), Pub. L. -, herein referred to as the Bipartisan Infrastructure Law (BIL). The purpose of the FCT competitive grant program is to make annual grants available to eligible airports for airport-owned airport traffic control tower (ATCT) projects that address the aging infrastructure of the nation&#;s airports. The FAA will consider ATCT projects that sustain, construct, repair, improve, rehabilitate, modernize, replace, or relocate non-approach control towers; or acquire and install air traffic control, communications, and related equipment to be used in those towers; or construct a remote tower certified by the FAA including acquisition and installation of air traffic control, communications, or related equipment. In addition, FT grants will align with DOT&#;s Strategic Framework FY- at https://www.transportation.gov/administrations/office-policy/fy--strategic-framework. The FY  FCT will be implemented consistent with law and in alignment with the priorities in Executive Order , Implementation of the Infrastructure Investments and Jobs Act ( FR ), which are to invest efficiently and equitably; promote the competitiveness of the U.S. economy; improve job opportunities by focusing on high labor standards; strengthen infrastructure resilience to all hazards including climate change; and to effectively coordinate with State, local, Tribal, and territorial government partners. \n",
            "Accuracy: 8/8 (1.0000)\n",
            "\n",
            "Class: FY Railroad Crossing Elimination Grant Program: The purpose of the RCE Program is to provide funding for highway-rail and pathway-rail grade crossing improvement projects that focus on improving the safety and mobility of people and goods.\n",
            "Accuracy: 9/9 (1.0000)\n",
            "\n",
            "Class: Reconnecting Communities Pilot Discretionary Grant Program: The Reconnecting Communities Pilot Program notice of funding opportunity was amended on September ,  to include technical corrections to the Key Information Table. See updated materials in the &quot;Manage Related Documents&quot; section, as well as on our website at https://www.transportation.gov/grants/reconnecting-communitiesThe purpose of this notice is to solicit applications for Reconnecting Communities Pilot (RCP) Program grants. Funds for the Fiscal Year (FY)  RCP Program are to be awarded on a competitive basis for projects that reconnect communities by removing, retrofitting, or mitigating highways or other transportation facilities that create barriers to community connectivity, including to mobility, access, or economic development.The variety of transformative solutions to knit communities back together can include: high-quality public transportation, infrastructure removal, pedestrian walkways and overpasses, capping over roadways, linear parks and trail connectors, roadway redesigns and complete streets conversions, and main street revitalization. The program will award two types of grants: Planning Grants and Capital Construction Grants. Please consult the full Notice of Funding Opportunity for full details about the application process, requirements, and forms.\n",
            "Accuracy: 6/7 (0.8571)\n",
            "\n",
            "Class: Safe Streets and Roads for All Discretionary Grant Program: THIS NOTICE OF FUNDING OPPORTUNITY WAS AMENDED ON AUGUST , . PLEASE SEE THE GRANTS.GOV PACKAGE UPDATES, AND THE SAFE STREETS AND ROADS FOR ALL (SSA) WEBSITE IN THE LINK PROVIDED BELOW FOR MORE INFORMATION. DEADLINE REMAINS SEPTEMBER , .The purpose of this notice is to solicit applications for SSA grants. Funds for the fiscal year (FY)  SSA grant program are to be awarded on a competitive basis to support planning, infrastructure, behavioral, and operational initiatives to prevent death and serious injury on roads and streets involving all roadway users, including pedestrians; bicyclists; public transportation, personal conveyance, and micromobility users; motorists; and commercial vehicle operators. Those interested in applying for Implementation Grants should use the application materials in PKG; those interested in applying for Action Plan Grants should use the application materials in PKG. See application instructions to determine which grant to apply for in this funding round.\n",
            "Accuracy: 5/6 (0.8333)\n",
            "\n",
            "Class: Strengthening Mobility and Revolutionizing Transportation (SMART) Grants Program: The purpose of this notice is to solicit applications for Strengthening Mobility and Revolutionizing Transportation (SMART) grants. Funds for the fiscal year (FY)  SMART Grants Program are to be awarded on a competitive basis to conduct demonstration projects focused on advanced smart city or community technologies and systems to improve transportation efficiency and safety.\n",
            "Accuracy: 9/9 (1.0000)\n",
            "\n",
            "Class: FY- Federal-State Partnership for Intercity Passenger Rail Grant Program for projects located on the Northeast Corridor: This program funds capital projects on the Northeast Corridor that reduce the state of good repair backlog, improve performance, or expand or establish new intercity passenger rail service.\n",
            "Accuracy: 4/4 (1.0000)\n",
            "\n",
            "Class: Fiscal Years - Wildlife Crossings Pilot Program (WCPP): The primary goals of the WCPP are to save lives, prevent serious injuries, and protect motorists and wildlife by reducing WVCs, and improve habitat connectivity for terrestrial and aquatic species. Reduction of wildlife vehicle collisions and improvement of terrestrial and aquatic habitat connectivity are the primary merit criteria that will be used in reviewing applications, and each of the primary merit criteria are of equal importance.\n",
            "Accuracy: 2/3 (0.6667)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity; Buses and Bus Facilities Program: The Federal Transit Administration (FTA) announces the availability of approximately $ million in competitive grants under the Buses and Bus Facilities Program is to assist in the financing of buses and bus facilities capital projects, including replacing, rehabilitating, purchasing or leasing buses or related equipment, and rehabilitating, purchasing, constructing or leasing bus-related facilities. Synopses and full announcement will be posted on Grants.gov as opportunity FTA---TPM-BUS. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time April , .\n",
            "Accuracy: 7/7 (1.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity; Low or No Emission Vehicle Program: The Federal Transit Administration (FTA) announces the availability of approximately $. billion in competitive grants under the Low or No Emission Grant Program (Low-No Program) for the purchase or lease of zero-emission and low-emission transit buses, including acquisition, construction, and leasing of required supporting facilities. Synopses and full announcement will be posted on Grants.gov as opportunity FTA---TPM-LWNO. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time April , .\n",
            "Accuracy: 0/12 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Areas of Persistent Poverty Program: To create new opportunities for those experiencing poverty, the U.S. Department of Transportation&#;s Federal Transit Administration (FTA) today announced a new Notice of Funding Opportunity (NOFO) to help improve transit in areas experiencing long-term economic distress. The $ million in competitive grant funding through FTA&#;s Areas of Persistent Poverty (AoPP) Program provides more resources to underserved and disadvantaged communities seeking to expand or improve transit. \n",
            "Accuracy: 7/7 (1.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Ferry Service for Rural Communities Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  Ferry Service for Rural Communities Program (FSRC) (Federal Assistance Listing #.). FSRC makes funding available competitively to assist in the financing of capital, planning, and operating assistance for eligible ferry services.\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $. million in competitive grants under the Fiscal Year (FY)  Passenger Ferry Grant Program (PFG) (Federal Assistance Listing #.). Of that amount, $ million is available only for low or zero-emission ferries or ferries using electric battery or fuel cell components and the infrastructure to support such ferries. As required by Federal public transportation law, funds will be awarded competitively to designated recipients or eligible direct recipients of Urbanized Area Formula funds to support capital projects to improve existing passenger ferry service, establish new ferry service, and repair and modernize ferry boats, terminals, and related facilities and equipment. \n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: FY  National Infrastructure Investments: The Infrastructure Investment and Jobs Act of  (&#;Bipartisan Infrastructure Law,&#; or &#;BIL&#;) authorized and appropriated $. billion to be awarded by the Department of Transportation (&#;DOT&#;) for Local and Regional Project Assistance Program Grants under National Infrastructure Investments in FY . This Notice of Funding Opportunity (NOFO) solicits applications for projects funded under the Local and Regional Project Assistance Program, known as the RAISE Grants program. If the FY  Appropriations Act provides additional funding for the RAISE grants program and/or significantly different requirements for National Infrastructure Investment funds, the Department will amend this Notice with guidance on additional requirements.As with previous rounds, funds for the FY  RAISE Transportation program are to be awarded on a competitive basis for projects that will have a significant local or regional impact.\n",
            "Accuracy: 17/24 (0.7083)\n",
            "\n",
            "Class: Neighborhood Access and Equity (NAE) Program: The Department of Transportation is combining two major discretionary grant programs, the Reconnecting Communities Pilot (RCP) and Neighborhood Access and Equity (NAE) programs, into one Notice of Funding Opportunity (NOFO). Together, this combined program is known as the Reconnecting Communities and Neighborhoods (RCN) Program. The primary goal of the NAE Program is to assist economically disadvantaged or undeserved communities for planning and construction activities. Funds for the fiscal year (FY)  NAE grant program are to be awarded on a competitive basis to support planning, capital construction, and regional partnership activities that aim to restore community connectivity through the removal, retrofit, mitigation or replacement of highways, roadways, or other infrastructure facilities that create barriers to mobility, access or economic development. Applicants must submit their applications via Valid Eval at the links below: Community Planning Grants: https://usg.valideval.com/teams/rcn_planning/signup Capital Construction Grants: https://usg.valideval.com/teams/rcn_capitalconstruction/signup Do not submit applications through Grants.gov. Applications must be submitted by : PM Eastern Time on Thursday, September , . Late applications will not be accepted. \n",
            "Accuracy: 21/21 (1.0000)\n",
            "\n",
            "Class: FY  Notice of Funding Opportunity for Small Shipyard Grants: This notice announces the intention of the Maritime Administration to provide grants to small shipyards. Under the Small Shipyard Grant Program, there is currently $,, available for grants for capital and related improvements to qualified shipyard facilities that will be effective in fostering efficiency, competitive operations, and quality ship construction, repair, and reconfiguration. Potential applicants are advised that it is expected, based on experience, that the number of applications will far exceed the funds available and that only a small percentage of applicants will be funded. Applications (SF- and the Addendum) will be accepted online through Grants.gov by  pm on the closing date (February , ). See full text under Related Documents Tab (FY  SSG NOFO) or the MARAD Web Page for detailed submission instructions.\n",
            "Accuracy: 4/4 (1.0000)\n",
            "\n",
            "Class: Safe Streets and Roads for All Funding Opportunity: The purpose of this notice is to solicit applications for Safe Streets and Roads for All (SSA) grants. Funds for the fiscal year (FY)  SSA grant program are to be awarded on a competitive basis to support planning and demonstration activities, as well as projects and strategies to prevent death and serious injury on roads and streets involving all roadway users, including pedestrians; bicyclists; public transportation, personal conveyance, and micromobility users; motorists; and commercial vehicle operators. Applicants must submit their applications via Valid Eval at https://usg.valideval.com/teams/usdot_ssa__implementation/signup for Implementation Grant applicants, and https://usg.valideval.com/teams/usdot_ssa__planning_demo/signup for Planning and Demonstration Grants. Do not submit your applications through Grants.Gov. Applications must be submitted by : PM Eastern Time on Monday, July , . Late applications will not be accepted.\n",
            "Accuracy: 57/58 (0.9828)\n",
            "\n",
            "Class: SMART Grants Notice of Funding: The purpose of this notice is to solicit applications for Strengthening Mobility and Revolutionizing Transportation (SMART) Stage  Planning and Prototyping grants. Funds for the fiscal year (FY)  SMART Grants Program are to be awarded on a competitive basis to conduct demonstration projects focused on advanced smart city or community technologies and systems to improve transportation efficiency and safety. Applications must be submitted via Valid Eval, an online submission proposal system used by USDOT, at https://usg.valideval.com/teams/USDOT_SMART_/signup. USDOT will not accept or review application materials submitted via Grants.gov. \n",
            "Accuracy: 0/5 (0.0000)\n",
            "\n",
            "Class:  Tribal Transportation Program Safety Fund: Eligible projects described in section (a)() are strategies, activities, and projects on a public road that are consistent with a transportation safety plan and that (i) correct or improve a hazardous road location or feature, or (ii) address a highway safety problem. TTPSF emphasizes the development of strategic transportation safety plans using a data-driven process as a means for Tribes to identify transportation safety needs and determine how those needs will be addressed in Tribal communities. FHWA has identified four eligibility categories: transportation safety plans; data assessment, improvement, and analysis activities; systemic roadway departure countermeasures; and infrastructure improvements and other eligible activities as listed in  U.S.C. &#; (a)().\n",
            "Accuracy: 13/13 (1.0000)\n",
            "\n",
            "Class: United States Marine Highway Grants: This funding opportunity solicits applications for fiscal year (FY)  United States Marine Highway Program (USMHP) grants. Funds for FY  USMHP grants will be awarded on a competitive basis to assist in funding eligible projects for the purpose of developing, expanding, or promoting marine highway transportation. This opportunity announces the availability of up to $,, million in funding for grants under this program and establishes selection criteria and application requirements. All USMHP grant recipients must meet all applicable Federal requirements, including domestic content (&#;Buy America&#;) requirements. This program was formerly known as the America&#;s Marine Highway Program\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n",
            "Class: INFRA Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families. The Infrastructure for Rebuilding America (INFRA) program is an existing competitive program that will see a more than  percent increase in this year&#;s funding due to the Bipartisan Infrastructure Law. These grants advance the Administration&#;s priorities of rebuilding America&#;s infrastructure and creating jobs by funding highway, multimodal freight and rail projects that position America to win the st century. Projects will improve safety, generate economic benefits, reduce congestion, enhance resiliency, and hold the greatest promise to eliminate supply chain bottlenecks and improve critical freight movements. Last year, DOT received over $ billion in application requests, but could only fund around $. billion of projects. To see last year&#;s INFRA grants, click here. The Bipartisan Infrastructure Law provides approximately $ billion for INFRA over  years, of which approximately $. billion will be made available through this NOFO. \n",
            "Accuracy: 0/4 (0.0000)\n",
            "\n",
            "Class: Mega Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The National Infrastructure Project Assistance (MEGA) program was created in the Bipartisan Infrastructure Law to fund major projects that are too large or complex for traditional funding programs. The program will provide grants on a competitive basis to support multijurisdictional or regional projects of significance that may also cut across multiple modes of transportation. Eligible projects could include highway, bridge, freight, port, passenger rail, and related public transportation projects of national and regional significance. DOT will award approximately  percent of funding to projects greater than $ million in cost, and approximately  percent to projects greater than $ million but less than $ million in cost.For more information about last year&apos;s Mega awards, visit here: https://www.transportation.gov/grants/mega-grant-program/FYawards\n",
            "Accuracy: 0/2 (0.0000)\n",
            "\n",
            "Class: Rural Surface Transportation Grant Progam: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The Rural Surface Transportation Grant Program (RURAL) was created in the Bipartisan Infrastructure Law and will support projects to improve and expand the surface transportation infrastructure in rural areas to increase connectivity, improve the safety and reliability of the movement of people and freight, and generate regional economic growth and improve quality of life. Eligible projects for Rural grants include highway, bridge, and tunnel projects that help improve freight, safety, and provide or increase access to an agricultural, commercial, energy, or transportation facilities that support the economy of a rural area. This year alone, DOT will award up to $ million in grants through the rural program&#;part of the $ billion included in the Bipartisan Infrastructure Law over five years. At least % of rural funding must be awarded in amounts of $ million or more. If you are seeking less than $ million, you are competing for only about $. million nationwide.For more information on last year&apos;s rural awards, see: https://www.transportation.gov/grants/rural--fact-sheets\n",
            "Accuracy: 2/5 (0.4000)\n",
            "\n",
            "Class: FY  Competitive Funding Opportunity: Rail Vehicle Replacement Grant Program (RAIL): The Federal Transit Administration (FTA) announces the opportunity to apply for approximately $ million in Fiscal Year (FY)  Section  State of Good Repair Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected rail vehicle replacement projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to States and local governmental authorities to assist in the funding of capital projects to replace rail rolling stock.\n",
            "Accuracy: 0/1 (0.0000)\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'FY  Notice of Funding Opportunity: Bipartisan Infrastructure Law (BIL) Airport Terminal Program (ATP) Grants: The Department of Transportation (DOT), Federal Aviation Administration (FAA) announces the opportunity to apply for approximately $ billion in FY  discretionary funds for the Airport Terminal Program (ATP), made available under the Infrastructure Investment and Jobs Act of  (IIJA), Pub. L. -, herein referred to as the Bipartisan Infrastructure Law (BIL). The purpose of the ATP is to make annual grants available to eligible airports for airport terminal and airport-owned Airport Traffic Control Towers development projects that address the aging infrastructure of our nation&#;s airports. In addition, ATP grants will align with DOT&#;s Strategic Framework FY- at https://www.transportation.gov/administrations/office-policy/fy--strategic-framework. The FY  ATP will be implemented consistent with law and in alignment with the priorities in Executive Order , Implementation of the Infrastructure Investments and Jobs Act ( FR ), which are to invest efficiently and equitably; promote the competitiveness of the U.S. economy; improve job opportunities by focusing on high labor standards; strengthen infrastructure resilience to all hazards including climate change; and to effectively coordinate with State, local, Tribal, and territorial government partners. ': 0.52,\n",
              " 'FY  Competitive Funding Opportunity: Airport Improvement Program Discretionary Grants: The U.S. Department of Transportation&#;s Federal Aviation Administration (FAA) announces the opportunity to apply for an estimated $. billion in Fiscal Year (FY)  discretionary grants under the Airport Improvement Program (AIP), per  U.S.C. &#; . The FAA awards these annually appropriated discretionary funds through the FAA&#;s long-standing iterative, competitive grant process. Prior to the publication of this Notice of Funding Opportunity (NOFO), the FAA identified eligible applicants in its National Plan of Integrated Airport Systems (NPIAS) and compiled potentially eligible projects through the -year Airports Capital Improvement Plan (ACIP). Both of these processes are described in FAA Order ., &#;Formulation of NPIAS and ACIP,&#; adhering to  U.S.C. &#; , which authorizes discretionary funds. The AIP funds airport capital improvements and rehabilitation projects. All discretionary grant funding is subject to appropriations, statutory requirements, and related program funding availability. ': 0.9938650306748467,\n",
              " 'Grants for Buses and Bus Facilities Program: The Federal Transit Administration (FTA) announces the availability of approximately $. million of Fiscal Year  funds for buses, bus facilities, and bus equipment. The purpose of the Grants for Buses and Bus Facilities Program is to assist in the financing of buses and bus facilities capital projects, including replacing, rehabilitating, purchasing or leasing buses or related equipment, and rehabilitating, purchasing, constructing or leasing bus-related facilities. ': 0.9285714285714286,\n",
              " 'Solicitation of Project Proposals for the Low or No Emission (Low-No) Program:  Notice of Funding Opportunity (NOFO): Solicitation of Project Proposals for the Low or No Emission Program (Low-No) Program. The Federal Transit Administration (FTA) announces the availability of $ million of Fiscal Year  funds for the purchase or lease of low or no emission vehicles as well as related equipment or facilities. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time on //. The main purpose of the Low-No Program is to support the transition of the nation&#;s transit fleet to the lowest polluting and most energy efficient transit vehicles. The Low-No Program provides funding to State and local governmental authorities for the purchase or lease of zero-emission and low-emission transit buses, including acquisition, construction, and leasing of required supporting facilities. ': 0.0,\n",
              " 'FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in Fiscal Year (FY)  Section  Urbanized Area Formula Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected passenger ferry projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to designated recipients or eligible direct recipients of Section  funds to assist in the financing of capital projects to support existing passenger ferry service, establish new ferry service, and to repair and modernize ferry boats, terminals, and related facilities and equipment. ': 0.0,\n",
              " 'Pilot Program for Transit-Oriented Development (TOD) Planning: Notice of Funding Opportunity for the Pilot Program for Transit-Oriented Development (TOD) Planning. The Federal Transit Administration (FTA) announces the availability of approximately $. million in Pilot Program for TOD Planning funding to support comprehensive planning associated with new fixed guideway and core capacity improvement projects. FTA may award amounts ranging from $, to $,,. The Pilot Program for TOD Planning provides funding to local communities to integrate land use and transportation planning in new fixed guideway and core capacity transit project corridors. As required by statute, any comprehensive planning funded through the pilot program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations. The statute also requires that the planning work be associated with a new fixed guideway or core capacity transit project as defined in Federal transit statute ( USC (a); also see the NOFO for the definitions). Consistent with statutory direction, FTA is seeking comprehensive planning projects covering an entire transit capital project corridor, rather than proposals that involve planning for individual station areas or only a small section of the corridor. To ensure that planning work reflects the needs and aspirations of the local community and results in concrete, specific deliverables and outcomes, FTA is requiring that transit project sponsors partner with entities with land use planning authority in the transit project corridor. ': 1.0,\n",
              " 'Public Transportation on Indian Reservations Program: The Federal Transit Administration (FTA) announces the availability of approximately $ million in funding provided by the Public Transportation on Indian Reservations Program (Tribal Transit Program (TTP)), as authorized by  U.S.C. (c)()(A), as amended by the Fixing America&apos;s Surface Transportation Act (FAST), Public Law - (December , ). The primary purpose of these competitively selected grants is to support planning, capital, and, in limited circumstances, operating assistance for tribal public transit services. Funds distributed to Indian tribes under the TTP should not replace or reduce funds that Indian tribes receive from states through FTA&#;s Section  program. ': 0.16666666666666666,\n",
              " 'INFRA Grants: The Nationally Significant Freight and Highway Projects (INFRA) program provides Federal financial assistance to highway and freight projects of national or regional significance. This notice solicits applications for awards under the program&#;s fiscal year (FY)  funding, subject to the availability of appropriated funds. The Department is focusing the competition on transportation infrastructure projects that support four key objectives, each of which is discussed in greater detail in the Notice of Funding Opportunity: () Supporting economic vitality at the national and regional level; () Leveraging Federal funding to attract non-Federal sources of infrastructure investment; () Deploying innovative technology, encouraging innovative approaches to project delivery, and incentivizing the use of innovative financing; and () Holding grant recipients accountable for their performance. ': 0.6666666666666666,\n",
              " ' Tribal Transportation Program Safety Fund: A Notice of Funding Availability (NOFA) for Tribal Transportation Program Safety Funds (TTPSF) has been published. This notice announces the availability of $. million for projects in three categories: safety plans; data assessment, improvement, and analysis activities; and infrastructure improvement and other eligible activities as listed in  U.S.C.(a)() The deadline for submissions is //. For complete instructions on preparing and submitting an application for TTPSF, please download the application information and NOFO at https://flh.fhwa.dot.gov/programs/ttp/safety/ttpsf.htm Please submit your application directly to FHWA through the &quot;apply now&quot; button at: https://flh.fhwa.dot.gov/programs/ttp/safety/ttpsf.htm ': 1.0,\n",
              " 'Grants for Buses and Bus Facilities Program: The purpose of the Grants for Buses and Bus Facilities Program is to assist in the financing of buses and bus facilities capital projects, including replacing, rehabilitating, purchasing or leasing buses or related equipment, and rehabilitating, purchasing, constructing or leasing bus-related facilities.': 0.8571428571428571,\n",
              " 'Low or No Emission Program (Low-No Program): The main purpose of the Low-No Program is to support the transition of the nation&#;s transit fleet to the lowest polluting and most energy efficient transit vehicles. The Low-No Program provides funding to State and local governmental authorities for the purchase or lease of zero-emission and low-emission transit buses, including acquisition, construction, and leasing of required supporting facilities.': 1.0,\n",
              " 'Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in Fiscal Year (FY)  Section  Urbanized Area Formula Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected passenger ferry projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to designated recipients or eligible direct recipients of Section  funds to assist in the financing of capital projects to support existing passenger ferry service, establish new ferry service, and to repair and modernize ferry boats, terminals, and related facilities and equipment. ': 0.0,\n",
              " 'Pilot Program for Transit-Oriented Development (TOD) Planning: Notice of Funding Opportunity for PILOT PROGRAM FOR TRANSIT-ORIENTED DEVELOPMENT (TOD) PLANNING. The Federal Transit Administration (FTA) announces the opportunity to apply for approximately $. million of funding under the Pilot Program for TOD Planning (Catalog of Federal Domestic Assistance #.). As required by federal transit law and subject to funding availability, funds will be awarded competitively to support comprehensive planning associated with new fixed guideway and core capacity improvement projects. FTA anticipates minimum grant awards of $, and maximum grant awards of $,,. The Pilot Program for TOD Planning is intended to fund comprehensive planning that supports economic development, ridership, multimodal connectivity and accessibility, increased transit access for pedestrian and bicycle traffic, and mixed-use development near transit stations. The program also encourages identification of infrastructure needs and engagement with the private sector. Consistent with statutory direction, FTA is seeking comprehensive planning projects covering an entire transit capital project corridor, rather than proposals that involve planning for individual station areas or only a small section of the corridor. To ensure any proposed planning work reflects the needs and aspirations of the local community and results in concrete, specific deliverables and outcomes, transit project sponsors must partner with entities with land use planning authority in the transit project corridor to conduct the planning work.The Pilot Program for TOD Planning helps support FTA&#;s mission of improving public transportation for America&#;s communities by providing funding to local communities to integrate land use and transportation planning around a new fixed guideway or core capacity improvement project. Per statute, any comprehensive planning funded through the program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations.': 0.0,\n",
              " 'FY  Competitive Funding Opportunity: Public Transportation on Indian Reservations Program: The Federal Transit Administration (FTA) announces the availability of approximately $ million in funding provided by the Public Transportation on Indian Reservations Program (Tribal Transit Program (TTP)), as authorized by  U.S.C. (c)()(A), as amended by the Fixing America&apos;s Surface Transportation Act (FAST), Public Law - (December , ). This is a national solicitation for project proposals and includes the selection criteria and program eligibility information for Fiscal Year  projects. The primary purpose of these competitively selected grants is to support planning, capital, and, in limited circumstances, operating assistance for tribal public transit services. Funds distributed to Indian tribes under the TTP should not replace or reduce funds that Indian tribes receive from states through FTA&#;s Section  program.': 0.0,\n",
              " '*INFRA Grants: The U.S. Department of Transportation (USDOT) is seeking applicants for the FY  round of the Infrastructure for Rebuilding America (INFRA) discretionary grant program to fund transportation projects of national and regional significance that result in good-paying jobs, improve safety, apply transformative technology, and explicitly address climate change and racial equity. The funding available for this year&#;s grants totals approximately $ million.The Department recognizes the role that infrastructure investment plays in economic development and job creation, and the added urgency of this funding at time when the COVID- pandemic has put stress on state and local budgets.For the first time, the USDOT seeks INFRA projects that address climate change and environmental justice. Projects will be evaluated on whether they were planned as part of a comprehensive strategy to address climate change, or whether they support strategies to reduce greenhouse gas emissions such as deploying zero-emission-vehicle infrastructure or encouraging modal shift and a reduction in vehicle-miles-traveled. Racial equity will also be considered as a selection criterion, to the extent that project sponsors have completed equity-focused community outreach, and projects are designed to benefit underserved communities. The Department will also consider whether the project is located in a federally designated community development zone, including qualified Opportunity Zones, Empowerment Zones, Promise Zones, or Choice Neighborhoods.USDOT seeks projects that apply innovative technology, delivery, or financing methods with proven outcomes to deliver projects in a cost effective manner. The Department will make awards under the INFRA program to both large and small projects. For a large project, the INFRA grant must be at least $ million. For a small project, the grant must be at least $ million. Under statutory requirements,  percent of available funds are reserved for small projects, and the Department must award at least  percent of funding for rural projects. INFRA grants may be used to fund a variety of components of an infrastructure project, however, the Department is specifically focused on projects in which the local sponsor is significantly invested and is positioned to proceed rapidly to construction. Eligible INFRA project costs may include: reconstruction, rehabilitation, acquisition of property (including land related to the project and improvements to the land), environmental mitigation, construction contingencies, equipment acquisition, and operational improvements directly related to system performance. The INFRA NOFO also announces the creation of the &#;INFRA Extra&#; Program, which will identify competitive INFRA applicants who do not receive an INFRA award and authorize them to seek a Transportation Infrastructure Finance and Innovation Act of  (TIFIA) loan up to  percent of their project cost.': 0.0,\n",
              " 'Fiscal Year  Competitive Funding Opportunity; Grants for Buses and Bus Facilities Program: The Federal Transit Administration (FTA) announces the availability of approximately $. million of Fiscal Year  funds for buses, bus facilities, and bus equipment. Synopses and full announcement will be posted on Grants.gov as opportunity FTA---BUS. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time November ,.': 0.1,\n",
              " 'FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  Passenger Ferry Grant Program (Ferry Program) (Federal Assistance Listing #.). Of that amount, $ million is available only for low or zero-emission ferries or ferries using electric battery or fuel cell components and the infrastructure to support such ferries. As required by Federal public transportation law, funds will be awarded competitively to designated recipients or eligible direct recipients of Urbanized Area Formula funds to support capital projects to improve existing passenger ferry service, establish new ferry service, and repair and modernize ferry boats, terminals, and related facilities and equipment.': 0.0,\n",
              " ' Competitive Funding Opportunity: Pilot Program for Transit-Oriented Development (TOD) Planning: The Pilot Program for TOD Planning helps support FTA&#;s mission of improving public transportation for America&#;s communities by providing funding to local communities to integrate land use and transportation planning around a new fixed guideway or core capacity improvement project. Per statute, any comprehensive planning funded through the program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations. Additional information is available at: https://www.transit.dot.gov/TODPilot.Notice of Funding Opportunity (NOFO): Notice of Funding Opportunity for PILOT PROGRAM FOR TRANSIT-ORIENTED DEVELOPMENT (TOD) PLANNING. The Federal Transit Administration (FTA) announces the opportunity to apply for $,, of funding under the Pilot Program for TOD Planning (Catalog of Federal Domestic Assistance #.). As required by federal transit law and subject to funding availability, funds will be awarded competitively to support comprehensive planning associated with new fixed guideway and core capacity improvement projects. FTA anticipates minimum grant awards of $, and maximum grant awards of $,,. Synopses and full announcement are posted on Grants.gov as opportunity FTA---TPE-TODP. The Pilot Program for TOD Planning is intended to fund comprehensive planning that supports economic development, ridership, multimodal connectivity and accessibility, increased transit access for pedestrian and bicycle traffic, and mixed-use development near transit stations. The program also encourages identification of infrastructure needs and engagement with the private sector.Consistent with statutory direction, FTA is seeking comprehensive planning projects covering an entire transit capital project corridor, rather than proposals that involve planning for individual station areas or only a small section of the corridor. To ensure any proposed planning work reflects the needs and aspirations of the local community and results in concrete, specific deliverables and outcomes, transit project sponsors must partner with entities with land use planning authority in the transit project corridor to conduct the planning work.Link and Instructions for attaching the supplemental form to the SF-: All applicants must complete a supplemental form (PDF) specific to the Pilot Program for TOD Planning and attach it to their submission in Grants.gov. See section D of the NOFO for information about additional required application contents.Webinar: FTA will conduct a webinar for prospective applicants after the NOFO is published in the Federal Register. Further information will be posted on FTA&#;s website (https://www.transit.dot.gov/TODPilot) when available. Dates: An applicant must submit a proposal electronically by : p.m. Eastern Daylight Time on June , . Any agency intending to apply should initiate the process of registering on the Grants.gov site immediately to ensure completion of registration before the submission deadline. ': 0.0,\n",
              " 'FY  Competitive Funding Opportunity:  Public Transportation on Indian Reservations Program; Tribal Transit Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants for the Fiscal Year (FY)  Public Transportation on Indian Reservations (Tribal Transit) Program (Federal Assistance Listing: .). As required by Federal public transportation law, funds will be awarded competitively for any purpose eligible under FTA&#;s Formula Grants for Rural Areas Program,  U.S.C. , including planning, capital, and operating assistance for tribal public transit services in rural areas. ': 0.6666666666666666,\n",
              " 'INFRA Grants: The U.S. Department of Transportation (USDOT) is seeking applicants for the FY  round of the Infrastructure for Rebuilding America (INFRA) discretionary grant program to fund transportation projects of national and regional significance that result in good-paying jobs, improve safety, apply transformative technology, and explicitly address climate change and racial equity. The funding available for this year&#;s grants totals approximately $ million.The Department recognizes the role that infrastructure investment plays in economic development and job creation, and the added urgency of this funding at time when the COVID- pandemic has put stress on state and local budgets.For the first time, the USDOT seeks INFRA projects that address climate change and environmental justice. Projects will be evaluated on whether they were planned as part of a comprehensive strategy to address climate change, or whether they support strategies to reduce greenhouse gas emissions such as deploying zero-emission-vehicle infrastructure or encouraging modal shift and a reduction in vehicle-miles-traveled. Racial equity will also be considered as a selection criterion, to the extent that project sponsors have completed equity-focused community outreach, and projects are designed to benefit underserved communities. The Department will also consider whether the project is located in a federally designated community development zone, including qualified Opportunity Zones, Empowerment Zones, Promise Zones, or Choice Neighborhoods.USDOT seeks projects that apply innovative technology, delivery, or financing methods with proven outcomes to deliver projects in a cost effective manner. The Department will make awards under the INFRA program to both large and small projects. For a large project, the INFRA grant must be at least $ million. For a small project, the grant must be at least $ million. Under statutory requirements,  percent of available funds are reserved for small projects, and the Department must award at least  percent of funding for rural projects. INFRA grants may be used to fund a variety of components of an infrastructure project, however, the Department is specifically focused on projects in which the local sponsor is significantly invested and is positioned to proceed rapidly to construction. Eligible INFRA project costs may include: reconstruction, rehabilitation, acquisition of property (including land related to the project and improvements to the land), environmental mitigation, construction contingencies, equipment acquisition, and operational improvements directly related to system performance. The INFRA NOFO also announces the creation of the &#;INFRA Extra&#; Program, which will identify competitive INFRA applicants who do not receive an INFRA award and authorize them to seek a Transportation Infrastructure Finance and Innovation Act of  (TIFIA) loan up to  percent of their project cost.': 0.29411764705882354,\n",
              " 'FY Tribal Transportation Program Safety Fund (TTPSF): A Notice of Funding Opportunity (NOFO) for Tribal Transportation Program Safety Funds (TTPSF) has been published. This notice announces the availability of approximately $. million for projects in four categories: safety plans; data assessment, improvement, and analysis activities; systemic roadway departure countermeasures; and infrastructure improvement and other eligible activities as listed in  U.S.C.(a)(). The goal of these projects is to reduce fatalities and serious injuries from transportation incidents, such as motor vehicle crashes. For complete instructions on preparing and submitting an application for TTPSF, please download the application, application information, systemic roadway departure risk assessment form (if applicable) and NOFO at https://highways.dot.gov/federal-lands/programs-tribal/safety/funds. Applications will NOT be submitted on Grants.gov. Instead, please use the &quot;apply now&quot; button on the program web page.': 0.7777777777777778,\n",
              " 'Advanced Transportation Technologies and  Innovative Mobility Deployment (ATTIMD) Program: AMENDMENT  (//): This amendment is for the purpose of correcting a typo contained in Table D. on page  of . This table, contained in Section D. for CONTENT AND FORM OF APPLICATION SUBMISSION, erroneously omitted the Project Outcome Criteria from that table&#;s submission requirements for Volume . This amendment also revises the verbiage included on page  of the NOFO to clarify that all resumes for key personnel should be included in item f) Appendix &#; R&#;sum&#;s for Key Personnel. Please note that Resumes do not count against the page limitations for Volume . Please refer to Amendment  in the &#;Related Documents&#; tab for more information. -- END OF AMENDMENT  -- AMENDMENT  (//): This amendment adds the weblink to the recording of the October , , informational webinar for this opportunity. The weblink is included in the attachments page for this opportunity as well as below: LINK: https://usdot.zoomgov.com/rec/share/eQNNRGISy-UxfqGpSDzINRbzvcdkAjDdoulZvwRCt-sARp.kdAJIUFsFZZEpf PASSCODE: c@X*&amp;@ -- END OF AMENDMENT  -- AMENDMENT  (//): This amendment adds the registration information for the October , , informational webinar for this opportunity. Please refer to the &quot;Related Documents&quot; tab for more information. -- END OF AMENDMENT  -- BASE NOFO (//): The Advanced Transportation Technologies and Innovative Mobility Deployment Program (ATTIMD), established in Section (c)() of title , United States Code (U.S.C.), also known as the Advanced Transportation Technology and Innovation (ATTAIN) Program, directs FHWA to award grants to eligible entities to deploy, install, and operate advanced transportation technologies to improve safety, mobility, efficiency, system performance, intermodal connectivity, and infrastructure return on investment. FHWA intends for these model technology deployments to help demonstrate how emerging transportation technologies, data, and their applications can be effectively deployed and integrated with existing systems to provide access to essential services and other destinations. This also includes efforts to increase connectivity to employment, education, services, and other opportunities; support workforce development; or contribute to increased mobility, particularly for persons with visible and hidden disabilities and elderly individuals. The Assistance Listing Number for this opportunity is . &#; Highway Research and Development. The Federal Highway Administration (FHWA) hereby requests applications to result in awards to eligible entities to deploy, install, and operate advanced transportation technologies to improve safety, mobility, efficiency, system performance, intermodal connectivity, and infrastructure return on investment. These model deployments are expected to provide benefits in the form of: reduced traffic-related fatalities and injuries; reduced traffic congestion and improved travel time reliability; reduced transportation-related emissions; optimized multimodal system performance; improved access to transportation alternatives, including for underserved populations; improved integration of payment systems; public access to real-time integrated traffic, transit, and multimodal transportation information to make informed travel decisions; cost savings to transportation agencies, businesses, and the traveling public; or other benefits to transportation users and the general public. This competitive ATTAIN Program will promote the use of innovative transportation solutions. The deployment of these technologies will provide Congress and FHWA with valuable real-life data and feedback to inform future decision-making. Cost Sharing: Cost-sharing or matching is required, with the maximum Federal share being  percent; hence, this NOFO requires a minimum non-Federal cost share of  percent. Cost sharing or matching means the portion of project costs not paid by Federal funds. NOTE: FHWA previously issued six NOFOs for the Advanced Transportation and Congestion Management Technologies Deployment (ATCMTD) Grant Program before the program was amended and renamed to the Advanced Transportation Technologies and Innovative Mobility Deployment Program by Section  of the Infrastructure Investment and Jobs Act (IIJA) (Public Law -, also known as the &#;Bipartisan Infrastructure Law&#; (BIL)).': 0.0,\n",
              " 'FY  and FY  Competitive Funding Opportunity: Competitive Grants for Rail Vehicle Replacement Program (Rail Program): The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in Fiscal Year (FY)  and  Section  State of Good Repair Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected rail vehicle replacement projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to States and local governmental authorities to assist in the funding of capital projects to replace rail rolling stock.': 1.0,\n",
              " 'Bridge Investment Program - Planning, Bridge Projects, and Large Bridge Projects: This Notice of Funding Opportunity (NOFO) is to solicit applications for three categories of Bridge Investment Program (BIP) funding opportunities: () Planning; () Bridge Projects (projects with eligible costs less than $ million); and () Large Bridge Projects (projects with eligible costs greater than $ million). Eligible applicants may submit applications for any of the three funding categories, but each category has distinct eligibility and selection criteria and application deadlines.These funds will be awarded on a competitive basis for planning, feasibility analysis, and revenue forecasting associated with the development of a project that would subsequently be eligible to apply for BIP funding under either the Bridge Projects or Large Bridge Projects funding categories.A total of $. billion in  BIP funds are available for the Bridge Projects and Large Bridge Projects funding opportunities. These funds will be awarded on a competitive basis for bridge replacement, rehabilitation, preservation, and protection projects that: () improve the safety, efficiency, and reliability of the movement of people and freight over bridges; and () improve the condition of bridges in the United States by reducing (a) the number of bridges, and total person miles traveled over bridges, that are in poor condition or that are in fair condition and at risk of falling into poor condition within the next three years, or (b) the number of bridges, and total person miles traveled over bridges, that do not meet current geometric design standards or cannot meet the load and traffic requirements typical of the regional transportation network. In addition, Large Bridge Projects that receive a BIP award of not less than $ million are eligible for multiyear grants, in which DOT can award available funds to a project over the course of several years in accordance with an agreement and in alignment with its schedule. In selecting Bridge Projects and Large Bridge Projects, FHWA will consider the extent to which BIP funds leverage non-Federal contributions from sponsors and stakeholders involved in the planning, design, and construction of eligible projects. ': 0.8333333333333334,\n",
              " 'FY  Competitive Funding Opportunity; Low or No Emission Grant Program: Low-No ProgramThe Federal Transit Administration (FTA) announces the opportunity to apply for approximately $. billion in competitive grants under the fiscal year (FY)  Low or No Emission Grant Program (Low-No Program) (Federal Assistance Listing: .) and approximately $ million in FY  funds under the Grants for Buses and Bus Facilities Program (Buses and Bus Facilities Program) (Federal Assistance Listing .), subject to availability of appropriated funding. Synopses and full announcement will be posted on Grants.gov as funding opportunity ID FTA--TPM for Low-No applications and FTA--TPM for Buses and Bus Facilities applications. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time May , .': 0.9333333333333333,\n",
              " 'FY  Competitive Funding Opportunity: All Stations Accessibility Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  All Stations Accessibility Program (ASAP) (Federal Assistance Listing #.). As required by Federal public transportation law, funds will be awarded competitively to eligible designated recipients that operate or allocate funds to inaccessible pre-ADA&#;or &#;legacy&#; &#; rail fixed guideway public transportation systems, and States (including territories and Washington, D.C.) and local governmental entities that operate or financially support legacy rail fixed guideway public transportation systems and corresponding legacy stations/facilities for capital projects to repair, improve, modify, retrofit, or relocate infrastructure of stations or facilities for passenger use, including load-bearing members that are an essential part of the structural frame; or () for planning projects to develop or modify a plan for pursuing public transportation accessibility projects, assessments of accessibility, or assessments of planned modifications to stations or facilities for passenger use projects; or programs of projects in an eligible area.': 1.0,\n",
              " 'FY  Competitive Funding Opportunity: Buses and Bus Facilities Competitive Program: Bus and Bus Facilities ProgramThe Federal Transit Administration (FTA) announces the opportunity to apply for approximately $. billion in competitive grants under the fiscal year (FY)  Low or No Emission Grant Program (Low-No Program) (Federal Assistance Listing: .) and approximately $ million in FY  funds under the Grants for Buses and Bus Facilities Program (Buses and Bus Facilities Program) (Federal Assistance Listing .), subject to availability of appropriated funding. Synopses and full announcement will be posted on Grants.gov as funding opportunity ID FTA--TPM for Low-No applications and FTA--TPM for Buses and Bus Facilities applications. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time May , ': 0.0,\n",
              " 'FY  Competitive Funding Opportunity: Electric or Low-Emitting Ferry Pilot Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  Electric or Low-Emitting Ferry Pilot Program (ELEF) (Federal Assistance Listing #.). ELEF makes funding available competitively to assist in the financing of capital projects for the purchase of electric or low-emitting ferry vessels that reduce emissions by using alternative fuels or on-board energy storage systems and related charging infrastructure to reduce emissions or produce zero onboard emissions under normal operation.': 1.0,\n",
              " 'FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $. million in competitive grants under the Fiscal Year (FY)  Passenger Ferry Grant Program (PFG) (Federal Assistance Listing #.). Of that amount, $. million is available only for low or zero-emission ferries or ferries using electric battery or fuel cell components and the infrastructure to support such ferries. As required by Federal public transportation law, funds will be awarded competitively to designated recipients or eligible direct recipients of Urbanized Area Formula funds to support capital projects to improve existing passenger ferry service, establish new ferry service, and repair and modernize ferry boats, terminals, and related facilities and equipment. ': 1.0,\n",
              " 'FY  Pilot Program for Transit-Oriented Development (TOD) Planning: The Pilot Program for TOD Planning helps support FTA&#;s mission of improving public transportation for America&#;s communities by providing funding to local communities to integrate land use and transportation planning around a new fixed guideway or core capacity improvement project. Per statute, any comprehensive or site specific planning funded through the program must examine ways to improve economic development and ridership, foster multimodal connectivity and accessibility, improve transit access for pedestrian and bicycle traffic, engage the private sector, identify infrastructure needs, and enable mixed-use development near transit stations.': 1.0,\n",
              " 'FY  Competitive Funding Opportunity: Public Transportation on Indian Reservations Program; Tribal Transit Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $. million in competitive grants for the Fiscal Year (FY)  Public Transportation on Indian Reservations (Tribal Transit) Program. As required by Federal public transportation law, funds will be awarded competitively for any purpose eligible under FTA&#;s Formula Grants for Rural Areas Program, including planning, capital, and operating assistance for tribal public transit services in rural areas. FTA may award additional funding that is made available to the program prior to the announcement of project selections.': 0.0,\n",
              " 'FY Consolidated Rail Infrastructure and Safety Improvements Grant Program: This program funds projects that improve the safety, efficiency, and reliability of intercity passenger and freight rail.': 1.0,\n",
              " 'FY Corridor Identification and Development Grant Program: This program facilitates the development of intercity passenger rail corridors.': 1.0,\n",
              " 'FY Federal-State Partnership for Intercity Passenger Rail Grant Program for projects not located on the Northeast Corridor: This program funds capital projects that reduce the state of good repair backlog, improve performance, or expand or establish new intercity passenger rail service.': 0.0,\n",
              " 'Fiscal Year  National Culvert Removal, Replacement, and Restoration Grant Program (Culvert AOP Program): -- ORIGINAL NOFO POSTING - // -- The primary goal of the Culvert AOP Program is to improve or restore anadromous fish passage through the replacement, removal, repair, or improvement of culverts or weirs. The grant program prioritizes projects that would improve fish passage for: (A) anadromous fish stocks listed as an endangered species or a threatened species under section  of the Endangered Species Act of  ( U.S.C. &#; ); (B) anadromous fish stocks identified by the National Marine Fisheries Service (NMFS) or the U.S. Fish and Wildlife Service (USFWS) that could reasonably become listed as an endangered species or a threatened species under that section; (C) anadromous fish stocks identified by the NMFS or the USFWS as prey for endangered species, threatened species, or protected species, including Southern resident orcas (Orcinus orca); or (D) anadromous fish stocks identified by the NMFS or the USFWS as climate resilient stocks ( U.S.C. &#; (e)()). The program also prioritizes projects that would open up more than  meters of upstream (anadromous) habitat before the end of the natural habitat ( U.S.C. &#; (e)()). ': 1.0,\n",
              " 'FY  National Infrastructure Investments: Agency Name: Department of TransportationDescription: The Infrastructure Investment and Jobs Act (Pub. L. -, November , , &#;Bipartisan Infrastructure Law,&#; or &#;BIL&#;) authorized and appropriated $. billion to be awarded by the Department of Transportation (&#;DOT&#;) for FY  for Local and Regional Project Assistance Program Grants under National Infrastructure Investments. This Notice of Funding Opportunity (NOFO) solicits applications for projects to be funded under the Local and Regional Project Assistance Program, known as the RAISE Grants program, including any additional funding appropriated for the RAISE Grants program under the FY  Appropriations Act. On March , , the Consolidated Appropriations Act,  (Pub. L. -, &#;FY  Appropriations Act&#;) appropriated an additional $ million for the FY  RAISE Grant Program. Therefore, a total of $. billion in funding is now available for the FY  RAISE Grant Program. Applicants should note that the two funding streams (BIL funding and FY  Appropriations Act funding) have slightly different funding restrictions and requirements that may affect competitiveness. Please see the Notice of Funding Opportunity Amendment No. for details.As with previous rounds, funds for the FY  RAISE Transportation program are to be awarded on a competitive basis for projects that will have a significant local or regional impact.': 0.6,\n",
              " 'FY  Natural Gas Distribution Infrastructure Safety and Modernization Grant: FY  Natural Gas Distribution Infrastructure Safety and Modernization Grant': 1.0,\n",
              " 'The Infrastructure Investment and Jobs Act (IIJA) Notice of Funding Opportunity for America&#;s Marine Highway Program: This notice announces the availability of funding for grants and establishes selection criteria and application requirements for the America&#;s Marine Highway Program (&#;AMHP&#;). The purpose of this program is to make grants available to previously designated Marine Highway Projects that support the development and expansion of documented vessels or port and landside infrastructure. The Department also seeks eligible grant projects that will strengthen American supply chains. The U.S. Department of Transportation (&#;DOT&#; or &#;Department&#;) will award Marine Highway Grants to implement projects or components of projects previously designated by the Secretary of Transportation (&#;Secretary&#;) under the AMHP. Only Marine Highway Projects the Secretary designates before the Notice of Funding Opportunity (&#;NOFO&#;) closing date are eligible for funding as described in this notice. TIMING OF GRANT APPLICATIONS: Applications must be received by the Maritime Administration (&#;MARAD&#;) by : p.m. E.D.T. on April , . ADDRESSES: Grant applications must be submitted electronically using Grants.gov (https://www.grants.gov). Please be aware that you must complete the Grants.gov registration process before submitting your application and that the registration process usually takes  to  weeks to complete. Applicants are strongly encouraged to make submissions in advance of the deadline. FOR FURTHER INFORMATION CONTACT: Fred Jones, Office of Ports &amp; Waterways Planning, Room W&#;, Maritime Administration, U.S. Department of Transportation,  New Jersey Avenue S.E., Washington, D.C. , phone --, or email Fred.Jones@dot.gov. Persons who use a telecommunications device for the deaf (TDD) may call the Federal Information Relay Service (FIRS) at --- to contact the above individual during business hours. The FIRS is available twenty-four hours a day, seven days a week, to leave a message or question with the above individual. You will receive a reply during regular business hours.': 1.0,\n",
              " '*FY  Notice of Funding Opportunity: Bipartisan Infrastructure Law (BIL) Airport Terminal Program (ATP) Grants: The Department of Transportation (DOT), Federal Aviation Administration (FAA) announces the opportunity to apply for approximately $ billion in FY  discretionary funds for the Airport Terminal Program (ATP), made available under the Infrastructure Investment and Jobs Act of  (IIJA), Pub. L. -, herein referred to as the Bipartisan Infrastructure Law (BIL). The purpose of the ATP is to make annual grants available to eligible airports for airport terminal and airport-owned Airport Traffic Control Towers development projects that address the aging infrastructure of our nation&#;s airports. In addition, ATP grants will align with DOT&#;s Strategic Framework FY- at https://www.transportation.gov/administrations/office-policy/fy--strategic-framework. The FY  ATP will be implemented consistent with law and in alignment with the priorities in Executive Order , Implementation of the Infrastructure Investments and Jobs Act ( FR ), which are to invest efficiently and equitably; promote the competitiveness of the U.S. economy; improve job opportunities by focusing on high labor standards; strengthen infrastructure resilience to all hazards including climate change; and to effectively coordinate with State, local, Tribal, and territorial government partners. ': 1.0,\n",
              " 'INFRA Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families. The Infrastructure for Rebuilding America (INFRA) program is an existing competitive program that will see a more than  percent increase in this year&#;s funding due to the Bipartisan Infrastructure Law. These grants advance the Administration&#;s priorities of rebuilding America&#;s infrastructure and creating jobs by funding highway, multimodal freight and rail projects that position America to win the st century. Projects will improve safety, generate economic benefits, reduce congestion, enhance resiliency, and hold the greatest promise to eliminate supply chain bottlenecks and improve critical freight movements. Last year, DOT received over $ billion of project applications, but could only fund around $ billion of projects. To see last year&#;s INFRA grants, click here. The Bipartisan Infrastructure Law provides approximately $ billion for INFRA over  years, of which approximately $. billion will be made available through this NOFO. ': 0.0,\n",
              " 'Mega Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The National Infrastructure Project Assistance (MEGA) program was created in the Bipartisan Infrastructure Law to fund major projects that are too large or complex for traditional funding programs. The program will provide grants on a competitive basis to support multijurisdictional or regional projects of significance that may also cut across multiple modes of transportation. Eligible projects could include highway, bridge, freight, port, passenger rail, and public transportation projects of national and regional significance. These could be bridges or tunnels connecting two states; new rail and transit lines that improve equity and reduce emissions; and freight hubs integrating ship, train and truck traffic while improving environmental justice. DOT will award  percent of funding to projects greater than $ million in cost, and  percent to projects greater than $ million but less than $ million in cost. The program will receive up to $ billion this year alone and be able to provide multi-year funding to projects.': 0.0,\n",
              " 'Rural Surface Transportation Grant Program: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The Rural Surface Transportation Grant Program (RURAL) was created in the Bipartisan Infrastructure Law and will support projects to improve and expand the surface transportation infrastructure in rural areas to increase connectivity, improve the safety and reliability of the movement of people and freight, and generate regional economic growth and improve quality of life. Eligible projects for Rural grants include highway, bridge, and tunnel projects that help improve freight, safety, and provide or increase access to an agricultural, commercial, energy, or transportation facilities that support the economy of a rural area. This year alone, DOT will award up to $ million in grants through the rural program&#;part of the $ billion included in the Bipartisan Infrastructure Law over five years.': 0.0,\n",
              " '*FY  Notice of Funding Opportunity: Bipartisan Infrastructure Law (BIL) FAA Contract Tower (FCT) Competitive Grant Program: The Department of Transportation (DOT), Federal Aviation Administration (FAA) announces the opportunity to apply for $ million in FY  Airport Infrastructure Grant funds for the FCT competitive grant program, made available under the Infrastructure Investment and Jobs Act of  (IIJA), Pub. L. -, herein referred to as the Bipartisan Infrastructure Law (BIL). The purpose of the FCT competitive grant program is to make annual grants available to eligible airports for airport-owned airport traffic control tower (ATCT) projects that address the aging infrastructure of the nation&#;s airports. The FAA will consider ATCT projects that sustain, construct, repair, improve, rehabilitate, modernize, replace, or relocate non-approach control towers; or acquire and install air traffic control, communications, and related equipment to be used in those towers; or construct a remote tower certified by the FAA including acquisition and installation of air traffic control, communications, or related equipment. In addition, FT grants will align with DOT&#;s Strategic Framework FY- at https://www.transportation.gov/administrations/office-policy/fy--strategic-framework. The FY  FCT will be implemented consistent with law and in alignment with the priorities in Executive Order , Implementation of the Infrastructure Investments and Jobs Act ( FR ), which are to invest efficiently and equitably; promote the competitiveness of the U.S. economy; improve job opportunities by focusing on high labor standards; strengthen infrastructure resilience to all hazards including climate change; and to effectively coordinate with State, local, Tribal, and territorial government partners. ': 1.0,\n",
              " 'FY Railroad Crossing Elimination Grant Program: The purpose of the RCE Program is to provide funding for highway-rail and pathway-rail grade crossing improvement projects that focus on improving the safety and mobility of people and goods.': 1.0,\n",
              " 'Reconnecting Communities Pilot Discretionary Grant Program: The Reconnecting Communities Pilot Program notice of funding opportunity was amended on September ,  to include technical corrections to the Key Information Table. See updated materials in the &quot;Manage Related Documents&quot; section, as well as on our website at https://www.transportation.gov/grants/reconnecting-communitiesThe purpose of this notice is to solicit applications for Reconnecting Communities Pilot (RCP) Program grants. Funds for the Fiscal Year (FY)  RCP Program are to be awarded on a competitive basis for projects that reconnect communities by removing, retrofitting, or mitigating highways or other transportation facilities that create barriers to community connectivity, including to mobility, access, or economic development.The variety of transformative solutions to knit communities back together can include: high-quality public transportation, infrastructure removal, pedestrian walkways and overpasses, capping over roadways, linear parks and trail connectors, roadway redesigns and complete streets conversions, and main street revitalization. The program will award two types of grants: Planning Grants and Capital Construction Grants. Please consult the full Notice of Funding Opportunity for full details about the application process, requirements, and forms.': 0.8571428571428571,\n",
              " 'Safe Streets and Roads for All Discretionary Grant Program: THIS NOTICE OF FUNDING OPPORTUNITY WAS AMENDED ON AUGUST , . PLEASE SEE THE GRANTS.GOV PACKAGE UPDATES, AND THE SAFE STREETS AND ROADS FOR ALL (SSA) WEBSITE IN THE LINK PROVIDED BELOW FOR MORE INFORMATION. DEADLINE REMAINS SEPTEMBER , .The purpose of this notice is to solicit applications for SSA grants. Funds for the fiscal year (FY)  SSA grant program are to be awarded on a competitive basis to support planning, infrastructure, behavioral, and operational initiatives to prevent death and serious injury on roads and streets involving all roadway users, including pedestrians; bicyclists; public transportation, personal conveyance, and micromobility users; motorists; and commercial vehicle operators. Those interested in applying for Implementation Grants should use the application materials in PKG; those interested in applying for Action Plan Grants should use the application materials in PKG. See application instructions to determine which grant to apply for in this funding round.': 0.8333333333333334,\n",
              " 'Strengthening Mobility and Revolutionizing Transportation (SMART) Grants Program: The purpose of this notice is to solicit applications for Strengthening Mobility and Revolutionizing Transportation (SMART) grants. Funds for the fiscal year (FY)  SMART Grants Program are to be awarded on a competitive basis to conduct demonstration projects focused on advanced smart city or community technologies and systems to improve transportation efficiency and safety.': 1.0,\n",
              " 'FY- Federal-State Partnership for Intercity Passenger Rail Grant Program for projects located on the Northeast Corridor: This program funds capital projects on the Northeast Corridor that reduce the state of good repair backlog, improve performance, or expand or establish new intercity passenger rail service.': 1.0,\n",
              " 'Fiscal Years - Wildlife Crossings Pilot Program (WCPP): The primary goals of the WCPP are to save lives, prevent serious injuries, and protect motorists and wildlife by reducing WVCs, and improve habitat connectivity for terrestrial and aquatic species. Reduction of wildlife vehicle collisions and improvement of terrestrial and aquatic habitat connectivity are the primary merit criteria that will be used in reviewing applications, and each of the primary merit criteria are of equal importance.': 0.6666666666666666,\n",
              " 'FY  Competitive Funding Opportunity; Buses and Bus Facilities Program: The Federal Transit Administration (FTA) announces the availability of approximately $ million in competitive grants under the Buses and Bus Facilities Program is to assist in the financing of buses and bus facilities capital projects, including replacing, rehabilitating, purchasing or leasing buses or related equipment, and rehabilitating, purchasing, constructing or leasing bus-related facilities. Synopses and full announcement will be posted on Grants.gov as opportunity FTA---TPM-BUS. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time April , .': 1.0,\n",
              " 'FY  Competitive Funding Opportunity; Low or No Emission Vehicle Program: The Federal Transit Administration (FTA) announces the availability of approximately $. billion in competitive grants under the Low or No Emission Grant Program (Low-No Program) for the purchase or lease of zero-emission and low-emission transit buses, including acquisition, construction, and leasing of required supporting facilities. Synopses and full announcement will be posted on Grants.gov as opportunity FTA---TPM-LWNO. Proposals must be submitted electronically through Grants.gov website by : PM Eastern Time April , .': 0.0,\n",
              " 'FY  Competitive Funding Opportunity: Areas of Persistent Poverty Program: To create new opportunities for those experiencing poverty, the U.S. Department of Transportation&#;s Federal Transit Administration (FTA) today announced a new Notice of Funding Opportunity (NOFO) to help improve transit in areas experiencing long-term economic distress. The $ million in competitive grant funding through FTA&#;s Areas of Persistent Poverty (AoPP) Program provides more resources to underserved and disadvantaged communities seeking to expand or improve transit. ': 1.0,\n",
              " 'FY  Competitive Funding Opportunity: Ferry Service for Rural Communities Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $ million in competitive grants under the Fiscal Year (FY)  Ferry Service for Rural Communities Program (FSRC) (Federal Assistance Listing #.). FSRC makes funding available competitively to assist in the financing of capital, planning, and operating assistance for eligible ferry services.': 0.0,\n",
              " 'FY  Competitive Funding Opportunity: Passenger Ferry Grant Program: The Federal Transit Administration (FTA) announces the opportunity to apply for $. million in competitive grants under the Fiscal Year (FY)  Passenger Ferry Grant Program (PFG) (Federal Assistance Listing #.). Of that amount, $ million is available only for low or zero-emission ferries or ferries using electric battery or fuel cell components and the infrastructure to support such ferries. As required by Federal public transportation law, funds will be awarded competitively to designated recipients or eligible direct recipients of Urbanized Area Formula funds to support capital projects to improve existing passenger ferry service, establish new ferry service, and repair and modernize ferry boats, terminals, and related facilities and equipment. ': 0.0,\n",
              " 'FY  National Infrastructure Investments: The Infrastructure Investment and Jobs Act of  (&#;Bipartisan Infrastructure Law,&#; or &#;BIL&#;) authorized and appropriated $. billion to be awarded by the Department of Transportation (&#;DOT&#;) for Local and Regional Project Assistance Program Grants under National Infrastructure Investments in FY . This Notice of Funding Opportunity (NOFO) solicits applications for projects funded under the Local and Regional Project Assistance Program, known as the RAISE Grants program. If the FY  Appropriations Act provides additional funding for the RAISE grants program and/or significantly different requirements for National Infrastructure Investment funds, the Department will amend this Notice with guidance on additional requirements.As with previous rounds, funds for the FY  RAISE Transportation program are to be awarded on a competitive basis for projects that will have a significant local or regional impact.': 0.7083333333333334,\n",
              " 'Neighborhood Access and Equity (NAE) Program: The Department of Transportation is combining two major discretionary grant programs, the Reconnecting Communities Pilot (RCP) and Neighborhood Access and Equity (NAE) programs, into one Notice of Funding Opportunity (NOFO). Together, this combined program is known as the Reconnecting Communities and Neighborhoods (RCN) Program. The primary goal of the NAE Program is to assist economically disadvantaged or undeserved communities for planning and construction activities. Funds for the fiscal year (FY)  NAE grant program are to be awarded on a competitive basis to support planning, capital construction, and regional partnership activities that aim to restore community connectivity through the removal, retrofit, mitigation or replacement of highways, roadways, or other infrastructure facilities that create barriers to mobility, access or economic development. Applicants must submit their applications via Valid Eval at the links below: Community Planning Grants: https://usg.valideval.com/teams/rcn_planning/signup Capital Construction Grants: https://usg.valideval.com/teams/rcn_capitalconstruction/signup Do not submit applications through Grants.gov. Applications must be submitted by : PM Eastern Time on Thursday, September , . Late applications will not be accepted. ': 1.0,\n",
              " 'FY  Notice of Funding Opportunity for Small Shipyard Grants: This notice announces the intention of the Maritime Administration to provide grants to small shipyards. Under the Small Shipyard Grant Program, there is currently $,, available for grants for capital and related improvements to qualified shipyard facilities that will be effective in fostering efficiency, competitive operations, and quality ship construction, repair, and reconfiguration. Potential applicants are advised that it is expected, based on experience, that the number of applications will far exceed the funds available and that only a small percentage of applicants will be funded. Applications (SF- and the Addendum) will be accepted online through Grants.gov by  pm on the closing date (February , ). See full text under Related Documents Tab (FY  SSG NOFO) or the MARAD Web Page for detailed submission instructions.': 1.0,\n",
              " 'Safe Streets and Roads for All Funding Opportunity: The purpose of this notice is to solicit applications for Safe Streets and Roads for All (SSA) grants. Funds for the fiscal year (FY)  SSA grant program are to be awarded on a competitive basis to support planning and demonstration activities, as well as projects and strategies to prevent death and serious injury on roads and streets involving all roadway users, including pedestrians; bicyclists; public transportation, personal conveyance, and micromobility users; motorists; and commercial vehicle operators. Applicants must submit their applications via Valid Eval at https://usg.valideval.com/teams/usdot_ssa__implementation/signup for Implementation Grant applicants, and https://usg.valideval.com/teams/usdot_ssa__planning_demo/signup for Planning and Demonstration Grants. Do not submit your applications through Grants.Gov. Applications must be submitted by : PM Eastern Time on Monday, July , . Late applications will not be accepted.': 0.9827586206896551,\n",
              " 'SMART Grants Notice of Funding: The purpose of this notice is to solicit applications for Strengthening Mobility and Revolutionizing Transportation (SMART) Stage  Planning and Prototyping grants. Funds for the fiscal year (FY)  SMART Grants Program are to be awarded on a competitive basis to conduct demonstration projects focused on advanced smart city or community technologies and systems to improve transportation efficiency and safety. Applications must be submitted via Valid Eval, an online submission proposal system used by USDOT, at https://usg.valideval.com/teams/USDOT_SMART_/signup. USDOT will not accept or review application materials submitted via Grants.gov. ': 0.0,\n",
              " ' Tribal Transportation Program Safety Fund: Eligible projects described in section (a)() are strategies, activities, and projects on a public road that are consistent with a transportation safety plan and that (i) correct or improve a hazardous road location or feature, or (ii) address a highway safety problem. TTPSF emphasizes the development of strategic transportation safety plans using a data-driven process as a means for Tribes to identify transportation safety needs and determine how those needs will be addressed in Tribal communities. FHWA has identified four eligibility categories: transportation safety plans; data assessment, improvement, and analysis activities; systemic roadway departure countermeasures; and infrastructure improvements and other eligible activities as listed in  U.S.C. &#; (a)().': 1.0,\n",
              " 'United States Marine Highway Grants: This funding opportunity solicits applications for fiscal year (FY)  United States Marine Highway Program (USMHP) grants. Funds for FY  USMHP grants will be awarded on a competitive basis to assist in funding eligible projects for the purpose of developing, expanding, or promoting marine highway transportation. This opportunity announces the availability of up to $,, million in funding for grants under this program and establishes selection criteria and application requirements. All USMHP grant recipients must meet all applicable Federal requirements, including domestic content (&#;Buy America&#;) requirements. This program was formerly known as the America&#;s Marine Highway Program': 0.0,\n",
              " 'INFRA Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families. The Infrastructure for Rebuilding America (INFRA) program is an existing competitive program that will see a more than  percent increase in this year&#;s funding due to the Bipartisan Infrastructure Law. These grants advance the Administration&#;s priorities of rebuilding America&#;s infrastructure and creating jobs by funding highway, multimodal freight and rail projects that position America to win the st century. Projects will improve safety, generate economic benefits, reduce congestion, enhance resiliency, and hold the greatest promise to eliminate supply chain bottlenecks and improve critical freight movements. Last year, DOT received over $ billion in application requests, but could only fund around $. billion of projects. To see last year&#;s INFRA grants, click here. The Bipartisan Infrastructure Law provides approximately $ billion for INFRA over  years, of which approximately $. billion will be made available through this NOFO. ': 0.0,\n",
              " 'Mega Grants: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The National Infrastructure Project Assistance (MEGA) program was created in the Bipartisan Infrastructure Law to fund major projects that are too large or complex for traditional funding programs. The program will provide grants on a competitive basis to support multijurisdictional or regional projects of significance that may also cut across multiple modes of transportation. Eligible projects could include highway, bridge, freight, port, passenger rail, and related public transportation projects of national and regional significance. DOT will award approximately  percent of funding to projects greater than $ million in cost, and approximately  percent to projects greater than $ million but less than $ million in cost.For more information about last year&apos;s Mega awards, visit here: https://www.transportation.gov/grants/mega-grant-program/FYawards': 0.0,\n",
              " 'Rural Surface Transportation Grant Progam: The Department is combining three major discretionary grant programs into one Multimodal Projects Discretionary Grant (MPDG) opportunity to reduce the burden for state and local applicants and increase the pipeline of &#;shovel-worthy&#; projects that are now possible because of the Bipartisan Infrastructure Law. These investments will create good-paying jobs, grow the economy, reduce emissions, improve safety, make our transportation more sustainable and resilient, and expand transportation options in rural America and other underserved communities. Thanks to the Bipartisan Infrastructure Law, this funding will help enable more communities to build vital infrastructure projects that also strengthen supply chains and reduce costs for American families.The Rural Surface Transportation Grant Program (RURAL) was created in the Bipartisan Infrastructure Law and will support projects to improve and expand the surface transportation infrastructure in rural areas to increase connectivity, improve the safety and reliability of the movement of people and freight, and generate regional economic growth and improve quality of life. Eligible projects for Rural grants include highway, bridge, and tunnel projects that help improve freight, safety, and provide or increase access to an agricultural, commercial, energy, or transportation facilities that support the economy of a rural area. This year alone, DOT will award up to $ million in grants through the rural program&#;part of the $ billion included in the Bipartisan Infrastructure Law over five years. At least % of rural funding must be awarded in amounts of $ million or more. If you are seeking less than $ million, you are competing for only about $. million nationwide.For more information on last year&apos;s rural awards, see: https://www.transportation.gov/grants/rural--fact-sheets': 0.4,\n",
              " 'FY  Competitive Funding Opportunity: Rail Vehicle Replacement Grant Program (RAIL): The Federal Transit Administration (FTA) announces the opportunity to apply for approximately $ million in Fiscal Year (FY)  Section  State of Good Repair Program funds (Catalog of Federal Domestic Assistance #.) authorized for competitively selected rail vehicle replacement projects. As required by Federal public transportation law and subject to appropriations, funds will be awarded competitively to States and local governmental authorities to assist in the funding of capital projects to replace rail rolling stock.': 0.0}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('finetuned_BERT_epoch_5.model', map_location=torch.device('cuda')))\n",
        "\n",
        "_, predictions, true_vals, _ = evaluate(dataloader_validation)\n",
        "accuracy_per_class(predictions, true_vals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFz3ThMomeX5"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "id": "5A0wrf68meX5",
        "outputId": "85be1aa9-6abc-43d3-c017-28221d86b077"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-24-9fd1ad228c05>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  simple_df['project_profile'] =  simple_df['Applicants'] + ': ' + simple_df['Project Description'] + ' ' + simple_df['Project Name']\n",
            "<ipython-input-24-9fd1ad228c05>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  simple_df['grant_profile'] =  simple_df['opportunitytitle'] + ': ' + simple_df['description']\n",
            "<ipython-input-24-9fd1ad228c05>:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  simple_df['label'] = simple_df['grant_profile'].replace(label_dict)\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'FY  Notice of Funding Opportunity: Bipartisan Infrastructure Law (BIL) Airport Terminal Program (ATP) Grants: The Department of Transportation (DOT), Federal Aviation Administration (FAA) announces the opportunity to apply for approximately $ billion in FY  discretionary funds for the Airport Terminal Program (ATP), made available under the Infrastructure Investment and Jobs Act of  (IIJA), Pub. L. -, herein referred to as the Bipartisan Infrastructure Law (BIL). The purpose of the ATP is to make annual grants available to eligible airports for airport terminal and airport-owned Airport Traffic Control Towers development projects that address the aging infrastructure of our nation&#;s airports. In addition, ATP grants will align with DOT&#;s Strategic Framework FY- at https://www.transportation.gov/administrations/office-policy/fy--strategic-framework. The FY  ATP will be implemented consistent with law and in alignment with the priorities in Executive Order , Implementation of the Infrastructure Investments and Jobs Act ( FR ), which are to invest efficiently and equitably; promote the competitiveness of the U.S. economy; improve job opportunities by focusing on high labor standards; strengthen infrastructure resilience to all hazards including climate change; and to effectively coordinate with State, local, Tribal, and territorial government partners. '"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create new dataset of project + project name and grant title\n",
        "simple_df = clean_dataset\n",
        "simple_df['project_profile'] =  simple_df['Applicants'] + ': ' + simple_df['Project Description'] + ' ' + simple_df['Project Name']\n",
        "simple_df['grant_profile'] =  simple_df['opportunitytitle'] + ': ' + simple_df['description']\n",
        "simple_df = simple_df[['project_profile','grant_profile'] ]\n",
        "possible_grants = simple_df['grant_profile'].unique()\n",
        "\n",
        "#substitute label with number instead\n",
        "label_dict = {}\n",
        "for index, possible_label in enumerate(possible_grants):\n",
        "    label_dict[possible_label] = index\n",
        "\n",
        "simple_df['label'] = simple_df['grant_profile'].replace(label_dict)\n",
        "simple_df['grant_profile'].unique()[0]\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cs224n",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "058de3ed86494076a7f46bb14f73dd0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e29f8df22452467594d82ba6a80428ef",
            "placeholder": "​",
            "style": "IPY_MODEL_74222ae9422a4159aba66d3b8d9f5919",
            "value": "model.safetensors: 100%"
          }
        },
        "08904902faf647b8badf670c2a896ccd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df4586f25a6c4747853b0eeaf92cb8be",
            "placeholder": "​",
            "style": "IPY_MODEL_6d720d71541c4273bad8ca1a193e3442",
            "value": " 232k/232k [00:00&lt;00:00, 3.00MB/s]"
          }
        },
        "09d677f517f44884b9c9ce51c74205b2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1537b7fdf8b24e77b61cd59c604ab9e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db7d101fceb34439a923c05ab5884473",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_35a7f22923664a0ebfe2560c75304408",
            "value": 231508
          }
        },
        "1dcabdf7b755433397f62e622fcac84f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a899e69a08114a3399c819396da3ba7d",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9ee448de73a442419c04e66f8c88c208",
            "value": 440449768
          }
        },
        "20f24451cfd54b8fb25ad406d83ed2cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_249752ef2ffd43879dee3ffa79a8085a",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6842cb3de0de4dd0a3e332475b201187",
            "value": 466062
          }
        },
        "2199b42250d64509a805a35ac0bbc2c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "249752ef2ffd43879dee3ffa79a8085a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35a7f22923664a0ebfe2560c75304408": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "36a7d955174445ecac05c9d8fa3fd9ec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3778647f0eee400ebc6f17f5dc5fa713": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38ea332441be4e72916786db98a20f4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b46d51217c44e4abb30b9fa0c6596cf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b903a5d88bf4737beb7299d165bab7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b46d51217c44e4abb30b9fa0c6596cf",
            "placeholder": "​",
            "style": "IPY_MODEL_5961f6c0192c452e844096f092580918",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "3c7679d3f7d948d59ad5b14b3a7d47df": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f7b7f0609c64acda60aca0f905b5544": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfeb6567dbd34663b653a3be11703f75",
            "placeholder": "​",
            "style": "IPY_MODEL_38ea332441be4e72916786db98a20f4d",
            "value": "config.json: 100%"
          }
        },
        "40d7cf3f22024298b7d1c84186a89d0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c7679d3f7d948d59ad5b14b3a7d47df",
            "placeholder": "​",
            "style": "IPY_MODEL_9b4baa66e7b04af9b40ad8ca40f78288",
            "value": "tokenizer.json: 100%"
          }
        },
        "4d7b3eb5467442bd87b7572a544bbf90": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54e0a3f25f2e4cc5944cdaebfc6d53d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_058de3ed86494076a7f46bb14f73dd0f",
              "IPY_MODEL_1dcabdf7b755433397f62e622fcac84f",
              "IPY_MODEL_f1b7e30c7f974117a2e5d86273a1ec37"
            ],
            "layout": "IPY_MODEL_3778647f0eee400ebc6f17f5dc5fa713"
          }
        },
        "5961f6c0192c452e844096f092580918": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61e82fa50b6c4b98b166f2a73c288f14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b903a5d88bf4737beb7299d165bab7c",
              "IPY_MODEL_9ddbe5383fdb48ef9d3a63e6e6106d44",
              "IPY_MODEL_e2962b7bfbe14310932793ea5e012e98"
            ],
            "layout": "IPY_MODEL_d2f1a606386243a981af6b56a1fe19ae"
          }
        },
        "6266808d204d482494a2b0a9a6ae334e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6305174988e94dbca05f031f77e46a1e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6842cb3de0de4dd0a3e332475b201187": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6d720d71541c4273bad8ca1a193e3442": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "702ab38848094b389eef42c893fd7acb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74222ae9422a4159aba66d3b8d9f5919": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7515b04cf8f74d1db3ba611c0c22f6ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d3d456323ac47d8b0bfbd7c7c790b10": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f042b55edf747bc95724cee68f82678": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85c8eb45f06a4ae7b6156cdd7154ac59",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_928c9bdd69c741c8b1b0cb9b235ab3cf",
            "value": 570
          }
        },
        "8077cfaafe5d48dcaa6b06c67af23bac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85c8eb45f06a4ae7b6156cdd7154ac59": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f19a8ebe77840c69de1be7c03e1ac28": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7515b04cf8f74d1db3ba611c0c22f6ba",
            "placeholder": "​",
            "style": "IPY_MODEL_4d7b3eb5467442bd87b7572a544bbf90",
            "value": "vocab.txt: 100%"
          }
        },
        "928c9bdd69c741c8b1b0cb9b235ab3cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9463c6aeaba5401fa6a7e180620fef2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b4baa66e7b04af9b40ad8ca40f78288": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ddbe5383fdb48ef9d3a63e6e6106d44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36a7d955174445ecac05c9d8fa3fd9ec",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6266808d204d482494a2b0a9a6ae334e",
            "value": 48
          }
        },
        "9ee448de73a442419c04e66f8c88c208": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9f8dd81cdc844e189773595dbae46f9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09d677f517f44884b9c9ce51c74205b2",
            "placeholder": "​",
            "style": "IPY_MODEL_702ab38848094b389eef42c893fd7acb",
            "value": " 466k/466k [00:00&lt;00:00, 3.44MB/s]"
          }
        },
        "a899e69a08114a3399c819396da3ba7d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acaa8d00d4a64d4c98fafefd39ba8227": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af2e59b19d5b4ec0baaaba2ee08d1705": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f19a8ebe77840c69de1be7c03e1ac28",
              "IPY_MODEL_1537b7fdf8b24e77b61cd59c604ab9e0",
              "IPY_MODEL_08904902faf647b8badf670c2a896ccd"
            ],
            "layout": "IPY_MODEL_7d3d456323ac47d8b0bfbd7c7c790b10"
          }
        },
        "b198510cfef749b9be826a7d72eb55e3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c71aa7dd97314bec83f2b3c68d0756fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_40d7cf3f22024298b7d1c84186a89d0f",
              "IPY_MODEL_20f24451cfd54b8fb25ad406d83ed2cb",
              "IPY_MODEL_9f8dd81cdc844e189773595dbae46f9f"
            ],
            "layout": "IPY_MODEL_2199b42250d64509a805a35ac0bbc2c5"
          }
        },
        "c84130085b164e83b5ea7856c89702e4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d111f49a84d541838dd2f88913335065": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1d9bdd9d14a4787bfc7d0b211922a54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f7b7f0609c64acda60aca0f905b5544",
              "IPY_MODEL_7f042b55edf747bc95724cee68f82678",
              "IPY_MODEL_ff903b7f4a00497da6f71e919e9abbc2"
            ],
            "layout": "IPY_MODEL_c84130085b164e83b5ea7856c89702e4"
          }
        },
        "d2f1a606386243a981af6b56a1fe19ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db7d101fceb34439a923c05ab5884473": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df4586f25a6c4747853b0eeaf92cb8be": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfeb6567dbd34663b653a3be11703f75": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2962b7bfbe14310932793ea5e012e98": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6305174988e94dbca05f031f77e46a1e",
            "placeholder": "​",
            "style": "IPY_MODEL_acaa8d00d4a64d4c98fafefd39ba8227",
            "value": " 48.0/48.0 [00:00&lt;00:00, 3.11kB/s]"
          }
        },
        "e29f8df22452467594d82ba6a80428ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1b7e30c7f974117a2e5d86273a1ec37": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8077cfaafe5d48dcaa6b06c67af23bac",
            "placeholder": "​",
            "style": "IPY_MODEL_d111f49a84d541838dd2f88913335065",
            "value": " 440M/440M [00:04&lt;00:00, 77.3MB/s]"
          }
        },
        "ff903b7f4a00497da6f71e919e9abbc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b198510cfef749b9be826a7d72eb55e3",
            "placeholder": "​",
            "style": "IPY_MODEL_9463c6aeaba5401fa6a7e180620fef2c",
            "value": " 570/570 [00:00&lt;00:00, 45.7kB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
