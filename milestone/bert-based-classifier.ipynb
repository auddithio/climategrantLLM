{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from transformers import BertForSequenceClassification\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_numbers(text):\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r'\\d+', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_years(text):\n",
    "    return text\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = pd.read_csv('/Users/peter/cs224n/climategrantLLM/milestone/BIL Launchpad Case studies - Sheet1.csv')\n",
    "\n",
    "\"\"\"\n",
    "Cleaning the data:\n",
    "- only include columns we will use: 'Project Name', 'Project Description', 'Applicants', 'opportunitytitle', 'description'\n",
    "- drop column with missing information\n",
    "- currently, the grant opportunities include years, i will omit this from their title to group recurring grants together\n",
    "\"\"\"\n",
    "dataset = dataset[['Project Name', 'Project Description', 'Applicants', 'opportunitytitle', 'description']]\n",
    "clean_dataset = dataset.dropna()\n",
    "print(len(clean_dataset['opportunitytitle'].unique()))\n",
    "# print(dataset['opportunitytitle'].unique())\n",
    "clean_dataset['opportunitytitle'] = clean_dataset['opportunitytitle'].apply(remove_numbers)\n",
    "print(len(clean_dataset['opportunitytitle'].unique()))\n",
    "# print(dataset['opportunitytitle'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Classification of project <> grant name (multi-class text classification) (ignore description of program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataset of project + project name and grant title\n",
    "simple_df = clean_dataset[['Project Name', 'Project Description', 'Applicants', 'opportunitytitle']]\n",
    "simple_df['project_profile'] =  simple_df['Applicants'] + ': ' + simple_df['Project Description'] + ' ' + simple_df['Project Name'] \n",
    "simple_df = simple_df[['project_profile','opportunitytitle'] ]\n",
    "possible_grants = simple_df['opportunitytitle'].unique()\n",
    "\n",
    "#substitute label with number instead\n",
    "label_dict = {}\n",
    "for index, possible_label in enumerate(possible_grants):\n",
    "    label_dict[possible_label] = index\n",
    "\n",
    "simple_df['label'] = simple_df['opportunitytitle'].replace(label_dict)\n",
    "simple_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_df['project_profile'][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(simple_df.index.values, \n",
    "                                                  simple_df.label.values, \n",
    "                                                  test_size=0.15, \n",
    "                                                  random_state=42, \n",
    "                                                  stratify=simple_df.label.values)\n",
    "\n",
    "simple_df['data_type'] = ['not_set']*simple_df.shape[0]\n",
    "\n",
    "simple_df.loc[X_train, 'data_type'] = 'train'\n",
    "simple_df.loc[X_val, 'data_type'] = 'val'\n",
    "\n",
    "simple_df.groupby(['opportunitytitle', 'label', 'data_type']).count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Constructs a BERT tokenizer. Based on WordPiece.\n",
    "Instantiate a pre-trained BERT model configuration to encode our data.\n",
    "To convert all the titles from text into encoded form, we use a function called batch_encode_plus , and we will proceed train and validation data separately.\n",
    "The 1st parameter inside the above function is the title text.\n",
    "add_special_tokens=True means the sequences will be encoded with the special tokens relative to their model.\n",
    "When batching sequences together, we set return_attention_mask=True, so it will return the attention mask according to the specific tokenizer defined by the max_length attribute.\n",
    "We also want to pad all the titles to certain maximum length.\n",
    "We actually do not need to set max_length=256, but just to play it safe.\n",
    "return_tensors='pt' to return PyTorch.\n",
    "And then we need to split the data into input_ids, attention_masks and labels.\n",
    "Finally, after we get encoded data set, we can create training data and validation data.\n",
    "\"\"\"\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          do_lower_case=True)\n",
    "                                          \n",
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    simple_df[simple_df.data_type=='train'].project_profile.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    #max_length=256, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    simple_df[simple_df.data_type=='val'].project_profile.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    #max_length=256, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(simple_df[simple_df.data_type=='train'].label.values)\n",
    "\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(simple_df[simple_df.data_type=='val'].label.values)\n",
    "\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(label_dict),\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.current_device()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 3\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, \n",
    "                              sampler=RandomSampler(dataset_train), \n",
    "                              batch_size=batch_size)\n",
    "\n",
    "dataloader_validation = DataLoader(dataset_val, \n",
    "                                   sampler=SequentialSampler(dataset_val), \n",
    "                                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=1e-5, \n",
    "                  eps=1e-8)\n",
    "                  \n",
    "epochs = 5\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=len(dataloader_train)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = torch.argmax(preds, dim=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat.cpu().numpy(), preds_flat.cpu().numpy(), average='weighted')\n",
    "\n",
    "def accuracy_per_class(preds, labels, label_dict):\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    \n",
    "    preds_flat = torch.argmax(preds, dim=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    class_accuracies = {}\n",
    "    for label in torch.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat == label]\n",
    "        y_true = labels_flat[labels_flat == label]\n",
    "        accuracy = (y_preds == label).sum().item() / len(y_true)\n",
    "        class_accuracies[label_dict_inverse[label.item()]] = accuracy\n",
    "        print(f'Class: {label_dict_inverse[label.item()]}')\n",
    "        print(f'Accuracy: {(y_preds == label).sum().item()}/{len(y_true)} ({accuracy:.4f})\\n')\n",
    "\n",
    "    return class_accuracies\n",
    "\n",
    "def recall_at_k(y_true, y_pred, k=5):\n",
    "    num_samples = y_true.size(0)\n",
    "    recall_count = 0\n",
    "\n",
    "    for true_label, predictions in zip(y_true, y_pred):\n",
    "        top_k_predictions = torch.topk(predictions, k).indices  # Get indices of the top k predictions\n",
    "        if true_label in top_k_predictions:\n",
    "            recall_count += 1\n",
    "\n",
    "    recall_at_k_score = recall_count / num_samples\n",
    "    return recall_at_k_score\n",
    "\n",
    "def evaluate(dataloader_val):\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in dataloader_val:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu()\n",
    "        label_ids = inputs['labels'].cpu()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total / len(dataloader_val)\n",
    "    \n",
    "    predictions = torch.cat(predictions, dim=0)\n",
    "    true_vals = torch.cat(true_vals, dim=0)\n",
    "    \n",
    "    recall_at_5 = recall_at_k(true_vals, predictions, k=5)\n",
    "    \n",
    "    return loss_val_avg, predictions, true_vals, recall_at_5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set the random seeds for reproducibility\n",
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Ensure to use GPU if available\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader_train, desc=f'Epoch {epoch}', leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {\n",
    "            'input_ids': batch[0],\n",
    "            'attention_mask': batch[1],\n",
    "            'labels': batch[2],\n",
    "        }       \n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': f'{loss.item() / len(batch):.3f}'})\n",
    "\n",
    "    torch.save(model.state_dict(), f'finetuned_BERT_epoch_{epoch}.model')\n",
    "    \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    loss_train_avg = loss_train_total / len(dataloader_train)            \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "        \n",
    "    loss_val_avg, predictions, true_vals, recall_at_5 = evaluate(dataloader_validation, model, device)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {loss_val_avg}')\n",
    "    tqdm.write(f'F1 Score (Weighted): {val_f1}')\n",
    "    tqdm.write(f'Recall@5: {recall_at_5}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('finetuned_BERT_epoch_1.model', map_location=torch.device('cpu')))\n",
    "\n",
    "_, predictions, true_vals = evaluate(dataloader_validation)\n",
    "accuracy_per_class(predictions, true_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "we will summarise the project down to a multi-class text classification. \n",
    "To do so:\n",
    "1. We will concatenate 'project name' and 'project description' together\n",
    "2. \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "cs224n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
